[{"question": "What is the autocorrelation for lag 1", "positive_ctxs": [{"text": "A lag 1 autocorrelation (i.e., k = 1 in the above) is the correlation between values that are one time period apart. More generally, a lag k autocorrelation is the correlation between values that are k time periods apart."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series. There are several autocorrelation coefficients, corresponding to each panel in the lag plot."}, {"text": "Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series. There are several autocorrelation coefficients, corresponding to each panel in the lag plot."}, {"text": "Autocorrelation measures the relationship between a variable's current value and its past values. An autocorrelation of +1 represents a perfect positive correlation, while an autocorrelation of negative 1 represents a perfect negative correlation."}, {"text": "Autocorrelation is diagnosed using a correlogram (ACF plot) and can be tested using the Durbin-Watson test. The auto part of autocorrelation is from the Greek word for self, and autocorrelation means data that is correlated with itself, as opposed to being correlated with some other data."}, {"text": "Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series.  The autocorrelation coefficients are plotted to show the autocorrelation function or ACF. The plot is also known as a correlogram."}, {"text": "If you select View/Residual Diagnostics/Correlogram-Q-statistics on the equation toolbar, EViews will display the autocorrelation and partial autocorrelation functions of the residuals, together with the Ljung-Box Q-statistics for high-order serial correlation."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "How do you interpret odds ratio", "positive_ctxs": [{"text": "Odds Ratio is a measure of the strength of association with an exposure and an outcome.OR > 1 means greater odds of association with the exposure and outcome.OR = 1 means there is no association between exposure and outcome.OR < 1 means there is a lower odds of association between the exposure and outcome."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The odds ratio is the measure of association for a case-control study. It tells us how much higher the odds of exposure is among cases of a disease compared with controls. The odds ratio compares the odds of exposure to the factor of interest among cases to the odds of exposure to the factor among controls."}, {"text": "The odds ratio tells us how much higher the odds of exposure are among case-patients than among controls. An odds ratio of \u2022 1.0 (or close to 1.0) indicates that the odds of exposure among case-patients are the same as, or similar to, the odds of exposure among controls. The exposure is not associated with the disease."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "To conclude, the important thing to remember about the odds ratio is that an odds ratio greater than 1 is a positive association (i.e., higher number for the predictor means group 1 in the outcome), and an odds ratio less than 1 is negative association (i.e., higher number for the predictor means group 0 in the outcome"}, {"text": "To conclude, the important thing to remember about the odds ratio is that an odds ratio greater than 1 is a positive association (i.e., higher number for the predictor means group 1 in the outcome), and an odds ratio less than 1 is negative association (i.e., higher number for the predictor means group 0 in the outcome"}, {"text": "An odds ratio is a measure of association between the presence or absence of two properties.  The value of the odds ratio tells you how much more likely someone under 25 might be to make a claim, for example, and the associated confidence interval indicates the degree of uncertainty associated with that ratio."}, {"text": "Odds ratios measure how many times bigger the odds of one outcome is for one value of an IV, compared to another value.  That odds ratio is an unstandardized effect size statistic. It tells you the direction and the strength of the relationship between water temperature and the odds that the plant is present."}]}, {"question": "What is temporal classification of data", "positive_ctxs": [{"text": "Data classification is usually based on measurements recorded at the same time. This. paper considers temporal data classification where the input is a temporal database that. describes measurements over a period of time in history while the predicted class is. expected to occur in the future."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior.  Both classes of networks exhibit temporal dynamic behavior."}, {"text": "Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.  Interrupted time series analysis is the analysis of interventions on a single time series. Time series data have a natural temporal ordering."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Use temporal data types to store date, time, and time-interval information. Although you can store this data in character strings, it is better to use temporal types for consistency and validation. An hour, minute, and second to six decimal places (microseconds), and the time zone offset from GMT."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.  KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point."}, {"text": "KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.  KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point."}]}, {"question": "How does transfer learning work", "positive_ctxs": [{"text": "Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task.  Transfer learning is an optimization that allows rapid progress or improved performance when modeling the second task."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "1 Answer. Transfer learning is when a model developed for one task is reused to work on a second task. Fine tuning is one approach to transfer learning."}, {"text": "1 Answer. Transfer learning is when a model developed for one task is reused to work on a second task. Fine tuning is one approach to transfer learning."}, {"text": "Transfer learning without any labeled data from the target domain is referred to as unsupervised transfer learning."}, {"text": "The basic premise of transfer learning is simple: take a model trained on a large dataset and transfer its knowledge to a smaller dataset. For object recognition with a CNN, we freeze the early convolutional layers of the network and only train the last few layers which make a prediction."}, {"text": "Neural style transfer is trained as a supervised learning task in which the goal is to input two images (x), and train a network to output a new, synthesized image (y)."}, {"text": "To convert a transfer function into state equations in phase variable form, we first convert the transfer function to a differential equation by cross-multiplying and taking the inverse Laplace transform, assuming zero initial conditions."}, {"text": "How to Prevent OverfittingCross-validation. Cross-validation is a powerful preventative measure against overfitting.  Train with more data. It won't work every time, but training with more data can help algorithms detect the signal better.  Remove features.  Early stopping.  Regularization.  Ensembling."}]}, {"question": "What is the point of hashing", "positive_ctxs": [{"text": "Hashing is the transformation of a string of characters into a usually shorter fixed-length value or key that represents the original string. Hashing is used to index and retrieve items in a database because it is faster to find the item using the shorter hashed key than to find it using the original value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A point estimate of a population parameter is a single value of a statistic. For example, the sample mean x is a point estimate of the population mean \u03bc. Similarly, the sample proportion p is a point estimate of the population proportion P."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}, {"text": "In computer science, locality-sensitive hashing (LSH) is an algorithmic technique that hashes similar input items into the same \"buckets\" with high probability.  It differs from conventional hashing techniques in that hash collisions are maximized, not minimized."}, {"text": "A point estimate is the value of a statistic that estimates the value of a parameter. For example, the sample mean is a point estimate of the population mean. The arithmetic mean is a single value meant to \"sum up\" a data set. To calculate the mean, add up all the values and divide by the number of values."}, {"text": "In general, prediction is the process of determining the magnitude of statistical variates at some future point of time."}]}, {"question": "What are the positives of artificial intelligence", "positive_ctxs": [{"text": "AI would have a low error rate compared to humans, if coded properly. They would have incredible precision, accuracy, and speed. They won't be affected by hostile environments, thus able to complete dangerous tasks, explore in space, and endure problems that would injure or kill us."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}, {"text": "Java, Python, Lisp, Prolog, and C++ are major AI programming language used for artificial intelligence capable of satisfying different needs in the development and designing of different software.  It answers the question, 'what is the language used for artificial intelligence?"}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "Explainable AI (XAI) refers to methods and techniques in the application of artificial intelligence technology (AI) such that the results of the solution can be understood by humans."}]}, {"question": "What is geometric distribution used for", "positive_ctxs": [{"text": "In such a sequence of trials, the geometric distribution is useful to model the number of failures before the first success. The distribution gives the probability that there are zero failures before the first success, one failure before the first success, two failures before the first success, and so on."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The exponential distribution is a continuous probability distribution used to model the time we need to wait before a given event occurs. It is the continuous counterpart of the geometric distribution, which is instead discrete."}, {"text": "It is very much like the exponential distribution, with \u03bb corresponding to 1/p, except that the geometric distribution is discrete while the exponential distribution is continuous."}, {"text": "The exponential distribution is a continuous probability distribution used to model the time we need to wait before a given event occurs. It is the continuous counterpart of the geometric distribution, which is instead discrete. Sometimes it is also called negative exponential distribution."}, {"text": "The exponential distribution is in continuous time what the geometric distribution is in discrete time. A positive integer random variable X has the geometric distribution with parameter p \u2208 (0, 1] if: P(X = n) = p(1 \u2212 p)n\u22121, \u2200n \u2265 1, or, equivalently, if: P(X>n) = (1 \u2212 p)n, \u2200n \u2208 N."}, {"text": "follows a negative binomial distribution with parameters r and p. The geometric distribution is a special case of discrete compound Poisson distribution."}, {"text": "This is because geometric mean involves product term. However, for a data which follows log-normal distribution, geometric mean should be same as median."}, {"text": "Let X be a discrete random variable with a geometric distribution with parameter p for some 0<p\u22641. Then the moment generating function MX of X is given by: MX(t)=p1\u2212(1\u2212p)et."}]}, {"question": "What does noise in data mean", "positive_ctxs": [{"text": "Noisy data are data with a large amount of additional meaningless information in it called noise. This includes data corruption and the term is often used as a synonym for corrupt data. It also includes any data that a user system cannot understand and interpret correctly."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Ordinary least squares assumes things like equal variance of the noise at every x location. Generalized least squares does not assume a diagonal co-variance matrix."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "OLS (linear regression, linear model) assumes normally distributed residuals.  Ordinary least squares assumes things like equal variance of the noise at every x location. Generalized least squares does not assume a diagonal co-variance matrix."}]}, {"question": "What are the matching types that birst employs while searching data in cache", "positive_ctxs": [{"text": "\uf0b7 Birst employs caching and aggregate awareness to send queries to the cache first, and then data to the user-ready data store. \uf0b7 If data is not cached, Birst generates one or more queries depending on how the data is sourced. \uf0b7 Birst's in-memory caching includes both exact and fuzzy matching."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Structured data is highly specific and is stored in a predefined format, where unstructured data is a conglomeration of many varied types of data that are stored in their native formats. This means that structured data takes advantage of schema-on-write and unstructured data employs schema-on-read."}, {"text": "Understanding the differences Detection refers to mining insights or information in a data pool when it is being processed.  Prediction or predictive analysis employs probability based on the data analyses and processing."}, {"text": "Structured data is clearly defined and searchable types of data, while unstructured data is usually stored in its native format. Structured data is quantitative, while unstructured data is qualitative. Structured data is often stored in data warehouses, while unstructured data is stored in data lakes."}, {"text": "Structured data is clearly defined and searchable types of data, while unstructured data is usually stored in its native format.  Structured data is often stored in data warehouses, while unstructured data is stored in data lakes."}, {"text": "Non-hierarchical cluster analysis aims to find a grouping of objects which maximises or minimises some evaluating criterion. Many of these algorithms will iteratively assign objects to different groups while searching for some optimal value of the criterion."}, {"text": "Discussion ForumQue.Which search implements stack operation for searching the states?a.Depth-limited searchb.Depth-first searchc.Breadth-first searchd.None of the mentioned1 more row"}, {"text": "SummaryWeighted Mean: A mean where some values contribute more than others.When the weights add to 1: just multiply each weight by the matching value and sum it all up.Otherwise, multiply each weight w by its matching value x, sum that all up, and divide by the sum of weights: Weighted Mean = \u03a3wx\u03a3w."}]}, {"question": "How do you tell if a logistic function is increasing or decreasing", "positive_ctxs": [{"text": "the limiting value C is 1 + A times larger than the initial output y(0)A is the number of times that the initial population must grow to reach C.if B is positive, the logistic function will always increase,while if B is negative, the function will always decrease."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "As you experiment with your algorithm to try and improve your model, your loss function will tell you if you're getting(or reaching) anywhere. At its core, a loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value)."}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "Chi Square distributions are positively skewed, with the degree of skew decreasing with increasing degrees of freedom. As the degrees of freedom increases, the Chi Square distribution approaches a normal distribution."}, {"text": "A function whose value increases more slowly to infinity than any nonconstant polynomial is said to be a logarithmically increasing function."}, {"text": "Logistic Regression is a special case of a Neural Network with no hidden layers, that uses the sigmoid activation function and uses the softmax with cross entropy loss.  neural network and logistic regressions are different techniques or algorithms to do the same thing, classification of data."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Multinomial logistic regression is used when you have a categorical dependent variable with two or more unordered levels (i.e. two or more discrete outcomes). It is practically identical to logistic regression, except that you have multiple possible outcomes instead of just one."}]}, {"question": "How do you apply multidimensional scaling", "positive_ctxs": [{"text": "Basic steps:Assign a number of points to coordinates in n-dimensional space.  Calculate Euclidean distances for all pairs of points.  Compare the similarity matrix with the original input matrix by evaluating the stress function.  Adjust coordinates, if necessary, to minimize stress."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "MinMax scaling will not affect the values of dummy variables but Standardised scaling will."}, {"text": "So you are model-free. This is when you apply Q learning.  With value iteration, you learn the expected cost when you are given a state x. With q-learning, you get the expected discounted cost when you are in state x and apply action a."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How TensorFlow works. TensorFlow allows developers to create dataflow graphs\u2014structures that describe how data moves through a graph, or a series of processing nodes. Each node in the graph represents a mathematical operation, and each connection or edge between nodes is a multidimensional data array, or tensor."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}]}, {"question": "What is deep learning Good For", "positive_ctxs": [{"text": "Deep learning networks can be successfully applied to big data for knowledge discovery, knowledge application, and knowledge-based prediction. In other words, deep learning can be a powerful engine for producing actionable results."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}, {"text": "Bayesian deep learning is a field at the intersection between deep learning and Bayesian probability theory.  Bayesian deep learning models typically form uncertainty estimates by either placing distributions over model weights, or by learning a direct mapping to probabilistic outputs."}]}, {"question": "What is correlation in computer vision", "positive_ctxs": [{"text": "Correlation is the process of moving a filter mask often referred to as kernel over the image and computing the sum of products at each location. Correlation is the function of displacement of the filter."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Multilayer Perceptron (MLP): used to apply in computer vision, now succeeded by Convolutional Neural Network (CNN). MLP is now deemed insufficient for modern advanced computer vision tasks. Has the characteristic of fully connected layers, where each perceptron is connected with every other perceptron."}, {"text": "Calculating the distance of various points in the scene relative to the position of the camera is one of the important tasks for a computer vision system."}, {"text": "Intel\u00ae Movidius\u2122 VPUs enable demanding computer vision and edge AI workloads with efficiency.  VPU technology enables intelligent cameras, edge servers and AI appliances with deep neural network and computer vision based applications in areas such as visual retail, security and safety, and industrial automation."}, {"text": "Object recognition is a computer vision technique for identifying objects in images or videos. Object recognition is a key output of deep learning and machine learning algorithms.  The goal is to teach a computer to do what comes naturally to humans: to gain a level of understanding of what an image contains."}, {"text": "Fei-Fei Li, computer vision is defined as \u201ca subset of mainstream artificial intelligence that deals with the science of making computers or machines visually enabled, i.e., they can analyze and understand an image.\u201d Human vision starts at the biological camera's \u201ceyes,\u201d which takes one picture about every 200"}, {"text": "Fei-Fei Li, computer vision is defined as \u201ca subset of mainstream artificial intelligence that deals with the science of making computers or machines visually enabled, i.e., they can analyze and understand an image.\u201d Human vision starts at the biological camera's \u201ceyes,\u201d which takes one picture about every 200"}]}, {"question": "Which one is the evaluation metric used to measure performance of a machine translation system", "positive_ctxs": [{"text": "The Word error rate (WER) is a metric based on the Levenshtein distance, where the Levenshtein distance works at the character level, WER works at the word level. It was originally used for measuring the performance of speech recognition systems, but is also used in the evaluation of machine translation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Loss value implies how poorly or well a model behaves after each iteration of optimization. An accuracy metric is used to measure the algorithm's performance in an interpretable way.  It is the measure of how accurate your model's prediction is compared to the true data."}, {"text": "The mean average precision (mAP) or sometimes simply just referred to as AP is a popular metric used to measure the performance of models doing document/information retrival and object detection tasks."}, {"text": "The most frequently used evaluation metric of survival models is the concordance index (c index, c statistic). It is a measure of rank correlation between predicted risk scores f^ and observed time points y that is closely related to Kendall's \u03c4."}, {"text": "Loss value implies how poorly or well a model behaves after each iteration of optimization. An accuracy metric is used to measure the algorithm's performance in an interpretable way. The accuracy of a model is usually determined after the model parameters and is calculated in the form of a percentage."}, {"text": "Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model."}, {"text": "A loss function is used to optimize a machine learning algorithm. The loss is calculated on training and validation and its interpretation is based on how well the model is doing in these two sets.  An accuracy metric is used to measure the algorithm's performance in an interpretable way."}, {"text": "A metric is a function that is used to judge the performance of your model. Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that you may use any loss function as a metric."}]}, {"question": "How accurate is Altman Z score", "positive_ctxs": [{"text": "In its initial test, the Altman Z-Score was found to be 72% accurate in predicting bankruptcy two years prior to the event. In subsequent tests over 31 years up until 1999, the model was found to be 80-90% accurate in predicting bankruptcy one year prior to the event."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "Proof: The integers Z are countable because the function f : Z \u2192 N given by f(n) = 2n if n is non-negative and f(n) = 3\u2212 n if n is negative, is an injective function."}]}, {"question": "What is data mining classification", "positive_ctxs": [{"text": "Classification is a data mining function that assigns items in a collection to target categories or classes. The goal of classification is to accurately predict the target class for each case in the data. For example, a classification model could be used to identify loan applicants as low, medium, or high credit risks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Association rules mining is another key unsupervised data mining method, after clustering, that finds interesting associations (relationships, dependencies) in large sets of data items."}, {"text": "Classification is a data mining function that assigns items in a collection to target categories or classes. The goal of classification is to accurately predict the target class for each case in the data. For example, a classification model could be used to identify loan applicants as low, medium, or high credit risks."}, {"text": "The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or"}, {"text": "Definition: In simple words, data mining is defined as a process used to extract usable data from a larger set of any raw data. It implies analysing data patterns in large batches of data using one or more software.  Data mining is also known as Knowledge Discovery in Data (KDD)."}, {"text": "Definition: In simple words, data mining is defined as a process used to extract usable data from a larger set of any raw data. It implies analysing data patterns in large batches of data using one or more software.  Data mining is also known as Knowledge Discovery in Data (KDD)."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Cross-validation is a standard tool in analytics and is an important feature for helping you develop and fine-tune data mining models.  Cross-validation has the following applications: Validating the robustness of a particular mining model. Evaluating multiple models from a single statement."}]}, {"question": "How do you find the marginal density function", "positive_ctxs": [{"text": "4:306:35Suggested clip \u00b7 77 secondsMarginal PDF from Joint PDF - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In the case of a pair of random variables (X, Y), when random variable X (or Y) is considered by itself, its density function is called the marginal density function."}, {"text": "their joint probability distribution at (x,y), the functions given by: g(x) = \u03a3y f (x,y) and h(y) = \u03a3x f (x,y) are the marginal distributions of X and Y , respectively. If you're great with equations, that's probably all you need to know. It tells you how to find a marginal distribution."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "2 Answers. By definition the probability density function is the derivative of the distribution function. But distribution function is an increasing function on R thus its derivative is always positive. Assume that probability density of X is -ve in the interval (a, b)."}, {"text": "The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable."}, {"text": "The cumulative density function gives you the probability of a random variable being on or below a certain value. The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value.  A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75."}]}, {"question": "What does IID stand for", "positive_ctxs": [{"text": "Independent and Identically Distributed"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "formal parameter \u2014 the identifier used in a method to stand for the value that is passed into the method by a caller. actual parameter \u2014 the actual value that is passed into the method by a caller."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}]}, {"question": "How does the implicit bias test work", "positive_ctxs": [{"text": "The Implicit Association Test (IAT) measures the strength of associations between concepts (e.g., black people, gay people) and evaluations (e.g., good, bad) or stereotypes (e.g., athletic, clumsy). The main idea is that making a response is easier when closely related items share the same response key."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Also known as implicit social cognition, implicit bias refers to the attitudes or stereotypes that affect our understanding, actions, and decisions in an unconscious manner."}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "A non-parametric test is a hypothesis test that does not make any assumptions about the distribution of the samples.  It does not rely on any properties of the distributions. The null hypothesis is that the samples were drawn from the same distribution."}, {"text": "There are two big reasons why you want homoscedasticity: While heteroscedasticity does not cause bias in the coefficient estimates, it does make them less precise. Lower precision increases the likelihood that the coefficient estimates are further from the correct population value."}, {"text": "There are two big reasons why you want homoscedasticity: While heteroscedasticity does not cause bias in the coefficient estimates, it does make them less precise. Lower precision increases the likelihood that the coefficient estimates are further from the correct population value."}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}]}, {"question": "What is adjusted R squared", "positive_ctxs": [{"text": "R-squared measures the proportion of the variation in your dependent variable (Y) explained by your independent variables (X) for a linear regression model. Adjusted R-squared adjusts the statistic based on the number of independent variables in the model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "AS a general thumb rule if adjusted R 2 increases when a new variables is added to the model, the variable should remain in the model. If the adjusted R2 decreases when the new variable is added then the variable should not remain in the model."}, {"text": "Coefficient of correlation is \u201cR\u201d value which is given in the summary table in the Regression output. R square is also called coefficient of determination. Multiply R times R to get the R square value. In other words Coefficient of Determination is the square of Coefficeint of Correlation."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The adjusted coefficient of determination (also known as adjusted R2 or. pronounced \u201cR bar squared\u201d) is a statistical measure that shows the proportion of variation explained by the estimated regression line."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance."}]}, {"question": "What is squashing in machine learning", "positive_ctxs": [{"text": "A squashing function is essentially defined as a function that squashes the input to one of the ends of a small interval. In Neural Networks, these can be used at nodes in a hidden layer to squash the input.  Popular ones that have been used include the sigmoid function, hyperbolic tangent function, etc."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "15 Most Used Machine Learning Tools By ExpertsKnime. Knime is again an open-source machine learning tool that is based on GUI.  Accord.net. Accord.net is a computational machine learning framework.  Scikit-Learn. Scikit-Learn is an open-source machine learning package.  TensorFlow.  Weka.  Pytorch.  RapidMiner.  Google Cloud AutoML.More items\u2022"}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}]}, {"question": "What is Version space in ML", "positive_ctxs": [{"text": "A version space is a hierarchial representation of knowledge that enables you to keep track of all the useful information supplied by a sequence of learning examples without remembering any of the examples."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The process of training an ML model involves providing an ML algorithm (that is, the learning algorithm) with training data to learn from. The term ML model refers to the model artifact that is created by the training process.  You can use the ML model to get predictions on new data for which you do not know the target."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The difference is very slim between machine learning (ML) and optimization theory. In ML the idea is to learn a function that minimizes an error or one that maximizes reward over punishment.  The goal for ML is similarly to optimize the performance of a model given an objective and the training data."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "A vector space is a space of vectors, ie. each element is a vector. A vector field is, at its core, a function between some space and some vector space, so every point in our base space has a vector assigned to it. A good example would be wind direction maps you see on weather reports."}]}, {"question": "Which of the following are differences between a field and a parameter", "positive_ctxs": [{"text": "-A field is a variable that exists inside of an object, while a parameter is a variable inside a method whose value is passed in from outside."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": ": the mean of the absolute values of the numerical differences between the numbers of a set (such as statistical data) and their mean or median."}, {"text": "In vector calculus and physics, a vector field is an assignment of a vector to each point in a subset of space. For instance, a vector field in the plane can be visualised as a collection of arrows with a given magnitude and direction, each attached to a point in the plane."}, {"text": "A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic , whereas Markov networks are undirected and may be cyclic.  The underlying graph of a Markov random field may be finite or infinite."}, {"text": "A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic , whereas Markov networks are undirected and may be cyclic.  The underlying graph of a Markov random field may be finite or infinite."}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "The main differences between an RMS Voltage and an Average Voltage, is that the mean value of a periodic wave is the average of all the instantaneous areas taken under the curve over a given period of the waveform, and in the case of a sinusoidal quantity, this period is taken as one-half of the cycle of the wave."}, {"text": "Now, every textbook on linear algebra gives the following definition of a linear operator: an operator T: V\u2014> W between two vector spaces V and W over the same field ! F is said to be linear if it satisfies the conditions of additivity, viz. T(u + v)=T(u)+T(v)"}]}, {"question": "What are coefficients in logistic regression", "positive_ctxs": [{"text": "Coef. A regression coefficient describes the size and direction of the relationship between a predictor and the response variable. Coefficients are the numbers by which the values of the term are multiplied in a regression equation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "OLS cannot be used because the regression function is not a linear function of the regression coefficients (the coefficients appear inside the nonlinear functions \u03a6 or \u039b)."}, {"text": "A collinearity is a special case when two or more variables are exactly correlated. This means the regression coefficients are not uniquely determined. In turn it hurts the interpretability of the model as then the regression coefficients are not unique and have influences from other features."}, {"text": "Introduction. Linear regression and logistic regression are two types of regression analysis techniques that are used to solve the regression problem using machine learning. They are the most prominent techniques of regression."}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}]}, {"question": "How is quantile calculated", "positive_ctxs": [{"text": "In simple terms, a quantile is where a sample is divided into equal-sized, adjacent, subgroups (that's why it's sometimes called a \u201cfractile\u201c).  The median cuts a distribution into two equal areas and so it is sometimes called 2-quantile. Quartiles are also quantiles; they divide the distribution into four equal parts."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value. So the median is the value of the quantile at the probability value of 0.5. A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75."}, {"text": "A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times. Given a prediction yi^p and outcome yi, the mean regression loss for a quantile q is. For a set of predictions, the loss will be its average."}, {"text": "The cumulative density function gives you the probability of a random variable being on or below a certain value. The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value.  A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75."}, {"text": "Relative Risk is calculated by dividing the probability of an event occurring for group 1 (A) divided by the probability of an event occurring for group 2 (B). Relative Risk is very similar to Odds Ratio, however, RR is calculated by using percentages, whereas Odds Ratio is calculated by using the ratio of odds."}, {"text": "The population standard deviation is a parameter, which is a fixed value calculated from every individual in the population. A sample standard deviation is a statistic. This means that it is calculated from only some of the individuals in a population."}, {"text": "A quantile defines a particular part of a data set, i.e. a quantile determines how many values in a distribution are above or below a certain limit. Special quantiles are the quartile (quarter), the quintile (fifth) and percentiles (hundredth)."}, {"text": "Qualitative Differences The population standard deviation is a parameter, which is a fixed value calculated from every individual in the population. A sample standard deviation is a statistic. This means that it is calculated from only some of the individuals in a population."}]}, {"question": "Why do we use logarithms in regression", "positive_ctxs": [{"text": "A regression model will have unit changes between the x and y variables, where a single unit change in x will coincide with a constant change in y. Taking the log of one or both variables will effectively change the case from a unit change to a percent change.  A logarithm is the base of a positive number."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}, {"text": "An algorithm that uses random numbers to decide what to do next anywhere in its logic is called a Randomized Algorithm. For example, in Randomized Quick Sort, we use a random number to pick the next pivot (or we randomly shuffle the array). And in Karger's algorithm, we randomly pick an edge."}, {"text": "Generally, we use linear regression for time series analysis, it is used for predicting the result for time series as its trends. For example, If we have a dataset of time series with the help of linear regression we can predict the sales with the time."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "So, For hidden layers the best option to use is ReLU, and the second option you can use as SIGMOID. For output layers the best option depends, so we use LINEAR FUNCTIONS for regression type of output layers and SOFTMAX for multi-class classification."}]}, {"question": "How do you optimize code", "positive_ctxs": [{"text": "Try to avoid implementing cheap tricks to make your code run faster.Optimize your Code using Appropriate Algorithm.  Optimize Your Code for Memory.  printf and scanf Vs cout and cin.  Using Operators.  if Condition Optimization.  Problems with Functions.  Optimizing Loops.  Data Structure Optimization.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to optimize your meta tags: A checklistCheck whether all your pages and your content have title tags and meta descriptions.Start paying more attention to your headings and how you structure your content.Don't forget to mark up your images with alt text.More items\u2022"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "n_estimators : This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Machine learning field allows you to code in a way so that the application or system can learn to solve the problem on it's own. Learning is a iterative process."}]}, {"question": "What is non numeric data", "positive_ctxs": [{"text": "An example is the weight of luggage loaded onto an airplane. Counting the number of times a ball dropped from a rooftop bounces before it comes to rest comprises numerical data.On the other hand, non-numerical data, also called categorical, qualitative or Yes/No data, is data that can be observed, not measured."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Numeric data types are numbers kept in database columns. Numerical data is data that is quantifiable, such as time, height, weight, amount, and so on. A non- numeric data bring up to categorical data."}, {"text": "- Categorical Variable Transformation: is turning a categorical variable to a numeric variable. Categorical variable transformation is mandatory for most of the machine learning models because they can handle only numeric values."}, {"text": "Any LTI filter with output and input can be represented by a difference equation in the form: If at least one of the is not null, the filter is recursive. If the are all zero, it is a non recursive filter usually called FIR (Finite Input Response) filter.  This happens both to recursive and non recursive filters."}, {"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}, {"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is ordinary least squares regression analysis", "positive_ctxs": [{"text": "Ordinary Least Squares regression (OLS) is more commonly named linear regression (simple or multiple depending on the number of explanatory variables).  The OLS method corresponds to minimizing the sum of square differences between the observed and predicted values."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Weighted least squares (WLS), also known as weighted linear regression, is a generalization of ordinary least squares and linear regression in which the errors covariance matrix is allowed to be different from an identity matrix."}, {"text": "Logistic regression works like ordinary least squares regression but on the logit of the dependent variable. Discriminant analysis is really used only for categorization. Logistic regression is often used when we aren't even interested in categorization but in getting the odds ratios for each variable."}, {"text": "Ridge regression is a term used to refer to a linear regression model whose coefficients are not estimated by ordinary least squares (OLS), but by an estimator, called ridge estimator, that is biased but has lower variance than the OLS estimator."}, {"text": "Logistic regression is quite different than linear regression in that it does not make several of the key assumptions that linear and general linear models (as well as other ordinary least squares algorithm based models) hold so close: (1) logistic regression does not require a linear relationship between the dependent"}, {"text": "1 Introduction. The partial least squares (PLS) algorithm was first introduced for regression tasks and then evolved into a classification method that is well known as PLS-discriminant analysis (PLS-DA)."}, {"text": "Robust regression is an alternative to least squares regression when data is contaminated with outliers or influential observations and it can also be used for the purpose of detecting influential observations. Please note: The purpose of this page is to show how to use various data analysis commands."}, {"text": "The least squares approach limits the distance between a function and the data points that the function explains. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data. Mathematicians use the least squares method to arrive at a maximum-likelihood estimate."}]}, {"question": "How do you use a TensorBoard", "positive_ctxs": [{"text": "Starting TensorBoardOpen up the command prompt (Windows) or terminal (Ubuntu/Mac)Go into the project home directory.If you are using Python virtuanenv, activate the virtual environment you have installed TensorFlow in.Make sure that you can see the TensorFlow library through Python.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Get startedPrepare your TensorBoard logs. (or download a sample from here).Upload the logs. Install the latest version of TensorBoard to use the uploader. $ pip install -U tensorboard.  View your experiment on TensorBoard. dev. Follow the link provided to view your experiment, or share it with others."}, {"text": "TensorBoard is a suite of web applications for inspecting and understanding your TensorFlow runs and graphs. TensorBoard currently supports five visualizations: scalars, images, audio, histograms, and graphs."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Specifically, you learned: That a key approach is to use word embeddings and convolutional neural networks for text classification. That a single layer model can do well on moderate-sized problems, and ideas on how to configure it."}, {"text": "Naive Bayes Classifier example by hand and how to do in Scikit-Learn, You can use any kind of predictor in a naive Bayes classifier, as long as you can specify a discriminative linear models take a mixture of categorical and continuous predictors."}]}, {"question": "Is a single layer feed forward neural network equivalent to a logistic regression algorithm", "positive_ctxs": [{"text": "Yes. This is the architechture of logistic regression, which is similar to a single layer feed forward neural network."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "8 Radial Basis Function Networks. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems.  An RBF network is a type of feed forward neural network composed of three layers, namely the input layer, the hidden layer and the output layer."}, {"text": "The general algorithm is The Backpropagation algorithm is suitable for the feed forward neural network on fixed sized input-output pairs. The Backpropagation Through Time is the application of Backpropagation training algorithm which is applied to the sequence data like the time series."}, {"text": "A feedforward neural network is an artificial neural network wherein connections between the units do not form a cycle.  The simplest kind of neural network is a single-layer perceptron network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights."}, {"text": "The algorithm is used to effectively train a neural network through a method called chain rule. In simple terms, after each forward pass through a network, backpropagation performs a backward pass while adjusting the model's parameters (weights and biases)."}, {"text": "A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.  If you accept most classes of problems can be reduced to functions, this statement implies a neural network can, in theory, solve any problem."}, {"text": "Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks. Perceptron is a linear classifier (binary). Also, it is used in supervised learning. It helps to classify the given input data."}, {"text": "1 Answer. The difference between a classification and regression is that a classification outputs a prediction probability for class/classes and regression provides a value. We can make a neural network to output a value by simply changing the activation function in the final layer to output the values."}]}, {"question": "What is the use of n grams", "positive_ctxs": [{"text": "Applications and considerations. n-gram models are widely used in statistical natural language processing. In speech recognition, phonemes and sequences of phonemes are modeled using a n-gram distribution. For parsing, words are modeled such that each n-gram is composed of n words."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, Bessel's correction is the use of n \u2212 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance."}, {"text": "Order the values of a data set of size n from smallest to largest. If n is odd, the sample median is the value in position (n + 1)/2; if n is even, it is the average of the values in positions n/2 and n/2 + 1."}, {"text": "We can compare the quality of two estimators by looking at the ratio of their MSE. If the two estimators are unbiased this is equivalent to the ratio of the variances which is defined as the relative efficiency. rndr = n + 1 n \u00b7 n n + 1 \u03b8."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The Slovin's Formula is given as follows: n = N/(1+Ne2), where n is the sample size, N is the population size and e is the margin of error to be decided by the researcher."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "A binomial random variable is the number of successes x in n repeated trials of a binomial experiment.Binomial DistributionThe mean of the distribution (\u03bcx) is equal to n * P .The variance (\u03c32x) is n * P * ( 1 - P ).The standard deviation (\u03c3x) is sqrt[ n * P * ( 1 - P ) ]."}]}, {"question": "What are the three axioms of probability", "positive_ctxs": [{"text": "The three axioms are: For any event A, P(A) \u2265 0. In English, that's \u201cFor any event A, the probability of A is greater or equal to 0\u201d. When S is the sample space of an experiment; i.e., the set of all possible outcomes, P(S) = 1."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "At the heart of this definition are three conditions, called the axioms of probability theory. Axiom 1: The probability of an event is a real number greater than or equal to 0. Axiom 2: The probability that at least one of all the possible outcomes of a process (such as rolling a die) will occur is 1."}, {"text": "The three axioms are:For any event A, P(A) \u2265 0. In English, that's \u201cFor any event A, the probability of A is greater or equal to 0\u201d.When S is the sample space of an experiment; i.e., the set of all possible outcomes, P(S) = 1.  If A and B are mutually exclusive outcomes, P(A \u222a B ) = P(A) + P(B)."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Convolution is used in the mathematics of many fields, such as probability and statistics. In linear systems, convolution is used to describe the relationship between three signals of interest: the input signal, the impulse response, and the output signal."}, {"text": "\u00b72 min read Here is a comparison of three basic pooling methods that are widely used. The three types of pooling operations are: Max pooling: The maximum pixel value of the batch is selected.  Average pooling: The average value of all the pixels in the batch is selected."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}]}, {"question": "What is a convolution in deep learning", "positive_ctxs": [{"text": "A convolution is the simple application of a filter to an input that results in an activation. Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in an input, such as an image."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep NN is just a deep neural network, with a lot of layers. It can be CNN, or just a plain multilayer perceptron. CNN, or convolutional neural network, is a neural network using convolution layer and pooling layer."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}, {"text": "Dictionary learning is learning a set of atoms so that a given image can be well approximated by a sparse linear combination of these learned atoms, while deep learning methods aim at extracting deep semantic feature representations via a deep network."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}]}, {"question": "How do you find the sample size for a stratified sample", "positive_ctxs": [{"text": "For example, if the researcher wanted a sample of 50,000 graduates using age range, the proportionate stratified random sample will be obtained using this formula: (sample size/population size) x stratum size. The table below assumes a population size of 180,000 MBA graduates per year."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A P value is also affected by sample size and the magnitude of effect. Generally the larger the sample size, the more likely a study will find a significant relationship if one exists. As the sample size increases the impact of random error is reduced."}, {"text": "Find a confidence level for a data set by taking half of the size of the confidence interval, multiplying it by the square root of the sample size and then dividing by the sample standard deviation. Look up the resulting Z or t score in a table to find the level."}, {"text": "Compared to simple random sampling, stratified sampling has two main disadvantages.Advantages and DisadvantagesA stratified sample can provide greater precision than a simple random sample of the same size.Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money.More items"}, {"text": "If you increase your sample size you increase the precision of your estimates, which means that, for any given estimate / size of effect, the greater the sample size the more \u201cstatistically significant\u201d the result will be."}, {"text": "Stratified sampling offers several advantages over simple random sampling. A stratified sample can provide greater precision than a simple random sample of the same size. Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money."}, {"text": "Stratified sampling offers several advantages over simple random sampling. A stratified sample can provide greater precision than a simple random sample of the same size. Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money."}, {"text": "As explained above, the shape of the t-distribution is affected by sample size.  As the sample size increases, so do degrees of freedom. When degrees of freedom are infinite, the t-distribution is identical to the normal distribution. As sample size increases, the sample more closely approximates the population."}]}, {"question": "What does univariate analysis mean", "positive_ctxs": [{"text": "Definition. Univariate analyses are used extensively in quality of life research. Univariate analysis is defined as analysis carried out on only one (\u201cuni\u201d) variable (\u201cvariate\u201d) to summarize or describe the variable (Babbie, 2007; Trochim, 2006)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Another common example of univariate analysis is the mean of a population distribution. Tables, charts, polygons, and histograms are all popular methods for displaying univariate analysis of a specific variable (e.g. mean, median, mode, standard variation, range, etc)."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.  Like univariate analysis, bivariate analysis can be descriptive or inferential."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "- Quora. Non-significant variables on univariate analysis became significant on multivariate analysis?  Yes, it is possible that when you add more predictors (X2, X3 and so forth) in a multiple regression, X1 can become a statistically significant predictor."}, {"text": "Because a chi-square test is a univariate test; it does not consider relationships among multiple variables at the same time. Therefore, dependencies detected by chi-square analyses may be unrealistic or non-causal. There may be other unseen factors that make the variables appear to be associated."}]}, {"question": "What is similarity matrix in clustering", "positive_ctxs": [{"text": "Cluster-Based Similarity Partitioning Algorithm For each input partition, an N \u00d7 N binary similarity matrix encodes the piecewise similarity between any two objects, that is, the similarity of one indicates that two objects are grouped into the same cluster and a similarity of zero otherwise."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Dissimilarity matrix is a matrix that expresses the similarity pair to pair between two sets. It's square and symmetric. The diagonal members are defined as zero, meaning that zero is the measure of dissimilarity between an element and itself."}, {"text": "The k-means clustering algorithm is one of the most widely used, effective, and best understood clustering methods.  In this paper we propose a supervised learning approach to finding a similarity measure so that k-means provides the desired clusterings for the task at hand."}, {"text": "Hierarchical clustering is an instance of the agglomerative or bottom-up approach, where we start with each data point as its own cluster and then combine clusters based on some similarity measure."}, {"text": "Low-rank matrix factorization (MF) is an important technique in data science. The key idea of MF is that there exists latent structures in the data, by uncovering which we could obtain a compressed representation of the data.  By properly adapting MF, we can go beyond the problem of clustering and matrix completion."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Computing accuracy for clustering can be done by reordering the rows (or columns) of the confusion matrix so that the sum of the diagonal values is maximal. The linear assignment problem can be solved in O(n3) instead of O(n!). Coclust library provides an implementation of the accuracy for clustering results."}, {"text": "An invertible matrix is a square matrix that has an inverse. We say that a square matrix is invertible if and only if the determinant is not equal to zero. In other words, a 2 x 2 matrix is only invertible if the determinant of the matrix is not 0."}]}, {"question": "How does stochastic gradient descent work", "positive_ctxs": [{"text": "Stochastic Gradient Descent (SGD): Hence, in Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration.  This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "The coefficients used in simple linear regression can be found using stochastic gradient descent.  Linear regression does provide a useful exercise for learning stochastic gradient descent which is an important algorithm used for minimizing cost functions by machine learning algorithms."}, {"text": "According to a senior data scientist, one of the distinct advantages of using Stochastic Gradient Descent is that it does the calculations faster than gradient descent and batch gradient descent. However, gradient descent is the best approach if one wants a speedier result."}, {"text": "Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data."}, {"text": "Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data."}, {"text": "The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated. The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset."}, {"text": "Adam optimizer. Implements the Adam optimization algorithm. Adam is a stochastic gradient descent method that computes individual adaptive learning rates for different parameters from estimates of first- and second-order moments of the gradients."}]}, {"question": "Is unit step function bounded", "positive_ctxs": [{"text": "It's true that the unit step function is bounded. However, a system which has the unit step function as its impulse response is not stable, because the integral (of the absolute value) is infinite. Bounded and stable are not the same thing."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "ReLu bounded negative outputs to 0 & above. This works well in hidden layers than the final output layer.  It is not typical, since in this case, the ouput value is not bounded in a range."}, {"text": "The rectifier is, as of 2017, the most popular activation function for deep neural networks. A unit employing the rectifier is also called a rectified linear unit (ReLU)."}, {"text": "A two-sided hypothesis is an alternative hypothesis which is not bounded from above or from below, as opposed to a one-sided hypothesis which is always bounded from either above or below. In fact, a two-sided hypothesis is nothing more than the union of two one-sided hypotheses."}, {"text": "Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2."}, {"text": "The receptive field in Convolutional Neural Networks (CNN) is the region of the input space that affects a particular unit of the network.  The numbers inside the pixels on the left image represent how many times this pixel was part of a convolution step (each sliding step of the filter)."}, {"text": "Relative Frequency Of A Class Is The Percentage Of The Data That Falls In That Class, While Cumulative Frequency Of A Class Is The Sum Of The Frequencies Of That Class And All Previous Classes."}]}, {"question": "What is feature map in convolutional neural network", "positive_ctxs": [{"text": "The feature map on CNN is the output of one filter applied to the previous layer. A filter that is given is drawn across the entire previous layer, moved one pixel at a time. Each position results in activation of the neuron and the output are collected in the feature map."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Convolutional layers are the major building blocks used in convolutional neural networks.  Convolutional neural networks apply a filter to an input to create a feature map that summarizes the presence of detected features in the input."}, {"text": "Convolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input. A pooling layer is a new layer added after the convolutional layer."}, {"text": "Advantages of Recurrent Neural Network It is useful in time series prediction only because of the feature to remember previous inputs as well. This is called Long Short Term Memory. Recurrent neural network are even used with convolutional layers to extend the effective pixel neighborhood."}, {"text": "Advantages of Recurrent Neural Network It is useful in time series prediction only because of the feature to remember previous inputs as well. This is called Long Short Term Memory. Recurrent neural network are even used with convolutional layers to extend the effective pixel neighborhood."}, {"text": "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery."}, {"text": "A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input."}]}, {"question": "What is an example of a two tailed test", "positive_ctxs": [{"text": "A test of a statistical hypothesis , where the region of rejection is on both sides of the sampling distribution , is called a two-tailed test. For example, suppose the null hypothesis states that the mean is equal to 10. The alternative hypothesis would be that the mean is less than 10 or greater than 10."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28. For a two tailed test, use \u03b1/2 = 0.05 and the critical region is below z = -1.645 and above z = 1.645."}, {"text": "Depending on the alternative hypothesis operator, greater than operator will be a right tailed test, less than operator is a left tailed test, and not equal operator is a two tailed test."}, {"text": "The definition of an ensemble is two or more people or things that function together as a whole. An example of an ensemble is a string quartet. An example of an ensemble is a group of actors in a play.  A small group of musicians playing or singing together."}, {"text": "An AB test is an example of statistical hypothesis testing, a process whereby a hypothesis is made about the relationship between two data sets and those data sets are then compared against each other to determine if there is a statistically significant relationship or not."}, {"text": "T-tests are called t-tests because the test results are all based on t-values. T-values are an example of what statisticians call test statistics. A test statistic is a standardized value that is calculated from sample data during a hypothesis test."}, {"text": "One or two of the sections is the \u201crejection region\u201c; if your test value falls into that region, then you reject the null hypothesis. A one tailed test with the rejection rejection in one tail. The critical value is the red line to the left of that region."}, {"text": "For example, an ANOVA test assumes that the variances of different populations are equal (i.e. homogeneous). One example of a test is the Chi-Square Test for Homogeneity. This tests to see if two populations come from the same unknown distribution (if they do, then they are homogeneous)."}]}, {"question": "How do neural networks choose the weights and biases", "positive_ctxs": [{"text": "Weights control the signal (or the strength of the connection) between two neurons. In other words, a weight decides how much influence the input will have on the output. Biases, which are constant, are an additional input into the next layer that will always have the value of 1."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A fancy name for training: the selection of parameter values, which are optimal in some desired sense (eg. minimize an objective function you choose over a dataset you choose). The parameters are the weights and biases of the network."}, {"text": "Neural network ensemble is a learning paradigm where many neural networks are jointly used to solve a problem.  Then it assigns random weights to those networks and employs genetic algorithm to evolve the weights so that they can characterize to some extent the fitness of the neural networks in constituting an ensemble."}, {"text": "Bayesian neural networks differ from plain neural networks in that their weights are assigned a probability distribution instead of a single value or point estimate. These probability distributions describe the uncertainty in weights and can be used to estimate uncertainty in predictions."}, {"text": "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use."}, {"text": "In a CNN, each layer has two kinds of parameters : weights and biases. The total number of parameters is just the sum of all weights and biases. Let's define, = Number of weights of the Conv Layer. = Number of biases of the Conv Layer."}, {"text": "Neural network momentum is a simple technique that often improves both training speed and accuracy. Training a neural network is the process of finding values for the weights and biases so that for a given set of input values, the computed output values closely match the known, correct, target values."}, {"text": "During training stage the residual network alters the weights until the output is equivalent to the identity function.  In turn the identity function helps in building a deeper network. The residual function then maps the identity, weights and biases to fit the actual value."}]}, {"question": "What is the difference between principal component analysis PCA and feature selection in machine learning Is PCA a means of feature selection", "positive_ctxs": [{"text": "The difference is that PCA will try to reduce dimensionality by exploring how one feature of the data is expressed in terms of the other features(linear dependecy). Feature selection instead, takes the target into consideration.  PCA is based on extracting the axes on which data shows the highest variability."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection. Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "The difference between factor analysis and principal component analysis.  Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables."}, {"text": "Establish face validity.Conduct a pilot test.Enter the pilot test in a spreadsheet.Use principal component analysis (PCA)Check the internal consistency of questions loading onto the same factors.Revise the questionnaire based on information from your PCA and CA."}, {"text": "Establish face validity.Conduct a pilot test.Enter the pilot test in a spreadsheet.Use principal component analysis (PCA)Check the internal consistency of questions loading onto the same factors.Revise the questionnaire based on information from your PCA and CA."}]}, {"question": "What is an intuitive explanation for the multivariate Gaussian distribution aka multivariate normal", "positive_ctxs": [{"text": "It's a Gaussian distribution in more than one dimension at a time. Nothing tricky in the combining itself, just a straightforward Cartesian-style combination."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions."}, {"text": "The multivariate normal distribution has two or more random variables \u2014 so the bivariate normal distribution is actually a special case of the multivariate normal distribution."}, {"text": "A univariate distribution refers to the distribution of a single random variable.  On the other hand, a multivariate distribution refers to the probability distribution of a group of random variables. For example, a multivariate normal distribution is used to specify the probabilities of returns of a group of n stocks."}, {"text": "A multivariate normal distribution is a vector in multiple normally distributed variables, such that any linear combination of the variables is also normally distributed."}, {"text": "As the name implies, multivariate regression is a technique that estimates a single regression model with more than one outcome variable. When there is more than one predictor variable in a multivariate regression model, the model is a multivariate multiple regression."}, {"text": "The Dirichlet is the multivariate generalization of the beta distribution.  The Dirichlet equals the uniform distribution when all parameters (\u03b11\u2026 \u03b1k) are equal. The Dirichlet distribution is a conjugate prior to the categorical distribution and multinomial distributions. A compound variant is the Dirichlet-multinomial."}, {"text": "In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed."}]}, {"question": "What are regression models used for", "positive_ctxs": [{"text": "Use regression analysis to describe the relationships between a set of independent variables and the dependent variable. Regression analysis produces a regression equation where the coefficients represent the relationship between each independent variable and the dependent variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Suggest Edits. Support Vector Machines (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis."}, {"text": "In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis."}, {"text": "In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis."}, {"text": "Classification and regression trees are machine-learning methods for constructing. prediction models from data. The models are obtained by recursively partitioning. the data space and fitting a simple prediction model within each partition."}, {"text": "Multiple regression models forecast a variable using a linear combination of predictors, whereas autoregressive models use a combination of past values of the variable.  These concepts and techniques are used by technical analysts to forecast security prices."}, {"text": "MSE is used to check how close estimates or forecasts are to actual values. Lower the MSE, the closer is forecast to actual. This is used as a model evaluation measure for regression models and the lower value indicates a better fit."}, {"text": "Linear models, generalized linear models, and nonlinear models are examples of parametric regression models because we know the function that describes the relationship between the response and explanatory variables.  If the relationship is unknown and nonlinear, nonparametric regression models should be used."}]}, {"question": "What is bias in classification", "positive_ctxs": [{"text": "Prediction bias is a quantity that measures how far apart those two averages are. That is: prediction bias = average of predictions \u2212 average of labels in data set. Note: \"Prediction bias\" is a different quantity than bias (the b in wx + b)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}]}, {"question": "What is the difference between ordinary least squares regression and linear regression with the least squares method", "positive_ctxs": [{"text": "Least squares is an estimation technique that allows you to estimate the parameters of models. OLS (ordinary least squares) is the least squares technique used for estimating the parameters of linear regression models.  The problem of linear regression is to fit a line to the data by minimizing the error."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Weighted least squares (WLS), also known as weighted linear regression, is a generalization of ordinary least squares and linear regression in which the errors covariance matrix is allowed to be different from an identity matrix."}, {"text": "Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable; the method estimates the relationship by minimizing the sum of the squares in the difference between the observed and predicted values of the"}, {"text": "Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable; the method estimates the relationship by minimizing the sum of the squares in the difference between the observed and predicted values of the"}, {"text": "The least squares approach limits the distance between a function and the data points that the function explains. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data. Mathematicians use the least squares method to arrive at a maximum-likelihood estimate."}, {"text": "The least squares approach limits the distance between a function and the data points that the function explains. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data. Mathematicians use the least squares method to arrive at a maximum-likelihood estimate."}, {"text": "Logistic regression is quite different than linear regression in that it does not make several of the key assumptions that linear and general linear models (as well as other ordinary least squares algorithm based models) hold so close: (1) logistic regression does not require a linear relationship between the dependent"}, {"text": "Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the"}]}, {"question": "How does K modes work", "positive_ctxs": [{"text": "The k-modes algorithm tries to minimize the sum of within-cluster Hamming distance from the mode of that cluster, summed over all clusters.  The procedure is similar to k-means: a number of clusters (k) is chosen, and k cluster-mode vectors are chosen at random (or according to accepted heuristics)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If the student does have multiple learning styles (multimodal), the advantages gained through multiple learning strategies include the ability to learn more quickly and at a deeper level so that recall at a later date will be more successful. Using various modes of learning also improves attention span."}, {"text": "A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical."}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "Chebyshev's inequality says that at least 1\u22121K2 of data from a sample must fall within K standard deviations from the mean, where K is any positive real number greater than one."}, {"text": "Examples of texts to create.  Live multimodal texts include dance, performance, oral storytelling, and presentations. Meaning is conveyed through combinations of various modes such as gestural, spatial, audio, and oral language."}, {"text": "How to Prevent OverfittingCross-validation. Cross-validation is a powerful preventative measure against overfitting.  Train with more data. It won't work every time, but training with more data can help algorithms detect the signal better.  Remove features.  Early stopping.  Regularization.  Ensembling."}, {"text": "Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows\u2022"}]}, {"question": "How do you know if F is conservative vector field", "positive_ctxs": [{"text": "As mentioned in the context of the gradient theorem, a vector field F is conservative if and only if it has a potential function f with F=\u2207f. Therefore, if you are given a potential function f or if you can find one, and that potential function is defined everywhere, then there is nothing more to do."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Now, every textbook on linear algebra gives the following definition of a linear operator: an operator T: V\u2014> W between two vector spaces V and W over the same field ! F is said to be linear if it satisfies the conditions of additivity, viz. T(u + v)=T(u)+T(v)"}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "A vector space is a space of vectors, ie. each element is a vector. A vector field is, at its core, a function between some space and some vector space, so every point in our base space has a vector assigned to it. A good example would be wind direction maps you see on weather reports."}, {"text": "In vector calculus and physics, a vector field is an assignment of a vector to each point in a subset of space. For instance, a vector field in the plane can be visualised as a collection of arrows with a given magnitude and direction, each attached to a point in the plane."}, {"text": "There are two sets of degrees of freedom; one for the numerator and one for the denominator. For example, if F follows an F distribution and the number of degrees of freedom for the numerator is four, and the number of degrees of freedom for the denominator is ten, then F ~ F 4,10."}, {"text": "The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time. A large F ratio means that the variation among group means is more than you'd expect to see by chance."}]}, {"question": "How do you calculate less than or more cumulative frequency", "positive_ctxs": [{"text": "1:314:30Suggested clip \u00b7 120 secondsCumulative Frequency Distribution (Less than and More than YouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Step 1: Prepare a table containing less than type cumulative frequency with the help of given frequencies. belongs. Class-interval of this cumulative frequency is the median class-interval. Step 3 : Find out the frequency f and lower limit l of this median class."}, {"text": "The distribution function , also called the cumulative distribution function (CDF) or cumulative frequency function, describes the probability that a variate takes on a value less than or equal to a number . The distribution function is sometimes also denoted. (Evans et al. 2000, p."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Cumulative relative frequency distribution\u2013showsthe proportionof items with values less than orequal to the upper limit of each class. Cumulative DistributionsCumulative percent frequency distribution\u2013showsthe percentageof items with values less than orequal to the upper limit of each class."}, {"text": "The cumulative distribution function (CDF) calculates the cumulative probability for a given x-value. Use the CDF to determine the probability that a random observation that is taken from the population will be less than or equal to a certain value."}, {"text": "A curve that represents the cumulative frequency distribution of grouped data on a graph is called a Cumulative Frequency Curve or an Ogive. Representing cumulative frequency data on a graph is the most efficient way to understand the data and derive results."}, {"text": "Given a probability density function, we define the cumulative distribution function (CDF) as follows. The cumulative distribution function (CDF) of a random variable X is denoted by F(x), and is defined as F(x) = Pr(X \u2264 x). where xn is the largest possible value of X that is less than or equal to x."}]}, {"question": "What is the interquartile range of a normal distribution", "positive_ctxs": [{"text": "In a standard normal distribution (with mean 0 and standard deviation 1), the first and third quartiles are located at -0.67448 and +0.67448 respectively. Thus the interquartile range (IQR) is 1.34896."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The interquartile range is the difference between the third quartile and the first quartile in a data set, giving the middle 50%. The interquartile range is a measure of spread; it's used to build box plots, determine normal distributions and as a way to determine outliers."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "There are 5 values above the median (upper half), the middle value is 77 which is the third quartile. The interquartile range is 77 \u2013 64 = 13; the interquartile range is the range of the middle 50% of the data.  When the sample size is odd, the median and quartiles are determined in the same way."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side.  The normal distribution is often called the bell curve because the graph of its probability density looks like a bell."}, {"text": "A normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution. Areas of the normal distribution are often represented by tables of the standard normal distribution.  For example, a Z of -2.5 represents a value 2.5 standard deviations below the mean."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}]}, {"question": "Is pore strip bad", "positive_ctxs": [{"text": "Not only are nose strips bad for those with sensitive skin, they also worsen other skin conditions. Pore strips exacerbate rosacea-prone skin , especially if they contain irritating ingredients like alcohol and astringents. They also aggravate extremely dry skin, eczema and psoriasis ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2."}, {"text": "Relative Frequency Of A Class Is The Percentage Of The Data That Falls In That Class, While Cumulative Frequency Of A Class Is The Sum Of The Frequencies Of That Class And All Previous Classes."}, {"text": "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater."}, {"text": "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The AUC value lies between 0.5 to 1 where 0.5 denotes a bad classifer and 1 denotes an excellent classifier."}]}, {"question": "What is the difference between tobit and logit regression", "positive_ctxs": [{"text": "Logit models are used for discrete outcome modeling. This can be for binary outcomes (0 and 1) or for three or more outcomes (multinomial logit).  It has nothing to do with binary or discrete outcomes. Tobit models are a form of linear regression."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The tobit model, also called a censored regression model, is designed to estimate linear relationships between variables when there is either left- or right-censoring in the dependent variable (also known as censoring from below and above, respectively)."}, {"text": "The tobit model, also called a censored regression model, is designed to estimate linear relationships between variables when there is either left- or right-censoring in the dependent variable (also known as censoring from below and above, respectively)."}, {"text": "The logit is a transformation. Logistic regression is a regression model. The logit transformation transforms a line to a logistic curve. Logistic regression fits a logistic curve to set of data where the dependent variable can only take the values 0 and 1."}, {"text": "If p is a probability, then p/(1 \u2212 p) is the corresponding odds; the logit of the probability is the logarithm of the odds, i.e.  For each choice of base, the logit function takes values between negative and positive infinity."}, {"text": "The essential difference between these two is that Logistic regression is used when the dependent variable is binary in nature. In contrast, Linear regression is used when the dependent variable is continuous and nature of the regression line is linear."}, {"text": "Thus logit regression is simply the GLM when describing it in terms of its link function, and logistic regression describes the GLM in terms of its activation function."}]}, {"question": "What is the difference between model and algorithm", "positive_ctxs": [{"text": "To summarize, an algorithm is a method or a procedure we follow to get something done or solve a problem. A model is a computation or a formula formed as a result of an algorithm that takes some values as input and produces some value as output."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The Sarsa algorithm is an On-Policy algorithm for TD-Learning. The major difference between it and Q-Learning, is that the maximum reward for the next state is not necessarily used for updating the Q-values."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term. Ridge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function."}, {"text": "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term. Ridge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function."}, {"text": "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term. Ridge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function."}]}, {"question": "What is population variance and sample variance", "positive_ctxs": [{"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data. Due to this value of denominator in the formula for variance in case of sample data is 'n-1', and it is 'n' for population data."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data."}, {"text": "Sample variance Concretely, the naive estimator sums the squared deviations and divides by n, which is biased.  The sample mean, on the other hand, is an unbiased estimator of the population mean \u03bc. Note that the usual definition of sample variance is. , and this is an unbiased estimator of the population variance."}, {"text": "5 Answers. N is the population size and n is the sample size. The question asks why the population variance is the mean squared deviation from the mean rather than (N\u22121)/N=1\u2212(1/N) times it."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}]}, {"question": "What does P value of 0.08 mean", "positive_ctxs": [{"text": "A p-value of 0.08 being more than the benchmark of 0.05 indicates non-significance of the test. This means that the null hypothesis cannot be rejected."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "This is because of the logistic distribution having heavier tails (than the normal distribution): Any outliers would not carry as much weight under the assumptions of the logistic (blue) distribution.  In a logistic regression does a very small P value for a predictor mean a good predictor or a bad predictor?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "A binomial random variable is the number of successes x in n repeated trials of a binomial experiment.Binomial DistributionThe mean of the distribution (\u03bcx) is equal to n * P .The variance (\u03c32x) is n * P * ( 1 - P ).The standard deviation (\u03c3x) is sqrt[ n * P * ( 1 - P ) ]."}, {"text": "The effect size is the main finding of a quantitative study. While a P value can inform the reader whether an effect exists, the P value will not reveal the size of the effect."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time.  The P value is determined from the F ratio and the two values for degrees of freedom shown in the ANOVA table."}]}, {"question": "What is the difference between backpropagation algorithm and Backpropagation through time Bptt algorithm", "positive_ctxs": [{"text": "The general algorithm is The Backpropagation algorithm is suitable for the feed forward neural network on fixed sized input-output pairs. The Backpropagation Through Time is the application of Backpropagation training algorithm which is applied to the sequence data like the time series."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The general algorithm is The Backpropagation algorithm is suitable for the feed forward neural network on fixed sized input-output pairs. The Backpropagation Through Time is the application of Backpropagation training algorithm which is applied to the sequence data like the time series."}, {"text": "Backpropagation through time is the method to overcome decay in information through RNN. BPTT helps a practitioner to solve the sequence prediction problems for recurrent neural networks. It is used as a training algorithm which can update its weight in RNN."}, {"text": "The algorithm is used to effectively train a neural network through a method called chain rule. In simple terms, after each forward pass through a network, backpropagation performs a backward pass while adjusting the model's parameters (weights and biases)."}, {"text": "The Sarsa algorithm is an On-Policy algorithm for TD-Learning. The major difference between it and Q-Learning, is that the maximum reward for the next state is not necessarily used for updating the Q-values."}, {"text": "What are the five steps in the backpropagation learning algorithm?Initialize weights with random values and set other parameters.Read in the input vector and the desired output.Compute the actual output via the calculations, working forward through the layers."}, {"text": "Backpropagation is an algorithm commonly used to train neural networks. When the neural network is initialized, weights are set for its individual elements, called neurons. Inputs are loaded, they are passed through the network of neurons, and the network provides an output for each one, given the initial weights."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is difference between bucketing and partitioning", "positive_ctxs": [{"text": "Partition divides large amount of data into multiple slices based on value of a table column(s).  Bucketing decomposes data into more manageable or equal parts. With partitioning, there is a possibility that you can create multiple small partitions based on column values."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Partitioning methods Horizontal partitioning involves putting different rows into different tables.  Vertical partitioning involves creating tables with fewer columns and using additional tables to store the remaining columns."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "The chief difference between MEMM and CRF is that MEMM is locally renormalized and suffers from the label bias problem, while CRFs are globally renormalized."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}]}, {"question": "How does the ReLu solve the vanishing gradient problem", "positive_ctxs": [{"text": "RELU activation solves this by having a gradient slope of 1, so during backpropagation, there isn't gradients passed back that are progressively getting smaller and smaller. but instead they are staying the same, which is how RELU solves the vanishing gradient problem."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "The ReLU activation solves the problem of vanishing gradient that is due to sigmoid-like non-linearities (the gradient vanishes because of the flat regions of the sigmoid). The other kind of \"vanishing\" gradient seems to be related to the depth of the network (e.g. see this for example)."}, {"text": "Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron."}, {"text": "Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron."}, {"text": "LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate's activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process."}]}, {"question": "Is artificial general intelligence possible", "positive_ctxs": [{"text": "A one-brain AI would still not be a true intelligence, only a better general-purpose AI\u2014Legg's multi-tool. But whether they're shooting for AGI or not, researchers agree that today's systems need to be made more general-purpose, and for those who do have AGI as the goal, a general-purpose AI is a necessary first step."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Sudharsan also noted that deep meta reinforcement learning will be the future of artificial intelligence where we will implement artificial general intelligence (AGI) to build a single model to master a wide variety of tasks. Thus each model will be capable to perform a wide range of complex tasks."}, {"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}, {"text": "Whereas AI is preprogrammed to carry out a task that a human can but more efficiently, artificial general intelligence (AGI) expects the machine to be just as smart as a human."}, {"text": "there are three general categories of learning that artificial intelligence (AI)/machine learning utilizes to actually learn. They are Supervised Learning, Unsupervised Learning and Reinforcement learning.  The machine then maps the inputs and the outputs."}, {"text": "Whereas AI is preprogrammed to carry out a task that a human can but more efficiently, artificial general intelligence (AGI) expects the machine to be just as smart as a human. This is the kind of AI we're used to seeing in blockbuster movies."}, {"text": "Whereas AI is preprogrammed to carry out a task that a human can but more efficiently, artificial general intelligence (AGI) expects the machine to be just as smart as a human.  A machine that was able to do this would be considered a fine example of AGI."}, {"text": "It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action. Some academic sources reserve the term \"strong AI\" for machines that can experience consciousness."}]}, {"question": "How do you use logistic regression for multi class classification", "positive_ctxs": [{"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Pros: It is easy and fast to predict class of test data set. It also perform well in multi class prediction. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "This binary classifier for multiclass can be used with one-vs-all or all-vs-all reduction method. Here you can go with logistic regression, decision tree algorithms. You can go with algorithms like Naive Bayes, Neural Networks and SVM to solve multi class problem."}, {"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss."}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}]}, {"question": "What are the uses of the boosting method for machine learning", "positive_ctxs": [{"text": "In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation."}, {"text": "Gradient Boosting Machines vs. XGBoost.  While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation."}, {"text": "Gradient Boosting Machines vs. XGBoost.  While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation."}, {"text": "Gradient Boosting Machines vs. XGBoost.  While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation."}, {"text": "Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner.  We will use a simple example to understand the GBM algorithm."}, {"text": "Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression.  A Random Forest operates by constructing several decision trees during training time and outputting the mean of the classes as the prediction of all the trees."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees."}]}, {"question": "How do you write logistic regression results", "positive_ctxs": [{"text": "Writing up resultsFirst, present descriptive statistics in a table.  Organize your results in a table (see Table 3) stating your dependent variable (dependent variable = YES) and state that these are \"logistic regression results.\"  When describing the statistics in the tables, point out the highlights for the reader.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to Formulate an Effective HypothesisState the problem that you are trying to solve. Make sure that the hypothesis clearly defines the topic and the focus of the experiment.Try to write the hypothesis as an if-then statement.  Define the variables."}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "Multinomial logistic regression is used when you have a categorical dependent variable with two or more unordered levels (i.e. two or more discrete outcomes). It is practically identical to logistic regression, except that you have multiple possible outcomes instead of just one."}, {"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "Standardization isn't required for logistic regression. The main goal of standardizing features is to help convergence of the technique used for optimization.  Otherwise, you can run your logistic regression without any standardization treatment on the features."}, {"text": "When the response categories are ordered, you could run a multinomial regression model. The disadvantage is that you are throwing away information about the ordering. An ordinal logistic regression model preserves that information, but it is slightly more involved."}]}, {"question": "What is the relationship between the p value of a t test and the Type I and Type II errors", "positive_ctxs": [{"text": "For example, a p-value of 0.01 would mean there is a 1% chance of committing a Type I error. However, using a lower value for alpha means that you will be less likely to detect a true difference if one really exists (thus risking a type II error)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The power of Hypothesis test is the probability of rejecting null hypothesis . As stated above we may commit Type I and Type II errors while testing a hypothesis.  Accordingly 1 \u2013 b value is the measure of how well the test is working or what is technically described as the power of the test."}, {"text": "Type I and type II errors are instrumental for the understanding of hypothesis testing in a clinical research scenario.  A type II error can be thought of as the opposite of a type I error and is when a researcher fails to reject the null hypothesis that is actually false in reality."}, {"text": "Type I and II Errors and Significance Levels. Rejecting the null hypothesis when it is in fact true is called a Type I error.  Most people would not consider the improvement practically significant. Caution: The larger the sample size, the more likely a hypothesis test will detect a small difference."}, {"text": "Type I error is equivalent to a False positive. Type II error is equivalent to a False negative. Type I error refers to non-acceptance of hypothesis which ought to be accepted. Type II error is the acceptance of hypothesis which ought to be rejected."}, {"text": "Type II Error and Power Calculations. Recall that in hypothesis testing you can make two types of errors \u2022 Type I Error \u2013 rejecting the null when it is true. \u2022 Type II Error \u2013 failing to reject the null when it is false.  = \u239b \u239e \u2212  \u2212 \u2212 = =  = \u239b \u239e \u2212"}, {"text": "The consequences of making a type I error mean that changes or interventions are made which are unnecessary, and thus waste time, resources, etc. Type II errors typically lead to the preservation of the status quo (i.e. interventions remain the same) when change is needed."}, {"text": "In terms of the courtroom example, a type I error corresponds to convicting an innocent defendant. Type II error. The second kind of error is the failure to reject a false null hypothesis as the result of a test procedure."}]}, {"question": "How is a scatterplot similar to a correlation coefficient", "positive_ctxs": [{"text": "A scatterplot displays the strength, direction, and form of the relationship between two quantitative variables. A correlation coefficient measures the strength of that relationship.  The correlation r measures the strength of the linear relationship between two quantitative variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The correlation coefficient is the specific measure that quantifies the strength of the linear relationship between two variables in a correlation analysis. The coefficient is what we symbolize with the r in a correlation report."}, {"text": "How to Read a Correlation Matrix-1 indicates a perfectly negative linear correlation between two variables.0 indicates no linear correlation between two variables.1 indicates a perfectly positive linear correlation between two variables."}, {"text": "Correlation Defined The closer the correlation coefficient is to +1.0, the closer the relationship is between the two variables.  If two variables have a correlation coefficient near zero, it indicates that there is no significant (linear) relationship between the variables."}, {"text": "A scatterplot displays data about two variables as a set of points in the x y xy xy -plane and is a useful tool for determining if there is a correlation between the variables. Causation means that one event causes another event to occur. Causation can only be determined from an appropriately designed experiment."}, {"text": "Moran's I is a correlation coefficient that measures the overall spatial autocorrelation of your data set. In other words, it measures how one object is similar to others surrounding it. If objects are attracted (or repelled) by each other, it means that the observations are not independent."}, {"text": "There are three big-picture methods to understand if a continuous and categorical are significantly correlated \u2014 point biserial correlation, logistic regression, and Kruskal Wallis H Test. The point biserial correlation coefficient is a special case of Pearson's correlation coefficient."}, {"text": "The cross product is a calculation used in order to define the correlation coefficient between two variables. SP is the sum of all cross products between two variables."}]}, {"question": "What is the difference between normal and T distribution", "positive_ctxs": [{"text": "The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution."}, {"text": "The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution."}, {"text": "The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}]}, {"question": "What is Simpson's Paradox example", "positive_ctxs": [{"text": "A baseball player can have higher batting average than another on each of two years, but lower than the other when the two are combined. In one case, David Justice had a higher batting average than Derek Jeter in 1995 and 1996, but across the two years, Jeter's average was higher."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "Which is most commonly used measure of similarity in clustering exercise", "positive_ctxs": [{"text": "Euclidean distance"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Clustering is done based on a similarity measure to group similar data objects together. This similarity measure is most commonly and in most applications based on distance functions such as Euclidean distance, Manhattan distance, Minkowski distance, Cosine similarity, etc. to group objects in clusters."}, {"text": "The k-means clustering algorithm is one of the most widely used, effective, and best understood clustering methods.  In this paper we propose a supervised learning approach to finding a similarity measure so that k-means provides the desired clusterings for the task at hand."}, {"text": "Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis."}, {"text": "Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis."}, {"text": "Taking the square root of the variance gives us the units used in the original scale and this is the standard deviation. Standard deviation is the measure of spread most commonly used in statistical practice when the mean is used to calculate central tendency. Thus, it measures spread around the mean."}, {"text": "Taking the square root of the variance gives us the units used in the original scale and this is the standard deviation. Standard deviation is the measure of spread most commonly used in statistical practice when the mean is used to calculate central tendency. Thus, it measures spread around the mean."}, {"text": "The group of functions that are minimized are called \u201closs functions\u201d. A loss function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. A most commonly used method of finding the minimum point of function is \u201cgradient descent\u201d."}]}, {"question": "Why do neural networks need an activation function", "positive_ctxs": [{"text": "Neural network activation functions are a crucial component of deep learning. Activation functions determine the output of a deep learning model, its accuracy, and also the computational efficiency of training a model\u2014which can make or break a large scale neural network."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An activation function is a function used in artificial neural networks which outputs a small value for small inputs, and a larger value if its inputs exceed a threshold.  Activation functions are useful because they add non-linearities into neural networks, allowing the neural networks to learn powerful operations."}, {"text": "It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.  The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better."}, {"text": "relu . The difference is that relu is an activation function whereas LeakyReLU is a Layer defined under keras. layers .  For activation functions you need to wrap around or use inside layers such Activation but LeakyReLU gives you a shortcut to that function with an alpha value."}, {"text": "7.2. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems. Radial basis function networks are distinguished from other neural networks due to their universal approximation and faster learning speed."}, {"text": "Activation functions cannot be linear because neural networks with a linear activation function are effective only one layer deep, regardless of how complex their architecture is.  Therefore, nonlinear functions must be continuous and differentiable between this range."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}]}, {"question": "What is a good sample percentage", "positive_ctxs": [{"text": "A good maximum sample size is usually 10% as long as it does not exceed 1000. A good maximum sample size is usually around 10% of the population, as long as this does not exceed 1000. For example, in a population of 5000, 10% would be 500. In a population of 200,000, 10% would be 20,000."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A good maximum sample size is usually 10% as long as it does not exceed 1000. A good maximum sample size is usually around 10% of the population, as long as this does not exceed 1000. For example, in a population of 5000, 10% would be 500. In a population of 200,000, 10% would be 20,000."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "For example, if we want to measure current obesity levels in a population, we could draw a sample of 1,000 people randomly from that population (also known as a cross section of that population), measure their weight and height, and calculate what percentage of that sample is categorized as obese."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "A frequency count is a measure of the number of times that an event occurs. Thus, a relative frequency of 0.50 is equivalent to a percentage of 50%."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is the difference between eager learning and lazy learning", "positive_ctxs": [{"text": "In artificial intelligence, eager learning is a learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main advantage gained in employing an eager learning method, such as an artificial neural network, is that the target function will be approximated globally during training, thus requiring much less space than using a lazy learning system."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A lazy learner delays abstracting from the data until it is asked to make a prediction while an eager learner abstracts away from the data during training and uses this abstraction to make predictions rather than directly compare queries with instances in the dataset."}, {"text": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables."}, {"text": "KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.  KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point."}, {"text": "KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.  KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}]}, {"question": "Is a support vector machine a neural network", "positive_ctxs": [{"text": "In simplest manner, svm without kernel is a single neural network neuron but with different cost function. If you add a kernel function, then it is comparable with 2 layer neural nets.  In simplest manner, svm without kernel is a single neural network neuron but with different cost function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An embedding is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables.  As input to a machine learning model for a supervised task."}, {"text": "In machine learning, the radial basis function kernel, or RBF kernel, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in support vector machine classification."}, {"text": "A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text. So you're working on a text classification problem."}, {"text": "A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text. So you're working on a text classification problem."}, {"text": "From Wikipedia, the free encyclopedia. In machine learning, the radial basis function kernel, or RBF kernel, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in support vector machine classification."}, {"text": "In machine learning, the polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models."}, {"text": "\u201cDeep learning is a branch of machine learning that uses neural networks with many layers. A deep neural network analyzes data with learned representations similarly to the way a person would look at a problem,\u201d Brock says. \u201cIn traditional machine learning, the algorithm is given a set of relevant features to analyze."}]}, {"question": "What is least square method in regression", "positive_ctxs": [{"text": "Key TakeawaysThe least squares method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve.Least squares regression is used to predict the behavior of dependent variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The least squares approach limits the distance between a function and the data points that the function explains. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data. Mathematicians use the least squares method to arrive at a maximum-likelihood estimate."}, {"text": "The least squares approach limits the distance between a function and the data points that the function explains. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data. Mathematicians use the least squares method to arrive at a maximum-likelihood estimate."}, {"text": "1 Introduction. The partial least squares (PLS) algorithm was first introduced for regression tasks and then evolved into a classification method that is well known as PLS-discriminant analysis (PLS-DA)."}, {"text": "Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable; the method estimates the relationship by minimizing the sum of the squares in the difference between the observed and predicted values of the"}]}, {"question": "What is convolutional neural network", "positive_ctxs": [{"text": "Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. ConvNets have been successful in identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery."}, {"text": "A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input."}, {"text": "A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input."}, {"text": "Deep NN is just a deep neural network, with a lot of layers. It can be CNN, or just a plain multilayer perceptron. CNN, or convolutional neural network, is a neural network using convolution layer and pooling layer."}, {"text": "Convolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input. A pooling layer is a new layer added after the convolutional layer."}, {"text": "Much of the modern innovations in image recognition is reliant on Deep Learning technology, an advanced type of Machine Learning, and the modern wonder of Artificial Intelligence.  For image recognition, the kind of neural network used is called convolutional neural networks."}]}, {"question": "How is random forest different from Decision Tree", "positive_ctxs": [{"text": "Each node in the decision tree works on a random subset of features to calculate the output. The random forest then combines the output of individual decision trees to generate the final output.  The Random Forest Algorithm combines the output of multiple (randomly created) Decision Trees to generate the final output."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Advantages and Disadvantages of Decision Trees in Machine Learning. Decision Tree is used to solve both classification and regression problems. But the main drawback of Decision Tree is that it generally leads to overfitting of the data."}, {"text": "Decision Tree can be used both in classification and regression problem. This article present the Decision Tree Regression Algorithm along with some advanced topics."}, {"text": "Multi-class Classification using Decision Tree, Random Forest and Extra Trees Algorithm in Python: An End-To-End Data Science Recipe \u2014 016. a) Different types of Machine Learning problems.  i) How to implement Decision Tree, Random Forest and Extra Tree Algorithms for Multiclass Classification in Python."}, {"text": "How to reduce False Positive and False Negative in binary classificationfirstly random forest overfits if the training data and testing data are not drawn from same distribution.check the data for linearity,multicollinearity ,outliers,etc.More items"}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed."}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed.  Decision trees can handle both categorical and numerical data"}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes"}]}, {"question": "What are examples of Geometric distribution in real life", "positive_ctxs": [{"text": "If the dice is thrown repeatedly until the first time a three appears. The probablility distribution of the number of times it is thrown not getting a three (not-a-threes number of failures to get a three) is a geometric distribution with the success_fraction = 1/6 = 0.1666 \u0307."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Disadvantages of randomised control trial study designTrials which test for efficacy may not be widely applicable. Trials which test for effectiveness are larger and more expensive.Results may not always mimic real life treatment situation (e.g. inclusion / exclusion criteria; highly controlled setting)"}, {"text": "Therefore the slope represents how much the y value changes when the x value changes by 1 unit. In statistics, especially regression analysis, the x value has real life meaning and so does the y value. Interpret the slope of the regression line in the context of the study."}, {"text": "An autonomous agent is an intelligent agent operating on an owner's behalf but without any interference of that ownership entity.  Non-biological examples include intelligent agents, autonomous robots, and various software agents, including artificial life agents, and many computer viruses."}, {"text": "1 Answer. A probability distribution is the theoretical outcome of an experiment whereas a sampling distribution is the real outcome of an experiment."}, {"text": "The goal of a company should be to achieve the target performance with minimal variation. That will minimize the customer dissatisfaction. A real life example of the Taguchi Loss Function would be the quality of food compared to expiration dates.  That is when the orange will taste the best (customer satisfaction)."}, {"text": "Bivariate analysis refers to the analysis of two variables to determine relationships between them. Bivariate analyses are often reported in quality of life research."}, {"text": "The exponential distribution is the only continuous distribution that is memoryless (or with a constant failure rate). Geometric distribution, its discrete counterpart, is the only discrete distribution that is memoryless."}]}, {"question": "What is the difference between class limits and class boundaries quizlet", "positive_ctxs": [{"text": "Class limits are the least and greatest numbers that can belong to the class. Class boundaries are the numbers that separate classes without forming gaps between them."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Class limits specify the span of data values that fall within a class. Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next."}, {"text": "Class limits specify the span of data values that fall within a class. Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next.  Class limits are not possible data values. Class boundaries specify the span of data values that fall within a class."}, {"text": "The lower class boundary is found by subtracting 0.5 units from the lower class limit and the upper class boundary is found by adding 0.5 units to the upper class limit. The difference between the upper and lower boundaries of any class."}, {"text": "(Select all that apply.) Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next. Class limits specify the span of data values that fall within a class."}, {"text": "In exclusive form, the lower and upper limits are known as true lower limit and true upper limit of the class interval. Thus, class limits of 10 - 20 class intervals in the exclusive form are 10 and 20. In inclusive form, class limits are obtained by subtracting 0.5 from lower limitand adding 0.5 to the upper limit."}, {"text": "The main difference is the behavior concerning inheritance: class variables are shared between a class and all its subclasses, while class instance variables only belong to one specific class."}, {"text": "Class Boundaries. Separate one class in a grouped frequency distribution from another. The boundaries have one more decimal place than the raw data and therefore do not appear in the data. There is no gap between the upper boundary of one class and the lower boundary of the next class."}]}, {"question": "What is generalization in deep learning", "positive_ctxs": [{"text": "Generalization refers to your model's ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model. Estimated Time: 5 minutes Learning Objectives."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array.  It is a term and set of techniques known in machine learning in the training and operation of deep learning models can be described in terms of tensors."}, {"text": "A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array.  It is a term and set of techniques known in machine learning in the training and operation of deep learning models can be described in terms of tensors."}, {"text": "Multi-view learning is an emerging direction in machine learning which considers learning with multiple views to improve the generalization performance. Multi-view learning is also known as data fusion or data integration from multiple feature sets."}, {"text": "Summary. Probably approximately correct (PAC) learning is a theoretical framework for analyzing the generalization error of a learning algorithm in terms of its error on a training set and some measure of complexity. The goal is typically to show that an algorithm achieves low generalization error with high probability"}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}]}, {"question": "What is the difference between class size and class width", "positive_ctxs": [{"text": "Each class will have a \u201clower class limit\u201d and an \u201cupper class limit\u201d which are the lowest and highest numbers in each class. The \u201cclass width\u201d is the distance between the lower limits of consecutive classes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The lower class boundary is found by subtracting 0.5 units from the lower class limit and the upper class boundary is found by adding 0.5 units to the upper class limit. The difference between the upper and lower boundaries of any class."}, {"text": "The main difference is the behavior concerning inheritance: class variables are shared between a class and all its subclasses, while class instance variables only belong to one specific class."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Definition. The class intervals are the subsets into which the data is grouped. The width of the class intervals will be a compromise between having intervals short enough so that not all of the observations fall in the same interval, but long enough so that you do not end up with only one observation per interval."}, {"text": "Definition. The class intervals are the subsets into which the data is grouped. The width of the class intervals will be a compromise between having intervals short enough so that not all of the observations fall in the same interval, but long enough so that you do not end up with only one observation per interval."}, {"text": "mAP (mean average precision) is the average of AP. In some context, we compute the AP for each class and average them. But in some context, they mean the same thing. For example, under the COCO context, there is no difference between AP and mAP."}, {"text": "This is answered by examining the meaning of each term in the phrase: modal means the one that occurs most often (averages: mode), a class interval is the width of one of your groups in the frequency table or, the class interval is what you use when grouping data together, e.g., if you counted the number of pencils in"}]}, {"question": "What is an example of bivariate data", "positive_ctxs": [{"text": "Data for two variables (usually two types of related data). Example: Ice cream sales versus the temperature on that day. The two variables are Ice Cream Sales and Temperature."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables."}, {"text": "In statistics, bivariate data is data on each of two variables, where each value of one of the variables is paired with a value of the other variable.  For example, bivariate data on a scatter plot could be used to study the relationship between stride length and length of legs."}, {"text": "In statistics, bivariate data is data on each of two variables, where each value of one of the variables is paired with a value of the other variable.  For example, bivariate data on a scatter plot could be used to study the relationship between stride length and length of legs."}, {"text": "Multivariate data analysis is a set of statistical models that examine patterns in multidimensional data by considering, at once, several data variables. It is an expansion of bivariate data analysis, which considers only two variables in its models."}, {"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}, {"text": "An example of statistics is a report of numbers saying how many followers of each religion there are in a particular country. An example of statistics is a math class offered in high schools and colleges. The definition of a statistic is a number, or a person who is an unnamed piece of data to be studied."}, {"text": "The definition of an ensemble is two or more people or things that function together as a whole. An example of an ensemble is a string quartet. An example of an ensemble is a group of actors in a play.  A small group of musicians playing or singing together."}]}, {"question": "How many neurons are in the input layer", "positive_ctxs": [{"text": "4 neurons"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Hidden layer of the neural network is the intermediate layer between Input and Output layer. Activation function applies on hidden layer if it is available.  Hidden nodes or hidden neurons are the neurons that are neither in the input layer nor the output layer [3]."}, {"text": "The number of neurons in the input layer equals the number of input variables in the data being processed. The number of neurons in the output layer equals the number of outputs associated with each input."}, {"text": "Each neuron in a layer receives an input from all the neurons present in the previous layer\u2014thus, they're densely connected. In other words, the dense layer is a fully connected layer, meaning all the neurons in a layer are connected to those in the next layer."}, {"text": "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."}, {"text": "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."}, {"text": "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."}, {"text": "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."}]}, {"question": "What are the layers in CNN", "positive_ctxs": [{"text": "We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A CNN LSTM can be defined by adding CNN layers on the front end followed by LSTM layers with a Dense layer on the output. It is helpful to think of this architecture as defining two sub-models: the CNN Model for feature extraction and the LSTM Model for interpreting the features across time steps."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "CNNs are trained to identify and extract the best features from the images for the problem at hand. That is their main strength. The latter layers of a CNN are fully connected because of their strength as a classifier."}, {"text": "Improve your model accuracy by Transfer Learning.Loading data using python libraries.Preprocess of data which includes reshaping, one-hot encoding and splitting.Constructing the model layers of CNN followed by model compiling, model training.Evaluating the model on test data.Finally, predicting the correct and incorrect labels."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers, and normalization layers. Here it simply means that instead of using the normal activation functions defined above, convolution and pooling functions are used as activation functions."}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}]}, {"question": "What are the implications of an outlier in a positively or negatively skewed distribution", "positive_ctxs": [{"text": "In a positively skewed distribution the outliers will be pulling the mean down the scale a great deal. The median might be slightly lower due to the outlier, but the mode will be unaffected. Thus, with a negatively skewed distribution the mean is numerically lower than the median or mode."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical."}, {"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}, {"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}, {"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}, {"text": "A left-skewed distribution has a long left tail.  The normal distribution is the most common distribution you'll come across. Next, you'll see a fair amount of negatively skewed distributions. For example, household income in the U.S. is negatively skewed with a very long left tail."}, {"text": "In statistics, a negatively skewed (also known as left-skewed) distribution is a type of distribution in which more values are concentrated on the right side (tail) of the distribution graph while the left tail of the distribution graph is longer."}, {"text": "In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right. In a negatively skewed distribution, the mean is usually less than the median because the few low scores tend to shift the mean to the left."}]}, {"question": "Which is a biased estimator", "positive_ctxs": [{"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "and is commonly used as an estimator for \u03c3. Nevertheless, S is a biased estimator of \u03c3."}, {"text": "An biased estimator is one which delivers an estimate which is consistently different from the parameter to be estimated. In a more formal definition we can define that the expectation E of a biased estimator is not equal to the parameter of a population."}, {"text": "However, for a general population it is not true that the sample median is an unbiased estimator of the population median. The sample mean is a biased estimator of the population median when the population is not symmetric.  It only will be unbiased if the population is symmetric."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}, {"text": "Firstly, while the sample variance (using Bessel's correction) is an unbiased estimator of the population variance, its square root, the sample standard deviation, is a biased estimate of the population standard deviation; because the square root is a concave function, the bias is downward, by Jensen's inequality."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}]}, {"question": "What is the difference between Bayes Theorem and conditional probability and how do I know when to apply them", "positive_ctxs": [{"text": "The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Answer. P(A \u2229 B) and P(A|B) are very closely related. Their only difference is that the conditional probability assumes that we already know something -- that B is true.  For P(A|B), however, we will receive a probability between 0, if A cannot happen when B is true, and P(B), if A is always true when B is true."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities .  The above statement is the general representation of the Bayes rule."}, {"text": "The One-Sample z-test is used when we want to know whether the difference between the mean of a sample mean and the mean of a population is large enough to be statistically significant, that is, if it is unlikely to have occurred by chance."}, {"text": "The linear Discriminant analysis estimates the probability that a new set of inputs belongs to every class.  LDA uses Bayes' Theorem to estimate the probabilities. If the output class is (k) and the input is (x), here is how Bayes' theorem works to estimate the probability that the data belongs to each class."}, {"text": "Simple linear regression is commonly used in forecasting and financial analysis\u2014for a company to tell how a change in the GDP could affect sales, for example. Microsoft Excel and other software can do all the calculations, but it's good to know how the mechanics of simple linear regression work."}]}, {"question": "What is factor analysis in psychometrics", "positive_ctxs": [{"text": "Factor analysis is a multivariant mathematical technique traditionally used in psychometrics to construct measures of psychologic and behavioral characteristics, such as intellectual abilities or personality traits (12)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two types of factor analyses, exploratory and confirmatory. Exploratory factor analysis (EFA) is method to explore the underlying structure of a set of observed variables, and is a crucial step in the scale development process. The first step in EFA is factor extraction."}, {"text": "In multivariate statistics, exploratory factor analysis (EFA) is a statistical method used to uncover the underlying structure of a relatively large set of variables. EFA is a technique within factor analysis whose overarching goal is to identify the underlying relationships between measured variables."}, {"text": "Structural equation modeling is a multivariate statistical analysis technique that is used to analyze structural relationships. This technique is the combination of factor analysis and multiple regression analysis, and it is used to analyze the structural relationship between measured variables and latent constructs."}, {"text": "The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models."}, {"text": "Factor analysis is an exploratory statistical technique to investigate dimensions and the factor structure underlying a set of variables (items) while cluster analysis is an exploratory statistical technique to group observations (people, things, events) into clusters or groups so that the degree of association is"}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Advantages. The main advantage of multivariate analysis is that since it considers more than one factor of independent variables that influence the variability of dependent variables, the conclusion drawn is more accurate."}]}, {"question": "What is statistical learning model", "positive_ctxs": [{"text": "Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Statistical modeling is the process of applying statistical analysis to a dataset. A statistical model is a mathematical representation (or mathematical model) of observed data."}, {"text": "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body."}, {"text": "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body."}]}, {"question": "What are some examples of artificial intelligence", "positive_ctxs": [{"text": "8 Examples of Artificial IntelligenceGoogle Maps and Ride-Hailing Applications. One doesn't have to put much thought into traveling to a new destination anymore.  Face Detection and Recognition.  Text Editors or Autocorrect.  Search and Recommendation Algorithms.  Chatbots.  Digital Assistants.  Social Media.  E-Payments."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "These are some of the most popular examples of artificial intelligence that's being used today. Everyone is familiar with Apple's personal assistant, Siri. She's the friendly voice-activated computer that we interact with on a daily basis."}, {"text": "\"AI is a computer system able to perform tasks that ordinarily require human intelligence Many of these artificial intelligence systems are powered by machine learning, some of them are powered by deep learning and some of them are powered by very boring things like rules.\""}, {"text": "Multimodal machine learning is a vibrant multi-disciplinary research field which addresses some of the original goals of artificial intelligence by integrating and modeling multiple communicative modalities, including linguistic, acoustic and visual messages."}, {"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "Here are some examples of discrete variables: Number of children per family. Number of students in a class. Number of citizens of a country."}, {"text": "Here are some examples of discrete variables: Number of children per family. Number of students in a class. Number of citizens of a country."}]}, {"question": "How does a permutation test construct a distribution", "positive_ctxs": [{"text": "A permutation test (also called a randomization test, re-randomization test, or an exact test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under all possible rearrangements of"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A non parametric test (sometimes called a distribution free test) does not assume anything about the underlying distribution (for example, that the data comes from a normal distribution).  It usually means that you know the population data does not have a normal distribution."}, {"text": "How to Calculate a Confusion MatrixYou need a test dataset or a validation dataset with expected outcome values.Make a prediction for each row in your test dataset.From the expected outcomes and predictions count: The number of correct predictions for each class."}, {"text": "A false negative is a test result that indicates a person does not have a disease or condition when the person actually does have it, according to the National Institute of Health (NIH)."}, {"text": "A non-parametric test is a hypothesis test that does not make any assumptions about the distribution of the samples.  It does not rely on any properties of the distributions. The null hypothesis is that the samples were drawn from the same distribution."}, {"text": "A permutation test5 is used to determine the statistical significance of a model by computing a test statistic on the dataset and then for many random permutations of that data. If the model is significant, the original test statistic value should lie at one of the tails of the null hypothesis distribution."}, {"text": "To convert a frequency distribution to a probability distribution, divide area of the bar or interval of x by the total area of all the Bars. A simpler formula is: , N is the total Frequency and w is the interval of x. Example (From a frequency distribution table construct a probability plot)."}, {"text": "Def: A uniform random permutation is one in which each of the n! possible permutations are equally likely.  Def Given a set of n elements, a k-permutation is a sequence containing k of the n elements."}]}, {"question": "Can you do principal component analysis on categorical variables", "positive_ctxs": [{"text": "No. Principal components analysis involves breaking down the variance structure of a group of variables. Categorical variables are not numerical at all, and thus have no variance structure."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors."}, {"text": "Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors."}, {"text": "If your data contains both numeric and categorical variables, the best way to carry out clustering on the dataset is to create principal components of the dataset and use the principal component scores as input into the clustering."}, {"text": "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  Factor analysis is related to principal component analysis (PCA), but the two are not identical."}, {"text": "The mathematics of factor analysis and principal component analysis (PCA) are different. Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables."}, {"text": "The difference between factor analysis and principal component analysis.  Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables."}, {"text": "Kmeans clustering algorithm is applied to reduced datasets which is done by principal component analysis dimension reduction method. Cluster analysis is one of the major data analysis methods widely used for many practical applications in emerging areas[12]."}]}, {"question": "What is a simple explanation of the Hidden Markov Model algorithm", "positive_ctxs": [{"text": "The Hidden Markov Model (HMM) is a relatively simple way to model sequential data. A hidden Markov model implies that the Markov Model underlying the data is hidden or unknown to you. More specifically, you only know observational data and not information about the states."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Hidden Markov Model (HMM) is a relatively simple way to model sequential data. A hidden Markov model implies that the Markov Model underlying the data is hidden or unknown to you. More specifically, you only know observational data and not information about the states."}, {"text": "Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process \u2013 call it \u2013 with unobservable (\"hidden\") states. HMM assumes that there is another process whose behavior \"depends\" on . The goal is to learn about by observing ."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset.  Model selection is the process of choosing one of the models as the final model that addresses the problem."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "A perceptron is a simple model of a biological neuron in an artificial neural network. Perceptron is also the name of an early algorithm for supervised learning of binary classifiers."}]}, {"question": "What does in group bias mean", "positive_ctxs": [{"text": "In-group favoritism, sometimes known as in-group\u2013out-group bias, in-group bias, intergroup bias, or in-group preference, is a pattern of favoring members of one's in-group over out-group members. This can be expressed in evaluation of others, in allocation of resources, and in many other ways."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Ridge regression does not really select variables in the many predictors situation.  Both ridge regression and the LASSO can outperform OLS regression in some predictive situations \u2013 exploiting the tradeoff between variance and bias in the mean square error."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "Response bias can be defined as the difference between the true values of variables in a study's net sample group and the values of variables obtained in the results of the same study.  Nonresponse bias occurs when some respondents included in the sample do not respond."}, {"text": "Response bias can be defined as the difference between the true values of variables in a study's net sample group and the values of variables obtained in the results of the same study.  Nonresponse bias occurs when some respondents included in the sample do not respond."}]}, {"question": "What is the difference between variance and sample variance", "positive_ctxs": [{"text": "Differences Between Population Variance and Sample Variance When calculating sample variance, n is the number of sample points (vs N for population size in the formula above). Unlike the population variance, the sample variance is simply a statistic of the sample."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data. Due to this value of denominator in the formula for variance in case of sample data is 'n-1', and it is 'n' for population data."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data."}, {"text": "Sampling Distribution of Sample Variance This is the variance of the population. The variance of this sampling distribution can be computed by finding the expected value of the square of the sample variance and subtracting the square of 2.92."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Calculating Standard Error of the MeanFirst, take the square of the difference between each data point and the sample mean, finding the sum of those values.Then, divide that sum by the sample size minus one, which is the variance.Finally, take the square root of the variance to get the SD."}, {"text": "Calculating Standard Error of the MeanFirst, take the square of the difference between each data point and the sample mean, finding the sum of those values.Then, divide that sum by the sample size minus one, which is the variance.Finally, take the square root of the variance to get the SD."}, {"text": "The variance estimated as the average squared difference from the sample mean will always be less than the variance estimated as the average squared difference from the population mean unless the sample mean equals the population mean in which case they will be the same."}]}, {"question": "What is the difference between binary logistic regression and multinomial logistic regression", "positive_ctxs": [{"text": "Multinomial logistic regression deals with situations where the outcome can have three or more possible types (e.g., \"disease A\" vs. \"disease B\" vs. \"disease C\") that are not ordered.  Binary logistic regression is used to predict the odds of being a case based on the values of the independent variables (predictors)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)\u2208{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits."}, {"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}, {"text": "Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative distribution function of logistic distribution."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}]}, {"question": "What is the cumulative distribution function of the standard normal distribution", "positive_ctxs": [{"text": "The cumulative distribution function of the standard normal distribution is, up to constant factors, the error function, erf ( x ) \u2261 2 \u03c0 \u222b 0 x exp ( \u2212 y 2 ) d y , (Exercise 7)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Returns the inverse, or critical value, of the cumulative standard normal distribution. This function computes the critical value so that the cumulative distribution is greater than or equal to a pre-specified value."}, {"text": "The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(\u2217). Both functions will take any number and rescale it to fall between 0 and 1."}, {"text": "The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(\u2217). Both functions will take any number and rescale it to fall between 0 and 1."}, {"text": "The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(\u2217). Both functions will take any number and rescale it to fall between 0 and 1."}, {"text": "The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(\u2217). Both functions will take any number and rescale it to fall between 0 and 1."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}]}, {"question": "How do you find the marginal probability distribution", "positive_ctxs": [{"text": "The joint probability mass function is P(X = x and Y = y). Conditional distributions are P(X = x given Y = y), P(Y = y given X = x). Marginal distributions are P(X = x), P(Y = y)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "their joint probability distribution at (x,y), the functions given by: g(x) = \u03a3y f (x,y) and h(y) = \u03a3x f (x,y) are the marginal distributions of X and Y , respectively. If you're great with equations, that's probably all you need to know. It tells you how to find a marginal distribution."}, {"text": "Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point.  Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood)."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "In probability theory and statistics, the marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables."}]}, {"question": "What makes a matrix Hermitian", "positive_ctxs": [{"text": "A matrix that has only real entries is Hermitian if and only if it is symmetric. A real and symmetric matrix is simply a special case of a Hermitian matrix."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "), while other elements may be complex. Hermitian matrices have real eigenvalues whose eigenvectors form a unitary basis. For real matrices, Hermitian is the same as symmetric."}, {"text": "In mathematics, specifically in functional analysis, each bounded linear operator on a complex Hilbert space has a corresponding Hermitian adjoint (or adjoint operator). Adjoints of operators generalize conjugate transposes of square matrices to (possibly) infinite-dimensional situations."}, {"text": "An invertible matrix is a square matrix that has an inverse. We say that a square matrix is invertible if and only if the determinant is not equal to zero. In other words, a 2 x 2 matrix is only invertible if the determinant of the matrix is not 0."}, {"text": "The determinant is a unique number associated with a square matrix. If the determinant of a matrix is equal to zero: The matrix is less than full rank. The matrix is singular."}, {"text": "The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler."}, {"text": "In mathematics, a nonnegative matrix, written. is a matrix in which all the elements are equal to or greater than zero, that is, A positive matrix is a matrix in which all the elements are strictly greater than zero."}, {"text": "When you multiply a matrix by a number, you multiply every element in the matrix by the same number. This operation produces a new matrix, which is called a scalar multiple. For example, if x is 5, and the matrix A is: A ="}]}, {"question": "Should I use correlation or regression", "positive_ctxs": [{"text": "Regression is primarily used to build models/equations to predict a key response, Y, from a set of predictor (X) variables. Correlation is primarily used to quickly and concisely summarize the direction and strength of the relationships between a set of 2 or more numeric variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Yes, this is possible and I have heard it termed as joint regression or multivariate regression.  Regression analysis involving more than one independent variable and more than one dependent variable is indeed (also) called multivariate regression. This methodology is technically known as canonical correlation analysis."}, {"text": "A partial correlation is basically the correlation between two variables when a third variable is held constant.  If we look at the relationship between exercise and weight loss, we see a negative correlation, which sounds bad but isn't. It means that the more I exercise, the more weight I lose."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "These pages demonstrate how to use Moran's I or a Mantel test to check for spatial autocorrelation in your data. Moran's I is a parametric test while Mantel's test is semi-parametric. Both will also indicate if your spatial autocorrelation is positive or negative and provide a p-value for the level of autocorrelation."}, {"text": "6 Types of Regression Models in Machine Learning You Should Know AboutLinear Regression.Logistic Regression.Ridge Regression.Lasso Regression.Polynomial Regression.Bayesian Linear Regression."}, {"text": "Both quantify the direction and strength of the relationship between two numeric variables. When the correlation (r) is negative, the regression slope (b) will be negative. When the correlation is positive, the regression slope will be positive."}, {"text": "Top Machine Learning Algorithms You Should KnowLinear Regression.Logistic Regression.Linear Discriminant Analysis.Classification and Regression Trees.Naive Bayes.K-Nearest Neighbors (KNN)Learning Vector Quantization (LVQ)Support Vector Machines (SVM)More items\u2022"}]}, {"question": "What is greedy approach in data structure", "positive_ctxs": [{"text": "In greedy algorithm approach, decisions are made from the given solution domain. As being greedy, the closest solution that seems to provide an optimum solution is chosen. Greedy algorithms try to find a localized optimum solution, which may eventually lead to globally optimized solutions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An Inverted file is an index data structure that maps content to its location within a database file, in a document or in a set of documents.  The inverted file is the most popular data structure used in document retrieval systems to support full text search."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "A greedy algorithm is used to construct a Huffman tree during Huffman coding where it finds an optimal solution. In decision tree learning, greedy algorithms are commonly used, however they are not guaranteed to find the optimal solution. One popular such algorithm is the ID3 algorithm for decision tree construction."}, {"text": "Unsupervised learning is where you only have input data (X) and no corresponding output variables. The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data."}, {"text": "Unsupervised learning is where you only have input data (X) and no corresponding output variables. The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data."}]}, {"question": "What is the relation between sample size and standard error", "positive_ctxs": [{"text": "The standard error of the sample mean depends on both the standard deviation and the sample size, by the simple relation SE = SD/\u221a(sample size)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "When a population is finite, the formula that determines the standard error of the mean \u03c3\u00afx. needs to be adjusted. If N is the size of the population and n is the size of the sample ( where n\u22650.05N) , then the standard error of the mean is \u03c3\u00afx=\u03c3\u221an\u221aN\u2212nN\u22121."}, {"text": "On this site, we use the normal distribution when the population standard deviation is known and the sample size is large. We might use either distribution when standard deviation is unknown and the sample size is very large."}, {"text": "Because our sample size is large.  It is called the standard error because it refers to how much the sample mean fluctuates or is in error around the actual population mean."}, {"text": "In general, as sample size increases, the difference between expected adjusted r-squared and expected r-squared approaches zero; in theory this is because expected r-squared becomes less biased. the standard error of adjusted r-squared would get smaller approaching zero in the limit."}, {"text": "The relationship between margin of error and sample size is simple: As the sample size increases, the margin of error decreases.  If you think about it, it makes sense that the more information you have, the more accurate your results are going to be (in other words, the smaller your margin of error will get)."}, {"text": "The population mean of the distribution of sample means is the same as the population mean of the distribution being sampled from.  Thus as the sample size increases, the standard deviation of the means decreases; and as the sample size decreases, the standard deviation of the sample means increases."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}]}, {"question": "Is ReLU a linear activation function", "positive_ctxs": [{"text": "As a simple definition, linear function is a function which has same derivative for the inputs in its domain. ReLU is not linear. The simple answer is that ReLU 's output is not a straight line, it bends at the x-axis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "ReLU has become the darling activation function of the neural network world. Short for Rectified Linear Unit, it is a piecewise linear function that is defined to be 0 for all negative values of x and equal to a \u00d7 x otherwise, where a is a learnable parameter."}, {"text": "Yes a perceptron (one fully connected unit) can be used for regression. It will just be a linear regressor. If you use no activation function you get a regressor and if you put a sigmoid activation you get a classifier.  That's why the loss function for classification is called \"logistic regression\"."}, {"text": "Delta learning does this using the difference between a target activation and an actual obtained activation. Using a linear activation function, network connections are adjusted. Another way to explain the Delta rule is that it uses an error function to perform gradient descent learning."}, {"text": "ReLU is an activation function, that nullifies negative neurons, and in its simplicity, it also aids computation speed. However, unlike ELU, it doesn't have a normalizing effect, so BatchNorm helps even better."}, {"text": "It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.  The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better."}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them."}]}, {"question": "How do you perform feature selection for regression data", "positive_ctxs": [{"text": "This tutorial is divided into four parts; they are:Regression Dataset.Numerical Feature Selection. Correlation Feature Selection. Mutual Information Feature Selection.Modeling With Selected Features. Model Built Using All Features. Model Built Using Correlation Features.  Tune the Number of Selected Features."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "So while L2 regularization does not perform feature selection the same way as L1 does, it is more useful for feature *interpretation*: a predictive feature will get a non-zero coefficient, which is often not the case with L1."}, {"text": "Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection. Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Supervised: Use the target variable (e.g. remove irrelevant variables).Wrapper: Search for well-performing subsets of features. RFE.Filter: Select subsets of features based on their relationship with the target. Feature Importance Methods.Intrinsic: Algorithms that perform automatic feature selection during training."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}]}, {"question": "How do I start learning neural networks", "positive_ctxs": [{"text": "Anyhow, you could start by reading Introduction to Artificial Neural Networks by Jacek M. Zurada. It's a very good and tells you all you need to know about neural networks. Try to follow the Machine Learning course offered by Stanford on Coursera."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "The reason why Convolutional Neural Networks (CNNs) do so much better than classic neural networks on images and videos is that the convolutional layers take advantage of inherent properties of images. Simple feedforward neural networks don't see any order in their inputs."}, {"text": "Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. In fact, it is the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three."}, {"text": "7.2. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems. Radial basis function networks are distinguished from other neural networks due to their universal approximation and faster learning speed."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}]}, {"question": "What is the purpose of multiple testing in statistical inference", "positive_ctxs": [{"text": "\"Recognize that any frequentist statistical test has a random chance of indicating significance when it is not really present. Running multiple tests on the same data set at the same stage of an analysis increases the chance of obtaining at least one invalid result."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The purpose of statistical inference is to estimate this sample to sample variation or uncertainty."}, {"text": "The purpose of statistical inference is to estimate this sample to sample variation or uncertainty."}, {"text": "The purpose of statistical inference is to estimate this sample to sample variation or uncertainty."}, {"text": "The purpose of hypothesis testing is to determine whether there is enough statistical evidence in favor of a certain belief, or hypothesis, about a parameter."}, {"text": "Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates."}, {"text": "In statistics, the multiple comparisons, multiplicity or multiple testing problem occurs when one considers a set of statistical inferences simultaneously or infers a subset of parameters selected based on the observed values."}, {"text": "A statistical model is a family of probability distributions, the central problem of statistical inference being to identify which member of the family generated the data currently of interest."}]}, {"question": "What are binomial distributions used for", "positive_ctxs": [{"text": "The binomial distribution model allows us to compute the probability of observing a specified number of \"successes\" when the process is repeated a specific number of times (e.g., in a set of patients) and the outcome for a given patient is either a success or a failure."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The following are examples of discrete probability distributions commonly used in statistics:Binomial distribution.Geometric Distribution.Hypergeometric distribution.Multinomial Distribution.Negative binomial distribution.Poisson distribution."}, {"text": "The chi-square test is the most commonly used to test the goodness of fit tests and is used for discrete distributions like the binomial distribution and the Poisson distribution, whereas The Kolmogorov-Smirnov and Anderson-Darling goodness of fit tests are used for continuous distributions."}, {"text": "Negative binomial regression \u2013 Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean."}, {"text": "Binomial counts successes in a fixed number of trials, while Negative binomial counts failures until a fixed number successes. The Bernoulli and Geometric distributions are the simplest cases of the Binomial and Negative Binomial distributions."}, {"text": "Binomial counts successes in a fixed number of trials, while Negative binomial counts failures until a fixed number successes. The Bernoulli and Geometric distributions are the simplest cases of the Binomial and Negative Binomial distributions."}, {"text": "Analysis methods you might considerNegative binomial regression \u2013 Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean.  Poisson regression \u2013 Poisson regression is often used for modeling count data.More items"}, {"text": "Poisson regression \u2013 Poisson regression is often used for modeling count data. Poisson regression has a number of extensions useful for count models. Negative binomial regression \u2013 Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean."}]}, {"question": "Why is NLP difficult", "positive_ctxs": [{"text": "Natural Language processing is considered a difficult problem in computer science. It's the nature of the human language that makes NLP difficult.  While humans can easily master a language, the ambiguity and imprecise characteristics of the natural languages are what make NLP difficult for machines to implement."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "NHST is difficult to describe in one sentence, particularly here."}, {"text": "Deep Learning is a part of Machine Learning which is applied to larger data-sets and based on ANN (Artificial Neural Networks). The main technology used in NLP (Natural Language Processing) which mainly focuses on teaching natural/human language to computers.  NLP is a part of AI which overlaps with ML & DL."}, {"text": "Let's Start with NLP and NLG Setting aside NLU for the moment, we can draw a really simple distinction: Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language."}, {"text": "Let's Start with NLP and NLG Setting aside NLU for the moment, we can draw a really simple distinction: Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language."}, {"text": "Developers can make use of NLP to perform tasks like speech recognition, sentiment analysis, translation, auto-correct of grammar while typing, and automated answer generation. NLP is a challenging field since it deals with human language, which is extremely diverse and can be spoken in a lot of ways."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "If there are only two variables, one is continuous and another one is categorical, theoretically, it would be difficult to capture the correlation between these two variables."}]}, {"question": "How do you know if A and B is mutually exclusive", "positive_ctxs": [{"text": "A and B are mutually exclusive events if they cannot occur at the same time. This means that A and B do not share any outcomes and P(A AND B) = 0. For example, suppose the sample space S = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Two events are mutually exclusive if the probability of them both occurring is zero, that is if Pr(A\u2229B)=0. With that definition, disjoint sets are necessarily mutually exclusive, but mutually exclusive events aren't necessarily disjoint."}, {"text": "Events are considered disjoint if they never occur at the same time; these are also known as mutually exclusive events. Events are considered independent if they are unrelated. Two events that do not occur at the same time. These are also known as mutually exclusive events."}, {"text": "Sometimes we want to know the probability of getting one result or another. When events are mutually exclusive and we want to know the probability of getting one event OR another, then we can use the OR rule.  P(A or B) = P(A) + P(B) for mutually exclusive events."}, {"text": "Two events are mutually exclusive if they cannot occur at the same time. Another word that means mutually exclusive is disjoint. If two events are disjoint, then the probability of them both occurring at the same time is 0."}, {"text": "Answer. P(A \u2229 B) and P(A|B) are very closely related. Their only difference is that the conditional probability assumes that we already know something -- that B is true.  For P(A|B), however, we will receive a probability between 0, if A cannot happen when B is true, and P(B), if A is always true when B is true."}, {"text": "An example of a mutually exclusive event is when a coin is a tossed and there are two events that can occur, either it will be a head or a tail. Hence, both the events here are mutually exclusive.Difference between Mutually exclusive and independent eventsMutually exclusive eventsIndependent events4 more rows"}, {"text": "The probability of the intersection of Events A and B is denoted by P(A \u2229 B). If Events A and B are mutually exclusive, P(A \u2229 B) = 0. The probability that Events A or B occur is the probability of the union of A and B."}]}, {"question": "What does measuring distance between clusters mean in case of complete linkage", "positive_ctxs": [{"text": "In complete-linkage clustering, the link between two clusters contains all element pairs, and the distance between clusters equals the distance between those two elements (one in each cluster) that are farthest away from each other."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters \u201cr\u201d and \u201cs\u201d to the left is equal to the length of the arrow between their two furthest points."}, {"text": "The hclust function in R uses the complete linkage method for hierarchical clustering by default. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components."}, {"text": "In Average linkage clustering, the distance between two clusters is defined as the average of distances between all pairs of objects, where each pair is made up of one object from each group. D(r,s) = Trs / ( Nr * Ns) Where Trs is the sum of all pairwise distances between cluster r and cluster s."}, {"text": "In Average linkage clustering, the distance between two clusters is defined as the average of distances between all pairs of objects, where each pair is made up of one object from each group. D(r,s) = Trs / ( Nr * Ns) Where Trs is the sum of all pairwise distances between cluster r and cluster s."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Mean DeviationFind the mean of all values.Find the distance of each value from that mean (subtract the mean from each value, ignore minus signs)Then find the mean of those distances."}, {"text": "In general, K-means is a heuristic algorithm that partitions a data set into K clusters by minimizing the sum of squared distance in each cluster.  In this paper, the simulation of basic k-means algorithm is done, which is implemented using Euclidian distance metric."}]}, {"question": "What is a Deep Markov Model", "positive_ctxs": [{"text": "Thus a deep markov model: we allow for the transition probabilities governing the dynamics of the latent variables as well as the the emission probabilities that govern how the observations are generated by the latent dynamics to be parameterized by (non-linear) neural networks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Hidden Markov Model (HMM) is a relatively simple way to model sequential data. A hidden Markov model implies that the Markov Model underlying the data is hidden or unknown to you. More specifically, you only know observational data and not information about the states."}, {"text": "Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process \u2013 call it \u2013 with unobservable (\"hidden\") states. HMM assumes that there is another process whose behavior \"depends\" on . The goal is to learn about by observing ."}, {"text": "It is a Markov random field. It was translated from statistical physics for use in cognitive science. The Boltzmann machine is based on stochastic spin-glass model with an external field, i.e., a Sherrington\u2013Kirkpatrick model that is a stochastic Ising Model and applied to machine learning."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph.  The underlying graph of a Markov random field may be finite or infinite."}]}, {"question": "What is AdaBoost algorithm in machine learning", "positive_ctxs": [{"text": "AdaBoost algorithm, short for Adaptive Boosting, is a Boosting technique that is used as an Ensemble Method in Machine Learning. It is called Adaptive Boosting as the weights are re-assigned to each instance, with higher weights to incorrectly classified instances."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "AdaBoost. AdaBoost is an ensemble machine learning algorithm for classification problems. It is part of a group of ensemble methods called boosting, that add new machine learning models in a series where subsequent models attempt to fix the prediction errors made by prior models."}, {"text": "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging."}, {"text": "XGboost is the most widely used algorithm in machine learning, whether the problem is a classification or a regression problem. It is known for its good performance as compared to all other machine learning algorithms."}, {"text": "The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems."}, {"text": "In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data."}, {"text": "The main differences therefore are that Gradient Boosting is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function. Hence, gradient boosting is much more flexible."}, {"text": "AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine multiple \u201cweak classifiers\u201d into a single \u201cstrong classifier\u201d.  \u2192 AdaBoost algorithms can be used for both classification and regression problem."}]}, {"question": "What is the difference between RGB and YUV", "positive_ctxs": [{"text": "RGB formats are usually straightforward: red, green, and blue with a given pixel size. RGB24 is the most common, allowing 8 bits and a value of 0-255 per color component.  YUV color-spaces are a more efficient coding and reduce the bandwidth more than RGB capture can."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "The Range is the difference between the lowest and highest values. Example: In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9. So the range is 9 \u2212 3 = 6."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "The chief difference between MEMM and CRF is that MEMM is locally renormalized and suffers from the label bias problem, while CRFs are globally renormalized."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}]}, {"question": "How do you find the p value of a probability", "positive_ctxs": [{"text": "If your test statistic is positive, first find the probability that Z is greater than your test statistic (look up your test statistic on the Z-table, find its corresponding probability, and subtract it from one). Then double this result to get the p-value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Here are the steps for finding any percentile for a normal distribution X: 1a. If you're given the probability (percent) less than x and you need to find x, you translate this as: Find a where p(X < a) = p (and p is the given probability). That is, find the pth percentile for X."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Graphically, the p value is the area in the tail of a probability distribution. It's calculated when you run hypothesis test and is the area to the right of the test statistic (if you're running a two-tailed test, it's the area to the left and to the right)."}, {"text": "Gaussian Distribution Function The nature of the gaussian gives a probability of 0.683 of being within one standard deviation of the mean. The mean value is a=np where n is the number of events and p the probability of any integer value of x (this expression carries over from the binomial distribution )."}, {"text": "Gaussian Distribution Function The nature of the gaussian gives a probability of 0.683 of being within one standard deviation of the mean. The mean value is a=np where n is the number of events and p the probability of any integer value of x (this expression carries over from the binomial distribution )."}, {"text": "If you're given the probability (percent) greater than x and you need to find x, you translate this as: Find b where p(X > b) = p (and p is given). Rewrite this as a percentile (less-than) problem: Find b where p(X < b) = 1 \u2013 p. This means find the (1 \u2013 p)th percentile for X."}, {"text": "The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value. So the median is the value of the quantile at the probability value of 0.5. A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75."}]}, {"question": "How do you derive the MGF of a normal distribution", "positive_ctxs": [{"text": "0:002:42Suggested clip \u00b7 110 secondsNormal distribution moment generating function - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A major difference is in its shape: the normal distribution is symmetrical, whereas the lognormal distribution is not. Because the values in a lognormal distribution are positive, they create a right-skewed curve.  A further distinction is that the values used to derive a lognormal distribution are normally distributed."}, {"text": "A normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution. Areas of the normal distribution are often represented by tables of the standard normal distribution.  For example, a Z of -2.5 represents a value 2.5 standard deviations below the mean."}, {"text": "The distribution becomes normal when you have several different forces of varying magnitude acting together. Generally, the more forces then the more normal the distribution will become. This occurs a lot in nature which is why the normal distribution is so prevalent."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side.  The normal distribution is often called the bell curve because the graph of its probability density looks like a bell."}, {"text": "A univariate distribution refers to the distribution of a single random variable.  On the other hand, a multivariate distribution refers to the probability distribution of a group of random variables. For example, a multivariate normal distribution is used to specify the probabilities of returns of a group of n stocks."}, {"text": "Normal distributions are symmetric around their mean. The mean, median, and mode of a normal distribution are equal. The area under the normal curve is equal to 1.0.  Approximately 95% of the area of a normal distribution is within two standard deviations of the mean."}]}, {"question": "When should you not normalize data in machine learning", "positive_ctxs": [{"text": "For machine learning, every dataset does not require normalization. It is required only when features have different ranges. For example, consider a data set containing two features, age, and income(x2). Where age ranges from 0\u2013100, while income ranges from 0\u2013100,000 and higher."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data."}, {"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "Multicollinearity might be a handful to pronounce but it's a topic you should be aware of in the machine learning field. Due to multicollinearity, regression coefficients will not be estimated precisely and cause a high standard error."}, {"text": "At a bare minimum, collect around 1000 examples. For most \"average\" problems, you should have 10,000 - 100,000 examples. For \u201chard\u201d problems like machine translation, high dimensional data generation, or anything requiring deep learning, you should try to get 100,000 - 1,000,000 examples."}, {"text": "At a bare minimum, collect around 1000 examples. For most \"average\" problems, you should have 10,000 - 100,000 examples. For \u201chard\u201d problems like machine translation, high dimensional data generation, or anything requiring deep learning, you should try to get 100,000 - 1,000,000 examples."}, {"text": "At a bare minimum, collect around 1000 examples. For most \"average\" problems, you should have 10,000 - 100,000 examples. For \u201chard\u201d problems like machine translation, high dimensional data generation, or anything requiring deep learning, you should try to get 100,000 - 1,000,000 examples."}, {"text": "Motivation. Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization.  Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance."}]}, {"question": "Which layer in a neural network allows it to learn more complicated features", "positive_ctxs": [{"text": "Hidden Layers, which are neuron nodes stacked in between inputs and outputs, allowing neural networks to learn more complicated features (such as XOR logic)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Convolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input. A pooling layer is a new layer added after the convolutional layer."}, {"text": "A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.  If you accept most classes of problems can be reduced to functions, this statement implies a neural network can, in theory, solve any problem."}, {"text": "One hidden layer is sufficient for the large majority of problems. Usually, each hidden layer contains the same number of neurons. The larger the number of hidden layers in a neural network, the longer it will take for the neural network to produce the output and the more complex problems the neural network can solve."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Jeff Heaton (see page 158 of the linked text), who states that one hidden layer allows a neural network to approximate any function involving \u201ca continuous mapping from one finite space to another.\u201d With two hidden layers, the network is able to \u201crepresent an arbitrary decision boundary to arbitrary accuracy.\u201d"}, {"text": "Forward propagation is how neural networks make predictions. Input data is \u201cforward propagated\u201d through the network layer by layer to the final layer which outputs a prediction."}, {"text": "The purpose of a neural network is to learn to recognize patterns in your data. Once the neural network has been trained on samples of your data, it can make predictions by detecting similar patterns in future data. Software that learns is truly \"Artificial Intelligence\"."}]}, {"question": "What is the goal of model selection in machine learning", "positive_ctxs": [{"text": "Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset. Model selection is a process that can be applied both across different types of models (e.g. logistic regression, SVM, KNN, etc.)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset.  Model selection is the process of choosing one of the models as the final model that addresses the problem."}, {"text": "I believe there are related in the optimization framework for both theories and they are somewhat familiar in the way the problem is stated but they differ in the main goal of each branch (optimal control goal is finding a control policy, while machine learning goal is to find a model to make prediction)."}, {"text": "Top reasons to use feature selection are: It enables the machine learning algorithm to train faster. It reduces the complexity of a model and makes it easier to interpret. It improves the accuracy of a model if the right subset is chosen."}, {"text": "Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation.  Its goal is to maximize the total reward."}, {"text": "Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation.  Its goal is to maximize the total reward."}, {"text": "The purpose of such selection is to determine a set of variables that will provide the best fit for the model so that accurate predictions can be made. Variable selection is one of the most difficult aspects of model building."}, {"text": "Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections."}]}, {"question": "What is prior likelihood and posterior", "positive_ctxs": [{"text": "Prior: Probability distribution representing knowledge or uncertainty of a data object prior or before observing it. Posterior: Conditional probability distribution representing what parameters are likely after observing the data object. Likelihood: The probability of falling under a specific category or class."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To put simply, likelihood is \"the likelihood of \u03b8 having generated D\" and posterior is essentially \"the likelihood of \u03b8 having generated D\" further multiplied by the prior distribution of \u03b8."}, {"text": "The posterior distribution is a way to summarize what we know about uncertain quantities in Bayesian analysis. It is a combination of the prior distribution and the likelihood function, which tells you what information is contained in your observed data (the \u201cnew evidence\u201d)."}, {"text": "It is a combination of the prior distribution and the likelihood function, which tells you what information is contained in your observed data (the \u201cnew evidence\u201d). In other words, the posterior distribution summarizes what you know after the data has been observed."}, {"text": "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis."}, {"text": "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis."}, {"text": "The output, y is generated from a normal (Gaussian) Distribution characterized by a mean and variance. In contrast to OLS, we have a posterior distribution for the model parameters that is proportional to the likelihood of the data multiplied by the prior probability of the parameters."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information. The posterior probability is calculated by updating the prior probability using Bayes' theorem."}]}, {"question": "What is forward and backward chaining in artificial intelligence", "positive_ctxs": [{"text": "Forward chaining as the name suggests, start from the known facts and move forward by applying inference rules to extract more data, and it continues until it reaches to the goal, whereas backward chaining starts from the goal, move backward by using inference rules to determine the facts that satisfy the goal."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Backward chaining is the logical process of inferring unknown truths from known conclusions by moving backward from a solution to determine the initial conditions and rules. Backward chaining is often applied in artificial intelligence (AI) and may be used along with its counterpart, forward chaining."}, {"text": "The difference between forward and backward chaining is: Backward chaining starts with a goal and then searches back through inference rules to find the facts that support the goal. Forward chaining starts with facts and searches forward through the rules to find a desired goal."}, {"text": "Backward chaining (or backward reasoning) is an inference method described colloquially as working backward from the goal. It is used in automated theorem provers, inference engines, proof assistants, and other artificial intelligence applications.  Both rules are based on the modus ponens inference rule."}, {"text": "Forward chaining starts from known facts and applies inference rule to extract more data unit it reaches to the goal. Backward chaining starts from the goal and works backward through inference rules to find the required facts that support the goal.  Backward chaining reasoning applies a depth-first search strategy."}, {"text": "Another strategy OTs typically recommend is something called \u201cbackward chaining.\" Backward chaining is working backward from the goal. For example, the goal is put on a T-shirt.  Pull shirt over head. Push right arm up through right sleeve."}, {"text": "The algorithm is used to effectively train a neural network through a method called chain rule. In simple terms, after each forward pass through a network, backpropagation performs a backward pass while adjusting the model's parameters (weights and biases)."}, {"text": "Backward chaining is known as goal-driven technique as we start from the goal and divide into sub-goal to extract the facts.  Backward chaining is suitable for diagnostic, prescription, and debugging application. 7. Forward chaining can generate an infinite number of possible conclusions."}]}, {"question": "Which is better logistic regression or decision tree", "positive_ctxs": [{"text": "Categorical data works well with Decision Trees, while continuous data work well with Logistic Regression. If your data is categorical, then Logistic Regression cannot handle pure categorical data (string format).  Therefore, if you have lots of categorical data, go with a Decision Tree."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes"}, {"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed."}, {"text": "The general regression tree building methodology allows input variables to be a mixture of continuous and categorical variables. A decision tree is generated when each decision node in the tree contains a test on some input variable's value. The terminal nodes of the tree contain the predicted output variable values."}, {"text": "Decision tree learning is a supervised machine learning technique for inducing a decision tree from training data. A decision tree (also referred to as a classification tree or a reduction tree) is a predictive model which is a mapping from observations about an item to conclusions about its target value."}]}, {"question": "What is predictive data modeling", "positive_ctxs": [{"text": "Predictive modeling, also called predictive analytics, is a mathematical process that seeks to predict future events or outcomes by analyzing patterns that are likely to forecast future results."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Predictive analytics uses predictors or known features to create predictive models that will be used in obtaining an output. A predictive model is able to learn how different points of data connect with each other. Two of the most widely used predictive modeling techniques are regression and neural networks."}, {"text": "Predictive modeling is a form of artificial intelligence that uses data mining and probability to forecast or estimate more granular, specific outcomes. For example, predictive modeling could help identify customers who are likely to purchase our new One AI software over the next 90 days."}, {"text": "Linear regressions are among the simplest types of predictive models.  Other more complex predictive models include decision trees, k-means clustering and Bayesian inference, to name just a few potential methods. The most complex area of predictive modeling is the neural network."}, {"text": "Predictive ModelingClean the data by removing outliers and treating missing data.Identify a parametric or nonparametric predictive modeling approach to use.Preprocess the data into a form suitable for the chosen modeling algorithm.Specify a subset of the data to be used for training the model.More items"}, {"text": "Overfitting is a modeling error that occurs when a function is too closely fit to a limited set of data points.  Thus, attempting to make the model conform too closely to slightly inaccurate data can infect the model with substantial errors and reduce its predictive power."}, {"text": "The steps are:Clean the data by removing outliers and treating missing data.Identify a parametric or nonparametric predictive modeling approach to use.Preprocess the data into a form suitable for the chosen modeling algorithm.Specify a subset of the data to be used for training the model.More items"}, {"text": "Predictive modeling is the process of using known results to create, process, and validate a model that can be used to forecast future outcomes. It is a tool used in predictive analytics, a data mining technique that attempts to answer the question \"what might possibly happen in the future?\""}]}, {"question": "Is Log loss the same as cross entropy", "positive_ctxs": [{"text": "1 Answer. They are essentially the same; usually, we use the term log loss for binary classification problems, and the more general cross-entropy (loss) for the general case of multi-class classification, but even this distinction is not consistent, and you'll often find the terms used interchangeably as synonyms."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cross Entropy is definitely a good loss function for Classification Problems, because it minimizes the distance between two probability distributions - predicted and actual.  So cross entropy make sure we are minimizing the difference between the two probability. This is the reason."}, {"text": "Cross Entropy is definitely a good loss function for Classification Problems, because it minimizes the distance between two probability distributions - predicted and actual.  So cross entropy make sure we are minimizing the difference between the two probability. This is the reason."}, {"text": "Logarithmic Loss, or simply Log Loss, is a classification loss function often used as an evaluation metric in Kaggle competitions.  Log Loss quantifies the accuracy of a classifier by penalising false classifications."}, {"text": "Logarithmic Loss, or simply Log Loss, is a classification loss function often used as an evaluation metric in Kaggle competitions.  Log Loss quantifies the accuracy of a classifier by penalising false classifications."}, {"text": "Logarithmic Loss, or simply Log Loss, is a classification loss function often used as an evaluation metric in Kaggle competitions.  Log Loss quantifies the accuracy of a classifier by penalising false classifications."}, {"text": "Logistic Regression is a special case of a Neural Network with no hidden layers, that uses the sigmoid activation function and uses the softmax with cross entropy loss.  neural network and logistic regressions are different techniques or algorithms to do the same thing, classification of data."}, {"text": "Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true ."}]}, {"question": "What is a random sample in statistics", "positive_ctxs": [{"text": "A simple random sample is a subset of a statistical population in which each member of the subset has an equal probability of being chosen. A simple random sample is meant to be an unbiased representation of a group.  Random sampling is used in science to conduct randomized control tests or for blinded experiments."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Statistical inference consists in the use of statistics to draw conclusions about some unknown aspect of a population based on a random sample from that population.  Point estimation is discussed in the statistics section of the encyclopedia."}, {"text": "Page 1. 1 Order Statistics. Definition The order statistics of a random sample X1,,Xn are the sample values placed in ascending order. They are denoted by X(1),,X(n). The order statistics are random variables that satisfy X(1) \u2264 X(2) \u2264 \u00b7\u00b7\u00b7 \u2264 X(n)."}, {"text": "Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables."}, {"text": "A simple random sample is a subset of a statistical population in which each member of the subset has an equal probability of being chosen. A simple random sample is meant to be an unbiased representation of a group."}, {"text": "Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population."}, {"text": "In particular, a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space."}, {"text": "In particular, a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space."}]}, {"question": "How do you deal with variance and bias", "positive_ctxs": [{"text": "You can reduce High variance, by reducing the number of features in the model. There are several methods available to check which features don't add much value to the model and which are of importance. Increasing the size of the training set can also help the model generalise."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "So, we have listed some of the ways where you can achieve trade-off between the two. Both bias and variance are related to each other, if you increase one the other decreases and vice versa. By a trade-off, there is an optimal balance in the bias and variance which gives us a model that is neither underfit nor overfit."}, {"text": "They are defined as follows: Bias: Bias describes how well a model matches the training set. A model with high bias won't match the data set closely, while a model with low bias will match the data set very closely.  Typically models with high bias have low variance, and models with high variance have low bias."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Differential calculus is usually taught first. I think most students find it more intuitive because they deal with rates of change in real life. Integral calculus is more abstract, and indefinite integrals are much easier to evaluate if you understand differentiation."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "What is null and alternative hypothesis", "positive_ctxs": [{"text": "The null hypothesis is a general statement that states that there is no relationship between two phenomenons under consideration or that there is no association between two groups. An alternative hypothesis is a statement that describes that there is a relationship between two selected variables in a study."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In research, there is a convention that the hypothesis is written in two forms, the null hypothesis, and the alternative hypothesis (called the experimental hypothesis when the method of investigation is an experiment)."}, {"text": "The null hypothesis is the one to be tested and the alternative is everything else. In our example, The null hypothesis would be: The mean data scientist salary is 113,000 dollars. While the alternative: The mean data scientist salary is not 113,000 dollars."}, {"text": "The null hypothesis is the one to be tested and the alternative is everything else. In our example, The null hypothesis would be: The mean data scientist salary is 113,000 dollars. While the alternative: The mean data scientist salary is not 113,000 dollars."}, {"text": "The null hypothesis (H0) for a one tailed test is that the mean is greater (or less) than or equal to \u00b5, and the alternative hypothesis is that the mean is < (or >, respectively) \u00b5."}, {"text": "Rejecting or failing to reject the null hypothesis If our statistical analysis shows that the significance level is below the cut-off value we have set (e.g., either 0.05 or 0.01), we reject the null hypothesis and accept the alternative hypothesis."}, {"text": "The critical region is the area that lies to the left of -1.645. If the z-value is less than -1.645 there we will reject the null hypothesis and accept the alternative hypothesis. If it is greater than -1.645, we will fail to reject the null hypothesis and say that the test was not statistically significant."}, {"text": "The purpose and importance of the null hypothesis and alternative hypothesis are that they provide an approximate description of the phenomena. The purpose is to provide the researcher or an investigator with a relational statement that is directly tested in a research study."}]}, {"question": "What use does multivariate Analyses of variance have if any", "positive_ctxs": [{"text": "In statistics, multivariate analysis of variance (MANOVA) is a procedure for comparing multivariate sample means. As a multivariate procedure, it is used when there are two or more dependent variables, and is often followed by significance tests involving individual dependent variables separately."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "For a large sample size, Sample Variance will be a better estimate of Population variance so even if population variance is unknown, we can use the Z test using sample variance. Similarly, for a Large Sample, we have a high degree of freedom."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "The one-way multivariate analysis of variance (one-way MANOVA) is used to determine whether there are any differences between independent groups on more than one continuous dependent variable. In this regard, it differs from a one-way ANOVA, which only measures one dependent variable."}, {"text": "So, assuming a 15% survey response rate, we see that you should send your NPS survey to 1,700 customers. What if you're a smaller company and don't have enough customers to send the recommended number of invitations?"}, {"text": "The main use of F-distribution is to test whether two independent samples have been drawn for the normal populations with the same variance, or if two independent estimates of the population variance are homogeneous or not, since it is often desirable to compare two variances rather than two averages."}, {"text": "The main use of F-distribution is to test whether two independent samples have been drawn for the normal populations with the same variance, or if two independent estimates of the population variance are homogeneous or not, since it is often desirable to compare two variances rather than two averages."}]}, {"question": "What is the cutoff frequency of a high pass filter", "positive_ctxs": [{"text": "The cutoff frequency for a high-pass filter is that frequency at which the output (load) voltage equals 70.7% of the input (source) voltage. Above the cutoff frequency, the output voltage is greater than 70.7% of the input, and vice versa."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A high-pass filter (HPF) is an electronic filter that passes signals with a frequency higher than a certain cutoff frequency and attenuates signals with frequencies lower than the cutoff frequency. The amount of attenuation for each frequency depends on the filter design."}, {"text": "The low-pass filter has a gain response with a frequency range from zero frequency (DC) to \u03c9C. Any input that has a frequency below the cutoff frequency \u03c9C gets a pass, and anything above it gets attenuated or rejected. The gain approaches zero as frequency increases to infinity."}, {"text": "Definition: The trend is the component of a time series that represents variations of low frequency in a time series, the high and medium frequency fluctuations having been filtered out."}, {"text": "A Second Order Low Pass Filter is to be design around a non-inverting op-amp with equal resistor and capacitor values in its cut-off frequency determining circuit. If the filters characteristics are given as: Q = 5, and \u0192c = 159Hz, design a suitable low pass filter and draw its frequency response."}, {"text": "A low pass filter is a fixed filter just filters out frequencies above a passband. A Kalman filter can be used for state estimation, prediction of values in time and smoothing. A Kalman filter is a consequence of state variable models and LQG system theory. It has a gain which changes at each time step."}, {"text": "A high pass filter can be formed by placing a capacitor in series with an inverting gain stage as shown in Figure 11.13."}, {"text": "The easiest way to convert categorical variables to continuous is by replacing raw categories with the average response value of the category. cutoff : minimum observations in a category. All the categories having observations less than the cutoff will be a different category."}]}, {"question": "How do you calculate entropy of information", "positive_ctxs": [{"text": "Entropy can be calculated for a random variable X with k in K discrete states as follows: H(X) = -sum(each k in K p(k) * log(p(k)))"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "joint entropy is the amount of information in two (or more) random variables; conditional entropy is the amount of information in one random variable given we already know the other."}, {"text": "In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes. The concept of information entropy was introduced by Claude Shannon in his 1948 paper \"A Mathematical Theory of Communication\"."}, {"text": "In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes. The concept of information entropy was introduced by Claude Shannon in his 1948 paper \"A Mathematical Theory of Communication\"."}, {"text": "Conditional entropy. In information theory, the conditional entropy quantifies the amount of information needed to describe the outcome of a random variable given that the value of another random variable is known."}, {"text": "Coding theory is one of the most important and direct applications of information theory.  Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source."}, {"text": "The intuition for entropy is that it is the average number of bits required to represent or transmit an event drawn from the probability distribution for the random variable. \u2026 the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution."}]}, {"question": "What does it mean that something is quantized", "positive_ctxs": [{"text": "In the simplest sense, if something is \u201cquantized\u201d, that means it can only take on certain specific values, rather than a continuous range of values. For example, the energy that an electron can have when it's bound to an atom is quantized."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "Quantization is the concept that a physical quantity can have only certain discrete values.  For example, matter is quantized because it is composed of individual particles that cannot be subdivided; it is not possible to have half an electron. Also, the energy levels of electrons in atoms are quantized."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "When the standard deviation or the mean change, something unusual is happening. To detect such changes, for each upcoming point \u201cp\u201d we create of window from \u201cp\u201d to \u201cp-100\u2033. Then, we calculate the standard deviation and mean of this window. If it changes too much, an anomaly has been detected."}]}, {"question": "What does the term linear mean", "positive_ctxs": [{"text": "1a(1) : of, relating to, resembling, or having a graph that is a line and especially a straight line : straight. (2) : involving a single dimension."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Logistic regression is quite different than linear regression in that it does not make several of the key assumptions that linear and general linear models (as well as other ordinary least squares algorithm based models) hold so close: (1) logistic regression does not require a linear relationship between the dependent"}, {"text": "The coefficient for a term represents the change in the mean response associated with a change in that term, while the other terms in the model are held constant. The sign of the coefficient indicates the direction of the relationship between the term and the response."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}]}, {"question": "What is the target concept in machine learning", "positive_ctxs": [{"text": "Target Concept Term used in the machine learning literature to denote the Bayes decision rule, or the regression function, depending on the context. The target concept is a member of the concept space. Synonyms: Bayes Decision Rule in classification, Regression Function in regression."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model.  The rows represent the predicted values of the target variable."}, {"text": "A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model."}, {"text": "Deep learning or hierarchical learning is the part of machine learning which mainly follows the widely used concepts of a neural network.  In this paper, we have used the concept of deep recurrent neural network (Deep-RNN) to train the model for a classification task."}, {"text": "Hypothesis Space (H): Hypothesis space is the set of all the possible legal hypothesis. This is the set from which the machine learning algorithm would determine the best possible (only one) which would best describe the target function or the outputs."}, {"text": "Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error."}, {"text": "Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error."}, {"text": "Deep reinforcement learning is a category of machine learning and artificial intelligence where intelligent machines can learn from their actions similar to the way humans learn from experience.  Actions that get them to the target outcome are rewarded (reinforced)."}]}, {"question": "Which technique can be implemented if you want to reduce the dimensionality of a certain statistical problem", "positive_ctxs": [{"text": "Here is a brief review of our original seven techniques for dimensionality reduction:Missing Values Ratio.  Low Variance Filter.  High Correlation Filter.  Random Forests/Ensemble Trees.  Principal Component Analysis (PCA).  Backward Feature Elimination.  Forward Feature Construction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "Principal Component Analysis (PCA) is an unsupervised, non-parametric statistical technique primarily used for dimensionality reduction in machine learning. High dimensionality means that the dataset has a large number of features.  PCA can also be used to filter noisy datasets, such as image compression."}, {"text": "Word Embedding is really all about improving the ability of networks to learn from text data. By representing that data as lower dimensional vectors.  This technique is used to reduce the dimensionality of text data but these models can also learn some interesting traits about words in a vocabulary."}, {"text": "In a simple case with two possible categories or the binary classification problem you have one boundary.  Negative means you want the output to be off/low when the classifier \u201csees\u201d that particular class. Positive means you want the output to be on/high when the classifier \u201csees\u201d that class."}, {"text": "For a statistical test to be valid, your sample size needs to be large enough to approximate the true distribution of the population being studied. To determine which statistical test to use, you need to know: whether your data meets certain assumptions. the types of variables that you're dealing with."}, {"text": "Data structure and algorithms help in understanding the nature of the problem at a deeper level and thereby a better understanding of the world. If you want to know more about Why Data Structures and Algorithms then you must watch this video of Mr."}, {"text": "Reduce Variance of an Estimate If we want to reduce the amount of variance in a prediction, we must add bias. Consider the case of a simple statistical estimate of a population parameter, such as estimating the mean from a small random sample of data. A single estimate of the mean will have high variance and low bias."}]}, {"question": "What is meant by Hyperplane", "positive_ctxs": [{"text": "In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "There is a direct relationship between the coefficients produced by logit and the odds ratios produced by logistic. First, let's define what is meant by a logit: A logit is defined as the log base e (log) of the odds. : [1] logit(p) = log(odds) = log(p/q) The range is negative infinity to positive infinity."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}]}, {"question": "How do you find the p value for a randomization test", "positive_ctxs": [{"text": "To get a p-value we compare our observed test- statistic to the randomization distribution of test- statistics obtained by assuming the null is true. The p-value will be the proportion of test- statistics in the randomization distribution that are as or more extreme than the observed test- statistic."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Here are the steps for finding any percentile for a normal distribution X: 1a. If you're given the probability (percent) less than x and you need to find x, you translate this as: Find a where p(X < a) = p (and p is the given probability). That is, find the pth percentile for X."}, {"text": "Graphically, the p value is the area in the tail of a probability distribution. It's calculated when you run hypothesis test and is the area to the right of the test statistic (if you're running a two-tailed test, it's the area to the left and to the right)."}, {"text": "If you're given the probability (percent) greater than x and you need to find x, you translate this as: Find b where p(X > b) = p (and p is given). Rewrite this as a percentile (less-than) problem: Find b where p(X < b) = 1 \u2013 p. This means find the (1 \u2013 p)th percentile for X."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "False negatives \u2014 that is, a test that says you don't have the virus when you actually do have the virus \u2014 may occur."}, {"text": "3.2 How to test for differences between samplesDecide on a hypothesis to test, often called the \u201cnull hypothesis\u201d (H0 ). In our case, the hypothesis is that there is no difference between sets of samples.  Decide on a statistic to test the truth of the null hypothesis.Calculate the statistic.Compare it to a reference value to establish significance, the P-value."}]}, {"question": "What kernel is used in SVM", "positive_ctxs": [{"text": "So, the rule of thumb is: use linear SVMs (or logistic regression) for linear problems, and nonlinear kernels such as the Radial Basis Function kernel for non-linear problems."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Gaussian RBF(Radial Basis Function) is another popular Kernel method used in SVM models for more. RBF kernel is a function whose value depends on the distance from the origin or from some point."}, {"text": "SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types."}, {"text": "Gaussian RBF(Radial Basis Function) is another popular Kernel method used in SVM models for more. RBF kernel is a function whose value depends on the distance from the origin or from some point. Gaussian Kernel is of the following format; ||X1 \u2014 X2 || = Euclidean distance between X1 & X2."}, {"text": "The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid."}, {"text": "SVM Kernel Functions SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form.  For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid."}, {"text": "SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs."}, {"text": "SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs."}]}, {"question": "Why do we need dimensionality reduction techniques", "positive_ctxs": [{"text": "Dimensionality reduction refers to techniques for reducing the number of input variables in training data. When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the \u201cessence\u201d of the data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Alternatively, general dimensionality reduction techniques are used such as:Independent component analysis.Isomap.Kernel PCA.Latent semantic analysis.Partial least squares.Principal component analysis.Multifactor dimensionality reduction.Nonlinear dimensionality reduction.More items"}, {"text": "I daresay that dimensionality reduction is necessary when we are lacking an acceptable balance between bias and variance. Some learning algorithms have some kind of 'built in' dimensionality reduction like the Relevance Vector Machine or Random Forests (to name two that are widely used)."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Dimensionality reduction refers to techniques for reducing the number of input variables in training data. When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the \u201cessence\u201d of the data."}, {"text": "The various methods used for dimensionality reduction include: Principal Component Analysis (PCA) Linear Discriminant Analysis (LDA) Generalized Discriminant Analysis (GDA)"}, {"text": "Answer. Answer: Explanation: Linear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications."}, {"text": "Principal Component Analysis (PCA) is an unsupervised, non-parametric statistical technique primarily used for dimensionality reduction in machine learning. High dimensionality means that the dataset has a large number of features.  PCA can also be used to filter noisy datasets, such as image compression."}]}, {"question": "How do you validate a model performance", "positive_ctxs": [{"text": "Using proper validation techniques helps you understand your model, but most importantly, estimate an unbiased generalization performance.Splitting your data.  k-Fold Cross-Validation (k-Fold CV)  Leave-one-out Cross-Validation (LOOCV)  Nested Cross-Validation.  Time Series CV.  Comparing Models."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The point of a test set is to give you a final, unbiased performance measure of your entire model building process."}, {"text": "In a dataset a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}, {"text": "In a dataset, a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How to train a Machine Learning model in 5 minutesModel Naming \u2014 Give Your Model a Name: Let's start with giving your model a name, describe your model and attach tags to your model.  Data Type Selection \u2014 Choose data type(Images/Text/CSV): It's time to tell us about the type of data you want to train your model.More items"}, {"text": "To \u201cconverge\u201d in machine learning is to have an error so close to local/global minimum, or you can see it aa having a performance so clise to local/global minimum. When the model \u201cconverges\u201d there is usually no significant error decrease / performance increase anymore. ( Unless a more modern optimizer is applied)"}]}, {"question": "What is difference between ARMA and Arima model", "positive_ctxs": [{"text": "ARMA stands for \u201cAutoregressive Moving Average\u201d and ARIMA stands for \u201cAutoregressive Integrated Moving Average.\u201d The only difference, then, is the \u201cintegrated\u201d part. Integrated refers to the number of times needed to difference a series in order to achieve stationarity, which is required for ARMA models to be valid."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "There is a clear difference between variables and parameters. A variable represents a model state, and may change during simulation. A parameter is commonly used to describe objects statically. A parameter is normally a constant in a single simulation, and is changed only when you need to adjust your model behavior."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term. Ridge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function."}, {"text": "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term. Ridge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function."}, {"text": "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term. Ridge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function."}]}, {"question": "How do you use linear regression to predict future values", "positive_ctxs": [{"text": "Statistical researchers often use a linear relationship to predict the (average) numerical value of Y for a given value of X using a straight line (called the regression line). If you know the slope and the y-intercept of that regression line, then you can plug in a value for X and predict the average value for Y."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Statistical researchers often use a linear relationship to predict the (average) numerical value of Y for a given value of X using a straight line (called the regression line). If you know the slope and the y-intercept of that regression line, then you can plug in a value for X and predict the average value for Y."}, {"text": "Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values."}, {"text": "A statistical model is autoregressive if it predicts future values based on past values. For example, an autoregressive model might seek to predict a stock's future prices based on its past performance."}, {"text": "You can use regression equations to make predictions. Regression equations are a crucial part of the statistical output after you fit a model.  However, you can also enter values for the independent variables into the equation to predict the mean value of the dependent variable."}, {"text": "Barto (2007), Scholarpedia, 2(11):1604. Temporal difference (TD) learning is an approach to learning how to predict a quantity that depends on future values of a given signal. The name TD derives from its use of changes, or differences, in predictions over successive time steps to drive the learning process."}, {"text": "The general guideline is to use linear regression first to determine whether it can fit the particular type of curve in your data. If you can't obtain an adequate fit using linear regression, that's when you might need to choose nonlinear regression."}, {"text": "The general guideline is to use linear regression first to determine whether it can fit the particular type of curve in your data. If you can't obtain an adequate fit using linear regression, that's when you might need to choose nonlinear regression."}]}, {"question": "What is Cohen's d in statistics", "positive_ctxs": [{"text": "Cohen's d is an effect size used to indicate the standardised difference between two means. It can be used, for example, to accompany reporting of t-test and ANOVA results. It is also widely used in meta-analysis. Cohen's d is an appropriate effect size for the comparison between two means."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cohen's kappa measures the agreement between two raters who each classify N items into C mutually exclusive categories.\u00b9 A simple way to think this is that Cohen's Kappa is a quantitative measure of reliability for two raters that are rating the same thing, corrected for how often that the raters may agree by chance."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "From Wikipedia, the free encyclopedia. Cohen's kappa coefficient (\u03ba) is a statistic that is used to measure inter-rater reliability (and also Intra-rater reliability) for qualitative (categorical) items."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Accuracy is the percentage of correctly classifies instances out of all instances.  Kappa or Cohen's Kappa is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "What kind of activation function is ReLU", "positive_ctxs": [{"text": "The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "The ReLU activation solves the problem of vanishing gradient that is due to sigmoid-like non-linearities (the gradient vanishes because of the flat regions of the sigmoid). The other kind of \"vanishing\" gradient seems to be related to the depth of the network (e.g. see this for example)."}, {"text": "ReLU has become the darling activation function of the neural network world. Short for Rectified Linear Unit, it is a piecewise linear function that is defined to be 0 for all negative values of x and equal to a \u00d7 x otherwise, where a is a learnable parameter."}, {"text": "An activation function is defined by and defines the output of a neuron in terms of its input (aka induced local field) . There are three types of activation functions. Threshhold function an example of which is. This function is also termed the Heaviside function. Piecewise Linear."}, {"text": "The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.  Due to this reason, during the backpropogation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}]}, {"question": "What are the uses of moments", "positive_ctxs": [{"text": "Moments are are very useful in statistics because they tell you much about your data. There are four commonly used moments in statistics: the mean, variance, skewness, and kurtosis. The mean gives you a measure of center of the data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Sample moments are those that are utilized to approximate the unknown population moments. Sample moments are calculated from the sample data. Such moments include mean, variance, skewness, and kurtosis."}, {"text": "Sample moments are those that are utilized to approximate the unknown population moments. Sample moments are calculated from the sample data. Such moments include mean, variance, skewness, and kurtosis."}, {"text": "A moment of a probability function taken about 0, (1) (2) The raw moments (sometimes also called \"crude moments\") can be expressed as terms of the central moments (i.e., those taken about the mean ) using the inverse binomial transform."}, {"text": "Moments are a set of statistical parameters to measure a distribution. Four moments are commonly used: 1st, Mean: the average."}, {"text": "Moments are a set of statistical parameters to measure a distribution. Four moments are commonly used: 1st, Mean: the average. 2d, Variance: Standard deviation is the square root of the variance: an indication of how closely the values are spread about the mean."}, {"text": "Moments are a set of statistical parameters to measure a distribution. Four moments are commonly used: 1st, Mean: the average. 2d, Variance: Standard deviation is the square root of the variance: an indication of how closely the values are spread about the mean."}, {"text": "Moments are a set of statistical parameters to measure a distribution. Four moments are commonly used: 1st, Mean: the average. 2d, Variance: Standard deviation is the square root of the variance: an indication of how closely the values are spread about the mean."}]}, {"question": "How many types of informed search methods are in artificial intelligence", "positive_ctxs": [{"text": "four types"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}, {"text": "In artificial intelligence and operations research, constraint satisfaction is the process of finding a solution to a set of constraints that impose conditions that the variables must satisfy.  Constraint propagation methods are also used in conjunction with search to make a given problem simpler to solve."}, {"text": "In artificial intelligence and operations research, constraint satisfaction is the process of finding a solution to a set of constraints that impose conditions that the variables must satisfy.  Constraint propagation methods are also used in conjunction with search to make a given problem simpler to solve."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "Explainable AI (XAI) refers to methods and techniques in the application of artificial intelligence technology (AI) such that the results of the solution can be understood by humans."}, {"text": "There are four types of artificial intelligence: reactive machines, limited memory, theory of mind and self-awareness."}, {"text": "There are four types of artificial intelligence: reactive machines, limited memory, theory of mind and self-awareness."}]}, {"question": "What is a variable answer", "positive_ctxs": [{"text": "Answer: A variable is a datatype whose value can not be fixed. It can be change based on other parameters. For example, Let X is a variable so that its value can be anything like 1,2,3 or a,p,r, or any word. It can not be fixed."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The answer to that is the Erlang distribution. The Gamma distribution is a generalization of that distribution using a continuous instead of a discrete parameter for the number of events."}, {"text": "The Monty Hall problem is one of those rare curiosities \u2013 a mathematical problem that has made the front pages of national news. Everyone now knows, or thinks they know, the answer but a realistic look at the problem demonstrates that the standard mathematician's answer is wrong."}, {"text": "The Monty Hall problem is one of those rare curiosities \u2013 a mathematical problem that has made the front pages of national news. Everyone now knows, or thinks they know, the answer but a realistic look at the problem demonstrates that the standard mathematician's answer is wrong."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}]}, {"question": "What is the formula for random sampling", "positive_ctxs": [{"text": "This approach works when the sample size is relatively large (greater than or equal to 30). Use the first or third formulas when the population size is known.How to Choose Sample Size for a Simple Random Sample.Sample statisticPopulation sizeSample sizeProportionUnknownn = [ ( z2 * p * q ) + ME2 ] / ( ME2 )3 more rows"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cluster sampling is best used when the clusters occur naturally in a population, when you don't have access to the entire population, and when the clusters are geographically convenient. However, cluster sampling is not as precise as simple random sampling or stratified random sampling."}, {"text": "Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "The formula for calculating a z-score is is z = (x-\u03bc)/\u03c3, where x is the raw score, \u03bc is the population mean, and \u03c3 is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation."}, {"text": "Non-probability sampling is a sampling technique where the odds of any member being selected for a sample cannot be calculated.  In addition, probability sampling involves random selection, while non-probability sampling does not\u2014it relies on the subjective judgement of the researcher."}, {"text": "In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).  In cluster sampling, the sampling unit is the whole cluster; Instead of sampling individuals from within each group, a researcher will study whole clusters."}, {"text": "The formula for calculating a z-score is is z = (x-\u03bc)/\u03c3, where x is the raw score, \u03bc is the population mean, and \u03c3 is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation. Figure 2."}, {"text": "Systematic random sampling: Systematic random sampling is a method to select samples at a particular preset interval. As a researcher, select a random starting point between 1 and the sampling interval.  (The number of elements in the population divided by the number of elements needed for the sample.)"}]}, {"question": "What is message passing algorithm", "positive_ctxs": [{"text": "Message passing algorithm which is an iterative decoding algorithm factorizes the global function of many variables into product of simpler local functions, whose arguments are the subset of variables. In order to visualize this factorization we use factor graph."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A linear model communication is one-way talking process But the disadvantage is that there is no feedback of the message by the receiver."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "For example RSA Encryption padding is randomized, ensuring that the same message encrypted multiple times looks different each time. It also avoids other weaknesses, such as encrypting the same message using different RSA keys leaking the message, or an attacker creating messages derived from some other ciphertexts."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "In cryptography, padding is any of a number of distinct practices which all include adding data to the beginning, middle, or end of a message prior to encryption."}]}, {"question": "Is thresholding a linear filter", "positive_ctxs": [{"text": "Nonlinear filters: Non-linear functions of signals. Examples: thresholding, image equalisation, or median filtering."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Convolution is a general purpose filter effect for images. \u25a1 Is a matrix applied to an image and a mathematical operation. comprised of integers. \u25a1 It works by determining the value of a central pixel by adding the. weighted values of all its neighbors together."}, {"text": "Convolution is a general purpose filter effect for images. \u25a1 Is a matrix applied to an image and a mathematical operation. comprised of integers. \u25a1 It works by determining the value of a central pixel by adding the. weighted values of all its neighbors together."}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them."}, {"text": "A blurring filter where you move over the image with a box filter (all the same values in the window) is an example of a linear filter. A non-linear filter is one that cannot be done with convolution or Fourier multiplication. A sliding median filter is a simple example of a non-linear filter."}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them. Sometimes this is incorrect."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise.  The Gaussian filter alone will blur edges and reduce contrast. The Median filter is a non-linear filter that is most commonly used as a simple way to reduce noise in an image."}, {"text": "1. The Canny edge detector is a linear filter because it uses the Gaussian filter to blur the image and then uses the linear filter to compute the gradient. Solution False. Though it does those things, it also has non-linear operations: thresholding, hysteresis, non-maximum suppression."}]}, {"question": "What type of thinking could be described as taking different directions", "positive_ctxs": [{"text": "Divergent Thinking. By contrast, divergent means \u201cdeveloping in different directions\u201d and so divergent thinking opens your mind in all directions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Machine learning can be described in many ways. Perhaps the most useful is as type of optimization.  This is done via what is known as an objective function, with \u201cobjective\u201d used in the sense of a goal. This function, taking data and model parameters as arguments, can be evaluated to return a number."}, {"text": "The probability of committing a type II error is equal to one minus the power of the test, also known as beta. The power of the test could be increased by increasing the sample size, which decreases the risk of committing a type II error."}, {"text": "Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components. Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting."}, {"text": "The binomial is a type of distribution that has two possible outcomes (the prefix \u201cbi\u201d means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail. A Binomial Distribution shows either (S)uccess or (F)ailure."}, {"text": "The binomial is a type of distribution that has two possible outcomes (the prefix \u201cbi\u201d means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail. A Binomial Distribution shows either (S)uccess or (F)ailure."}, {"text": "No, moment of inertia is a tensor quantity. Sometimes it behaves as scalar & sometimes as a vector. Sometimes it depends on the directions and sometimes depends on distribution of mass of the particles in the object."}, {"text": "A statistical hypothesis is a formal claim about a state of nature structured within the framework of a statistical model. For example, one could claim that the median time to failure from (acce]erated) electromigration of the chip population described in Section 6.1."}]}, {"question": "Why is ReLU a good activation function", "positive_ctxs": [{"text": "The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.  Due to this reason, during the backpropogation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "ReLU has become the darling activation function of the neural network world. Short for Rectified Linear Unit, it is a piecewise linear function that is defined to be 0 for all negative values of x and equal to a \u00d7 x otherwise, where a is a learnable parameter."}, {"text": "ReLU is an activation function, that nullifies negative neurons, and in its simplicity, it also aids computation speed. However, unlike ELU, it doesn't have a normalizing effect, so BatchNorm helps even better."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}]}, {"question": "What is the use of iterator", "positive_ctxs": [{"text": "An Iterator is an object that can be used to loop through collections, like ArrayList and HashSet. It is called an \"iterator\" because \"iterating\" is the technical term for looping. To use an Iterator, you must import it from the java."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In computer programming, an iterator is an object that enables a programmer to traverse a container, particularly lists. Various types of iterators are often provided via a container's interface.  An iterator is behaviorally similar to a database cursor."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}]}, {"question": "What are the advantages and disadvantages of Dimensional Analysis", "positive_ctxs": [{"text": "(i) The value of dimensionless constants cannot be determined by this method. (ii) This method cannot be applied to equations involving exponential and trigonometric functions. (iii) It cannot be applied to an equation involving more than three physical quantities."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Disadvantages include its \"black box\" nature, greater computational burden, proneness to overfitting, and the empirical nature of model development. An overview of the features of neural networks and logistic regression is presented, and the advantages and disadvantages of using this modeling technique are discussed."}, {"text": "Major advantages include its simplicity and lack of bias. Among the disadvantages are difficulty gaining access to a list of a larger population, time, costs, and that bias can still occur under certain circumstances."}, {"text": "Let's discuss some advantages and disadvantages of Linear Regression. Logistic regression is easier to implement, interpret, and very efficient to train. If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting."}, {"text": "1:009:32Suggested clip \u00b7 114 secondsStoichiometry Using Dimensional Analysis - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}, {"text": "0:278:54Suggested clip \u00b7 121 secondsDeriving Engineering Equations Using Dimensional Analysis YouTubeStart of suggested clipEnd of suggested clip"}, {"text": "2:528:15Suggested clip \u00b7 90 secondsUnit Conversion Using Dimensional Analysis Tutorial (Factor Label YouTubeStart of suggested clipEnd of suggested clip"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is batch normalization layer", "positive_ctxs": [{"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling.  Others sustain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks."}, {"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling."}, {"text": "To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "Batch normalization is a technique that can improve the learning rate of a neural network. It does so by minimizing internal covariate shift which is essentially the phenomenon of each layer's input distribution changing as the parameters of the layer above it change during training."}, {"text": "Batch normalization works best after the activation function, and here or here is why: it was developed to prevent internal covariate shift. Internal covariate shift occurs when the distribution of the activations of a layer shifts significantly throughout training."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}]}, {"question": "What are the two conditions for omitted variable bias", "positive_ctxs": [{"text": "For omitted variable bias to occur, the omitted variable \u201dZ\u201d must satisfy two conditions: The omitted variable is correlated with the included regressor (i.e. The omitted variable is a determinant of the dependent variable (i.e. expensive and the alternative funding is loan or scholarship which is harder to acquire."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Intuitively, omitted variable bias occurs when the independent variable (the X) that we have included in our model picks up the effect of some other variable that we have omitted from the model. The reason for the bias is that we are attributing effects to X that should be attributed to the omitted variable."}, {"text": "If the correlation between education and unobserved ability is positive, omitted variables bias will occur in an upward direction. Conversely, if the correlation between an explanatory variable and an unobserved relevant variable is negative, omitted variables bias will occur in a downward direction."}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "Fixed effects models remove omitted variable bias by measuring changes within groups across time, usually by including dummy variables for the missing or unknown characteristics."}, {"text": "Omitted variable bias occurs when a regression model leaves out relevant independent variables, which are known as confounding variables. This condition forces the model to attribute the effects of omitted variables to variables that are in the model, which biases the coefficient estimates."}, {"text": "In the development of the probability function for a discrete random variable, two conditions must be satisfied: (1) f(x) must be nonnegative for each value of the random variable, and (2) the sum of the probabilities for each value of the random variable must equal one."}, {"text": "In the development of the probability function for a discrete random variable, two conditions must be satisfied: (1) f(x) must be nonnegative for each value of the random variable, and (2) the sum of the probabilities for each value of the random variable must equal one."}]}, {"question": "How do you explain clustering", "positive_ctxs": [{"text": "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "If you have outliers, the best way is to use a clustering algorithm that can handle them. For example DBSCAN clustering is robust against outliers when you choose minpts large enough. Don't use k-means: the squared error approach is sensitive to outliers. But there are variants such as k-means-- for handling outliers."}, {"text": "False negatives \u2014 that is, a test that says you don't have the virus when you actually do have the virus \u2014 may occur."}]}, {"question": "What are the features of a binomial distribution", "positive_ctxs": [{"text": "1: The number of observations n is fixed. 2: Each observation is independent. 3: Each observation represents one of two outcomes (\"success\" or \"failure\"). 4: The probability of \"success\" p is the same for each outcome."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution.  Suppose we flip a coin repeatedly and count the number of heads (successes)."}, {"text": "Remember that the Poisson distribution assumes that the mean and variance are the same.  The negative binomial distribution has one parameter more than the Poisson regression that adjusts the variance independently from the mean. In fact, the Poisson distribution is a special case of the negative binomial distribution."}, {"text": "The binomial distribution model allows us to compute the probability of observing a specified number of \"successes\" when the process is repeated a specific number of times (e.g., in a set of patients) and the outcome for a given patient is either a success or a failure.  The binomial equation also uses factorials."}, {"text": "The binomial distribution is a probability distribution that summarizes the likelihood that a value will take one of two independent values under a given set of parameters or assumptions."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The geometric distribution describes the probability of \"x trials are made before a success\", and the negative binomial distribution describes that of \"x trials are made before r successes are obtained\", where r is fixed. So you see that the latter is a particular case of the former, namely, when r=1."}]}, {"question": "Is clustering supervised or unsupervised How do you classify it", "positive_ctxs": [{"text": "Clustering is considered unsupervised learning, because there's no labeled target variable in clustering. Clustering algorithms try to, well, cluster data points into similar groups (or\u2026 clusters) based on different characteristics of the data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In the absence of a class label, clustering analysis is also called unsupervised learning, as opposed to supervised learning that includes classification and regression. Accordingly, approaches to clustering analysis are typically quite different from supervised learning."}, {"text": "k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}, {"text": "They are often confused with each other. The 'K' in K-Means Clustering has nothing to do with the 'K' in KNN algorithm. k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}, {"text": "They are often confused with each other. The 'K' in K-Means Clustering has nothing to do with the 'K' in KNN algorithm. k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}, {"text": "KNN represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points, while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters."}, {"text": "KNN represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points, while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters."}, {"text": "You can use an unsupervised learning algorithm (like clustering) to create your training data for the supervised learning algorithm but you cannot simply convert an unsupervised learning algorithm into a supervised one."}]}, {"question": "What is Multicollinearity and why is it a problem", "positive_ctxs": [{"text": "Multicollinearity exists whenever an independent variable is highly correlated with one or more of the other independent variables in a multiple regression equation. Multicollinearity is a problem because it undermines the statistical significance of an independent variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant."}, {"text": "Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant."}, {"text": "Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant."}, {"text": "Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results."}, {"text": "Collinearity is a linear association between two predictors. Multicollinearity is a situation where two or more predictors are highly linearly related.  But, correlation 'among the predictors' is a problem to be rectified to be able to come up with a reliable model."}, {"text": "If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier."}, {"text": "An example of a nonlinear classifier is kNN.  If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier."}]}, {"question": "What are the methods of classification", "positive_ctxs": [{"text": "Sequence classification methods can be organized into three categories: (1) feature-based classification, which transforms a sequence into a feature vector and then applies conventional classification methods; (2) sequence distance\u2013based classification, where the distance function that measures the similarity between"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two main methods for tackling a multi-label classification problem: problem transformation methods and algorithm adaptation methods. Problem transformation methods transform the multi-label problem into a set of binary classification problems, which can then be handled using single-class classifiers."}, {"text": "Definition. Multivariate statistics refers to methods that examine the simultaneous effect of multiple variables. Traditional classification of multivariate statistical methods suggested by Kendall is based on the concept of dependency between variables (Kendall 1957)."}, {"text": "Several methods could be used to measure the performance of the classification model. Some of them are log-loss, AUC, confusion matrix, and precision-recall. Accuracy is the measure of correct prediction of the classifier compared to the overall data points."}, {"text": "Adaptive learning rate methods are an optimization of gradient descent methods with the goal of minimizing the objective function of a network by using the gradient of the function and the parameters of the network."}, {"text": "Data Clustering Basics. The classification of observations into groups requires some methods for computing the distance or the (dis)similarity between each pair of observations. The result of this computation is known as a dissimilarity or distance matrix."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks."}]}, {"question": "What does binning data mean", "positive_ctxs": [{"text": "Data binning is the process of grouping individual data values into specific bins or groups according to defined criteria. For example, census data can be binned into defined age groups."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "The optimal binning is the optimal discretization of a variable into bins given a discrete or continuous numeric target."}]}, {"question": "How is AI used in different recommender systems", "positive_ctxs": [{"text": "Due to AI, recommendation engines make quick and to-the-point recommendations tailored to each customer's needs and preferences. With the usage of artificial intelligence, online searching is improving as well, since it makes recommendations related to the user's visual preferences rather than product descriptions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Content-based recommendation systems uses their knowledge about each product to recommend new ones. Recommendations are based on attributes of the item. Content-based recommender systems work well when descriptive data on the content is provided beforehand. \u201cSimilarity\u201d is measured against product attributes."}, {"text": "Offline evaluations test the effectiveness of recommender system algorithms on a certain dataset. Online evaluation attempts to evaluate recommender systems by a method called A/B testing where a part of users are served by recommender system A and the another part of users by recommender system B."}, {"text": "Difference between rule-based AI and machine learning Machine learning systems are probabilistic and rule-based AI models are deterministic.  Machine learning systems require more data as compared to rule-based models. Rule-based AI models can operate with simple basic information and data."}, {"text": "CNNs can be used in tons of applications from image and video recognition, image classification, and recommender systems to natural language processing and medical image analysis.  This is the way that a CNN works! Image by NatWhitePhotography on Pixabay. CNNs have an input layer, and output layer, and hidden layers."}, {"text": "Java, Python, Lisp, Prolog, and C++ are major AI programming language used for artificial intelligence capable of satisfying different needs in the development and designing of different software.  It answers the question, 'what is the language used for artificial intelligence?"}, {"text": "The term cognitive computing is typically used to describe AI systems that aim to simulate human thought.  A number of AI technologies are required for a computer system to build cognitive models that mimic human thought processes, including machine learning, deep learning, neural networks, NLP and sentiment analysis."}, {"text": "The AI is programmed to do something devastating: Autonomous weapons are artificial intelligence systems that are programmed to kill. In the hands of the wrong person, these weapons could easily cause mass casualties. Moreover, an AI arms race could inadvertently lead to an AI war that also results in mass casualties."}]}, {"question": "Is an estimated standard deviation of a sampling distribution", "positive_ctxs": [{"text": "The standard error (SE) of a statistic (usually an estimate of a parameter) is the standard deviation of its sampling distribution or an estimate of that standard deviation.  Mathematically, the variance of the sampling distribution obtained is equal to the variance of the population divided by the sample size."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, the t-statistic is the ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.  For example, the T-statistic is used in estimating the population mean from a sampling distribution of sample means if the population standard deviation is unknown."}, {"text": "The T distribution is a continuous probability distribution of the z-score when the estimated standard deviation is used in the denominator rather than the true standard deviation.  T-tests are used in statistics to estimate significance."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean."}, {"text": "A standard deviation is a measure of variability for a distribution of scores in a single sample or in a population of scores. A standard error is the standard deviation in a distribution of means of all possible samples of a given size from a particular population of individual scores."}, {"text": "A normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution. Areas of the normal distribution are often represented by tables of the standard normal distribution.  For example, a Z of -2.5 represents a value 2.5 standard deviations below the mean."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "Suppose it is of interest to estimate the population mean, \u03bc, for a quantitative variable. Data collected from a simple random sample can be used to compute the sample mean, x\u0304, where the value of x\u0304 provides a point estimate of \u03bc.  The standard deviation of a sampling distribution is called the standard error."}]}, {"question": "How do you calculate sample size", "positive_ctxs": [{"text": "How to Find a Sample Size Given a Confidence Interval and Width (unknown population standard deviation)za/2: Divide the confidence interval by two, and look that area up in the z-table: .95 / 2 = 0.475.  E (margin of error): Divide the given width by 2. 6% / 2.  : use the given percentage. 41% = 0.41.  : subtract. from 1."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "When the sample size is small, we use the t-distribution to calculate the p-value. In this case, we calculate the degrees of freedom, df= n-1. We then use df, along with the test statistic, to calculate the p-value. If the sample is greater than 30 (n>30), we consider this a large sample size."}, {"text": "If you increase your sample size you increase the precision of your estimates, which means that, for any given estimate / size of effect, the greater the sample size the more \u201cstatistically significant\u201d the result will be."}, {"text": "As explained above, the shape of the t-distribution is affected by sample size.  As the sample size increases, so do degrees of freedom. When degrees of freedom are infinite, the t-distribution is identical to the normal distribution. As sample size increases, the sample more closely approximates the population."}]}, {"question": "What are some of the statistical methods that are useful for data analyst", "positive_ctxs": [{"text": "5 Most Important Methods For Statistical Data AnalysisMean. The arithmetic mean, more commonly known as \u201cthe average,\u201d is the sum of a list of numbers divided by the number of items on the list.  Standard Deviation.  Regression.  Sample Size Determination.  Hypothesis Testing."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cluster analysis divides data into groups (clusters) that are meaningful, useful, or both. If meaningful groups are the goal, then the clusters should capture the natural structure of the data. In some cases, however, cluster analysis is only a useful starting point for other purposes, such as data summarization."}, {"text": "A sampling error is a statistical error that occurs when an analyst does not select a sample that represents the entire population of data and the results found in the sample do not represent the results that would be obtained from the entire population."}, {"text": "A sampling error is a statistical error that occurs when an analyst does not select a sample that represents the entire population of data and the results found in the sample do not represent the results that would be obtained from the entire population."}, {"text": "A sampling error is a statistical error that occurs when an analyst does not select a sample that represents the entire population of data and the results found in the sample do not represent the results that would be obtained from the entire population."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar."}, {"text": "In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar."}]}, {"question": "How is implicit bias reduced", "positive_ctxs": [{"text": "Introspection: Explore and identify your own prejudices by taking implicit association tests or through other means of self-analysis. Mindfulness: Since you're more likely to give in to your biases when you're under pressure, practice ways to reduce stress and increase mindfulness, such as focused breathing."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Also known as implicit social cognition, implicit bias refers to the attitudes or stereotypes that affect our understanding, actions, and decisions in an unconscious manner."}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "In statistics and machine learning, the bias\u2013variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters.  The bias error is an error from erroneous assumptions in the learning algorithm."}, {"text": "In-group bias is notoriously difficult to avoid completely, but research shows it can be reduced through interaction with other groups, and by giving people an incentive to act in an unbiased manner."}, {"text": "In-group bias is notoriously difficult to avoid completely, but research shows it can be reduced through interaction with other groups, and by giving people an incentive to act in an unbiased manner."}, {"text": "In statistics and machine learning, the bias\u2013variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters."}, {"text": "So, How Does a Neural Network Work Exactly?Information is fed into the input layer which transfers it to the hidden layer.The interconnections between the two layers assign weights to each input randomly.A bias added to every input after weights are multiplied with them individually.More items\u2022"}]}, {"question": "What is negative sampling in Word2Vec", "positive_ctxs": [{"text": "Word2Vec slightly customizes the process and calls it negative sampling. In Word2Vec, the words for the negative samples (used for the corrupted pairs) are drawn from a specially designed distribution, which favours less frequent words to be drawn more often."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Negative sampling is a technique used to train machine learning models that generally have several order of magnitudes more negative observations compared to positive ones. And in most cases, these negative observations are not given to us explicitly and instead, must be generated somehow."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}]}, {"question": "How do you calculate estimation in statistics", "positive_ctxs": [{"text": "Estimation in StatisticsPoint estimate. A point estimate of a population parameter is a single value of a statistic. For example, the sample mean x is a point estimate of the population mean \u03bc.  Interval estimate. An interval estimate is defined by two numbers, between which a population parameter is said to lie."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Statistical inference consists in the use of statistics to draw conclusions about some unknown aspect of a population based on a random sample from that population.  Point estimation is discussed in the statistics section of the encyclopedia."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Inferential statistics lets you draw conclusions about populations by using small samples. Consequently, inferential statistics provide enormous benefits because typically you can't measure an entire population."}, {"text": "Generally a cosine similarity between two documents is used as a similarity measure of documents. In Java, you can use Lucene (if your collection is pretty large) or LingPipe to do this. The basic concept would be to count the terms in every document and calculate the dot product of the term vectors."}]}, {"question": "What is logistic regression simple explanation", "positive_ctxs": [{"text": "Logistic Regression, also known as Logit Regression or Logit Model, is a mathematical model used in statistics to estimate (guess) the probability of an event occurring having been given some previous data. Logistic Regression works with binary data, where either the event happens (1) or the event does not happen (0)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "The logit is a transformation. Logistic regression is a regression model. The logit transformation transforms a line to a logistic curve. Logistic regression fits a logistic curve to set of data where the dependent variable can only take the values 0 and 1."}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}, {"text": "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)\u2208{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits."}, {"text": "In biostatistics, logistic regression is often used when the outcome variable is dichotomous. You also list \"SPSS\" as a topic. A Google search on <SPSS logistic regression example> will no doubt yield many hits, including the UCLA \"textbook examples\"."}]}, {"question": "How do you select best features in regression", "positive_ctxs": [{"text": "Supervised: Use the target variable (e.g. remove irrelevant variables).Wrapper: Search for well-performing subsets of features. RFE.Filter: Select subsets of features based on their relationship with the target. Feature Importance Methods.Intrinsic: Algorithms that perform automatic feature selection during training."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "If you are working on a classification problem, the best score is 100% accuracy. If you are working on a regression problem, the best score is 0.0 error. These scores are an impossible to achieve upper/lower bound."}, {"text": "If you are working on a classification problem, the best score is 100% accuracy. If you are working on a regression problem, the best score is 0.0 error. These scores are an impossible to achieve upper/lower bound."}, {"text": "If you are working on a classification problem, the best score is 100% accuracy. If you are working on a regression problem, the best score is 0.0 error. These scores are an impossible to achieve upper/lower bound."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How to Annotate an Image in WordIn the \u201cIllustrations\u201d section, click \u201cPictures\u201d.  The cursor changes to a big \u201c+\u201d symbol.  Right-click on the callout and select \u201cFill\u201d from the popup box above the popup menu.  Once you've moved the callout, you may need to reposition the callout arrow to point where you want."}, {"text": "If you are working on a classification problem, the best score is 100% accuracy. If you are working on a regression problem, the best score is 0.0 error. These scores are an impossible to achieve upper/lower bound. All predictive modeling problems have prediction error."}]}, {"question": "Can you Factorise matrices", "positive_ctxs": [{"text": "In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Yes. We Can Always Use The Normal Distribution To Approximate The Binomial Distribution."}, {"text": "Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. Confusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives."}, {"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "SVD gives you the whole nine-yard of diagonalizing a matrix into special matrices that are easy to manipulate and to analyze. It lay down the foundation to untangle data into independent components. PCA skips less significant components."}, {"text": "A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes. When writing a TensorFlow program, the main object you manipulate and pass around is the tf$Tensor ."}, {"text": "The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative. Using the original matrix (A), NMF will give you two matrices (W and H)."}]}, {"question": "What is the difference between T distribution and Z distribution", "positive_ctxs": [{"text": "The standard normal or z-distribution assumes that you know the population standard deviation. The t-distribution is based on the sample standard deviation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The parameters of the distribution are m and s2, where m is the mean (expectation) of the distribution and s2 is the variance. We write X ~ N(m, s2) to mean that the random variable X has a normal distribution with parameters m and s2. If Z ~ N(0, 1), then Z is said to follow a standard normal distribution."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The mean for the standard normal distribution is zero, and the standard deviation is one. The transformation z=x\u2212\u03bc\u03c3 z = x \u2212 \u03bc \u03c3 produces the distribution Z ~ N(0, 1)."}]}, {"question": "What is a simple explanation of how artificial neural networks work 1", "positive_ctxs": [{"text": "An artificial neural network is an attempt to simulate the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a humanlike manner. ANNs are created by programming regular computers to behave as though they are interconnected brain cells."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A perceptron is a simple model of a biological neuron in an artificial neural network. Perceptron is also the name of an early algorithm for supervised learning of binary classifiers."}, {"text": "Neural networks are often compared to decision trees because both methods can model data that has nonlinear relationships between variables, and both can handle interactions between variables.  A neural network is more of a \u201cblack box\u201d that delivers results without an explanation of how the results were derived."}, {"text": "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior.  Both classes of networks exhibit temporal dynamic behavior."}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "The ways in which they function Another fundamental difference between traditional computers and artificial neural networks is the way in which they function. While computers function logically with a set of rules and calculations, artificial neural networks can function via images, pictures, and concepts."}, {"text": "word2vec itself is a simple bi-layered neural network architecture, it turns text into meaningful vectors form that deeper networks can understand. In other words the out put of simple neural word2vec model is used as input for Deep Networks."}]}, {"question": "What is decision tree explain with diagram", "positive_ctxs": [{"text": "A decision tree is a flowchart-like diagram that shows the various outcomes from a series of decisions. It can be used as a decision-making tool, for research analysis, or for planning strategy. A primary advantage for using a decision tree is that it is easy to follow and understand."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes"}, {"text": "Decision tree learning is a supervised machine learning technique for inducing a decision tree from training data. A decision tree (also referred to as a classification tree or a reduction tree) is a predictive model which is a mapping from observations about an item to conclusions about its target value."}, {"text": "Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter.  An example of a decision tree can be explained using above binary tree."}, {"text": "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value."}]}, {"question": "Why do we use standard deviation over variance", "positive_ctxs": [{"text": "Standard deviation looks at how spread out a group of numbers is from the mean, by looking at the square root of the variance. The variance measures the average degree to which each point differs from the mean\u2014the average of all data points."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "On this site, we use the normal distribution when the population standard deviation is known and the sample size is large. We might use either distribution when standard deviation is unknown and the sample size is very large."}, {"text": "Introduction. The standard deviation is a measure of the spread of scores within a set of data. Usually, we are interested in the standard deviation of a population. However, as we are often presented with data from a sample only, we can estimate the population standard deviation from a sample standard deviation."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "s2 (sample variance) is the best point estimate for population variance o2. s (sample standard deviation) is the best point estimate for the population standard deviation o."}, {"text": "The standard deviation of the sample mean \u02c9X that we have just computed is the standard deviation of the population divided by the square root of the sample size: \u221a10=\u221a20/\u221a2."}]}, {"question": "What is the limitations behind rule generation in association rule mining", "positive_ctxs": [{"text": "Some of the main drawbacks of association rule algorithms in e-learning are: the used algorithms have too many parameters for somebody non expert in data mining and the obtained rules are far too many, most of them non-interesting and with low comprehensibility."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The confidence of an association rule is a percentage value that shows how frequently the rule head occurs among all the groups containing the rule body.  Thus, the confidence of a rule is the percentage equivalent of m/n, where the values are: m. The number of groups containing the joined rule head and rule body."}, {"text": "Applications of association rule mining are stock analysis, web log mining, medical diagnosis, customer market analysis bioinformatics etc. In past, many algorithms were developed by researchers for Boolean and Fuzzy association rule mining such as Apriori, FP-tree, Fuzzy FP-tree etc."}, {"text": "As opposed to decision tree and rule set induction, which result in classification models, association rule learning is an unsupervised learning method, with no class labels assigned to the examples.  This would then be a Supervised Learning task , where the NN learns from pre-calssified examples."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm. #"}, {"text": "An artificial neural network's learning rule or learning process is a method, mathematical logic or algorithm which improves the network's performance and/or training time. Usually, this rule is applied repeatedly over the network."}]}, {"question": "How long do stripe tokens last", "positive_ctxs": [{"text": "1 Answer. The card token is valid for a few minutes (usually up to 10). What Stripe recommends in that case is to use the token now to create a customer via the API first to save its card and then let your background job handle the charge part after the fact."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If you don't have enough time to read through the entire post, the following hits on the key components: Bag-of-words: How to break up long text into individual words. Filtering: Different approaches to remove uninformative words. Bag of n-grams: Retain some context by breaking long text into sequences of words."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Task scheduler last run result 0x1 mostly cause by privilege issue. For example, user do not have sufficient privilege to execute the task at the specified location or the process unable to locate the file for some reason."}, {"text": "The length of time between each transit is the planet's \"orbital period\", or the length of a year on that particular planet. Not all planets have years as long as a year on the Earth! Some planets discovered by Kepler orbit around their stars so quickly that their years only last about four hours!"}, {"text": "Naive Bayes Classifier example by hand and how to do in Scikit-Learn, You can use any kind of predictor in a naive Bayes classifier, as long as you can specify a discriminative linear models take a mixture of categorical and continuous predictors."}, {"text": "It is easier to reject the null hypothesis with a one-tailed than with a two-tailed test as long as the effect is in the specified direction. Therefore, one-tailed tests have lower Type II error rates and more power than do two-tailed tests."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}]}, {"question": "What is Slo in Sre", "positive_ctxs": [{"text": "Service-Level Objective (SLO) Availability, in SRE terms, defines whether a system is able to fulfill its intended function at a point in time."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is the meaning of latent variable", "positive_ctxs": [{"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables.  Because measurement error is by definition unique variance, it is not captured in the latent variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "On a technical note, estimation of a latent variable is done by analyzing the variance and covariance of the indicators. The measurement model of a latent variable with effect indicators is the set of relationships (modeled as equations) in which the latent variable is set as the predictor of the indicators."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "The standard solution that psychologists take to measuring latent variables is to use a series of questions that are all designed to measure the latent variable. This is known as a multi-item scale, where an \u201citem\u201d is a question, and a \u201cscale\u201d is the resulting estimate of the latent variable."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The Poisson distribution is a discrete function, meaning that the event can only be measured as occurring or not as occurring, meaning the variable can only be measured in whole numbers. Fractional occurrences of the event are not a part of the model. it was named after French mathematician Sim\u00e9on Denis Poisson."}, {"text": "The parameters are the ones that we specify a prior distribution for. The latent variables are usually the ones that we describe using a conditional distribution of the latent variable given the parameters."}]}, {"question": "Is boosting an ensemble method", "positive_ctxs": [{"text": "Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.  Explicit regression gradient boosting algorithms were subsequently developed by Jerome H."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.  Explicit regression gradient boosting algorithms were subsequently developed by Jerome H."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. ("}, {"text": "Boosting is an ensemble modeling technique which attempts to build a strong classifier from the number of weak classifiers. It is done building a model by using weak models in series.  AdaBoost was the first really successful boosting algorithm developed for the purpose of binary classification."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}]}, {"question": "What is multi label image classification", "positive_ctxs": [{"text": "Multi-label classification is a type of classification in which an object can be categorized into more than one class. For example, In the above dataset, we will classify a picture as the image of a dog or cat and also classify the same image based on the breed of the dog or cat."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "For multi class classification using SVM; It is NOT (one vs one) and NOT (one vs REST). Instead learn a two-class classifier where the feature vector is (x, y) where x is data and y is the correct label associated with the data."}, {"text": "Fundamentally, classification is about predicting a label and regression is about predicting a quantity.  That classification is the problem of predicting a discrete class label output for an example. That regression is the problem of predicting a continuous quantity output for an example."}, {"text": "The role of a fully connected layer in a CNN architecture The objective of a fully connected layer is to take the results of the convolution/pooling process and use them to classify the image into a label (in a simple classification example)."}, {"text": "Difference between multi-class classification & multi-label classification is that in multi-class problems the classes are mutually exclusive, whereas for multi-label problems each label represents a different classification task, but the tasks are somehow related."}, {"text": "Classification Algorithms in Data Mining. It is one of the Data Mining. That is used to analyze a given data set and takes each instance of it. It assigns this instance to a particular class.  So classification is the process to assign class label from a data set whose class label is unknown."}, {"text": "Convolutional Neural Networks (CNNs) is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough."}, {"text": "Convolutional Neural Networks (CNNs) is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough."}]}, {"question": "What is the relation between convolution and correlation", "positive_ctxs": [{"text": "Theoretically, convolution are linear operations on the signal or signal modifiers, whereas correlation is a measure of similarity between two signals. As you rightly mentioned, the basic difference between convolution and correlation is that the convolution process rotates the matrix by 180 degrees."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Theoretically, convolution are linear operations on the signal or signal modifiers, whereas correlation is a measure of similarity between two signals.  Also, correlation or auto-correlation is the measure of similarity of signal with itself which has a different time lag between them."}, {"text": "If correlation =1, then it shows there exists a directly proportional relationship between the two variables and if the same is - 1 then it denotes that there exists a inversely proportional relation between the two two variables and if we fit a regression line for the same then we'll get a straight line having"}, {"text": "Textual entailment (TE) in natural language processing is a directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "Convolution is used in the mathematics of many fields, such as probability and statistics. In linear systems, convolution is used to describe the relationship between three signals of interest: the input signal, the impulse response, and the output signal."}]}, {"question": "What is the acceptable value of skewness", "positive_ctxs": [{"text": "As a general rule of thumb: If skewness is less than -1 or greater than 1, the distribution is highly skewed. If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed. If skewness is between -0.5 and 0.5, the distribution is approximately symmetric."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined."}, {"text": "If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical."}, {"text": "The consistency of the sampling distribution is dependent on the sample size not on the distribution of the population. As the sample size decreases the absolute value of the skewness and kurtosis of the sampling distribution increases. This sample size relationship is expressed in the central limit theorem."}, {"text": "Properties of F-DistributionThe F-distribution is positively skewed and with the increase in the degrees of freedom \u03bd1 and \u03bd2, its skewness decreases.The value of the F-distribution is always positive, or zero since the variances are the square of the deviations and hence cannot assume negative values.More items"}, {"text": "If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical.  If skewness is less than \u22121 or greater than +1, the distribution is highly skewed."}, {"text": "Moments are used to find the central tendency(In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution), dispersion, skewness and kurtosis( the sharpness of the peak of a frequency-distribution curve).."}, {"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}]}, {"question": "What is the difference between linear and nonlinear filters", "positive_ctxs": [{"text": "Linear filtering is the filtering method in which the value of output pixel is linear combinations of the neighbouring input pixels.  A non-linear filtering is one that cannot be done with convolution or Fourier multiplication. A sliding median filter is a simple example of a non-linear filter."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In signal processing, a nonlinear (or non-linear) filter is a filter whose output is not a linear function of its input.  Like linear filters, nonlinear filters may be shift invariant or not. Non-linear filters have many applications, especially in the removal of certain types of noise that are not additive."}, {"text": "Linear models, generalized linear models, and nonlinear models are examples of parametric regression models because we know the function that describes the relationship between the response and explanatory variables.  If the relationship is unknown and nonlinear, nonparametric regression models should be used."}, {"text": "Linear means something related to a line.  A non-linear equation is such which does not form a straight line. It looks like a curve in a graph and has a variable slope value. The major difference between linear and nonlinear equations is given here for the students to understand it in a more natural way."}, {"text": "An example of a nonlinear classifier is kNN.  If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier."}, {"text": "Two classes of digital filters are Finite Impulse Response (FIR) and Infinite Impulse Response (IIR). The term 'Impulse Response' refers to the appearance of the filter in the time domain.  The mathematical difference between the IIR and FIR implementation is that the IIR filter uses some of the filter output as input."}, {"text": "PCA is designed to model linear variabilities in high-dimensional data. However, many high dimensional data sets have a nonlinear nature. In these cases the high-dimensional data lie on or near a nonlinear manifold (not a linear subspace) and therefore PCA can not model the variability of the data correctly."}]}, {"question": "What increases the margin of error", "positive_ctxs": [{"text": "The margin of error increases as the level of confidence increases because the larger the expected proportion of intervals that will contain the\u200b parameter, the larger the margin of error.  The larger the level of confidence\u200b is, the larger number of intervals that will contain the parameter."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The relationship between margin of error and sample size is simple: As the sample size increases, the margin of error decreases.  If you think about it, it makes sense that the more information you have, the more accurate your results are going to be (in other words, the smaller your margin of error will get)."}, {"text": "The margin of error is a statistic expressing the amount of random sampling error in the results of a survey. The larger the margin of error, the less confidence one should have that a poll result would reflect the result of a survey of the entire population."}, {"text": "An error term represents the margin of error within a statistical model; it refers to the sum of the deviations within the regression line, which provides an explanation for the difference between the theoretical value of the model and the actual observed results."}, {"text": "The Slovin's Formula is given as follows: n = N/(1+Ne2), where n is the sample size, N is the population size and e is the margin of error to be decided by the researcher."}, {"text": "n =(z\u03b12p\u2032q\u2032EBP2 ( z \u03b1 2 p \u2032 q \u2032 E B P 2 provides the number of participants needed to estimate the population proportion with confidence 1 \u2013 \u03b1 and margin of error EBP."}, {"text": "A P value is also affected by sample size and the magnitude of effect. Generally the larger the sample size, the more likely a study will find a significant relationship if one exists. As the sample size increases the impact of random error is reduced."}, {"text": "It maximizes the margin of the hyperplane. This is the best hyperplane because it reduces the generalization error the most. If we add new data, the Maximum Margin Classifier is the best hyperplane to correctly classify the new data. The Maximum Margin Classifier is our first SVM."}]}, {"question": "How do you prove covariance stationary", "positive_ctxs": [{"text": "A sequence of random variables is covariance stationary if all the terms of the sequence have the same mean, and if the covariance between any two terms of the sequence depends only on the relative positions of the two terms, that is, on how far apart they are located from each other, and not on their absolute position"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to find accuracy of ARIMA model?Problem description: Prediction on CPU utilization.  Step 1: From Elasticsearch I collected 1000 observations and exported on Python.Step 2: Plotted the data and checked whether data is stationary or not.Step 3: Used log to convert the data into stationary form.Step 4: Done DF test, ACF and PACF.More items\u2022"}, {"text": "A stationary time series is one whose properties do not depend on the time at which the series is observed. 14. Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Stationarity. A common assumption in many time series techniques is that the data are stationary. A stationary process has the property that the mean, variance and autocorrelation structure do not change over time."}, {"text": "Stationary Time Series Time series are stationary if they do not have trend or seasonal effects. Summary statistics calculated on the time series are consistent over time, like the mean or the variance of the observations."}, {"text": "A covariance matrix is a square matrix which gives two types of information. If you are looking at the population covariance matrix then. each diagonal element is the variance of the corresponding random variable. each off-diagonal element is the covariance of the corresponding pair of random variables."}]}, {"question": "How do you explain central tendency", "positive_ctxs": [{"text": "A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data.  The mean (often called the average) is most likely the measure of central tendency that you are most familiar with, but there are others, such as the median and the mode."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution.  The most common measures of central tendency are the arithmetic mean, the median, and the mode."}, {"text": "A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data.  The mean (often called the average) is most likely the measure of central tendency that you are most familiar with, but there are others, such as the median and the mode."}, {"text": "Moments are used to find the central tendency(In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution), dispersion, skewness and kurtosis( the sharpness of the peak of a frequency-distribution curve).."}, {"text": "These measures indicate where most values in a distribution fall and are also referred to as the central location of a distribution. You can think of it as the tendency of data to cluster around a middle value. In statistics, the three most common measures of central tendency are the mean, median, and mode."}, {"text": "The median is usually preferred in these situations because the value of the mean can be distorted by the outliers. However, it will depend on how influential the outliers are. If they do not significantly distort the mean, using the mean as the measure of central tendency will usually be preferred."}, {"text": "Deviation means change or distance.  Hence standard deviation is a measure of change or the distance from a measure of central tendency - which is normally the mean. Hence, standard deviation is different from a measure of central tendency."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}]}, {"question": "Is there such a thing as the law of averages", "positive_ctxs": [{"text": "The law of averages is the commonly held belief that a particular outcome or event will over certain periods of time occur at a frequency that is similar to its probability. Depending on context or application it can be considered a valid common-sense observation or a misunderstanding of probability."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The law of averages is often mistaken by many people as the law of large numbers, but there is a big difference. The law of averages is a spurious belief that any deviation in expected probability will have to average out in a small sample of consecutive experiments, but this is not necessarily true."}, {"text": "The law of averages is a false belief, sometimes known as the 'gambler's fallacy,' that is derived from the law of large numbers.  The law of averages is a misconception that probability occurs with a small number of consecutive experiments so they will certainly have to 'average out' sooner rather than later."}, {"text": "The law of averages is not a mathematical principle, whereas the law of large numbers is. In probability theory, the law of large numbers is a theorem that describes the result of performing the same experiment a large number of times."}, {"text": "The law of averages is not a mathematical principle, whereas the law of large numbers is.  According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed."}, {"text": "The law of averages is sometimes known as \u201cGambler's Fallacy. \u201d It evokes the idea that an event is \u201cdue\u201d to happen.  The law of averages says it's due to land on black! \u201d Of course, the wheel has no memory and its probabilities do not change according to past results."}, {"text": "Functions of Random Variables One law is called the \u201cweak\u201d law of large numbers, and the other is called the \u201cstrong\u201d law of large numbers. The weak law describes how a sequence of probabilities converges, and the strong law describes how a sequence of random variables behaves in the limit."}, {"text": "The law of averages is the commonly held belief that a particular outcome or event will over certain periods of time occur at a frequency that is similar to its probability. Depending on context or application it can be considered a valid common-sense observation or a misunderstanding of probability."}]}, {"question": "What is the difference between time series data and cross sectional data", "positive_ctxs": [{"text": "Time-series data is a set of observations collected at usually discrete and equally spaced time intervals.  Cross-sectional data are observations that come from different individuals or groups at a single point in time."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The key difference between time series and panel data is that time series focuses on a single individual at multiple time intervals while panel data (or longitudinal data) focuses on multiple individuals at multiple time intervals.  Fields such as Econometrics and statistics relies on data."}, {"text": "No. Stock return is not always stationary.  Using non-stationary time series data in financial models produces unreliable and spurious results and leads to poor understanding and forecasting. The solution to the problem is to transform the time series data so that it becomes stationary."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "When Longitudinal data looks like a time series is when we measure the same thing over time. The big difference is that in a time series we can measure the overall change in the measurement over time (or by group) while in a longitudinal analysis you actually have the measurement of change at the individual level."}, {"text": "Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.  Interrupted time series analysis is the analysis of interventions on a single time series. Time series data have a natural temporal ordering."}, {"text": "A stationary time series is one whose properties do not depend on the time at which the series is observed. 14. Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times."}, {"text": "A time series is a sequence of numerical data points in successive order. In investing, a time series tracks the movement of the chosen data points, such as a security's price, over a specified period of time with data points recorded at regular intervals."}]}, {"question": "How is 11+ standardized calculated", "positive_ctxs": [{"text": "When your child sits the eleven plus exam, the number of questions answered correctly decides the \"Raw Score\". If there are more than one tests, the score may be the sum of the raw scores.  A standardized test score is calculated by translating the raw score into a completely different scale."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A test statistic is a standardized value that is calculated from sample data during a hypothesis test.  A t-value of 0 indicates that the sample results exactly equal the null hypothesis."}, {"text": "T-tests are called t-tests because the test results are all based on t-values. T-values are an example of what statisticians call test statistics. A test statistic is a standardized value that is calculated from sample data during a hypothesis test."}, {"text": "Coefficient of variation is a measure used to assess the total risk per unit of return of an investment. It is calculated by dividing the standard deviation of an investment by its expected rate of return.  Coefficient of variation provides a standardized measure of comparing risk and return of different investments."}, {"text": "A t score is one form of a standardized test statistic (the other you'll come across in elementary statistics is the z-score). The t score formula enables you to take an individual score and transform it into a standardized form>one which helps you to compare scores."}, {"text": "The statistical output displays the coded coefficients, which are the standardized coefficients. Temperature has the standardized coefficient with the largest absolute value. This measure suggests that Temperature is the most important independent variable in the regression model."}, {"text": "Absolute standardized differences for baseline covariates comparing treated to untreated subjects in the original and the matched sample.  Thus, when the standardized difference is equal to 0.10, the percentage of non-overlap between the distributions of the continuous covariate in the two groups is 7.7 per cent."}, {"text": "Betas are calculated by subtracting the mean from the variable and dividing by its standard deviation. This results in standardized variables having a mean of zero and a standard deviation of 1. Standardized beta coefficients are also called: Betas."}]}, {"question": "How do you estimate in agile", "positive_ctxs": [{"text": "How to Estimate an Agile/Scrum Story Backlog with PointsThe goal of agile/scrum estimation.  A few terms.  Set an estimation range.  Set some reference points.  Estimate stories with planning poker.  Estimate bugs, chores, and spikes.  Set aside a couple of days.  Use the big numbers: 20, 40, 100.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "he confidence interval tells you more than just the possible range around the estimate. It also tells you about how stable the estimate is. A stable estimate is one that would be close to the same value if the survey were repeated."}, {"text": "A parametric model is one where we assume the 'shape' of the data, and therefore only have to estimate the coefficients of the model. A non-parametric model is one where we do not assume the 'shape' of the data, and we have to estimate the most suitable form of the model, along with the coefficients."}]}, {"question": "What is the importance of Gaussian distribution", "positive_ctxs": [{"text": "The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "You can get the feature importance of each feature of your dataset by using the feature importance property of the model. Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The Gaussian Processes Classifier is a classification machine learning algorithm. Gaussian Processes are a generalization of the Gaussian probability distribution and can be used as the basis for sophisticated non-parametric machine learning algorithms for classification and regression."}, {"text": "In statistics, importance sampling is a general technique for estimating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest. It is related to umbrella sampling in computational physics."}, {"text": "In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions."}, {"text": "Gaussian Distribution Function The nature of the gaussian gives a probability of 0.683 of being within one standard deviation of the mean. The mean value is a=np where n is the number of events and p the probability of any integer value of x (this expression carries over from the binomial distribution )."}, {"text": "Gaussian Distribution Function The nature of the gaussian gives a probability of 0.683 of being within one standard deviation of the mean. The mean value is a=np where n is the number of events and p the probability of any integer value of x (this expression carries over from the binomial distribution )."}]}, {"question": "Why are SIFT descriptors scale invariant", "positive_ctxs": [{"text": "One of the stages that SIFT uses is to create a pyramid of scales of the image.  The feature detector then works by finding features that have a peak response not only in the image space, but in scale space too. This means that it finds the scale of the image which the feature will produce the highest response."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The subject of this chapter is image key points which we define as a distinctive point in an input image which is invariant to rotation, scale and distortion."}, {"text": "Given two random variables X and Y, the correlation is scale and location invariant in the sense that cor(X,Y)=cor(XT,YT), if XT=a+bX, and YT=c+dY, and b and d have the same sign (either both positive or both negative)."}, {"text": "Statistical stationarity: A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time.  Such statistics are useful as descriptors of future behavior only if the series is stationary."}, {"text": "The difference between a ratio scale and an interval scale is that the zero point on an interval scale is some arbitrarily agreed value, whereas on a ratio scale it is a true zero."}, {"text": "Some of the algorithms used in image recognition (Object Recognition, Face Recognition) are SIFT (Scale-invariant Feature Transform), SURF (Speeded Up Robust Features), PCA (Principal Component Analysis), and LDA (Linear Discriminant Analysis)."}, {"text": "Linear time invariant (LTI) filters are linear applications that transform a signal into another signal, as such that the application commutes with time shifts."}, {"text": "The interval scale of measurement is a type of measurement scale that is characterized by equal intervals between scale units. A perfect example of an interval scale is the Fahrenheit scale to measure temperature.  For example, suppose it is 60 degrees Fahrenheit on Monday and 70 degrees on Tuesday."}]}, {"question": "How do you do principal component analysis", "positive_ctxs": [{"text": "7:3021:58Suggested clip \u00b7 120 secondsStatQuest: Principal Component Analysis (PCA), Step-by-Step YouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors."}, {"text": "Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Kmeans clustering algorithm is applied to reduced datasets which is done by principal component analysis dimension reduction method. Cluster analysis is one of the major data analysis methods widely used for many practical applications in emerging areas[12]."}, {"text": "Image compression with principal component analysis is a frequently occurring application of the dimension reduction technique.  As the number of principal components used to project the new data increases, the quality and representation compared to the original image improve."}, {"text": "The mathematics of factor analysis and principal component analysis (PCA) are different. Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables."}, {"text": "The difference between factor analysis and principal component analysis.  Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables."}]}, {"question": "Which distance function is used in K means clustering", "positive_ctxs": [{"text": "The k-means clustering algorithm uses the Euclidean distance [1,4] to measure the similarities between objects. Both iterative algorithm and adaptive algorithm exist for the standard k-means clustering. K-means clustering algorithms need to assume that the number of groups (clusters) is known a priori."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical."}, {"text": "Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows\u2022"}, {"text": "The hclust function in R uses the complete linkage method for hierarchical clustering by default. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components."}, {"text": "In general, K-means is a heuristic algorithm that partitions a data set into K clusters by minimizing the sum of squared distance in each cluster.  In this paper, the simulation of basic k-means algorithm is done, which is implemented using Euclidian distance metric."}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "For most common hierarchical clustering software, the default distance measure is the Euclidean distance. This is the square root of the sum of the square differences. However, for gene expression, correlation distance is often used. The distance between two vectors is 0 when they are perfectly correlated."}]}, {"question": "How do you partition data", "positive_ctxs": [{"text": "Designing partitions for query performanceLimit the size of each partition so that the query response time is within target.If you use horizontal partitioning, design the shard key so that the application can easily select the right partition.  Consider the location of a partition."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In my opinion the LVM partition is more usefull cause then after installation you can later change partition sizes and number of partitions easily. In standard partition also you can do resizing, but total number of physical partitions are limited to 4. With LVM you have much greater flexibility."}, {"text": "Dynamic Partition takes more time in loading data compared to static partition. When you have large data stored in a table then the Dynamic partition is suitable. If you want to partition a number of columns but you don't know how many columns then also dynamic partition is suitable."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "A partition of a number is any combination of integers that adds up to that number. For example, 4 = 3+1 = 2+2 = 2+1+1 = 1+1+1+1, so the partition number of 4 is 5. It sounds simple, yet the partition number of 10 is 42, while 100 has more than 190 million partitions."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}]}, {"question": "What is the difference between neural networks and machine learning", "positive_ctxs": [{"text": "Neural network structures/arranges algorithms in layers of fashion, that can learn and make intelligent decisions on its own. Whereas in Machine learning the decisions are made based on what it has learned only. Machine learning models/methods or learnings can be two types supervised and unsupervised learnings."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}, {"text": "Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. In fact, it is the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three."}, {"text": "The ways in which they function Another fundamental difference between traditional computers and artificial neural networks is the way in which they function. While computers function logically with a set of rules and calculations, artificial neural networks can function via images, pictures, and concepts."}, {"text": "Learning Rate and Gradient Descent Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}, {"text": "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}, {"text": "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}]}, {"question": "What is a variance in statistics", "positive_ctxs": [{"text": "Variance (\u03c32) in statistics is a measurement of the spread between numbers in a data set. That is, it measures how far each number in the set is from the mean and therefore from every other number in the set."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "In statistics and machine learning, the bias\u2013variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters."}, {"text": "Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables."}, {"text": "The variance of a set of numbers is the mean squared deviation from the mean. It is a measure of how spread out the set of numbers is.  The estimation variance is the variance of that large set of values. It measures how much, well, variance there is in an estimator from sample to sample."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "An example of statistics is a report of numbers saying how many followers of each religion there are in a particular country. An example of statistics is a math class offered in high schools and colleges. The definition of a statistic is a number, or a person who is an unnamed piece of data to be studied."}]}, {"question": "How do you do weighted regression", "positive_ctxs": [{"text": "Fit the regression model by unweighted least squares and analyze the residuals.  Estimate the variance function or the standard deviation function.  Use the fitted values from the estimated variance or standard deviation function to obtain the weights.  Estimate the regression coefficients using these weights."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "The fundamental counting principle states that if there are p ways to do one thing, and q ways to do another thing, then there are p\u00d7q ways to do both things. possible outcomes of the experiment. The counting principle can be extended to situations where you have more than 2 choices."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "False negatives \u2014 that is, a test that says you don't have the virus when you actually do have the virus \u2014 may occur."}]}, {"question": "Why is systematic sampling not random", "positive_ctxs": [{"text": "This makes systematic sampling functionally similar to simple random sampling (SRS). However it is not the same as SRS because not every possible sample of a certain size has an equal chance of being chosen (e.g. samples with at least two elements adjacent to each other will never be chosen by systematic sampling)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "In simple random sampling, each data point has an equal probability of being chosen. Meanwhile, systematic sampling chooses a data point per each predetermined interval. While systematic sampling is easier to execute than simple random sampling, it can produce skewed results if the data set exhibits patterns."}, {"text": "In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).  In cluster sampling, the sampling unit is the whole cluster; Instead of sampling individuals from within each group, a researcher will study whole clusters."}, {"text": "Systematic sampling is easier to do than random sampling. In systematic sampling, the list of elements is \"counted off\". That is, every kth element is taken.  Stratified sampling also divides the population into groups called strata."}, {"text": "Despite the sample population being selected in advance, systematic sampling is still thought of as being random if the periodic interval is determined beforehand and the starting point is random."}, {"text": "The main types of probability sampling methods are simple random sampling, stratified sampling, cluster sampling, multistage sampling, and systematic random sampling."}, {"text": "Under simple random sampling, a sample of items is chosen randomly from a population, and each item has an equal probability of being chosen. Meanwhile, systematic sampling involves selecting items from an ordered population using a skip or sampling interval."}]}, {"question": "How is root mean square error RMSE and classification related", "positive_ctxs": [{"text": "The RMSE measures the standard deviation of the predictions from the ground-truth. This is the relationship between RMSE and classification.  The RMSE measures the standard deviation of the predictions from the ground-truth. This is the relationship between RMSE and classification."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "MSE is the average of the squared error that is used as the loss function for least squares regression: It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE."}, {"text": "A kind of average sometimes used in statistics and engineering, often abbreviated as RMS. To find the root mean square of a set of numbers, square all the numbers in the set and then find the arithmetic mean of the squares. Take the square root of the result. This is the root mean square."}, {"text": "In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."}, {"text": "In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."}, {"text": "The most commonly used metric for regression tasks is RMSE (Root Mean Square Error). This is defined as the square root of the average squared distance between the actual score and the predicted score: rmse=\u221a\u2211ni=1(yi\u2212^yi)2n."}, {"text": "Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit."}, {"text": "Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit."}]}, {"question": "Who invented AB testing", "positive_ctxs": [{"text": "William Sealy Gosset"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Optuna is an automated hyperparameter optimization software framework that is knowingly invented for the machine learning-based tasks. It emphasizes an authoritative, define-by-run approach user API."}, {"text": "An AB test is an example of statistical hypothesis testing, a process whereby a hypothesis is made about the relationship between two data sets and those data sets are then compared against each other to determine if there is a statistically significant relationship or not."}, {"text": "split testing"}, {"text": "Mathematically test efficiency is calculated as a percentage of the number of alpha testing (in-house or on-site) defects divided by sum of a number of alpha testing and a number of beta testing (off-site) defects."}, {"text": "Benefits of Usability TestingUsability testing provides an unbiased, accurate, and direct examination of your product or website's user experience.  Usability testing is convenient.  Usability testing can tell you what your users do on your site or product and why they take these actions.More items\u2022"}, {"text": "Chernoff faces, invented by Herman Chernoff in 1973, display multivariate data in the shape of a human face. The individual parts, such as eyes, ears, mouth and nose represent values of the variables by their shape, size, placement and orientation."}, {"text": "The machine operates on an infinite memory tape divided into discrete \"cells\". The machine positions its \"head\" over a cell and \"reads\" or \"scans\" the symbol there.  The Turing machine was invented in 1936 by Alan Turing, who called it an \"a-machine\" (automatic machine)."}]}, {"question": "What is the difference between time series and regression", "positive_ctxs": [{"text": "Regression: This is a tool used to evaluate the relationship of a dependent variable in relation to multiple independent variables. A regression will analyze the mean of the dependent variable in relation to changes in the independent variables. Time Series: A time series measures data over a specific period of time."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A stationary time series is one whose properties do not depend on the time at which the series is observed. 14. Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times."}, {"text": "The key difference between time series and panel data is that time series focuses on a single individual at multiple time intervals while panel data (or longitudinal data) focuses on multiple individuals at multiple time intervals.  Fields such as Econometrics and statistics relies on data."}, {"text": "Generally, we use linear regression for time series analysis, it is used for predicting the result for time series as its trends. For example, If we have a dataset of time series with the help of linear regression we can predict the sales with the time."}, {"text": "An autoregressive model is when a value from a time series is regressed on previous values from that same time series.  In this regression model, the response variable in the previous time period has become the predictor and the errors have our usual assumptions about errors in a simple linear regression model."}, {"text": "When Longitudinal data looks like a time series is when we measure the same thing over time. The big difference is that in a time series we can measure the overall change in the measurement over time (or by group) while in a longitudinal analysis you actually have the measurement of change at the individual level."}, {"text": "No. Stock return is not always stationary.  Using non-stationary time series data in financial models produces unreliable and spurious results and leads to poor understanding and forecasting. The solution to the problem is to transform the time series data so that it becomes stationary."}]}, {"question": "What is at test and p value", "positive_ctxs": [{"text": "When you perform a t-test, you're usually trying to find evidence of a significant difference between population means (2-sample t) or between the population mean and a hypothesized value (1-sample t). The t-value measures the size of the difference relative to the variation in your sample data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Graphically, the p value is the area in the tail of a probability distribution. It's calculated when you run hypothesis test and is the area to the right of the test statistic (if you're running a two-tailed test, it's the area to the left and to the right)."}, {"text": "The test statistic is a z-score (z) defined by the following equation. z=(p\u2212P)\u03c3 where P is the hypothesized value of population proportion in the null hypothesis, p is the sample proportion, and \u03c3 is the standard deviation of the sampling distribution."}, {"text": "Test statistic. The test statistic is a z-score (z) defined by the following equation. where P is the hypothesized value of population proportion in the null hypothesis, p is the sample proportion, and \u03c3 is the standard deviation of the sampling distribution."}, {"text": "A proposition of the form \u201cif p then q\u201d or \u201cp implies q\u201d, represented \u201cp \u2192 q\u201d is called a conditional proposition.  The proposition p is called hypothesis or antecedent, and the proposition q is the conclusion or consequent. Note that p \u2192 q is true always except when p is true and q is false."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Definition: Given data the maximum likelihood estimate (MLE) for the parameter p is the value of p that maximizes the likelihood P(data |p). That is, the MLE is the value of p for which the data is most likely. 100 P(55 heads|p) = ( 55 ) p55(1 \u2212 p)45."}]}, {"question": "What is machine learning name its types and discuss the difference between classification and clustering", "positive_ctxs": [{"text": "Classification is the process of classifying the data with the help of class labels whereas, in clustering, there are no predefined class labels. 2. Classification is supervised learning, while clustering is unsupervised learning."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The most significant difference between regression vs classification is that while regression helps predict a continuous quantity, classification predicts discrete class labels. There are also some overlaps between the two types of machine learning algorithms."}, {"text": "A machine learning task is the type of prediction or inference being made, based on the problem or question that is being asked, and the available data. For example, the classification task assigns data to categories, and the clustering task groups data according to similarity."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Regression and classification are categorized under the same umbrella of supervised machine learning.  The main difference between them is that the output variable in regression is numerical (or continuous) while that for classification is categorical (or discrete)."}, {"text": "Regression and classification are categorized under the same umbrella of supervised machine learning.  The main difference between them is that the output variable in regression is numerical (or continuous) while that for classification is categorical (or discrete)."}, {"text": "The Gaussian Processes Classifier is a classification machine learning algorithm. Gaussian Processes are a generalization of the Gaussian probability distribution and can be used as the basis for sophisticated non-parametric machine learning algorithms for classification and regression."}]}, {"question": "How do you find the critical region", "positive_ctxs": [{"text": "\u27a2 To determine the critical region for a normal distribution, we use the table for the standard normal distribution. If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "\u27a2 To determine the critical region for a normal distribution, we use the table for the standard normal distribution. If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28."}, {"text": "Normal Distribution For a one-tailed test, the critical value is 1.645. So the critical region is Z<\u22121.645 for a left-tailed test and Z>1.645 for a right-tailed test. For a two-tailed test, the critical value is 1.96."}, {"text": "Lets do this step by step:Step 1: find the mean.Step 2: fin the standard deviation of the mean (using the population SD)Step 3: find the Z score.Step 4: compare to the critical Z score. From the stated hypothesis, we know that we are dealing with a 1-tailed hypothesis test.  Step 4 : compare to the critical Z score."}, {"text": "If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28. For a two tailed test, use \u03b1/2 = 0.05 and the critical region is below z = -1.645 and above z = 1.645."}, {"text": "To find the critical value, follow these steps.Compute alpha (\u03b1): \u03b1 = 1 - (confidence level / 100)Find the critical probability (p*): p* = 1 - \u03b1/2.To express the critical value as a z-score, find the z-score having a cumulative probability equal to the critical probability (p*).More items"}, {"text": "The range containing values that are consistent with the null hypothesis is the \"acceptance region\"; the other range, in which the null hypothesis is rejected, is the rejection region (or critical region)."}]}, {"question": "How do you find conditional probability from joint probability", "positive_ctxs": [{"text": "The joint probability is symmetrical, meaning that P(A and B) is the same as P(B and A). The calculation using the conditional probability is also symmetrical, for example: P(A and B) = P(A given B)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A Generative Model \u200clearns the joint probability distribution p(x,y). It predicts the conditional probability with the help of Bayes Theorem. A Discriminative model \u200clearns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems."}, {"text": "The nominator is the joint probability and the denominator is the probability of the given outcome.  This is the conditional probability: P(A\u2223B)=P(A\u2229B)P(B) This is the Bayes' rule: P(A\u2223B)=P(B|A)\u2217P(A)P(B)."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "their joint probability distribution at (x,y), the functions given by: g(x) = \u03a3y f (x,y) and h(y) = \u03a3x f (x,y) are the marginal distributions of X and Y , respectively. If you're great with equations, that's probably all you need to know. It tells you how to find a marginal distribution."}]}, {"question": "What are the advantages of machine learning", "positive_ctxs": [{"text": "Advantages of Machine LearningContinuous Improvement. Machine Learning algorithms are capable of learning from the data we provide.  Automation for everything.  Trends and patterns identification.  Wide range of applications.  Data Acquisition.  Highly error-prone.  Algorithm Selection.  Time-consuming."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Automated machine learning (AutoML) is the process of automating the process of applying machine learning to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model."}, {"text": "Random initialization refers to the practice of using random numbers to initialize the weights of a machine learning model. Random initialization is one way of performing symmetry breaking, which is the act of preventing all of the weights in the machine learning model from being the same."}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}, {"text": "Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model."}, {"text": "If the student does have multiple learning styles (multimodal), the advantages gained through multiple learning strategies include the ability to learn more quickly and at a deeper level so that recall at a later date will be more successful. Using various modes of learning also improves attention span."}, {"text": "Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset.  Model selection is the process of choosing one of the models as the final model that addresses the problem."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "How do you interpret intercepts in logistic regression", "positive_ctxs": [{"text": "The intercept (often labeled the constant) is the expected mean value of Y when all X=0. Start with a regression equation with one predictor, X. If X sometimes equals 0, the intercept is simply the expected mean value of Y at that value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A logistic regression estimates the mean of your response given that your data is distributed Bernoulli or is a Binomial trial. Since the mean of a Binomial trial is the probability of success, you can interpret the output from a Logistic regression (after logit transformation) as a probability of success."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "Multinomial logistic regression is used when you have a categorical dependent variable with two or more unordered levels (i.e. two or more discrete outcomes). It is practically identical to logistic regression, except that you have multiple possible outcomes instead of just one."}, {"text": "Use simple logistic regression when you have one nominal variable and one measurement variable, and you want to know whether variation in the measurement variable causes variation in the nominal variable."}, {"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "Standardization isn't required for logistic regression. The main goal of standardizing features is to help convergence of the technique used for optimization.  Otherwise, you can run your logistic regression without any standardization treatment on the features."}]}, {"question": "What is the significance of sufficiency in statistical inference", "positive_ctxs": [{"text": "We know from the sufficiency principle that if we have a sufficient statistic Y = T (X) and a statistical model, the inferences we can make about \u03b8 from our model and X (the data set) must be the same as from that model and Y."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "A statistical model is a family of probability distributions, the central problem of statistical inference being to identify which member of the family generated the data currently of interest."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.  Bayesian updating is particularly important in the dynamic analysis of a sequence of data."}, {"text": "This is why it is important to distinguish between the statistical significance of a result and the practical significance of that result.  Null hypothesis testing is a formal approach to deciding whether a statistical relationship in a sample reflects a real relationship in the population or is just due to chance."}, {"text": "A t-value is the relative error difference in contrast to the null hypothesis. A p-value, is the statistical significance of a measurement in how correct a statistical evidence part, is."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics."}]}, {"question": "Is RMSE the same as standard deviation", "positive_ctxs": [{"text": "Nonetheless, they are not the same. Standard deviation is used to measure the spread of data around the mean, while RMSE is used to measure distance between some values and prediction for those values.  If you use mean as your prediction for all the cases, then RMSE and SD will be exactly the same."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit."}, {"text": "Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit."}, {"text": "Root Mean Squared Error or RMSE RMSE is the standard deviation of the errors which occur when a prediction is made on a dataset. This is the same as MSE (Mean Squared Error) but the root of the value is considered while determining the accuracy of the model. from sklearn."}, {"text": "In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."}, {"text": "In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."}, {"text": "The population mean of the distribution of sample means is the same as the population mean of the distribution being sampled from.  Thus as the sample size increases, the standard deviation of the means decreases; and as the sample size decreases, the standard deviation of the sample means increases."}, {"text": "Standard deviation is never negative. Standard deviation is sensitive to outliers. A single outlier can raise the standard deviation and in turn, distort the picture of spread. For data with approximately the same mean, the greater the spread, the greater the standard deviation."}]}, {"question": "How do you state the null and alternative hypothesis in words", "positive_ctxs": [{"text": "In a hypothesis test, we:Evaluate the null hypothesis, typically denoted with H0.  Always write the alternative hypothesis, typically denoted with Ha or H1, using less than, greater than, or not equals symbols, i.e., (\u2260, >, or <).More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How to Conduct Hypothesis TestsState the hypotheses. Every hypothesis test requires the analyst to state a null hypothesis and an alternative hypothesis.  Formulate an analysis plan. The analysis plan describes how to use sample data to accept or reject the null hypothesis.  Analyze sample data.  Interpret the results."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "In research, there is a convention that the hypothesis is written in two forms, the null hypothesis, and the alternative hypothesis (called the experimental hypothesis when the method of investigation is an experiment)."}, {"text": "Rejecting or failing to reject the null hypothesis If our statistical analysis shows that the significance level is below the cut-off value we have set (e.g., either 0.05 or 0.01), we reject the null hypothesis and accept the alternative hypothesis."}, {"text": "Compare the P-value to the \u03b1 significance level stated earlier. If it is less than \u03b1, reject the null hypothesis. If the result is greater than \u03b1, fail to reject the null hypothesis. If you reject the null hypothesis, this implies that your alternative hypothesis is correct, and that the data is significant."}, {"text": "Statistical analysts test a hypothesis by measuring and examining a random sample of the population being analyzed. All analysts use a random population sample to test two different hypotheses: the null hypothesis and the alternative hypothesis."}, {"text": "The null hypothesis is the one to be tested and the alternative is everything else. In our example, The null hypothesis would be: The mean data scientist salary is 113,000 dollars. While the alternative: The mean data scientist salary is not 113,000 dollars."}]}, {"question": "Is artificial intelligence intelligent", "positive_ctxs": [{"text": "Artificial intelligence (AI) is the attempt to let computers perform services for which humans need intelligence. However, this is still not possible today. AI systems are capable of recognizing patterns, learning and making decisions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep reinforcement learning is a category of machine learning and artificial intelligence where intelligent machines can learn from their actions similar to the way humans learn from experience.  Actions that get them to the target outcome are rewarded (reinforced)."}, {"text": "In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats\u2014artificial systems that exhibit complex behaviour in an agent environment.  The term is also sometimes used in ethology or animal behavior."}, {"text": "It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action. Some academic sources reserve the term \"strong AI\" for machines that can experience consciousness."}, {"text": "It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action. Some academic sources reserve the term \"strong AI\" for machines that can experience consciousness."}, {"text": "In the real world, knowledge plays a vital role in intelligence as well as creating artificial intelligence. It demonstrates the intelligent behavior in AI agents or systems. It is possible for an agent or system to act accurately on some input only when it has the knowledge or experience about the input."}, {"text": "An autonomous agent is an intelligent agent operating on an owner's behalf but without any interference of that ownership entity.  Non-biological examples include intelligent agents, autonomous robots, and various software agents, including artificial life agents, and many computer viruses."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}]}, {"question": "What is the probability of getting at least one six when rolling six fair dice", "positive_ctxs": [{"text": "66.5%"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If outcomes are equally likely, then the probability of an event occurring is the number in the event divided by the number in the sample space. The probability of rolling a six on a single roll of a die is 1/6 because there is only 1 way to roll a six out of 6 ways it could be rolled."}, {"text": "Any situation in which every outcome in a sample space is equally likely will use a uniform distribution. One example of this in a discrete case is rolling a single standard die. There are a total of six sides of the die, and each side has the same probability of being rolled face up."}, {"text": "At the heart of this definition are three conditions, called the axioms of probability theory. Axiom 1: The probability of an event is a real number greater than or equal to 0. Axiom 2: The probability that at least one of all the possible outcomes of a process (such as rolling a die) will occur is 1."}, {"text": "In probability, two events are independent if the incidence of one event does not affect the probability of the other event. If the incidence of one event does affect the probability of the other event, then the events are dependent. There is a red 6-sided fair die and a blue 6-sided fair die."}, {"text": "The comparison - wise error rate is the probability of a Type I error set by the experimentor for evaluating each comparison. The experiment - wise error rate is the probability of making at least one Type I error when performing the whole set of comparisons."}, {"text": "Joint probability is calculated by multiplying the probability of event A, expressed as P(A), by the probability of event B, expressed as P(B). For example, suppose a statistician wishes to know the probability that the number five will occur twice when two dice are rolled at the same time."}, {"text": "Joint probability is calculated by multiplying the probability of event A, expressed as P(A), by the probability of event B, expressed as P(B). For example, suppose a statistician wishes to know the probability that the number five will occur twice when two dice are rolled at the same time."}]}, {"question": "What is a normal population distribution", "positive_ctxs": [{"text": "\"Normal\" data are data that are drawn (come from) a population that has a normal distribution. This distribution is inarguably the most important and the most frequently used distribution in both the theory and application of statistics. If X is a normal random variable, then the probability distribution of X is."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Particular distributions are associated with hypothesis testing. Perform tests of a population mean using a normal distribution or a Student's t-distribution. (Remember, use a Student's t-distribution when the population standard deviation is unknown and the distribution of the sample mean is approximately normal.)"}, {"text": "The normal curve is called Mesokurtic curve. If the curve of a distribution is peaked than a normal or mesokurtic curve then it is referred to as a Leptokurtic curve. If a curve is less peaked than a normal curve, it is called as a Platykurtic curve. That's why kurtosis of normal distribution equal to three."}, {"text": "If a population is known to be normally distributed, then it follows that the sample mean must equal the population mean. If the sampled population distribution is skewed, then in most cases the sampling distribution of the mean can be approximated by the normal distribution if the sample size n is at least 30."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "Because our sample size is greater than 30, the Central Limit Theorem tells us that the sampling distribution will approximate a normal distribution.  Because we know the population standard deviation and the sample size is large, we'll use the normal distribution to find probability."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}]}, {"question": "What are some applications of Eigenvalues and Eigenvectors", "positive_ctxs": [{"text": "Here are just some of the many uses of eigenvectors and eigenvalues:Using singular value decomposition for image compression.  Deriving Special Relativity is more natural in the language of linear algebra.  Spectral Clustering.  Dimensionality Reduction/PCA.  Low rank factorization for collaborative prediction.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "0:0013:40Suggested clip \u00b7 120 secondsFinding Eigenvalues and Eigenvectors : 2 x 2 Matrix Example YouTubeStart of suggested clipEnd of suggested clip"}, {"text": "There are multiple uses of eigenvalues and eigenvectors: Eigenvalues and Eigenvectors have their importance in linear differential equations where you want to find a rate of change or when you want to maintain relationships between two variables."}, {"text": "There are numerous applications of integrals. Using technology such as computer software, internet sources, graphing calculators and smartphone apps can make solving integral problems easier. Some applications of integrals are: Displacement, which is the integral of velocity with respect to time."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "Three of the most common applications of exponential and logarithmic functions have to do with interest earned on an investment, population growth, and carbon dating."}, {"text": "Bayes' Theorem has many applications in areas such as mathematics, medicine, finance, marketing, engineering and many other. This paper covers Bayes' Theorem at a basic level and explores how the formula was derived. We also, look at some extended forms of the formula and give an explicit example."}]}, {"question": "What is the meaning of in group and out group how do they both affect the opinions and behavior of individuals", "positive_ctxs": [{"text": "In relation to out-group, a social group toward which a person feels a sense of competition or opposition. They both affect the opinions and behavior of individuals because In-groups and Out- groups are based on the idea that \"we\" have valued traits that \"they\" lack."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "Collective intelligence (CI) is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals and appears in consensus decision making."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Rather, the swarm of humans uses software to input their opinions in real time, thus making micro-changes to the rest of the swarm and the inputs of other members. Studies show that swarm intelligence consistently outperforms individuals and crowds working without the algorithms."}, {"text": "Cluster analysis tries to maximize in-group homogeneity and maximize between group heterogeneity. Multiple discriminant analysis is different. It starts with a discrete DV and tries to determine how much the levels of the IV's distinguish the members of the groups."}, {"text": "A/B testing (also known as split testing) is the process of comparing two versions of a web page, email, or other marketing asset and measuring the difference in performance. You do this giving one version to one group and the other version to another group. Then you can see how each variation performs."}, {"text": "Discriminant function analysis (DFA) is a statistical procedure that classifies unknown individuals and the probability of their classification into a certain group (such as sex or ancestry group). Discriminant function analysis makes the assumption that the sample is normally distributed for the trait."}]}, {"question": "What should be the learning rate", "positive_ctxs": [{"text": "A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem. \u2014 Practical recommendations for gradient-based training of deep architectures, 2012."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Learning rate decay (lrDecay) is a \\emph{de facto} technique for training modern neural networks.  We provide another novel explanation: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns."}, {"text": "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use."}, {"text": "A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck.  The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate."}, {"text": "No. If the learning rate is too high, then the model can diverge.  If the validation error consistently goes up, that means the model could be diverging because of high learning rate."}, {"text": "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}, {"text": "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}]}, {"question": "What is an association rule in data mining", "positive_ctxs": [{"text": "Association rule mining is a procedure which aims to observe frequently occurring patterns, correlations, or associations from datasets found in various kinds of databases such as relational databases, transactional databases, and other forms of repositories."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Applications of association rule mining are stock analysis, web log mining, medical diagnosis, customer market analysis bioinformatics etc. In past, many algorithms were developed by researchers for Boolean and Fuzzy association rule mining such as Apriori, FP-tree, Fuzzy FP-tree etc."}, {"text": "The confidence of an association rule is a percentage value that shows how frequently the rule head occurs among all the groups containing the rule body.  Thus, the confidence of a rule is the percentage equivalent of m/n, where the values are: m. The number of groups containing the joined rule head and rule body."}, {"text": "As opposed to decision tree and rule set induction, which result in classification models, association rule learning is an unsupervised learning method, with no class labels assigned to the examples.  This would then be a Supervised Learning task , where the NN learns from pre-calssified examples."}, {"text": "The Apriori algorithm is used for mining frequent itemsets and devising association rules from a transactional database. The parameters \u201csupport\u201d and \u201cconfidence\u201d are used. Support refers to items' frequency of occurrence; confidence is a conditional probability. Items in a transaction form an item set."}, {"text": "An association rule has two parts: an antecedent (if) and a consequent (then). An antecedent is an item found within the data.  Support is an indication of how frequently the items appear in the data. Confidence indicates the number of times the if-then statements are found true."}, {"text": "Association rules mining is another key unsupervised data mining method, after clustering, that finds interesting associations (relationships, dependencies) in large sets of data items."}, {"text": "Cross-validation is a standard tool in analytics and is an important feature for helping you develop and fine-tune data mining models.  Cross-validation has the following applications: Validating the robustness of a particular mining model. Evaluating multiple models from a single statement."}]}, {"question": "What is gradient descent algorithm with example", "positive_ctxs": [{"text": "Gradient Descent is an optimization algorithm used for minimizing the cost function in various machine learning algorithms. It is basically used for updating the parameters of the learning model.  But if the number of training examples is large, then batch gradient descent is computationally very expensive."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. One cycle through the entire training dataset is called a training epoch."}, {"text": "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning."}, {"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}]}, {"question": "Is standard deviation affected by outliers", "positive_ctxs": [{"text": "Standard deviation is sensitive to outliers. A single outlier can raise the standard deviation and in turn, distort the picture of spread. For data with approximately the same mean, the greater the spread, the greater the standard deviation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "We can calculate the mean and standard deviation of a given sample, then calculate the cut-off for identifying outliers as more than 3 standard deviations from the mean. We can then identify outliers as those examples that fall outside of the defined lower and upper limits."}, {"text": "The difference between the two norms is that the standard deviation is calculating the square of the difference whereas the mean absolute deviation is only looking at the absolute difference. Hence large outliers will create a higher dispersion when using the standard deviation instead of the other method."}, {"text": "The sum of squared errors is a 'total' and is, therefore, affected by the number of data points. The variance is the 'average' variability but in units squared. The standard deviation is the average variation but converted back to the original units of measurement."}, {"text": "Center: The center is not affected by sample size. The mean of the sample means is always approximately the same as the population mean \u00b5 = 3,500. Spread: The spread is smaller for larger samples, so the standard deviation of the sample means decreases as sample size increases."}, {"text": "Center: The center is not affected by sample size. The mean of the sample means is always approximately the same as the population mean \u00b5 = 3,500. Spread: The spread is smaller for larger samples, so the standard deviation of the sample means decreases as sample size increases."}, {"text": "Center: The center is not affected by sample size. The mean of the sample means is always approximately the same as the population mean \u00b5 = 3,500. Spread: The spread is smaller for larger samples, so the standard deviation of the sample means decreases as sample size increases."}, {"text": "The coefficient of variation is a better risk measure than the standard deviation alone because the CV adjusts for the size of the project. The CV measures the standard deviation divided by the mean and therefore puts the standard deviation into context."}]}, {"question": "What does risk mean to you interview question", "positive_ctxs": [{"text": "Interview Answer Risk means potential threat that calls for identification and careful monitoring of KPI's."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "In Reinforcement Learning, this type of decision is called exploitation when you keep doing what you were doing, and exploration when you try something new. Naturally this raises a question about how much to exploit and how much to explore."}, {"text": "With a continuous variable, the hazard ratio indicates the change in the risk of death if the parameter in question rises by one unit, for example if the patient is one year older on diagnosis. For every additional year of patient age on diagnosis, the risk of death falls by 7% (hazard ratio 0.93)."}]}, {"question": "What are the differences between simple random sampling and restricted random sampling", "positive_ctxs": [{"text": "Simple random sampling is where individuals are chosen completely by chance from a population. The addition of SRS increases the chance a guilty person will be found."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cluster sampling is best used when the clusters occur naturally in a population, when you don't have access to the entire population, and when the clusters are geographically convenient. However, cluster sampling is not as precise as simple random sampling or stratified random sampling."}, {"text": "Systematic random samplingCalculate the sampling interval (the number of households in the population divided by the number of households needed for the sample)Select a random start between 1 and sampling interval.Repeatedly add sampling interval to select subsequent households."}, {"text": "Systematic random samplingCalculate the sampling interval (the number of households in the population divided by the number of households needed for the sample)Select a random start between 1 and sampling interval.Repeatedly add sampling interval to select subsequent households."}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "Steps in selecting a systematic random sample:Calculate the sampling interval (the number of households in the population divided by the number of households needed for the sample)Select a random start between 1 and sampling interval.Repeatedly add sampling interval to select subsequent households."}, {"text": "Cluster Sampling: Advantages and Disadvantages Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "The main types of probability sampling methods are simple random sampling, stratified sampling, cluster sampling, multistage sampling, and systematic random sampling."}]}, {"question": "What is hinge loss in machine learning", "positive_ctxs": [{"text": "In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs). For an intended output t = \u00b11 and a classifier score y, the hinge loss of the prediction y is defined as."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs). For an intended output t = \u00b11 and a classifier score y, the hinge loss of the prediction y is defined as."}, {"text": "Squared hinge loss is nothing else but a square of the output of the hinge's max(\u2026) function. It generates a loss function as illustrated above, compared to regular hinge loss."}, {"text": "Fortunately, hinge loss, logistic loss and square loss are all convex functions. Convexity ensures global minimum and it's computationally appleaing."}, {"text": "The squared hinge loss is differentiable because the term from the chain rule forces the limits to converge to the same number from both sides."}, {"text": "Gram matrix is simply the matrix of the inner product of each vector and its corresponding vectors in same. It found use in the current machine learning is due to deep learning loss where while style transferring the loss function is computed using the gram matrix."}, {"text": "There are several different common loss functions to choose from: the cross-entropy loss, the mean-squared error, the huber loss, and the hinge loss \u2013 just to name a few.\u201d"}, {"text": "A loss function is used to optimize a machine learning algorithm. The loss is calculated on training and validation and its interpretation is based on how well the model is doing in these two sets.  An accuracy metric is used to measure the algorithm's performance in an interpretable way."}]}, {"question": "What is a latent variable example", "positive_ctxs": [{"text": "Examples of latent variables from the field of economics include quality of life, business confidence, morale, happiness and conservatism: these are all variables which cannot be measured directly."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "On a technical note, estimation of a latent variable is done by analyzing the variance and covariance of the indicators. The measurement model of a latent variable with effect indicators is the set of relationships (modeled as equations) in which the latent variable is set as the predictor of the indicators."}, {"text": "A latent variable is a variable that is inferred using models from observed data.  Approaches to inferring latent variables from data include: using a single observed variable, multi-item scales, predictive models, dimension reduction techniques such as factor analysis, structural equation models, and mixture models."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "A manifest variable is a variable or factor that can be directly measured or observed. It is the opposite of a latent variable, which is a factor that cannot be directly observed, and which needs a manifest variable assigned to it as an indicator to test whether it is present."}, {"text": "The standard solution that psychologists take to measuring latent variables is to use a series of questions that are all designed to measure the latent variable. This is known as a multi-item scale, where an \u201citem\u201d is a question, and a \u201cscale\u201d is the resulting estimate of the latent variable."}, {"text": "SEM uses latent variables to account for measurement error. Latent Variables. A latent variable is a hypothetical construct that is invoked to explain observed covariation in behavior. Examples in psychology include intelligence (a.k.a. cognitive ability), Type A personality, and depression."}]}, {"question": "How do you do a randomization test in R", "positive_ctxs": [{"text": "2:585:44Suggested clip \u00b7 117 secondsC1 R: Using R to Conduct a Randomization Test - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "False negatives \u2014 that is, a test that says you don't have the virus when you actually do have the virus \u2014 may occur."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Statistics can never \"prove\" anything. All a statistical test can do is assign a probability to the data you have, indicating the likelihood (or probability) that these numbers come from random fluctuations in sampling."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "\u201cThe decision of whether to use a one\u2010 or a two\u2010tailed test is important because a test statistic that falls in the region of rejection in a one\u2010tailed test may not do so in a two\u2010tailed test, even though both tests use the same probability level.\u201d"}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "What is Mnist Load_data ()", "positive_ctxs": [{"text": "load_data function Loads the MNIST dataset. This is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More info can be found at the MNIST homepage."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "How do you avoid Simpson's paradox", "positive_ctxs": [{"text": "Simpson's paradox can be avoided by selecting an appropriate experimental design and analysis that incorporates the confounding variable in such a way as to obtain unconfounded estimates of treatment effects, thus more accurately answering the research question."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How to Deal with MulticollinearityRedesign the study to avoid multicollinearity.  Increase sample size.  Remove one or more of the highly-correlated independent variables.  Define a new variable equal to a linear combination of the highly-correlated variables."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "The problem is a paradox of the veridical type, because the correct choice (that one should switch doors) is so counterintuitive it can seem absurd, but is nevertheless demonstrably true."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How to avoid selection biasesUsing random methods when selecting subgroups from populations.Ensuring that the subgroups selected are equivalent to the population at large in terms of their key characteristics (this method is less of a protection than the first, since typically the key characteristics are not known)."}]}, {"question": "What does Communalities mean in factor analysis", "positive_ctxs": [{"text": "Communalities \u2013 This is the proportion of each variable's variance that can be explained by the factors (e.g., the underlying latent continua). It is also noted as h2 and can be defined as the sum of squared factor loadings for the variables.  They are the reproduced variances from the factors that you have extracted."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Communalities \u2013 This is the proportion of each variable's variance that can be explained by the factors (e.g., the underlying latent continua). It is also noted as h2 and can be defined as the sum of squared factor loadings for the variables."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "\"Correlation is not causation\" means that just because two things correlate does not necessarily mean that one causes the other.  Correlations between two things can be caused by a third factor that affects both of them."}, {"text": "\"Correlation is not causation\" means that just because two things correlate does not necessarily mean that one causes the other.  Correlations between two things can be caused by a third factor that affects both of them."}, {"text": "There are two types of factor analyses, exploratory and confirmatory. Exploratory factor analysis (EFA) is method to explore the underlying structure of a set of observed variables, and is a crucial step in the scale development process. The first step in EFA is factor extraction."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}]}, {"question": "What is a predictor variable in multiple regression", "positive_ctxs": [{"text": "Multiple regression (an extension of simple linear regression) is used to predict the value of a dependent variable (also known as an outcome variable) based on the value of two or more independent variables (also known as predictor variables)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "As the name implies, multivariate regression is a technique that estimates a single regression model with more than one outcome variable. When there is more than one predictor variable in a multivariate regression model, the model is a multivariate multiple regression."}, {"text": "Test for Significance of Regression. The test for significance of regression in the case of multiple linear regression analysis is carried out using the analysis of variance. The test is used to check if a linear statistical relationship exists between the response variable and at least one of the predictor variables."}, {"text": "A Latent Class regression model: Is used to predict a dependent variable as a function of predictor variables (Regression model). Includes a K-category latent variable X to cluster cases (LC model)  Each case may contain multiple records (Regression with repeated measurements)."}, {"text": "Regression analysis consists of a set of machine learning methods that allow us to predict a continuous outcome variable (y) based on the value of one or multiple predictor variables (x). Briefly, the goal of regression model is to build a mathematical equation that defines y as a function of the x variables."}, {"text": "Regression analysis consists of a set of machine learning methods that allow us to predict a continuous outcome variable (y) based on the value of one or multiple predictor variables (x). Briefly, the goal of regression model is to build a mathematical equation that defines y as a function of the x variables."}, {"text": "In multivariate regression there are more than one dependent variable with different variances (or distributions).  But when we say multiple regression, we mean only one dependent variable with a single distribution or variance. The predictor variables are more than one."}, {"text": "Whereas multiple regression predicts a single dependent variable from a set of multiple independent variables, canonical correlation simultaneously predicts multiple dependent variables from multiple independent variables."}]}, {"question": "What is detection and estimation theory", "positive_ctxs": [{"text": "Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Detection theory or signal detection theory is a means to measure the ability to differentiate between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the"}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Outlier detection is extensively used in a wide variety of applications such as military surveillance for enemy activities to prevent attacks, intrusion detection in cyber security, fraud detection for credit cards, insurance or health care and fault detection in safety critical systems and in various kind of images."}, {"text": "The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986. Canny also produced a computational theory of edge detection explaining why the technique works."}, {"text": "There are two types of estimations used: point and interval. A point estimation is a type of estimation that uses a single value, a sample statistic, to infer information about the population.  Interval estimation is the range of numbers in which a population parameter lies considering margin of error."}]}, {"question": "How do you predict confidence intervals in R", "positive_ctxs": [{"text": "To find the confidence interval in R, create a new data. frame with the desired value to predict. The prediction is made with the predict() function. The interval argument is set to 'confidence' to output the mean interval."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition. The class intervals are the subsets into which the data is grouped. The width of the class intervals will be a compromise between having intervals short enough so that not all of the observations fall in the same interval, but long enough so that you do not end up with only one observation per interval."}, {"text": "Definition. The class intervals are the subsets into which the data is grouped. The width of the class intervals will be a compromise between having intervals short enough so that not all of the observations fall in the same interval, but long enough so that you do not end up with only one observation per interval."}, {"text": "False confidence in stepwise results The standard errors of the coefficient estimates are underestimated, which makes the confidence intervals too narrow, the t statistics too high, and the p values too low\u2014which leads to overfitting and creates a false confidence in the final model."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Probability sampling gives you the best chance to create a sample that is truly representative of the population. Using probability sampling for finding sample sizes means that you can employ statistical techniques like confidence intervals and margins of error to validate your results."}, {"text": "Confidence intervals measure the degree of uncertainty or certainty in a sampling method. They can take any number of probability limits, with the most common being a 95% or 99% confidence level. Confidence intervals are conducted using statistical methods, such as a t-test."}, {"text": "OLS does not require that the error term follows a normal distribution to produce unbiased estimates with the minimum variance. However, satisfying this assumption allows you to perform statistical hypothesis testing and generate reliable confidence intervals and prediction intervals."}]}, {"question": "Why is transfer learning useful", "positive_ctxs": [{"text": "Transfer learning is useful when you have insufficient data for a new domain you want handled by a neural network and there is a big pre-existing data pool that can be transferred to your problem."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Transfer learning without any labeled data from the target domain is referred to as unsupervised transfer learning."}, {"text": "1 Answer. Transfer learning is when a model developed for one task is reused to work on a second task. Fine tuning is one approach to transfer learning."}, {"text": "1 Answer. Transfer learning is when a model developed for one task is reused to work on a second task. Fine tuning is one approach to transfer learning."}, {"text": "Neural style transfer is trained as a supervised learning task in which the goal is to input two images (x), and train a network to output a new, synthesized image (y)."}, {"text": "The basic premise of transfer learning is simple: take a model trained on a large dataset and transfer its knowledge to a smaller dataset. For object recognition with a CNN, we freeze the early convolutional layers of the network and only train the last few layers which make a prediction."}, {"text": "Fine tuning is one approach to transfer learning, and it is very popular in computer vision and NLP. The most common example given is when a model is trained on ImageNet is fine-tuned on a second task.  Transfer learning is when a model developed for one task is reused for a model on a second task."}, {"text": "In deep learning, transfer learning is a technique whereby a neural network model is first trained on a problem similar to the problem that is being solved. One or more layers from the trained model are then used in a new model trained on the problem of interest."}]}, {"question": "How do you find the marginal distribution of X", "positive_ctxs": [{"text": "g(x) = \u03a3y f (x,y) and h(y) = \u03a3x f (x,y) are the marginal distributions of X and Y , respectively. If you're great with equations, that's probably all you need to know. It tells you how to find a marginal distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "their joint probability distribution at (x,y), the functions given by: g(x) = \u03a3y f (x,y) and h(y) = \u03a3x f (x,y) are the marginal distributions of X and Y , respectively. If you're great with equations, that's probably all you need to know. It tells you how to find a marginal distribution."}, {"text": "First, to find the conditional distribution of X given a value of Y, we can think of fixing a row in Table 1 and dividing the values of the joint pmf in that row by the marginal pmf of Y for the corresponding value. For example, to find pX|Y(x|1), we divide each entry in the Y=1 row by pY(1)=1/2."}, {"text": "Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point.  Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood)."}, {"text": "A marginal distribution is the percentages out of totals, and conditional distribution is the percentages out of some column."}, {"text": "If we know the joint CDF of X and Y, we can find the marginal CDFs, FX(x) and FY(y). Specifically, for any x\u2208R, we have FXY(x,\u221e)=P(X\u2264x,Y\u2264\u221e)=P(X\u2264x)=FX(x). Here, by FXY(x,\u221e), we mean limy\u2192\u221eFXY(x,y). Similarly, for any y\u2208R, we have FY(y)=FXY(\u221e,y)."}, {"text": "You can find the decision boundary analytically. For Bayesian hypothesis testing, the decision boundary corresponds to the values of X that have equal posteriors, i.e., you need to solve: for X = (x1, x2)."}, {"text": "In the case of a pair of random variables (X, Y), when random variable X (or Y) is considered by itself, its density function is called the marginal density function."}]}, {"question": "What is the drift in data collection", "positive_ctxs": [{"text": "Data drift is the sum of data changes \u2014 think mobile interactions, sensor logs and web clickstreams \u2014 that started life as well-meaning business tweaks or system updates, as CMSWire contributor, Girish Pancha, explains in greater detail here."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Data Drift Defined Data drift is unexpected and undocumented changes to data structure, semantics, and infrastructure that is a result of modern data architectures. Data drift breaks processes and corrupts data, but can also reveal new opportunities for data use."}, {"text": "The ability to detect and adapt to changes in the distribution of examples is paramount for data stream mining algorithms. The shift in the underlying distribution of examples arriving from a data stream is referred to as concept drift. Concept drift occurs over time and the rate at which the drifts occurs varies."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Yes, there are. One example is the WEKA MOA framework [1]. This framework implements standard algorithms in the literature of concept drift detection.  The nice thing about this framework is that it allows users to generate new data streams which contains concept drifts of different types."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Sampling is done because you usually cannot gather data from the entire population. Even in relatively small populations, the data may be needed urgently, and including everyone in the population in your data collection may take too long."}, {"text": "Sampling is done because you usually cannot gather data from the entire population. Even in relatively small populations, the data may be needed urgently, and including everyone in the population in your data collection may take too long."}]}, {"question": "What is the main difference between recursive and non recursive filters in DSP", "positive_ctxs": [{"text": "1 Answers found. A recursive filter has a system in which the output is directly dependent on one or more of its past outputs. But in a non recursive filter the system followed is the one in which the output is independent of any of the past outputs like, the feed-forward system where the system is having no feedback."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Any LTI filter with output and input can be represented by a difference equation in the form: If at least one of the is not null, the filter is recursive. If the are all zero, it is a non recursive filter usually called FIR (Finite Input Response) filter.  This happens both to recursive and non recursive filters."}, {"text": "Since this impulse response in infinitely long, recursive filters are often called infinite impulse response (IIR) filters. In effect, recursive filters convolve the input signal with a very long filter kernel, although only a few coefficients are involved."}, {"text": "Since this impulse response in infinitely long, recursive filters are often called infinite impulse response (IIR) filters. In effect, recursive filters convolve the input signal with a very long filter kernel, although only a few coefficients are involved."}, {"text": "Recursive and Nonrecursive Discrete-Time Systems This is a recursive system which means the output at time n depends on any number of a past output values. So, a recursive system has feed back output of the system into the input."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Answer: Recursive function is a function which calls itself again and again.  A recursive function in general has an extremely high time complexity while a non-recursive one does not. A recursive function generally has smaller code size whereas a non-recursive one is larger."}, {"text": "The infinite impulse response (IIR) filter is a recursive filter in that the output from the filter is computed by using the current and previous inputs and previous outputs. Because the filter uses previous values of the output, there is feedback of the output in the filter structure."}]}, {"question": "What does it mean when homogeneity of variance is violated", "positive_ctxs": [{"text": "The assumption of homogeneity of variance means that the level of variance for a particular variable is constant across the sample.  In ANOVA, when homogeneity of variance is violated there is a greater probability of falsely rejecting the null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Both tests relate the mean difference to the variance (variability of measurements) (and to the sample size). The z-test assumes that the variance is known, whereas the t-test does not make this assumption. Usually one does not know the variance, so one needs to estimate it from the available data."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "The variance of the sampling distribution of the mean is computed as follows: That is, the variance of the sampling distribution of the mean is the population variance divided by N, the sample size (the number of scores used to compute a mean)."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}]}, {"question": "In Experimental Design what is the difference between blocking and stratified sampling", "positive_ctxs": [{"text": "Blocks and strata are different. Blocking refers to classifying experimental units into blocks whereas stratification refers to classifying individuals of a population into strata. The samples from the strata in a stratified random sample can be the blocks in an experiment."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "The main difference between quota and stratified sampling can be explained in a way that in quota sampling researchers use non-random sampling methods to gather data from one stratum until the required quota fixed by the researcher is fulfilled."}, {"text": "The difference between quota sampling and stratified sampling is: although both \"group\" participants by an important characteristic, stratified sampling relies on random selection within each group, while quota sampling relies on convenience sampling within each group."}, {"text": "Connection to stratified sampling Quota sampling is the non-probability version of stratified sampling. In stratified sampling, subsets of the population are created so that each subset has a common characteristic, such as gender."}, {"text": "Experimental probability is the result of an experiment. Theoretical probability is what is expected to happen. Three students tossed a coin 50 times individually."}]}, {"question": "What is time complexity of an algorithm explain with example", "positive_ctxs": [{"text": "An algorithm is said to be constant time (also written as O(1) time) if the value of T(n) is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an array takes constant time as only one operation has to be performed to locate it."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Abstract: The k-means algorithm is known to have a time complexity of O(n 2 ), where n is the input data size. This quadratic complexity debars the algorithm from being effectively used in large applications."}, {"text": "Time Complexity and Space Complexity are two factors which determine which algorithm is better than the other. Time Complexity in a simple way means the amount of time an algorithm takes to run. Space complexity means the amount of space required by the algorithm."}, {"text": "Knuth Morris Pratt (KMP) is an algorithm, which checks the characters from left to right. When a pattern has a sub-pattern appears more than one in the sub-pattern, it uses that property to improve the time complexity, also for in the worst case. The time complexity of KMP is O(n)."}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}, {"text": "Definition. Multi-label learning is an extension of the standard supervised learning setting. In contrast to standard supervised learning where one training example is associated with a single class label, in multi-label learning, one training example is associated with multiple class labels simultaneously."}, {"text": "Asymptotic Analysis is the big idea that handles above issues in analyzing algorithms. In Asymptotic Analysis, we evaluate the performance of an algorithm in terms of input size (we don't measure the actual running time). We calculate, how the time (or space) taken by an algorithm increases with the input size."}]}, {"question": "When did artificial intelligence start", "positive_ctxs": [{"text": "1950s"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multi-agent reinforcement learning is the study of numerous artificial intelligence agents cohabitating in an environment, often collaborating toward some end goal. When focusing on collaboration, it derives inspiration from other social structures in the animal kingdom. It also draws heavily on game theory."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}, {"text": "Sudharsan also noted that deep meta reinforcement learning will be the future of artificial intelligence where we will implement artificial general intelligence (AGI) to build a single model to master a wide variety of tasks. Thus each model will be capable to perform a wide range of complex tasks."}, {"text": "To put put it bluntly, Artificial intelligence (AI) relies on machines, whereas Collective Intelligence (CI) relies on people. AI stands for the simulation of human intelligence by machines, computers or software systems.  In fact, artificial and collective intelligence can -and should \u2013 reinforce each other."}, {"text": "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}]}, {"question": "What is meant by relative efficiency", "positive_ctxs": [{"text": "The relative efficiency of two procedures is the ratio of their efficiencies, although often this concept is used where the comparison is made between a given procedure and a notional \"best possible\" procedure."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "FAQ Explanation: Volumetric efficiency is the ratio of the volume of charge admitted at N.T.P. to the swept volume of the piston while mechanical efficiency is the ratio of the brake power to the indicated power and relative efficiency is the ratio of the indicated thermal efficiency to the air standard efficiency"}, {"text": "The relative efficiency of two procedures is the ratio of their efficiencies, although often this concept is used where the comparison is made between a given procedure and a notional \"best possible\" procedure."}, {"text": "The work efficiency formula is efficiency = output / input, and you can multiply the result by 100 to get work efficiency as a percentage. This is used across different methods of measuring energy and work, whether it's energy production or machine efficiency."}, {"text": "Efficiency: For an unbiased estimator, efficiency indicates how much its precision is lower than the theoretical limit of precision provided by the Cramer-Rao inequality. A measure of efficiency is the ratio of the theoretically minimal variance to the actual variance of the estimator."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "There is a direct relationship between the coefficients produced by logit and the odds ratios produced by logistic. First, let's define what is meant by a logit: A logit is defined as the log base e (log) of the odds. : [1] logit(p) = log(odds) = log(p/q) The range is negative infinity to positive infinity."}, {"text": "The only difference between a relative frequency distribution graph and a frequency distribution graph is that the vertical axis uses proportional or relative frequency rather than simple frequency. Cumulative relative frequency (also called an ogive) is the accumulation of the previous relative frequencies."}]}, {"question": "How do you test for normality", "positive_ctxs": [{"text": "value of the Shapiro-Wilk Test is greater than 0.05, the data is normal. If it is below 0.05, the data significantly deviate from a normal distribution. If you need to use skewness and kurtosis values to determine normality, rather the Shapiro-Wilk test, you will find these in our enhanced testing for normality guide."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Yes, you should check normality of errors AFTER modeling. In linear regression, errors are assumed to follow a normal distribution with a mean of zero. Let's do some simulations and see how normality influences analysis results and see what could be consequences of normality violation."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "False negatives \u2014 that is, a test that says you don't have the virus when you actually do have the virus \u2014 may occur."}, {"text": "Nonparametric tests have the following limitations: Nonparametric tests are usually less powerful than corresponding parametric test when the normality assumption holds. Thus, you are less likely to reject the null hypothesis when it is false if the data comes from the normal distribution."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above . 05 indicates normality."}]}, {"question": "What is the mean of a Bernoulli distribution", "positive_ctxs": [{"text": "A Bernouilli distribution is a discrete probability distribution for a Bernouilli trial \u2014 a random experiment that has only two outcomes (usually called a \u201cSuccess\u201d or a \u201cFailure\u201d).  The expected value for a random variable, X, from a Bernoulli distribution is: E[X] = p. For example, if p = . 04, then E[X] = 0.4."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Bernoulli distribution represents the success or failure of a single Bernoulli trial. The Binomial Distribution represents the number of successes and failures in n independent Bernoulli trials for some given value of n.  Another example is the number of heads obtained in tossing a coin n times."}, {"text": "A logistic regression estimates the mean of your response given that your data is distributed Bernoulli or is a Binomial trial. Since the mean of a Binomial trial is the probability of success, you can interpret the output from a Logistic regression (after logit transformation) as a probability of success."}, {"text": "The mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean \u03bc, then the mean of the sampling distribution of the mean is also \u03bc. The symbol \u03bcM is used to refer to the mean of the sampling distribution of the mean."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Let X be a discrete random variable with the Bernoulli distribution with parameter p: X\u223cBern(p) Then the variance of X is given by: var(X)=p(1\u2212p)"}, {"text": "The geometric distribution represents the number of failures before you get a success in a series of Bernoulli trials. This discrete probability distribution is represented by the probability density function: f(x) = (1 \u2212 p)x \u2212 1p."}, {"text": "Typically by the time the sample size is 30 the distribution of the sample mean is practically the same as a normal distribution.  \u00afX, the mean of the measurements in a sample of size n; the distribution of \u00afX is its sampling distribution, with mean \u03bc\u00afX=\u03bc and standard deviation \u03c3\u00afX=\u03c3\u221an."}]}, {"question": "How can algorithms be biased", "positive_ctxs": [{"text": "Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Humans are error-prone and biased, but that doesn't mean that algorithms are necessarily better.  But these systems can be biased based on who builds them, how they're developed, and how they're ultimately used. This is commonly known as algorithmic bias."}, {"text": "Humans are error-prone and biased, but that doesn't mean that algorithms are necessarily better.  But these systems can be biased based on who builds them, how they're developed, and how they're ultimately used. This is commonly known as algorithmic bias."}, {"text": "Humans are error-prone and biased, but that doesn't mean that algorithms are necessarily better.  But these systems can be biased based on who builds them, how they're developed, and how they're ultimately used. This is commonly known as algorithmic bias."}, {"text": "Bias can creep into algorithms in several ways. AI systems learn to make decisions based on training data, which can include biased human decisions or reflect historical or social inequities, even if sensitive variables such as gender, race, or sexual orientation are removed."}, {"text": "An biased estimator is one which delivers an estimate which is consistently different from the parameter to be estimated. In a more formal definition we can define that the expectation E of a biased estimator is not equal to the parameter of a population."}, {"text": "How to Prevent OverfittingCross-validation. Cross-validation is a powerful preventative measure against overfitting.  Train with more data. It won't work every time, but training with more data can help algorithms detect the signal better.  Remove features.  Early stopping.  Regularization.  Ensembling."}, {"text": "Popular algorithms that can be used for binary classification include:Logistic Regression.k-Nearest Neighbors.Decision Trees.Support Vector Machine.Naive Bayes."}]}, {"question": "Can we change the value of static variable", "positive_ctxs": [{"text": "Static final variables 2) The variable MY_VAR is public which means any class can use it. It is a static variable so you won't need any object of class in order to access it. It's final so the value of this variable can never be changed in the current or in any class."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The formula for a simple linear regression is:y is the predicted value of the dependent variable (y) for any given value of the independent variable (x).B0 is the intercept, the predicted value of y when the x is 0.B1 is the regression coefficient \u2013 how much we expect y to change as x increases.More items\u2022"}, {"text": "Linear regression is the next step up after correlation. It is used when we want to predict the value of a variable based on the value of another variable. The variable we want to predict is called the dependent variable (or sometimes, the outcome variable)."}, {"text": "Linear regression is the next step up after correlation. It is used when we want to predict the value of a variable based on the value of another variable. The variable we want to predict is called the dependent variable (or sometimes, the outcome variable)."}, {"text": "We can use the regression line to predict a value of \"Y\" for any \"X\" score. The steepness of the angle of the regression line is called its slope. It is the amount of change in \"Y\" that we can expect for any unit change in \"X\"."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}]}, {"question": "What does Covariance indicate", "positive_ctxs": [{"text": "Covariance indicates the relationship of two variables whenever one variable changes. If an increase in one variable results in an increase in the other variable, both variables are said to have a positive covariance.  Both variables move together in the same direction when they change."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Covariance is the measure of how much two sets of data vary. The Covariance determines the degree to which the two variables are related or how they vary together. The Covariance is the average of the product of deviations of data points from their respective means, based on the following formula."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "A Correlation of 0 means that there is no linear relationship between the two variables. We already know that if two random variables are independent, the Covariance is 0. We can see that if we plug in 0 for the Covariance to the equation for Correlation, we will get a 0 for the Correlation."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}]}, {"question": "How do you calculate true positive from sensitivity and specificity", "positive_ctxs": [{"text": "Multiply the Total with disease by the Sensitivity to get the number of True positives. Multiply the Total without disease by the Specificity to get the number of True Negatives. Compute the number of False positives and False negatives by subtraction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An ROC curve shows the relationship between clinical sensitivity and specificity for every possible cut-off. The ROC curve is a graph with: The x-axis showing 1 \u2013 specificity (= false positive fraction = FP/(FP+TN)) The y-axis showing sensitivity (= true positive fraction = TP/(TP+FN))"}, {"text": "An ROC curve shows the relationship between clinical sensitivity and specificity for every possible cut-off. The ROC curve is a graph with: The x-axis showing 1 \u2013 specificity (= false positive fraction = FP/(FP+TN)) The y-axis showing sensitivity (= true positive fraction = TP/(TP+FN))"}, {"text": "Sensitivity and specificity are inversely proportional, meaning that as the sensitivity increases, the specificity decreases and vice versa."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "The sensitivity and specificity of a test often vary with disease prevalence; this effect is likely to be the result of mechanisms, such as patient spectrum, that affect prevalence, sensitivity and specificity."}, {"text": "The sensitivity of the test reflects the probability that the screening test will be positive among those who are diseased. In contrast, the specificity of the test reflects the probability that the screening test will be negative among those who, in fact, do not have the disease."}, {"text": "In machine learning, the true positive rate, also referred to sensitivity or recall, is used to measure the percentage of actual positives which are correctly identified."}]}, {"question": "How do you improve validation", "positive_ctxs": [{"text": "2 AnswersUse weight regularization. It tries to keep weights low which very often leads to better generalization.  Corrupt your input (e.g., randomly substitute some pixels with black or white).  Expand your training set.  Pre-train your layers with denoising critera.  Experiment with network architecture."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "The second reason you may see validation loss lower than training loss is due to how the loss value are measured and reported: Training loss is measured during each epoch. While validation loss is measured after each epoch."}, {"text": "The second reason you may see validation loss lower than training loss is due to how the loss value are measured and reported: Training loss is measured during each epoch. While validation loss is measured after each epoch."}, {"text": "The second reason you may see validation loss lower than training loss is due to how the loss value are measured and reported: Training loss is measured during each epoch. While validation loss is measured after each epoch."}, {"text": "How to Calculate a Confusion MatrixYou need a test dataset or a validation dataset with expected outcome values.Make a prediction for each row in your test dataset.From the expected outcomes and predictions count: The number of correct predictions for each class."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "What is the difference between a one tailed test and a two tailed test", "positive_ctxs": [{"text": "This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28. For a two tailed test, use \u03b1/2 = 0.05 and the critical region is below z = -1.645 and above z = 1.645."}, {"text": "Depending on the alternative hypothesis operator, greater than operator will be a right tailed test, less than operator is a left tailed test, and not equal operator is a two tailed test."}, {"text": "The null hypothesis (H0) for a one tailed test is that the mean is greater (or less) than or equal to \u00b5, and the alternative hypothesis is that the mean is < (or >, respectively) \u00b5."}, {"text": "\u27a2 To determine the critical region for a normal distribution, we use the table for the standard normal distribution. If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28."}, {"text": "One or two of the sections is the \u201crejection region\u201c; if your test value falls into that region, then you reject the null hypothesis. A one tailed test with the rejection rejection in one tail. The critical value is the red line to the left of that region."}, {"text": "Before you can figure out if you have a left tailed test or right tailed test, you have to make sure you have a single tail to begin with. A tail in hypothesis testing refers to the tail at either end of a distribution curve. Area under a normal distribution curve. Two tails (both left and right) are shaded."}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}]}, {"question": "What does it mean for a Markov chain to be aperiodic", "positive_ctxs": [{"text": "You can show that all states in the same communicating class have the same period. A class is said to be periodic if its states are periodic. Similarly, a class is said to be aperiodic if its states are aperiodic. Finally, a Markov chain is said to be aperiodic if all of its states are aperiodic."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed."}, {"text": "A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed."}, {"text": "A Markov chain is ergodic if it is both irreducible and aperiodic. This condition is equivalent to the transition matrix being a primitive nonnegative matrix."}, {"text": "Regular Markov Chains. \u25cb A transition matrix P is regular if some power of P has only positive entries. A Markov chain is a regular Markov chain if its transition matrix is regular. For example, if you take successive powers of the matrix D, the entries of D will always be positive (or so it appears)."}, {"text": "A Markov chain in which every state can be reached from every other state is called an irreducible Markov chain. If a Markov chain is not irreducible, but absorbable, the sequences of microscopic states may be trapped into some independent closed states and never escape from such undesirable states."}, {"text": "In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain."}]}, {"question": "What is alpha level", "positive_ctxs": [{"text": "Before you run any statistical test, you must first determine your alpha level, which is also called the \u201csignificance level.\u201d By definition, the alpha level is the probability of rejecting the null hypothesis when the null hypothesis is true.  Like all probabilities, alpha ranges from 0 to 1."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Why is an alpha level of . 05 commonly used? Seeing as the alpha level is the probability of making a Type I error, it seems to make sense that we make this area as tiny as possible.  The smaller the alpha level, the smaller the area where you would reject the null hypothesis."}, {"text": "Rejection Regions and Alpha Levels You, as a researcher, choose the alpha level you are willing to accept. For example, if you wanted to be 95% confident that your results are significant, you would choose a 5% alpha level (100% \u2013 95%). That 5% level is the rejection region."}, {"text": "The significance level, also denoted as alpha or \u03b1, is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference."}, {"text": "Alpha levels and beta levels are related: An alpha level is the probability of a type I error, or rejecting the null hypothesis when it is true. A beta level, usually just called beta(\u03b2), is the opposite; the probability of of accepting the null hypothesis when it's false."}, {"text": "The probability of Type 1 error is alpha -- the criterion that we set as the level at which we will reject the null hypothesis. The p value is something else -- it tells you how UNUSUAL the data are, given the assumption that the null hypothesis is true."}, {"text": "Medical Definition of alpha state : a state of wakeful relaxation that is associated with increased alpha wave activity When electroencephalograms show a brain wave pattern of 9 to 12 cycles per second, the subject is said to be in alpha state, usually described as relaxed, peaceful, or floating.\u2014"}, {"text": "When we refer to values as being \u201cstatistically equivalent\u201d or to a \u201cconclusion of statistical equivalence,\u201d we mean the difference between groups is smaller than what is considered meaningful and statistically falls within the interval indicated by the equivalence bounds. In any one-sided test, for an alpha level of ."}]}, {"question": "What is truncated Bptt", "positive_ctxs": [{"text": "Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "How does random forest predict", "positive_ctxs": [{"text": "Random forest is a supervised learning algorithm which is used for both classification as well as regression.  Similarly, random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How to reduce False Positive and False Negative in binary classificationfirstly random forest overfits if the training data and testing data are not drawn from same distribution.check the data for linearity,multicollinearity ,outliers,etc.More items"}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "Random Forest is less computationally expensive and does not require a GPU to finish training. A random forest can give you a different interpretation of a decision tree but with better performance. Neural Networks will require much more data than an everyday person might have on hand to actually be effective."}, {"text": "Random forest will reduce variance part of error rather than bias part, so on a given training data set decision tree may be more accurate than a random forest. But on an unexpected validation data set, Random forest always wins in terms of accuracy."}, {"text": "The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree."}, {"text": "There is really only one advantage to using a random forest over a decision tree: It reduces overfitting and is therefore more accurate."}]}, {"question": "What is the use of statistics in machine learning", "positive_ctxs": [{"text": "Statistics is generally considered a prerequisite to the field of applied machine learning. We need statistics to help transform observations into information and to answer questions about samples of observations."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Most machine learning roles will require the use of Python or C/C++ (though Python is often preferred). Background in the theory behind machine learning algorithms and an understanding of how they can be efficiently implemented in terms of both space and time is critical."}, {"text": "Gram matrix is simply the matrix of the inner product of each vector and its corresponding vectors in same. It found use in the current machine learning is due to deep learning loss where while style transferring the loss function is computed using the gram matrix."}, {"text": "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data."}, {"text": "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data."}, {"text": "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data."}, {"text": "In statistics and machine learning, the bias\u2013variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters.  The bias error is an error from erroneous assumptions in the learning algorithm."}, {"text": "Statistical inference consists in the use of statistics to draw conclusions about some unknown aspect of a population based on a random sample from that population.  Point estimation is discussed in the statistics section of the encyclopedia."}]}, {"question": "Are parameters random", "positive_ctxs": [{"text": "Another view however is that the parameter value used to generate the data that are obtained in your study is just one drawn parameter value, where the draw is from some distribution (the prior).  as parameters, but rather as random or latent effects."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "\u201cThe difference is that, in the Bayesian approach, the parameters that we are trying to estimate are treated as random variables. In the frequentist approach, they are fixed. Random variables are governed by their parameters (mean, variance, etc.) and distributions (Gaussian, Poisson, binomial, etc)."}, {"text": "Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data."}, {"text": "The Gamma distribution can be thought of as a generalization of the Chi-square distribution. If a random variable has a Chi-square distribution with degrees of freedom and is a strictly positive constant, then the random variable defined as has a Gamma distribution with parameters and ."}, {"text": "The parameters of the distribution are m and s2, where m is the mean (expectation) of the distribution and s2 is the variance. We write X ~ N(m, s2) to mean that the random variable X has a normal distribution with parameters m and s2. If Z ~ N(0, 1), then Z is said to follow a standard normal distribution."}, {"text": "Estimation is the process used to calculated these population parameters by analyzing only a small random sample from the population. The value or range of values used to approximate a parameter is called an estimate."}, {"text": "Grid-searching is the process of scanning the data to configure optimal parameters for a given model. Depending on the type of model utilized, certain parameters are necessary.  Grid-searching can be applied across machine learning to calculate the best parameters to use for any given model."}]}, {"question": "What is the difference between a one sided and two sided t test", "positive_ctxs": [{"text": "A one-tailed test has the entire 5% of the alpha level in one tail (in either the left, or the right tail). A two-tailed test splits your alpha level in half (as in the image to the left)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The difference between these two statistical measurements is that correlation measures the degree of a relationship between two variables (x and y), whereas regression is how one variable affects another."}, {"text": "The difference is a matter of design. In the test of independence, observational units are collected at random from a population and two categorical variables are observed for each unit.  In the goodness-of-fit test there is only one observed variable."}, {"text": "The difference is a matter of design. In the test of independence, observational units are collected at random from a population and two categorical variables are observed for each unit.  In the goodness-of-fit test there is only one observed variable."}]}, {"question": "What is the mean of a discrete random variable", "positive_ctxs": [{"text": "The mean of a discrete random variable X is a weighted average of the possible values that the random variable can take. Unlike the sample mean of a group of observations, which gives each observation equal weight, the mean of a random variable weights each outcome xi according to its probability, pi."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To find the expected value, E(X), or mean \u03bc of a discrete random variable X, simply multiply each value of the random variable by its probability and add the products. The formula is given as E(X)=\u03bc=\u2211xP(x)."}, {"text": "The probability of each value of the discrete random variable is between 0 and\u200b 1, inclusive, and the sum of all the probabilities is 1.  It is the expected value of a discrete random variable."}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "A random variable is a variable whose value is a numerical outcome of a random phenomenon. A discrete random variable X has a countable number of possible values. Example: Let X represent the sum of two dice.  A continuous random variable X takes all values in a given interval of numbers."}, {"text": "A random variable is a numerical description of the outcome of a statistical experiment.  For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x). This function provides the probability for each value of the random variable."}, {"text": "Definition: Entropy is a measure of uncertainty of a random variable. The entropy of a discrete random variable X with alphabet X is H(X) = -) p(x) log p(2) DEX When the base of the logarithm is 2, entropy is measured in bits."}]}, {"question": "Can we use sets in calculated fields", "positive_ctxs": [{"text": "Sets can be used in calculated fields Sets can be used in calculated fields as if they were a field.  Or you can have the calculation return a specific value, or return another field instead, the main point is that they are not very different than normal dimensions in this respect."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Clustering methods are used to identify groups of similar objects in a multivariate data sets collected from fields such as marketing, bio-medical and geo-spatial. They are different types of clustering methods, including: Partitioning methods. Hierarchical clustering."}, {"text": "Clustering methods are used to identify groups of similar objects in a multivariate data sets collected from fields such as marketing, bio-medical and geo-spatial. They are different types of clustering methods, including: Partitioning methods. Hierarchical clustering."}, {"text": "among the constituent fields of anthropology. Physical anthropology has made the most use of statistics, while archeology, linguistics, and cultural anthropology have employed them much less frequently."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Yes. We Can Always Use The Normal Distribution To Approximate The Binomial Distribution."}, {"text": "Dual Booting Can Impact Disk Swap Space. In most cases there shouldn't be too much impact on your hardware from dual booting.  Both Linux and Windows use chunks of the hard disk drive to improve performance while the computer is running."}, {"text": "In Gradient Descent or Batch Gradient Descent, we use the whole training data per epoch whereas, in Stochastic Gradient Descent, we use only single training example per epoch and Mini-batch Gradient Descent lies in between of these two extremes, in which we can use a mini-batch(small portion) of training data per epoch"}]}, {"question": "What is the difference between unimodal and multimodal", "positive_ctxs": [{"text": "13. What is the difference between unimodal, bimodal, and multimodal data? Unimodal data has a distribution that is single-peaked (one mode). Bimodal data has two peaks (2 modes) and multimodal data refer to distributions with more than two clear peaks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A unimodal distribution only has one peak in the distribution, a bimodal distribution has two peaks, and a multimodal distribution has three or more peaks. Another way to describe the shape of histograms is by describing whether the data is skewed or symmetric."}, {"text": "The Shape of a Histogram A histogram is unimodal if there is one hump, bimodal if there are two humps and multimodal if there are many humps. A nonsymmetric histogram is called skewed if it is not symmetric. If the upper tail is longer than the lower tail then it is positively skewed."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities."}, {"text": "The Range is the difference between the lowest and highest values. Example: In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9. So the range is 9 \u2212 3 = 6."}]}, {"question": "What is the relationship between word embedding and topic modeling Can we use word embedding to enhance topic modeling", "positive_ctxs": [{"text": "Word embedding and topic modeling come from two different research communities. Word embeddings come from the neural net research tradition, while topic modelings come from Bayesian model research tradition. Word embedding can be used to improve topic models like Lda2Vec."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The skip-gram model. Both the input vector x and the output y are one-hot encoded word representations. The hidden layer is the word embedding of size N."}, {"text": "fastText is another word embedding method that is an extension of the word2vec model. Instead of learning vectors for words directly, fastText represents each word as an n-gram of characters.  This helps capture the meaning of shorter words and allows the embeddings to understand suffixes and prefixes."}, {"text": "fastText is another word embedding method that is an extension of the word2vec model. Instead of learning vectors for words directly, fastText represents each word as an n-gram of characters.  This helps capture the meaning of shorter words and allows the embeddings to understand suffixes and prefixes."}, {"text": "LDA stands for Latent Dirichlet Allocation, and it is a type of topic modeling algorithm. The purpose of LDA is to learn the representation of a fixed number of topics, and given this number of topics learn the topic distribution that each document in a collection of documents has."}, {"text": "Photo Credit: Pixabay. Topic modeling is a type of statistical modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic."}, {"text": "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.  That you can either train a new embedding or use a pre-trained embedding on your natural language processing task."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is the advantage of CNN", "positive_ctxs": [{"text": "The main advantage of CNN compared to its predecessors is that it automatically detects the important features without any human supervision. For example, given many pictures of cats and dogs, it can learn the key features for each class by itself."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Translational Invariance makes the CNN invariant to translation. Invariance to translation means that if we translate the inputs the CNN will still be able to detect the class to which the input belongs. Translational Invariance is a result of the pooling operation."}, {"text": "Translational Invariance makes the CNN invariant to translation. Invariance to translation means that if we translate the inputs the CNN will still be able to detect the class to which the input belongs. Translational Invariance is a result of the pooling operation."}, {"text": "Advantages. The main advantage of multivariate analysis is that since it considers more than one factor of independent variables that influence the variability of dependent variables, the conclusion drawn is more accurate."}, {"text": "A CNN LSTM can be defined by adding CNN layers on the front end followed by LSTM layers with a Dense layer on the output. It is helpful to think of this architecture as defining two sub-models: the CNN Model for feature extraction and the LSTM Model for interpreting the features across time steps."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig."}]}, {"question": "What is null hypothesis and alternative hypothesis in statistics", "positive_ctxs": [{"text": "The null hypothesis is a general statement that states that there is no relationship between two phenomenons under consideration or that there is no association between two groups. An alternative hypothesis is a statement that describes that there is a relationship between two selected variables in a study."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In research, there is a convention that the hypothesis is written in two forms, the null hypothesis, and the alternative hypothesis (called the experimental hypothesis when the method of investigation is an experiment)."}, {"text": "A null hypothesis is a type of conjecture used in statistics that proposes that there is no difference between certain characteristics of a population or data-generating process. The alternative hypothesis proposes that there is a difference."}, {"text": "A null hypothesis is a type of conjecture used in statistics that proposes that there is no difference between certain characteristics of a population or data-generating process. The alternative hypothesis proposes that there is a difference."}, {"text": "The null hypothesis is the one to be tested and the alternative is everything else. In our example, The null hypothesis would be: The mean data scientist salary is 113,000 dollars. While the alternative: The mean data scientist salary is not 113,000 dollars."}, {"text": "The null hypothesis is the one to be tested and the alternative is everything else. In our example, The null hypothesis would be: The mean data scientist salary is 113,000 dollars. While the alternative: The mean data scientist salary is not 113,000 dollars."}, {"text": "The null hypothesis (H0) for a one tailed test is that the mean is greater (or less) than or equal to \u00b5, and the alternative hypothesis is that the mean is < (or >, respectively) \u00b5."}, {"text": "Rejecting or failing to reject the null hypothesis If our statistical analysis shows that the significance level is below the cut-off value we have set (e.g., either 0.05 or 0.01), we reject the null hypothesis and accept the alternative hypothesis."}]}, {"question": "What is a positive skew in statistics", "positive_ctxs": [{"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A distribution is skewed if one of its tails is longer than the other. The first distribution shown has a positive skew. This means that it has a long tail in the positive direction. The distribution below it has a negative skew since it has a long tail in the negative direction."}, {"text": "A distribution is skewed if one of its tails is longer than the other. The first distribution shown has a positive skew. This means that it has a long tail in the positive direction. The distribution below it has a negative skew since it has a long tail in the negative direction."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Understanding the Correlation Coefficient A value of exactly 1.0 means there is a perfect positive relationship between the two variables. For a positive increase in one variable, there is also a positive increase in the second variable."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "In a normal distribution the mean is zero and the standard deviation is 1. It has zero skew and a kurtosis of 3. Normal distributions are symmetrical, but not all symmetrical distributions are normal."}, {"text": "Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables."}]}, {"question": "What is binning in histogram", "positive_ctxs": [{"text": "In a histogram, the total range of data set (i.e from minimum value to maximum value) is divided into 8 to 15 equal parts. These equal parts are known as bins or class intervals. Each and every observation (or value) in the data set is placed in the appropriate bin."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The optimal binning is the optimal discretization of a variable into bins given a discrete or continuous numeric target."}, {"text": "The Shape of a Histogram A histogram is unimodal if there is one hump, bimodal if there are two humps and multimodal if there are many humps. A nonsymmetric histogram is called skewed if it is not symmetric. If the upper tail is longer than the lower tail then it is positively skewed."}, {"text": "An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value. By looking at the histogram for a specific image a viewer will be able to judge the entire tonal distribution at a glance."}, {"text": "This is calculated as the outer product between the actual rating's histogram vector of ratings and the predicted rating's histogram vector of ratings, normalized such that E and O have the same sum. From these three matrices, the quadratic weighted kappa is calculated."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "Why do we omit one dummy variable", "positive_ctxs": [{"text": "2 Answers. Simply put because one level of your categorical feature (here location) become the reference group during dummy encoding for regression and is redundant. I am quoting form here \"A categorical variable of K categories, or levels, usually enters a regression as a sequence of K-1 dummy variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The definition of a dummy dependent variable model is quite simple: If the dependent, response, left-hand side, or Y variable is a dummy variable, you have a dummy dependent variable model. The reason dummy dependent variable models are important is that they are everywhere."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "A dummy variable is a numerical variable used in regression analysis to represent subgroups of the sample in your study. In research design, a dummy variable is often used to distinguish different treatment groups."}, {"text": "2 Answers. Simply put because one level of your categorical feature (here location) become the reference group during dummy encoding for regression and is redundant. I am quoting form here \"A categorical variable of K categories, or levels, usually enters a regression as a sequence of K-1 dummy variables."}, {"text": "A dummy variable (aka, an indicator variable) is a numeric variable that represents categorical data, such as gender, race, political affiliation, etc.  For example, suppose we are interested in political affiliation, a categorical variable that might assume three values - Republican, Democrat, or Independent."}, {"text": "If there are other predictor variables, all coefficients will be changed.  All the coefficients are jointly estimated, so every new variable changes all the other coefficients already in the model. This is one reason we do multiple regression, to estimate coefficient B1 net of the effect of variable Xm."}, {"text": "Just like the post period dummy variable controls for factors changing over time that are common to both treatment and control groups, the year fixed effects (i.e. year dummy variables) control for factors changing each year that are common to all cities for a given year."}]}, {"question": "Is decision tree linear classifier", "positive_ctxs": [{"text": "Decision trees is a non-linear classifier like the neural networks, etc. It is generally used for classifying non-linearly separable data. Even when you consider the regression example, decision tree is non-linear."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Decision tree classifier \u2013 Decision tree classifier is a systematic approach for multiclass classification. It poses a set of questions to the dataset (related to its attributes/features). The decision tree classification algorithm can be visualized on a binary tree."}, {"text": "Decision tree classifier \u2013 Decision tree classifier is a systematic approach for multiclass classification. It poses a set of questions to the dataset (related to its attributes/features). The decision tree classification algorithm can be visualized on a binary tree."}, {"text": "Naive Bayes is a linear classifier Naive Bayes leads to a linear decision boundary in many common cases.  The red decision line indicates the decision boundary where P(y=1|x)=P(y=2|x)."}, {"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value."}]}, {"question": "What are association rules in machine learning", "positive_ctxs": [{"text": "Association Rule Mining, as the name suggests, association rules are simple If/Then statements that help discover relationships between seemingly independent relational databases or other data repositories. Most machine learning algorithms work with numeric datasets and hence tend to be mathematical."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In data mining, association rules are useful for analyzing and predicting customer behavior. They play an important part in customer analytics, market basket analysis, product clustering, catalog design and store layout. Programmers use association rules to build programs capable of machine learning."}, {"text": "Association Rule Mining, as the name suggests, association rules are simple If/Then statements that help discover relationships between seemingly independent relational databases or other data repositories. Most machine learning algorithms work with numeric datasets and hence tend to be mathematical."}, {"text": "In data science, association rules are used to find correlations and co-occurrences between data sets. They are ideally used to explain patterns in data from seemingly independent information repositories, such as relational databases and transactional databases."}, {"text": "Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness."}, {"text": "Rule-based machine learning approaches include learning classifier systems, association rule learning, artificial immune systems, and any other method that relies on a set of rules, each covering contextual knowledge."}, {"text": "The descriptive analysis uses mainly unsupervised learning approaches for summarizing, classifying, extracting rules to answer what happens was happened in the past. While Predictive analysis is about machine learning approaches for the aim forecasting future data based on past data."}, {"text": "The Apriori algorithm is used for mining frequent itemsets and devising association rules from a transactional database. The parameters \u201csupport\u201d and \u201cconfidence\u201d are used. Support refers to items' frequency of occurrence; confidence is a conditional probability. Items in a transaction form an item set."}]}, {"question": "How do you find the parameter of a lognormal distribution", "positive_ctxs": [{"text": "If x is a lognormally distributed random variable, then y = ln(x) is a normally distributed random variable. The location parameter is equal to the mean of the logarithm of the data points, and the shape parameter is equal to the standard deviation of the logarithm of the data points."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Whereas the normal distribution is the sum/difference of lots of things, the lognormal (because it is the log transform) is the product/quotient of lots of things. So if you are multiplying a bunch of variables together, the resultant distribution approaches lognormal as the number of variables gets large."}, {"text": "The lognormal distribution is commonly used to model the lives of units whose failure modes are of a fatigue-stress nature. Since this includes most, if not all, mechanical systems, the lognormal distribution can have widespread application."}, {"text": "A major difference is in its shape: the normal distribution is symmetrical, whereas the lognormal distribution is not. Because the values in a lognormal distribution are positive, they create a right-skewed curve.  A further distinction is that the values used to derive a lognormal distribution are normally distributed."}, {"text": "The lognormal distribution is a probability distribution whose logarithm has a normal distribution. The mean m and variance v of a lognormal random variable are functions of the lognormal distribution parameters \u00b5 and \u03c3: m = exp ( \u03bc + \u03c3 2 / 2 ) v = exp ( 2 \u03bc + \u03c3 2 ) ( exp ( \u03c3 2 ) \u2212 1 )"}, {"text": "Maximum likelihood estimation is a method that will find the values of \u03bc and \u03c3 that result in the curve that best fits the data.  The goal of maximum likelihood is to find the parameter values that give the distribution that maximise the probability of observing the data."}, {"text": "The answer to that is the Erlang distribution. The Gamma distribution is a generalization of that distribution using a continuous instead of a discrete parameter for the number of events."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}]}, {"question": "Why do we use probability distribution", "positive_ctxs": [{"text": "Probability distributions are a fundamental concept in statistics. They are used both on a theoretical level and a practical level. Some practical uses of probability distributions are: To calculate confidence intervals for parameters and to calculate critical regions for hypothesis tests."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point.  Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood)."}, {"text": "To plot the probability density function for a log normal distribution in R, we can use the following functions: dlnorm(x, meanlog = 0, sdlog = 1) to create the probability density function. curve(function, from = NULL, to = NULL) to plot the probability density function."}, {"text": "In short, the beta distribution can be understood as representing a probability distribution of probabilities- that is, it represents all the possible values of a probability when we don't know what that probability is."}, {"text": "On this site, we use the normal distribution when the population standard deviation is known and the sample size is large. We might use either distribution when standard deviation is unknown and the sample size is very large."}, {"text": "Why is an alpha level of . 05 commonly used? Seeing as the alpha level is the probability of making a Type I error, it seems to make sense that we make this area as tiny as possible.  The smaller the alpha level, the smaller the area where you would reject the null hypothesis."}, {"text": "These are generally used when direct sampling from the probability distribution would be difficult. Some of the use cases of MCMC methods are to approximate a target probability distribution or to compute an integral."}]}, {"question": "What are clusters used for", "positive_ctxs": [{"text": "At a high level, a computer cluster is a group of two or more computers, or nodes, that run in parallel to achieve a common goal. This allows workloads consisting of a high number of individual, parallelizable tasks to be distributed among the nodes in the cluster."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Cluster sampling is best used when the clusters occur naturally in a population, when you don't have access to the entire population, and when the clusters are geographically convenient. However, cluster sampling is not as precise as simple random sampling or stratified random sampling."}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Agglomerative clustering uses a bottom-up approach, wherein each data point starts in its own cluster. These clusters are then joined greedily, by taking the two most similar clusters together and merging them.  For each cluster, you further divide it down to two clusters until you hit the desired number of clusters."}, {"text": "The Agglomerative Hierarchical Clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity."}, {"text": "A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical."}]}, {"question": "How do you calculate the Z score", "positive_ctxs": [{"text": "The formula for calculating a z-score is is z = (x-\u03bc)/\u03c3, where x is the raw score, \u03bc is the population mean, and \u03c3 is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}]}, {"question": "How does sift achieve rotation invariance", "positive_ctxs": [{"text": "In this way the keypoint is \"orientation invariant\" in the sense that if the same keypoint were found in a rotated image, the dominant orientation alignment/subtraction would guarantee the same (or similar) set of orientation histograms and therefore, keypoint signature."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How big data analytics worksdata mining, which sift through data sets in search of patterns and relationships;predictive analytics, which build models to forecast customer behavior and other future developments;machine learning, which taps algorithms to analyze large data sets; and.More items"}, {"text": "How many parity check bits must be included with the data word to achieve single-bit error correction and double error correction when data words are as follows: 16 bits."}, {"text": "Introduction[edit] Shift Invariance simply refers to the 'invariance' that a CNN has to recognising images. It allows the CNN to detect features/objects even if it does not look exactly like the images in it's training period. Shift invariance covers 'small' differences, such as movements shifts of a couple of pixels."}, {"text": "Achieving translation invariance in Convolutional NNs: Then the max pooling layer takes the output from the convolutional layer and reduces its resolution and complexity. It does so by outputting only the max value from a grid.So the information about the exact position of the max value in the grid is discarded."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Discretion traces back to the Latin verb discernere, \"to separate, to discern,\" from the prefix dis-, \"off, away,\" plus cernere, \"separate, sift.\" If you use discretion, you sift away what is not desirable, keeping only the good."}, {"text": "Ensemble methods are learning models that achieve performance by combining the opinions of multiple learners.  Ensemble methods are learning models that achieve performance by combining the opinions of multiple learners."}]}, {"question": "What is the difference between a linear model using glm and a linear model using lm in R", "positive_ctxs": [{"text": "You'll get the same answer, but the technical difference is glm uses likelihood (if you want AIC values) whereas lm uses least squares. Consequently lm is faster, but you can't do as much with it."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x)."}, {"text": "Linear regression is a classical model for predicting a numerical quantity.  Coefficients of a linear regression model can be estimated using a negative log-likelihood function from maximum likelihood estimation. The negative log-likelihood function can be used to derive the least squares solution to linear regression."}, {"text": "Alternative procedures include: Different linear model: fitting a linear model with additional X variable(s) Nonlinear model: fitting a nonlinear model when the linear model is inappropriate.  Weighted least squares linear regression: dealing with unequal variances in Y by performing a weighted least squares fit."}, {"text": "A linear regression model attempts to explain the relationship between two or more variables using a straight line. Consider the data obtained from a chemical process where the yield of the process is thought to be related to the reaction temperature (see the table below)."}, {"text": "One assumption of Poisson Models is that the mean and the variance are equal, but this assumption is often violated. This can be dealt with by using a dispersion parameter if the difference is small or a negative binomial regression model if the difference is large."}, {"text": "Delta learning does this using the difference between a target activation and an actual obtained activation. Using a linear activation function, network connections are adjusted. Another way to explain the Delta rule is that it uses an error function to perform gradient descent learning."}]}, {"question": "What is the importance of statistical significance in research", "positive_ctxs": [{"text": "\u201cStatistical significance helps quantify whether a result is likely due to chance or to some factor of interest,\u201d says Redman."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "This is why it is important to distinguish between the statistical significance of a result and the practical significance of that result.  Null hypothesis testing is a formal approach to deciding whether a statistical relationship in a sample reflects a real relationship in the population or is just due to chance."}, {"text": "The purpose and importance of the null hypothesis and alternative hypothesis are that they provide an approximate description of the phenomena. The purpose is to provide the researcher or an investigator with a relational statement that is directly tested in a research study."}, {"text": "A t-value is the relative error difference in contrast to the null hypothesis. A p-value, is the statistical significance of a measurement in how correct a statistical evidence part, is."}, {"text": "While statistical significance relates to whether an effect exists, practical significance refers to the magnitude of the effect. However, no statistical test can tell you whether the effect is large enough to be important in your field of study.  An effect of 4 points or less is too small to care about."}, {"text": "A statistical project is the process of answering a research question using statistical techniques and presenting the work in a written report. The research question may arise from any field of scientific endeavor, such as athletics, advertising, aerodynamics, or nutrition."}, {"text": "You can get the feature importance of each feature of your dataset by using the feature importance property of the model. Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable."}]}, {"question": "Why do we differentiate between universal Turing machines and normal Turing machines", "positive_ctxs": [{"text": "The set of all \"normal\" Turing machines, i.e., the set of all Turing machines, can compute all computable functions.  The difference is that a single universal Turing machine can simulate the computation of all computable functions depending on how you interpret its input."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In computer science, a universal Turing machine (UTM) is a Turing machine that simulates an arbitrary Turing machine on arbitrary input.  In terms of computational complexity, a multi-tape universal Turing machine need only be slower by logarithmic factor compared to the machines it simulates."}, {"text": "The universality property of Turing machines states that there exists a Turing machine, which can simulate the behaviour of any other Turing machine.  It says that a Turing machine can be adapted to different tasks by programming; from the viewpoint of computability it is not necessary to build special-purpose machines."}, {"text": "No. A universal Turing machine is a Turing machine that takes as its input a string of the form where is the representation of the transition table of Turing machine and is a string over the input alphabet of ."}, {"text": "On a broad level, we can differentiate both AI and ML as: AI is a bigger concept to create intelligent machines that can simulate human thinking capability and behavior, whereas, machine learning is an application or subset of AI that allows machines to learn from data without being programmed explicitly."}, {"text": "Artificial Intelligence is the broader concept of machines being able to carry out tasks in a way that we would consider \u201csmart\u201d.  Machine Learning is a current application of AI based around the idea that we should really just be able to give machines access to data and let them learn for themselves."}, {"text": "Artificial Intelligence is the broader concept of machines being able to carry out tasks in a way that we would consider \u201csmart\u201d.  Machine Learning is a current application of AI based around the idea that we should really just be able to give machines access to data and let them learn for themselves."}, {"text": "Neural Turing Machines can take input and output and learn algorithms that map from one to the other.  This means that once they have learned that algorithm, they can take a given input, and they can extrapolate based on that algorithm to any variable output."}]}, {"question": "Is lower log likelihood better", "positive_ctxs": [{"text": "Log-likelihood values cannot be used alone as an index of fit because they are a function of sample size but can be used to compare the fit of different coefficients. Because you want to maximize the log-likelihood, the higher value is better. For example, a log-likelihood value of -3 is better than -7."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The log likelihood This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. Therefore we can work with the simpler log-likelihood instead of the original likelihood."}, {"text": "The log likelihood This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. Therefore we can work with the simpler log-likelihood instead of the original likelihood."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "The log likelihood This means that if the value on the x-axis increases, the value on the y-axis also increases (see figure below). This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function."}, {"text": "It is the sum of the likelihood residuals. At record level, the natural log of the error (residual) is calculated for each record, multiplied by minus one, and those values are totaled."}, {"text": "There is no correct value for MSE. Simply put, the lower the value the better and 0 means the model is perfect."}, {"text": "Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2."}]}, {"question": "What is the probability density function of x", "positive_ctxs": [{"text": "The function fX(x) gives us the probability density at point x. It is the limit of the probability of the interval (x,x+\u0394] divided by the length of the interval as the length of the interval goes to 0."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Relationship Between a CDF and a PDF In technical terms, a probability density function (pdf) is the derivative of a cumulative density function (cdf). Futhermore, the area under the curve of a pdf between negative infinity and x is equal to the value of x on the cdf."}, {"text": "The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable."}, {"text": "2 Answers. By definition the probability density function is the derivative of the distribution function. But distribution function is an increasing function on R thus its derivative is always positive. Assume that probability density of X is -ve in the interval (a, b)."}, {"text": "For values of x > 0, the gamma function is defined using an integral formula as \u0393(x) = Integral on the interval [0, \u221e ] of \u222b 0\u221et x \u22121 e\u2212t dt. The probability density function for the gamma distribution is given by. The mean of the gamma distribution is \u03b1\u03b2 and the variance (square of the standard deviation) is \u03b1\u03b22."}, {"text": "The area to the left of x (point of interest) is equal to probability of the x-axis variable being less than the value of x (point of interest).  The probability density is the y-axis. The PDF works for discrete and continuous data distributions."}, {"text": "We capture the notion of being close to a number with a probability density function which is often denoted by \u03c1(x). If the probability density around a point x is large, that means the random variable X is likely to be close to x. If, on the other hand, \u03c1(x)=0 in some interval, then X won't be in that interval."}, {"text": "It is the limit of the probability of the interval (x,x+\u0394] divided by the length of the interval as the length of the interval goes to 0. Remember that P(x<X\u2264x+\u0394)=FX(x+\u0394)\u2212FX(x). =dFX(x)dx=F\u2032X(x),if FX(x) is differentiable at x. is called the probability density function (PDF) of X."}]}, {"question": "What is machine learning error", "positive_ctxs": [{"text": "In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data."}, {"text": "From Wikipedia, the free encyclopedia. Error-driven learning is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback. It is a type of reinforcement learning."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "To \u201cconverge\u201d in machine learning is to have an error so close to local/global minimum, or you can see it aa having a performance so clise to local/global minimum. When the model \u201cconverges\u201d there is usually no significant error decrease / performance increase anymore. ( Unless a more modern optimizer is applied)"}, {"text": "In statistics and machine learning, the bias\u2013variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters.  The bias error is an error from erroneous assumptions in the learning algorithm."}, {"text": "In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a"}, {"text": "The clear sign of a machine learning overfitting is if its error on testing set is much greater than the error on training set.  For instance if the model accuracy for train data is 85% and the accuracy for test/validation data is 65% then its very obvious that the model has overlearned and you should check that."}]}, {"question": "Where are receptive fields located", "positive_ctxs": [{"text": "Receptive field, region in the sensory periphery within which stimuli can influence the electrical activity of sensory cells."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In the visual system, visual receptive fields are volumes in visual space.  The receptive field is often identified as the region of the retina where the action of light alters the firing of the neuron."}, {"text": "any of various neurons located in extrastriate visual areas, particularly those in the inferotemporal cortex, that respond regardless of the location of a stimulus in the receptive field."}, {"text": "Receptive fields are defined portion of space or spatial construct containing units that provide input to a set of units within a corresponding layer. The receptive field is defined by the filter size of a layer within a convolution neural network."}, {"text": "Spatial pooling mimics the action of the receptive fields of the various layers of the cortex, primarily layers L2/3, L5 & L6. This also incorporates the inhibitory action of the inter-neurons. This inhibitory bit is simulated with the k-winner part of the Numenta implementation."}, {"text": "The receptive field size of a unit can be increased in a number of ways. One option is to stack more layers to make the network deeper, which increases the receptive field size linearly by theory, as each extra layer increases the receptive field size by the kernel size."}, {"text": "A CNN has multiple layers. Weight sharing happens across the receptive field of the neurons(filters) in a particular layer. Weights are the numbers within each filter.  These filters act on a certain receptive field/ small section of the image. When the filter moves through the image, the filter does not change."}, {"text": "PGMs with undirected edges are known as Markov networks (MNs) or Markov random fields (MRFs)."}]}, {"question": "What is topic Modelling used for", "positive_ctxs": [{"text": "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "LDA stands for Latent Dirichlet Allocation, and it is a type of topic modeling algorithm. The purpose of LDA is to learn the representation of a fixed number of topics, and given this number of topics learn the topic distribution that each document in a collection of documents has."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Photo Credit: Pixabay. Topic modeling is a type of statistical modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic."}, {"text": "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body."}, {"text": "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "Why do statistics and uncertainties matter", "positive_ctxs": [{"text": "Measurement uncertainty is critical to risk assessment and decision making. Organizations make decisions every day based on reports containing quantitative measurement data. If measurement results are not accurate, then decision risks increase. Selecting the wrong suppliers, could result in poor product quality."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Spectroscopy in chemistry and physics, a method of analyzing the properties of matter from their electromagnetic interactions. Spectral estimation, in statistics and signal processing, an algorithm that estimates the strength of different frequency components (the power spectrum) of a time-domain signal."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Here are 25 phases that you can use to increase confidence and self-esteem in your children.\u201cYou are capable.\"  \u201cThat was brave.\"  \u201cYou've got this.\"  \u201cI believe in you.\"  \u201cYou can do hard things.\"  \u201cNo matter what happens, I love you.\"  \u201cLet's try it together.\"  \u201cHow'd you do that?\"More items"}, {"text": "If the order doesn't matter then we have a combination, if the order do matter then we have a permutation. One could say that a permutation is an ordered combination. The number of permutations of n objects taken r at a time is determined by the following formula: P(n,r)=n!"}, {"text": "If the order doesn't matter then we have a combination, if the order do matter then we have a permutation.  One could say that a permutation is an ordered combination. The number of permutations of n objects taken r at a time is determined by the following formula: P(n,r)=n!"}, {"text": "Without dark matter stars would escape their galaxies and galaxy clusters would come unbound.  Dark matter appears to be some entirely new kind of particle that has mass, and therefore gravity, but otherwise interacts only via the (appropriately named) weak force. Even those interactions are rare."}, {"text": "Model calibration is the process of adjustment of the model parameters and forcing within the margins of the uncertainties (in model parameters and / or model forcing) to obtain a model representation of the processes of interest that satisfies pre-agreed criteria (Goodness-of-Fit or Cost Function)."}]}, {"question": "Is OLS the same as linear regression", "positive_ctxs": [{"text": "Yes, although 'linear regression' refers to any approach to model the relationship between one or more variables, OLS is the method used to find the simple linear regression of a set of data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "OLS cannot be used because the regression function is not a linear function of the regression coefficients (the coefficients appear inside the nonlinear functions \u03a6 or \u039b)."}, {"text": "Yes, although 'linear regression' refers to any approach to model the relationship between one or more variables, OLS is the method used to find the simple linear regression of a set of data."}, {"text": "In ordinary least squares, the relevant assumption of the classical linear regression model is that the error term is uncorrelated with the regressors. The presence of omitted-variable bias violates this particular assumption. The violation causes the OLS estimator to be biased and inconsistent."}, {"text": "Consequences of Heteroscedasticity The OLS estimators and regression predictions based on them remains unbiased and consistent. The OLS estimators are no longer the BLUE (Best Linear Unbiased Estimators) because they are no longer efficient, so the regression predictions will be inefficient too."}, {"text": "Ridge regression is a term used to refer to a linear regression model whose coefficients are not estimated by ordinary least squares (OLS), but by an estimator, called ridge estimator, that is biased but has lower variance than the OLS estimator."}, {"text": "Content validity: Is the test fully representative of what it aims to measure?  Criterion validity: Do the results correspond to a different test of the same thing?"}, {"text": "Generally speaking, gradient boosted trees are more robust in multicollinearity situations than OLS regression.  When two independent variables are highly correlated, applying OLS regression could create problems. For example, p-values may not be reliable or even worse the OLS solution can't even be calculated."}]}, {"question": "Can machine learning predict stock market", "positive_ctxs": [{"text": "So, the prediction of stock Prices using machine learning is 100% correct and not 99%. This is theoritically true, and one can prove this mathematically. BUT THE MACHINE LEARNING TECHNIQUES FOR PREDICTION, DOES NOT ABLE TO PREDECT THE PSYCHOLOGICAL FACTORS OF HUMEN , ON THE PRICES OF THE STOCKS and others."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "Applications of association rule mining are stock analysis, web log mining, medical diagnosis, customer market analysis bioinformatics etc. In past, many algorithms were developed by researchers for Boolean and Fuzzy association rule mining such as Apriori, FP-tree, Fuzzy FP-tree etc."}, {"text": "In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data."}, {"text": "The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered. In machine learning, one aims to construct algorithms that are able to learn to predict a certain target output."}, {"text": "The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered. In machine learning, one aims to construct algorithms that are able to learn to predict a certain target output."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Yes. We Can Always Use The Normal Distribution To Approximate The Binomial Distribution."}]}, {"question": "How binning can handle noisy data", "positive_ctxs": [{"text": "Binning method is used to smoothing data or to handle noisy data. In this method, the data is first sorted and then the sorted values are distributed into a number of buckets or bins. As binning methods consult the neighborhood of values, they perform local smoothing."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}, {"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}]}, {"question": "How do you find the accuracy of a precision and recall", "positive_ctxs": [{"text": "Precision is a metric that quantifies the number of correct positive predictions made. Precision, therefore, calculates the accuracy for the minority class. It is calculated as the ratio of correctly predicted positive examples divided by the total number of positive examples that were predicted."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Definition. Average precision is a measure that combines recall and precision for ranked retrieval results. For one information need, the average precision is the mean of the precision scores after each relevant document is retrieved."}, {"text": "The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate."}, {"text": "Recall quantifies the number of positive class predictions made out of all positive examples in the dataset. F-Measure provides a single score that balances both the concerns of precision and recall in one number."}, {"text": "Calculate precision and recall for all objects present in the image. You also need to consider the confidence score for each object detected by the model in the image. Consider all of the predicted bounding boxes with a confidence score above a certain threshold."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}]}, {"question": "Why do we use AI", "positive_ctxs": [{"text": "Artificial Intelligence enhances the speed, precision and effectiveness of human efforts. In financial institutions, AI techniques can be used to identify which transactions are likely to be fraudulent, adopt fast and accurate credit scoring, as well as automate manually intense data management tasks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "The AI is programmed to do something devastating: Autonomous weapons are artificial intelligence systems that are programmed to kill. In the hands of the wrong person, these weapons could easily cause mass casualties. Moreover, an AI arms race could inadvertently lead to an AI war that also results in mass casualties."}, {"text": "An algorithm that uses random numbers to decide what to do next anywhere in its logic is called a Randomized Algorithm. For example, in Randomized Quick Sort, we use a random number to pick the next pivot (or we randomly shuffle the array). And in Karger's algorithm, we randomly pick an edge."}, {"text": "Action selection in AI systems is a basic system in which the problem can be analyzed by the AI machine to understand what it has to do next to get closer to the solution of the problem.  AI agents and action selection form to be very important entities to help devise an intelligent solution to a problem."}, {"text": "In the AI lexicon this is known as \u201cinference.\u201d Inference is where capabilities learned during deep learning training are put to work. Inference can't happen without training. Makes sense. That's how we gain and use our own knowledge for the most part."}, {"text": "On a broad level, we can differentiate both AI and ML as: AI is a bigger concept to create intelligent machines that can simulate human thinking capability and behavior, whereas, machine learning is an application or subset of AI that allows machines to learn from data without being programmed explicitly."}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}]}, {"question": "What is meant by linear regression", "positive_ctxs": [{"text": "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data.  A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Quantile regression is an extension of linear regression used when the conditions of linear regression are not met."}, {"text": "In statistics, Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference."}, {"text": "Thus, Linear regression is better for simpler modelling while neural net is better for complex or multiple-level/category modelling. Neural networks generally outperform linear regression as they have more degrees of freedom. In linear regression variables are treated as a linear combination."}, {"text": "3 Answers. Since your response is ordinal then you should use ordinal regression. At a very high level, the main difference ordinal regression and linear regression is that with linear regression the dependent variable is continuous and ordinal the dependent variable is ordinal."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Ridge regression is a term used to refer to a linear regression model whose coefficients are not estimated by ordinary least squares (OLS), but by an estimator, called ridge estimator, that is biased but has lower variance than the OLS estimator."}]}, {"question": "What makes a function divergent", "positive_ctxs": [{"text": "In mathematics, a divergent series is an infinite series that is not convergent, meaning that the infinite sequence of the partial sums of the series does not have a finite limit. If a series converges, the individual terms of the series must approach zero."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Rabin-Karp algorithm makes use of hash functions and the rolling hash technique. A hash function is essentially a function that maps one thing to a value. In particular, hashing can map data of arbitrary size to a value of fixed size."}, {"text": "A log-linear model is a mathematical model that takes the form of a function whose logarithm equals a linear combination of the parameters of the model, which makes it possible to apply (possibly multivariate) linear regression."}, {"text": "Discriminant function analysis (DFA) is a statistical procedure that classifies unknown individuals and the probability of their classification into a certain group (such as sex or ancestry group). Discriminant function analysis makes the assumption that the sample is normally distributed for the trait."}, {"text": "Discriminant function analysis (DFA) is a statistical procedure that classifies unknown individuals and the probability of their classification into a certain group (such as sex or ancestry group). Discriminant function analysis makes the assumption that the sample is normally distributed for the trait."}, {"text": "Discriminant function analysis (DFA) is a statistical procedure that classifies unknown individuals and the probability of their classification into a certain group (such as sex or ancestry group). Discriminant function analysis makes the assumption that the sample is normally distributed for the trait."}, {"text": "matplotlib. pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc."}, {"text": "We use binary cross-entropy loss for classification models which output a probability p. The range of the sigmoid function is [0, 1] which makes it suitable for calculating probability."}]}, {"question": "Which is the best model used in word2vec algorithm for word embedding", "positive_ctxs": [{"text": "Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are: Continuous Bag-of-Words, or CBOW model. Continuous Skip-Gram Model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "fastText is another word embedding method that is an extension of the word2vec model. Instead of learning vectors for words directly, fastText represents each word as an n-gram of characters.  This helps capture the meaning of shorter words and allows the embeddings to understand suffixes and prefixes."}, {"text": "fastText is another word embedding method that is an extension of the word2vec model. Instead of learning vectors for words directly, fastText represents each word as an n-gram of characters.  This helps capture the meaning of shorter words and allows the embeddings to understand suffixes and prefixes."}, {"text": "The skip-gram model. Both the input vector x and the output y are one-hot encoded word representations. The hidden layer is the word embedding of size N."}, {"text": "To assess which word2vec model is best, simply calculate the distance for each pair, do it 200 times, sum up the total distance, and the smallest total distance will be your best model."}, {"text": "Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."}, {"text": "Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."}, {"text": "The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."}]}, {"question": "In which case we should use kernel tricks", "positive_ctxs": [{"text": "When talking about kernels in machine learning, most likely the first thing that comes into your mind is the support vector machines (SVM) model because the kernel trick is widely used in the SVM model to bridge linearity and non-linearity."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In our categorical case we would use a simple regression equation for each group to investigate the simple slopes. It is common practice to standardize or center variables to make the data more interpretable in simple slopes analysis; however, categorical variables should never be standardized or centered."}, {"text": "Kernel function A kernel (or covariance function) describes the covariance of the Gaussian process random variables. Together with the mean function the kernel completely defines a Gaussian process. In the first post we introduced the concept of the kernel which defines a prior on the Gaussian process distribution."}, {"text": "The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."}, {"text": "The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."}, {"text": "The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."}, {"text": "The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."}, {"text": "In Gradient Descent or Batch Gradient Descent, we use the whole training data per epoch whereas, in Stochastic Gradient Descent, we use only single training example per epoch and Mini-batch Gradient Descent lies in between of these two extremes, in which we can use a mini-batch(small portion) of training data per epoch"}]}, {"question": "How do you calculate Chebyshev's inequality", "positive_ctxs": [{"text": "Illustration of the Inequality For K = 2 we have 1 \u2013 1/K2 = 1 - 1/4 = 3/4 = 75%. So Chebyshev's inequality says that at least 75% of the data values of any distribution must be within two standard deviations of the mean. For K = 3 we have 1 \u2013 1/K2 = 1 - 1/9 = 8/9 = 89%."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "One way to prove Chebyshev's inequality is to apply Markov's inequality to the random variable Y = (X \u2212 \u03bc)2 with a = (k\u03c3)2. Chebyshev's inequality then follows by dividing by k2\u03c32."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Chebyshev's inequality says that at least 1\u22121K2 of data from a sample must fall within K standard deviations from the mean, where K is any positive real number greater than one."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Chebyshev's inequality, also known as Chebyshev's theorem, is a statistical tool that measures dispersion in a data population.  The theorem states that no more than 1 / k2 of the distribution's values will be more than k standard deviations away from the mean."}, {"text": "A Lorenz curve is a graphical representation of income inequality or wealth inequality developed by American economist Max Lorenz in 1905. The graph plots percentiles of the population on the horizontal axis according to income or wealth."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "How do you determine the interquartile range", "positive_ctxs": [{"text": "To find the interquartile range (IQR), \u200bfirst find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The interquartile range is the difference between the third quartile and the first quartile in a data set, giving the middle 50%. The interquartile range is a measure of spread; it's used to build box plots, determine normal distributions and as a way to determine outliers."}, {"text": "There are 5 values above the median (upper half), the middle value is 77 which is the third quartile. The interquartile range is 77 \u2013 64 = 13; the interquartile range is the range of the middle 50% of the data.  When the sample size is odd, the median and quartiles are determined in the same way."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "To find the interquartile range (IQR), \u200bfirst find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1."}, {"text": "To find the interquartile range (IQR), \u200bfirst find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1."}, {"text": "To find the interquartile range (IQR), \u200bfirst find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1."}, {"text": "The median is a robust measure of central tendency.  The median absolute deviation and interquartile range are robust measures of statistical dispersion, while the standard deviation and range are not. Trimmed estimators and Winsorised estimators are general methods to make statistics more robust."}]}, {"question": "What is an example of an independent event", "positive_ctxs": [{"text": "Definition: Two events, A and B, are independent if the fact that A occurs does not affect the probability of B occurring. Some other examples of independent events are: Landing on heads after tossing a coin AND rolling a 5 on a single 6-sided die. Choosing a marble from a jar AND landing on heads after tossing a coin."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An independent event is an event in which the outcome isn't affected by another event. A dependent event is affected by the outcome of a second event."}, {"text": "An independent event is an event in which the outcome isn't affected by another event. A dependent event is affected by the outcome of a second event."}, {"text": "A Poisson Process is a model for a series of discrete event where the average time between events is known, but the exact timing of events is random . The arrival of an event is independent of the event before (waiting time between events is memoryless)."}, {"text": "Binomial. The binomial distribution function specifies the number of times (x) that an event occurs in n independent trials where p is the probability of the event occurring in a single trial. It is an exact probability distribution for any number of discrete trials."}, {"text": "Characteristics of a Poisson Distribution The probability that an event occurs in a given time, distance, area, or volume is the same. Each event is independent of all other events. For example, the number of people who arrive in the first hour is independent of the number who arrive in any other hour."}, {"text": "The dependent variable is the variable being tested and measured in an experiment, and is 'dependent' on the independent variable. An example of a dependent variable is depression symptoms, which depends on the independent variable (type of therapy)."}, {"text": "The dependent variable is the variable being tested and measured in an experiment, and is 'dependent' on the independent variable. An example of a dependent variable is depression symptoms, which depends on the independent variable (type of therapy)."}]}, {"question": "Is matrix factorization supervised or unsupervised", "positive_ctxs": [{"text": "Unsupervised: Given only samples X of the data, we compute a function f such that y = f(X) is \u201csimpler\u201d. Clustering: y is discrete \u2022 Y is continuous: Matrix factorization, Kalman filtering, unsupervised neural networks. Unsupervised: Cluster some hand-written digit data into 10 classes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Matrix factorization using the alternating least squares algorithm for collaborative filtering. Alternating least squares (ALS) is an optimization technique to solve the matrix factorization problem. This technique achieves good performance and has proven relatively easy to implement."}, {"text": "You can use an unsupervised learning algorithm (like clustering) to create your training data for the supervised learning algorithm but you cannot simply convert an unsupervised learning algorithm into a supervised one."}, {"text": "These are three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning."}, {"text": "These are three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning."}, {"text": "Non negative matrix factorization only takes positive values as input while SVD can take both positive and negative values.  SVD and NMF are both matrix decomposition techniques but they are very different and are generally used for different purposes. SVD helps in giving Eigen vectors of the input matrix."}]}, {"question": "What is the importance of studying social network", "positive_ctxs": [{"text": "Social media plays an important role in every student's life. It is often easier and more convenient to access information, provide information and communicate via social media. Tutors and students can be connected to each other and can make good use of these platforms for the benefit of their learning and teaching."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "You can get the feature importance of each feature of your dataset by using the feature importance property of the model. Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable."}, {"text": "Explanation: Correlation is the process of studying the cause and effect relationship that exists between two variables. Correlation coefficient is the measure of the correlation that exists between two variables."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Facebook Trending is a feature of the social network designed to show each user a list of topics that are spiking in popularity in updates, posts, and comments. Facebook Trending appears as a short list of keywords and phrases in a small module at the top right of the user's News Feed."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "The logarithm is to exponentiation as division is to multiplication: The logarithm is the inverse of the exponent: it undoes exponentiation. When studying logarithms, always remember the following fundamental equivalence: if and only if . Whenever one of these is true, so is the other."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is an example of sample size", "positive_ctxs": [{"text": "Sample size measures the number of individual samples measured or observations used in a survey or experiment. For example, if you test 100 samples of soil for evidence of acid rain, your sample size is 100. If an online survey returned 30,500 completed questionnaires, your sample size is 30,500."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "The larger the sample size is the smaller the effect size that can be detected. The reverse is also true; small sample sizes can detect large effect sizes.  Thus an appropriate determination of the sample size used in a study is a crucial step in the design of a study."}, {"text": "Sample size measures the number of individual samples measured or observations used in a survey or experiment. For example, if you test 100 samples of soil for evidence of acid rain, your sample size is 100. If an online survey returned 30,500 completed questionnaires, your sample size is 30,500."}, {"text": "The consistency of the sampling distribution is dependent on the sample size not on the distribution of the population. As the sample size decreases the absolute value of the skewness and kurtosis of the sampling distribution increases. This sample size relationship is expressed in the central limit theorem."}, {"text": "The main aim of a sample size calculation is to determine the number of participants needed to detect a clinically relevant treatment effect. Pre-study calculation of the required sample size is warranted in the majority of quantitative studies."}, {"text": "Typically by the time the sample size is 30 the distribution of the sample mean is practically the same as a normal distribution.  \u00afX, the mean of the measurements in a sample of size n; the distribution of \u00afX is its sampling distribution, with mean \u03bc\u00afX=\u03bc and standard deviation \u03c3\u00afX=\u03c3\u221an."}, {"text": "The minimum sample size is 100 Most statisticians agree that the minimum sample size to get any kind of meaningful result is 100. If your population is less than 100 then you really need to survey all of them."}]}, {"question": "What is the difference between AZ score and standard deviation", "positive_ctxs": [{"text": "Key Takeaways. Standard deviation defines the line along which a particular data point lies. Z-score indicates how much a given value differs from the standard deviation. The Z-score, or standard score, is the number of standard deviations a given data point lies above or below mean."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "The difference between the two norms is that the standard deviation is calculating the square of the difference whereas the mean absolute deviation is only looking at the absolute difference. Hence large outliers will create a higher dispersion when using the standard deviation instead of the other method."}, {"text": "Definition. A score that is derived from an individual's raw score within a distribution of scores. The standard score describes the difference of the raw score from a sample mean, expressed in standard deviations. Standard scores preserve the absolute differences between scores."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Quartile deviation is the difference between \u201cfirst and third quartiles\u201d in any distribution. Standard deviation measures the \u201cdispersion of the data set\u201d that is relative to its mean."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}]}, {"question": "How is business analytics different from data science", "positive_ctxs": [{"text": "Data Science vs Business Analytics, often used interchangeably, are very different domains.  Simply put, Data science is the study of Data using statistics which provides key insights but not business changing decisions whereas Business Analytics is the analysis of data to make key business decisions for the company."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The answer is a big NO. Data science gets solutions and results to specific business problems using AI as a tool. If data science is to insights, machine learning is to predictions and artificial intelligence is to actions."}, {"text": "Big data analytics is the use of advanced analytic techniques against very large, diverse data sets that include structured, semi-structured and unstructured data, from different sources, and in different sizes from terabytes to zettabytes."}, {"text": "While many people use the terms interchangeably, data science and big data analytics are unique fields, with the major difference being the scope.  Data science produces broader insights that concentrate on which questions should be asked, while big data analytics emphasizes discovering answers to questions being asked."}, {"text": "While many people use the terms interchangeably, data science and big data analytics are unique fields, with the major difference being the scope.  Data science produces broader insights that concentrate on which questions should be asked, while big data analytics emphasizes discovering answers to questions being asked."}, {"text": "Predictive analytics requires a data-driven culture: 5 steps to startDefine the business result you want to achieve.  Collect relevant data from all available sources.  Improve the quality of data using data cleaning techniques.  Choose predictive analytics solutions or build your own models to test the data.More items\u2022"}, {"text": "Real-time big data analytics means that big data is processed as it arrives and either a business user gets consumable insights without exceeding a time period allocated for decision-making or an analytical system triggers an action or a notification."}, {"text": "Big data analytics and data mining are not the same. Both of them involve the use of large data sets, handling the collection of the data or reporting of the data which is mostly used by businesses. However, both big data analytics and data mining are both used for two different operations."}]}, {"question": "How does power affect sample size", "positive_ctxs": [{"text": "The power of a hypothesis test is affected by three factors. Sample size (n). Other things being equal, the greater the sample size, the greater the power of the test.  The greater the difference between the \"true\" value of a parameter and the value specified in the null hypothesis, the greater the power of the test."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Events are independent if the outcome of one event does not affect the outcome of another. For example, if you throw a die and a coin, the number on the die does not affect whether the result you get on the coin."}, {"text": "Sample size refers to the number of participants or observations included in a study.  The size of a sample influences two statistical properties: 1) the precision of our estimates and 2) the power of the study to draw conclusions."}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "A good maximum sample size is usually 10% as long as it does not exceed 1000. A good maximum sample size is usually around 10% of the population, as long as this does not exceed 1000. For example, in a population of 5000, 10% would be 500. In a population of 200,000, 10% would be 20,000."}, {"text": "How to Perform Systematic Sampling: StepsStep 1: Assign a number to every element in your population.  Step 2: Decide how large your sample size should be.  Step 3: Divide the population by your sample size.  Step 1: Assign a number to every element in your population.Step 2: Decide how large your sample size should be.More items\u2022"}]}, {"question": "What is partitioning in ETL", "positive_ctxs": [{"text": "Partitioning is when an area of data storage is sub-divided to improve performance. Think of it as an organizational tool. If all your collected data is in one large space without organization the digital tools used for analyzing it will have a more difficult time finding the information in order to analyze it."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Partitioning methods Horizontal partitioning involves putting different rows into different tables.  Vertical partitioning involves creating tables with fewer columns and using additional tables to store the remaining columns."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects).  Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images."}]}, {"question": "How do you test for Multicollinearity", "positive_ctxs": [{"text": "Multicollinearity can also be detected with the help of tolerance and its reciprocal, called variance inflation factor (VIF). If the value of tolerance is less than 0.2 or 0.1 and, simultaneously, the value of VIF 10 and above, then the multicollinearity is problematic."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "False negatives \u2014 that is, a test that says you don't have the virus when you actually do have the virus \u2014 may occur."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Multicollinearity doesn't affect how well the model fits. In fact, if you want to use the model to make predictions, both models produce identical results for fitted values and prediction intervals!"}, {"text": "How to Calculate a Confusion MatrixYou need a test dataset or a validation dataset with expected outcome values.Make a prediction for each row in your test dataset.From the expected outcomes and predictions count: The number of correct predictions for each class."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "What is PDF and CDF in probability", "positive_ctxs": [{"text": "The pdf and cdf give a complete description of the probability distribution of a random variable.  The pdf represents the relative frequency of failure times as a function of time. The cdf is a function, F(x)\\,\\!, of a random variable X\\,\\!, and is defined for a number x\\,\\! by: F(x)=P(X\\le x)=\\int_{0}^{x}f(s)ds\\ \\,\\!"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "PDF according to input X being discrete or continuous generates probability mass functions and CDF does the same but generates cumulative mass function. That means, PDF is derivative of CDF and CDF can be applied at any point where PDF has been applied.  The cumulative function is the integral of the density function."}, {"text": "PDF according to input X being discrete or continuous generates probability mass functions and CDF does the same but generates cumulative mass function. That means, PDF is derivative of CDF and CDF can be applied at any point where PDF has been applied.  The cumulative function is the integral of the density function."}, {"text": "So a CDF is a function whose output is a probability. The PDF is a function whose output is a nonnegative number. The PDF itself is not a probability (unlike the CDF), but it can be used to calculate probabilities."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The Relationship Between a CDF and a PDF In technical terms, a probability density function (pdf) is the derivative of a cumulative density function (cdf). Futhermore, the area under the curve of a pdf between negative infinity and x is equal to the value of x on the cdf."}, {"text": "It is usually more straightforward to start from the CDF and then to find the PDF by taking the derivative of the CDF. Note that before differentiating the CDF, we should check that the CDF is continuous. As we will see later, the function of a continuous random variable might be a non-continuous random variable."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What are the limitations of Mohr's method", "positive_ctxs": [{"text": "A) (ii) Disadvantages of Mohr Method \uf0a7 Mohr's method is suitable only for titration of chloride, bromide and cyanide alone. \uf0a7 Errors can be introduced due to the need of excess titrant before the endpoint colour is visible."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In robust statistics, robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Regression analysis seeks to find the relationship between one or more independent variables and a dependent variable."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}, {"text": "This clustering method classifies the information into multiple groups based on the characteristics and similarity of the data.  There are many algorithms that come under partitioning method some of the popular ones are K-Mean, PAM(K-Mediods), CLARA algorithm (Clustering Large Applications) etc."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}]}, {"question": "Can the median and mean be the same", "positive_ctxs": [{"text": "If the set has an even number of terms, the median is the average of the middle two terms. For example, in the set {10, 12, 15, 20}, the median is the average of 12 and 15: 13.5.  Since the numbers are consecutive, the mean and the median are the same."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In a normal distribution, the mean and the median are the same number while the mean and median in a skewed distribution become different numbers: A left-skewed, negative distribution will have the mean to the left of the median. A right-skewed distribution will have the mean to the right of the median."}, {"text": "In case of mean and median, it is not necessary. However, the accuracy of the mean would be higher if the class intervals are short. Similarly the median would be more accurate if the 'median class', class interval in which median falls, is of short length."}, {"text": "In these situations, the median is generally considered to be the best representative of the central location of the data. The more skewed the distribution, the greater the difference between the median and mean, and the greater emphasis should be placed on using the median as opposed to the mean."}, {"text": "Mean, variance, and standard deviation The mean of the sampling distribution of the sample mean will always be the same as the mean of the original non-normal distribution. In other words, the sample mean is equal to the population mean."}, {"text": "The median is the middle number in a sorted, ascending or descending, list of numbers and can be more descriptive of that data set than the average.  If there is an odd amount of numbers, the median value is the number that is in the middle, with the same amount of numbers below and above."}, {"text": "In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right.  In a positively skewed distribution, the mode is always less than the mean and median."}, {"text": "In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right. In a negatively skewed distribution, the mean is usually less than the median because the few low scores tend to shift the mean to the left."}]}, {"question": "What are the two types of hierarchical clustering", "positive_ctxs": [{"text": "There are two types of hierarchical clustering, Divisive and Agglomerative."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cluster analysis is applied in many fields such as the natural sciences, the medical sciences, economics, marketing, etc. There are essentially two types of clustering methods: hierarchical algorithms and partioning algorithms. The hierarchical algorithms can be divided into agglomerative and splitting procedures."}, {"text": "Clustering and Association are two types of Unsupervised learning.  Important clustering types are: 1)Hierarchical clustering 2) K-means clustering 3) K-NN 4) Principal Component Analysis 5) Singular Value Decomposition 6) Independent Component Analysis."}, {"text": "For most common hierarchical clustering software, the default distance measure is the Euclidean distance. This is the square root of the sum of the square differences. However, for gene expression, correlation distance is often used. The distance between two vectors is 0 when they are perfectly correlated."}, {"text": "A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical."}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "The hclust function in R uses the complete linkage method for hierarchical clustering by default. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components."}, {"text": "Unsupervised machine learning helps you to finds all kind of unknown patterns in data. Clustering and Association are two types of Unsupervised learning. Four types of clustering methods are 1) Exclusive 2) Agglomerative 3) Overlapping 4) Probabilistic."}]}, {"question": "What is recommender system in machine learning", "positive_ctxs": [{"text": "Recommender systems are an important class of machine learning algorithms that offer \"relevant\" suggestions to users. Categorized as either collaborative filtering or a content-based system, check out how these approaches work along with implementations to follow from example code."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Offline evaluations test the effectiveness of recommender system algorithms on a certain dataset. Online evaluation attempts to evaluate recommender systems by a method called A/B testing where a part of users are served by recommender system A and the another part of users by recommender system B."}, {"text": "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  Feature learning can be either supervised or unsupervised."}, {"text": "Artificial intelligence is a technology which enables a machine to simulate human behavior. Machine learning is a subset of AI which allows a machine to automatically learn from past data without programming explicitly. The goal of AI is to make a smart computer system like humans to solve complex problems."}, {"text": "Collaborative filtering (CF) is a technique used by recommender systems.  For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes)."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Linear programming is a special case of mathematical programming (mathematical optimization). Now linear programming is a subset of machine learning known as supervised learning. In a supervised learning, the system knows the patterns and the pattern is well defined based on previous data and information."}, {"text": "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  In supervised feature learning, features are learned using labeled input data."}]}, {"question": "Which algorithm is best for classification", "positive_ctxs": [{"text": "3.1 Comparison MatrixClassification AlgorithmsAccuracyF1-ScoreNa\u00efve Bayes80.11%0.6005Stochastic Gradient Descent82.20%0.5780K-Nearest Neighbours83.56%0.5924Decision Tree84.23%0.63083 more rows\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Logistic regression is a powerful machine learning algorithm that utilizes a sigmoid function and works best on binary classification problems, although it can be used on multi-class classification problems through the \u201cone vs. all\u201d method. Logistic regression (despite its name) is not fit for regression tasks."}, {"text": "Logistic regression is a powerful machine learning algorithm that utilizes a sigmoid function and works best on binary classification problems, although it can be used on multi-class classification problems through the \u201cone vs. all\u201d method."}, {"text": "Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability."}, {"text": "Logistic regression is a classification algorithm traditionally limited to only two-class classification problems. If you have more than two classes then Linear Discriminant Analysis is the preferred linear classification technique."}]}, {"question": "How the chi square test for independence and the chi square goodness of fit test are related", "positive_ctxs": [{"text": "The chi-square test may be used both as a test of goodness-of-fit (comparing frequencies of one nominal variable to theoretical expectations) and as a test of independence (comparing frequencies of one nominal variable for different values of a second nominal variable)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship."}, {"text": "The chi-square test is the most commonly used to test the goodness of fit tests and is used for discrete distributions like the binomial distribution and the Poisson distribution, whereas The Kolmogorov-Smirnov and Anderson-Darling goodness of fit tests are used for continuous distributions."}, {"text": "In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution."}, {"text": "In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution."}, {"text": "In statistics, the likelihood-ratio test assesses the goodness of fit of two competing statistical models based on the ratio of their likelihoods, specifically one found by maximization over the entire parameter space and another found after imposing some constraint."}, {"text": "In statistics, the likelihood-ratio test assesses the goodness of fit of two competing statistical models based on the ratio of their likelihoods, specifically one found by maximization over the entire parameter space and another found after imposing some constraint."}, {"text": "Definition. Pearson's chi-squared test is used to assess three types of comparison: goodness of fit, homogeneity, and independence. A test of goodness of fit establishes whether an observed frequency distribution differs from a theoretical distribution."}]}, {"question": "What is pattern recognition algorithm", "positive_ctxs": [{"text": "Pattern recognition is the process of recognizing patterns by using machine learning algorithm. Pattern recognition can be defined as the classification of data based on knowledge already gained or on statistical information extracted from patterns and/or their representation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Recognizing patterns allows us to predict and expect what is coming. The process of pattern recognition involves matching the information received with the information already stored in the brain. Making the connection between memories and information perceived is a step of pattern recognition called identification."}, {"text": "Feature extraction describes the relevant shape information contained in a pattern so that the task of classifying the pattern is made easy by a formal procedure. In pattern recognition and in image processing, feature extraction is a special form of dimensionality reduction."}, {"text": "1. A pattern recognition technique that is used to categorize a huge number of data into different classes."}, {"text": "Pattern recognition is a process in which we use multiple senses in order to make decisions. As we go through our day, our brain's pattern recognition abilities help us recognise certain objects and situations."}, {"text": "In computer science and machine learning, pattern recognition is a technology that matches the information stored in the database with the incoming data."}, {"text": "An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is \"spam\" or \"non-spam\").  This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns."}, {"text": "Rabin-Karp is another pattern searching algorithm to find the pattern in a more efficient way. It also checks the pattern by moving window one by one, but without checking all characters for all cases, it finds the hash value. When the hash value is matched, then only it tries to check each character."}]}, {"question": "What does it mean for an estimator to be more efficient than another estimator", "positive_ctxs": [{"text": "Essentially, a more efficient estimator, experiment, or test needs fewer observations than a less efficient one to achieve a given performance.  An efficient estimator is characterized by a small variance or mean square error, indicating that there is a small deviance between the estimated value and the \"true\" value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Whereas GLS is more efficient than OLS under heteroscedasticity or autocorrelation, this is not true for FGLS. The feasible estimator is, provided the errors covariance matrix is consistently estimated, asymptotically more efficient, but for a small or medium size sample, it can be actually less efficient than OLS."}, {"text": "In statistics, an efficient estimator is an estimator that estimates the quantity of interest in some \u201cbest possible\u201d manner."}, {"text": "In statistics, an efficient estimator is an estimator that estimates the quantity of interest in some \u201cbest possible\u201d manner."}, {"text": "Say we want to estimate the mean of a population. While the most used estimator is the average of the sample, another possible estimator is simply the first number drawn from the sample.  In theory, you could have an unbiased estimator whose variance is asymptotically nonzero, and that would be inconsistent."}, {"text": "The sample mean is a consistent estimator for the population mean. A consistent estimate has insignificant errors (variations) as sample sizes grow larger.  In other words, the more data you collect, a consistent estimator will be close to the real population parameter you're trying to measure."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "The Cram\u00e9r-Rao Inequality provides a lower bound for the variance of an unbiased estimator of a parameter. It allows us to conclude that an unbiased estimator is a minimum variance unbiased estimator for a parameter."}]}, {"question": "How can you tell the difference between a strong linear association and a weak linear association", "positive_ctxs": [{"text": "Values near \u22121 indicate a strong negative linear relationship, values near 0 indicate a weak linear relationship, and values near 1 indicate a strong positive linear relationship. The correlation is an appropriate numerical measure only for linear relationships and is sensitive to outliers."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Regression analysis refers to assessing the relationship between the outcome variable and one or more variables.  For example, a correlation of r = 0.8 indicates a positive and strong association among two variables, while a correlation of r = -0.3 shows a negative and weak association."}, {"text": "Correlation analysis explores the association between two or more variables and makes inferences about the strength of the relationship.  Technically, association refers to any relationship between two variables, whereas correlation is often used to refer only to a linear relationship between two variables."}, {"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}, {"text": "The Correlation Coefficient When the r value is closer to +1 or -1, it indicates that there is a stronger linear relationship between the two variables. A correlation of -0.97 is a strong negative correlation while a correlation of 0.10 would be a weak positive correlation."}, {"text": "Collinearity is a linear association between two predictors. Multicollinearity is a situation where two or more predictors are highly linearly related.  But, correlation 'among the predictors' is a problem to be rectified to be able to come up with a reliable model."}, {"text": "A coefficient of correlation of +0.8 or -0.8 indicates a strong correlation between the independent variable and the dependent variable. An r of +0.20 or -0.20 indicates a weak correlation between the variables."}, {"text": "Linear regression attempts to model the relationship between two variables by fitting a linear equation (= a straight line) to the observed data.  If you have a hunch that the data follows a straight line trend, linear regression can give you quick and reasonably accurate results."}]}, {"question": "How do you find the mean of a Beta distribution", "positive_ctxs": [{"text": "0:003:12Suggested clip \u00b7 104 secondsBeta distribution: mean - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Beta distribution is a continuous probability distribution having two parameters. One of its most common uses is to model one's uncertainty about the probability of success of an experiment."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "The mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean \u03bc, then the mean of the sampling distribution of the mean is also \u03bc. The symbol \u03bcM is used to refer to the mean of the sampling distribution of the mean."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}]}, {"question": "Is age a continuous variable", "positive_ctxs": [{"text": "A variable is said to be continuous if it can assume an infinite number of real values. Examples of a continuous variable are distance, age and temperature. The measurement of a continuous variable is restricted by the methods used, or by the accuracy of the measuring instruments."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Time is a continuous variable. You could turn age into a discrete variable and then you could count it. For example: A person's age in years."}, {"text": "For example, you could be: 25 years, 10 months, 2 days, 5 hours, 4 seconds, 4 milliseconds, 8 nanoseconds, 99 picosends\u2026and so on. Time is a continuous variable. You could turn age into a discrete variable and then you could count it."}, {"text": "Binning is a way to group a number of more or less continuous values into a smaller number of \"bins\". For example, if you have data about a group of people, you might want to arrange their ages into a smaller number of age intervals."}, {"text": "A Latent Class regression model: Is used to predict a dependent variable as a function of predictor variables (Regression model). Includes a K-category latent variable X to cluster cases (LC model)  Each case may contain multiple records (Regression with repeated measurements)."}, {"text": "A continuous random variable is a random variable whose statistical distribution is continuous. Formally: A continuous random variable is a function X X X on the outcomes of some probabilistic experiment which takes values in a continuous set V V V."}, {"text": "It is possible to find the correlation between a categorical variable and a continuous variable using the analysis of covariance technique."}, {"text": "A discrete variable is a variable whose value is obtained by counting. A continuous variable is a variable whose value is obtained by measuring.  A discrete random variable X has a countable number of possible values."}]}, {"question": "Does the central limit theorem apply to variance", "positive_ctxs": [{"text": "The central limit theorem applies to almost all types of probability distributions, but there are exceptions. For example, the population must have a finite variance. That restriction rules out the Cauchy distribution because it has infinite variance."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The law of large numbers states that the sample mean of independent and identically distributed observations converges to a certain value. The central limit theorem describes the distribution of the difference between the sample mean and that value."}, {"text": "The central limit theorem states that the CDF of Zn converges to the standard normal CDF. converges in distribution to the standard normal random variable as n goes to infinity, that is limn\u2192\u221eP(Zn\u2264x)=\u03a6(x), for all x\u2208R,  The Xi's can be discrete, continuous, or mixed random variables."}, {"text": "The central limit theorem has been extended to the case of dependent random variables by several authors (Bruns, Markoff, S.  The conditions under which these theorems are stated either are very restrictive or involve conditional distributions, which makes them difficult to apply."}, {"text": "normal approximation: The process of using the normal curve to estimate the shape of the distribution of a data set. central limit theorem: The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed."}, {"text": "The central limit theorem states that the sampling distribution of the mean approaches a normal distribution, as the sample size increases.  Therefore, as a sample size increases, the sample mean and standard deviation will be closer in value to the population mean \u03bc and standard deviation \u03c3 ."}, {"text": "Key Terms. normal approximation: The process of using the normal curve to estimate the shape of the distribution of a data set. central limit theorem: The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed."}, {"text": "In the study of probability theory, the central limit theorem (CLT) states that the distribution of sample approximates a normal distribution (also known as a \u201cbell curve\u201d) as the sample size becomes larger, assuming that all samples are identical in size, and regardless of the population distribution shape."}]}, {"question": "What is the covariance of a matrix", "positive_ctxs": [{"text": "In probability theory and statistics, a covariance matrix (also known as auto-covariance matrix, dispersion matrix, variance matrix, or variance\u2013covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A covariance matrix is a square matrix which gives two types of information. If you are looking at the population covariance matrix then. each diagonal element is the variance of the corresponding random variable. each off-diagonal element is the covariance of the corresponding pair of random variables."}, {"text": "The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of observed values of two of the variables and whose i, i element is the sample variance of the observed values of one of the variables."}, {"text": "In probability theory and statistics, a covariance matrix (also known as auto-covariance matrix, dispersion matrix, variance matrix, or variance\u2013covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector."}, {"text": "The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal. Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA.  If the matrix A is a real matrix, then U and V are also real."}, {"text": "LDA tends to be a better than QDA when you have a small training set. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major issue, or if the assumption of a common covariance matrix for the K classes is clearly untenable."}, {"text": "Principle Component Analysis (PCA) is a common feature extraction method in data science. Technically, PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues and then uses those to project the data into a new subspace of equal or less dimensions."}, {"text": "Principle Component Analysis (PCA) is a common feature extraction method in data science. Technically, PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues and then uses those to project the data into a new subspace of equal or less dimensions."}]}, {"question": "Can a Boxplot be used to detect outliers", "positive_ctxs": [{"text": "Box plots are useful as they show outliers within a data set. An outlier is an observation that is numerically distant from the rest of the data. When reviewing a box plot, an outlier is defined as a data point that is located outside the whiskers of the box plot."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Predictive ModelingClean the data by removing outliers and treating missing data.Identify a parametric or nonparametric predictive modeling approach to use.Preprocess the data into a form suitable for the chosen modeling algorithm.Specify a subset of the data to be used for training the model.More items"}, {"text": "The steps are:Clean the data by removing outliers and treating missing data.Identify a parametric or nonparametric predictive modeling approach to use.Preprocess the data into a form suitable for the chosen modeling algorithm.Specify a subset of the data to be used for training the model.More items"}, {"text": "Z Score is free of any scale, hence it is used as a transformation technique while we need to make any variable unit free in various statistical techniques. Also, it is used to identifying outliers in a univarite way.  Z-test is a statistical technique to test the Null Hypothesis against the Alternate Hypothesis."}, {"text": "Z Score is free of any scale, hence it is used as a transformation technique while we need to make any variable unit free in various statistical techniques. Also, it is used to identifying outliers in a univarite way.  Z-test is a statistical technique to test the Null Hypothesis against the Alternate Hypothesis."}, {"text": "Neural networks offer a number of advantages, including requiring less formal statistical training, ability to implicitly detect complex nonlinear relationships between dependent and independent variables, ability to detect all possible interactions between predictor variables, and the availability of multiple training"}, {"text": "5 ways to deal with outliers in dataSet up a filter in your testing tool. Even though this has a little cost, filtering out outliers is worth it.  Remove or change outliers during post-test analysis.  Change the value of outliers.  Consider the underlying distribution.  Consider the value of mild outliers."}, {"text": "5 ways to deal with outliers in dataSet up a filter in your testing tool. Even though this has a little cost, filtering out outliers is worth it.  Remove or change outliers during post-test analysis.  Change the value of outliers.  Consider the underlying distribution.  Consider the value of mild outliers."}]}, {"question": "What is big data Course", "positive_ctxs": [{"text": "Our Big Data Hadoop certification training course lets you master the concepts of the Hadoop framework, Big Data tools, and methodologies to prepare you for success in your role as a Big Data Developer. Learn how various components of the Hadoop ecosystem fit into the Big Data processing lifecycle."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A posterior probability value is a prior probability value that has been a | Course Hero. Study Resources. by Textbook. by Literature Title."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Real-time big data analytics means that big data is processed as it arrives and either a business user gets consumable insights without exceeding a time period allocated for decision-making or an analytical system triggers an action or a notification."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The answer is a big NO. Data science gets solutions and results to specific business problems using AI as a tool. If data science is to insights, machine learning is to predictions and artificial intelligence is to actions."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "What is the finite population correction factor", "positive_ctxs": [{"text": "The finite population correction (fpc) factor is used to adjust a variance estimate for an estimated mean or total, so that this variance only applies to the portion of the population that is not in the sample."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The finite population correction (fpc) factor is used to adjust a variance estimate for an estimated mean or total, so that this variance only applies to the portion of the population that is not in the sample."}, {"text": "Improving the PF can maximize current-carrying capacity, improve voltage to equipment, reduce power losses, and lower electric bills. The simplest way to improve power factor is to add PF correction capacitors to the electrical system. PF correction capacitors act as reactive current generators."}, {"text": "The short answer is \"no\"--there is no unbiased estimator of the population standard deviation (even though the sample variance is unbiased). However, for certain distributions there are correction factors that, when multiplied by the sample standard deviation, give you an unbiased estimator."}, {"text": "In statistics, Bessel's correction is the use of n \u2212 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance."}, {"text": "When a sampling unit is drawn from a finite population and is returned to that population, after its characteristic(s) have been recorded, before the next unit is drawn, the sampling is said to be \u201cwith replacement\u201d."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A continuity correction factor is used when you use a continuous probability distribution to approximate a discrete probability distribution. For example, when you want to use the normal to approximate a binomial.  p = probability of an event (e.g. 60%), q = probability the event doesn't happen (100% \u2013 p)."}]}, {"question": "How can a guy be more attractive and confident", "positive_ctxs": [{"text": "11 Ways to Build Your Confidence and Appear More Attractive.  Always be ready to tell a good story.  Demonstrate inquisitiveness.  Practice good posture.  Stop worrying about what people think.  Eliminate negative self-talk.  Smile.  Learn from your mistakes without dwelling on them.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Every time you conduct a t-test there is a chance that you will make a Type I error.  An ANOVA controls for these errors so that the Type I error remains at 5% and you can be more confident that any statistically significant result you find is not just running lots of tests."}, {"text": "The correct interpretation of a 95% confidence interval is that \"we are 95% confident that the population parameter is between X and X.\""}, {"text": "Data Processing is a task of converting data from a given form to a much more usable and desired form i.e. making it more meaningful and informative. Using Machine Learning algorithms, mathematical modelling and statistical knowledge, this entire process can be automated."}, {"text": "Rejection Regions and Alpha Levels You, as a researcher, choose the alpha level you are willing to accept. For example, if you wanted to be 95% confident that your results are significant, you would choose a 5% alpha level (100% \u2013 95%). That 5% level is the rejection region."}, {"text": "p-value helps you to decide whether there is a relationship between two variables or not. The smaller the p-value this mean the more confident you are about the existence of relationship between the two variables."}, {"text": "Results of a study can be made more accurate by controlling for the variation in the covariate. So, a covariate is in fact, a type of control variable.  A control variable is a nominal variable (not continuous) and although it has more than one value, the values are categorical and not infinite."}, {"text": "An example of a mutually exclusive event is when a coin is a tossed and there are two events that can occur, either it will be a head or a tail. Hence, both the events here are mutually exclusive.Difference between Mutually exclusive and independent eventsMutually exclusive eventsIndependent events4 more rows"}]}, {"question": "Whats the difference between derivation sample and validation sample", "positive_ctxs": [{"text": "Validation and Derivation Procedures serve two different purposes. Validation Procedures compare multiple Question responses for the same patient for the purpose of ensuring that patient data is valid. Derivation Procedures use calculations to derive values from collected data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The law of large numbers states that the sample mean of independent and identically distributed observations converges to a certain value. The central limit theorem describes the distribution of the difference between the sample mean and that value."}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "Given these assumptions, we know the following.The expected value of the difference between all possible sample means is equal to the difference between population means. Thus,  The standard deviation of the difference between sample means (\u03c3d) is approximately equal to: \u03c3d = sqrt( \u03c312 / n1 + \u03c322 / n2 )"}, {"text": "Response bias can be defined as the difference between the true values of variables in a study's net sample group and the values of variables obtained in the results of the same study.  Nonresponse bias occurs when some respondents included in the sample do not respond."}, {"text": "Response bias can be defined as the difference between the true values of variables in a study's net sample group and the values of variables obtained in the results of the same study.  Nonresponse bias occurs when some respondents included in the sample do not respond."}]}, {"question": "What is KTH in statistics", "positive_ctxs": [{"text": "In statistics, the kth order statistic of a statistical sample is equal to its kth-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables."}]}, {"question": "What does a negative path coefficient mean", "positive_ctxs": [{"text": "Answer. A negative path loading is basically the same as a negative regression coefficient. I.e., For a path loading from X to Y it is the predicted increase in Y for a one unit increase on X holding all other variables constant. So a negative coefficient just means that as X increases, Y is predicted to decrease."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Answer. A negative path loading is basically the same as a negative regression coefficient. I.e., For a path loading from X to Y it is the predicted increase in Y for a one unit increase on X holding all other variables constant. So a negative coefficient just means that as X increases, Y is predicted to decrease."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "The sign of a regression coefficient tells you whether there is a positive or negative correlation between each independent variable the dependent variable. A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase."}, {"text": "A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease. The coefficient value signifies how much the mean of the dependent variable changes given a one-unit shift in the independent variable while holding other variables in the model constant."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "False negative would therefore mean that there was a object (result should be positive) but the algorithm did not detect it (and therefore returned negative). A true negative is simply the algorithm correctly stating that the area it checked does not hold an object."}, {"text": "A false negative is a test result that indicates a person does not have a disease or condition when the person actually does have it, according to the National Institute of Health (NIH)."}]}, {"question": "What is a prior in Bayesian statistics", "positive_ctxs": [{"text": "In Bayesian statistical inference, a prior probability distribution, often simply called the prior, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account.  Priors can be created using a number of methods."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bayesian decision making is the process in which a decision is made based on the probability of a successful outcome, where this probability is informed by both prior information and new evidence that the decision maker obtains."}, {"text": "Bayesian decision making is the process in which a decision is made based on the probability of a successful outcome, where this probability is informed by both prior information and new evidence that the decision maker obtains."}, {"text": "Bayesian statistics are indispensable when what you need is to evaluate a decision or conclusion in light of the available evidence. Quality control would be impossible without Bayesian statistics."}, {"text": "The posterior distribution is a way to summarize what we know about uncertain quantities in Bayesian analysis. It is a combination of the prior distribution and the likelihood function, which tells you what information is contained in your observed data (the \u201cnew evidence\u201d)."}, {"text": "In Bayesian statistical inference, a prior probability distribution, often simply called the prior, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account.  Priors can be created using a number of methods."}, {"text": "The prior distribution is a distribution for the parameters whereas the prior predictive distribution is a distribution for the observations.  The last line is based on the assumption that the upcoming observation is independent of X given \u03b8."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "What is probability and random process", "positive_ctxs": [{"text": "In probability theory and related fields, a stochastic or random process is a mathematical object usually defined as a family of random variables.  Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed."}, {"text": "In probability theory and related fields, a stochastic or random process is a mathematical object usually defined as a family of random variables.  Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A time series is a stochastic process that operates in continuous state space and discrete time set. A stochastic process is nothing but a set of random variables. It is a time dependent random phenomenon. Same is time series."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed.  A log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive."}, {"text": "Stochastic (from from Greek \u03c3\u03c4\u03cc\u03c7\u03bf\u03c2 (st\u00f3khos) 'aim, guess'.) is any randomly determined process. In mathematics the terms stochastic process and random process are interchangeable."}]}, {"question": "Why is the sigmoid activation function not recommended for hidden units but it is fine for an output unit", "positive_ctxs": [{"text": "Sigmoid and tanh should not be used as activation function for the hidden layer. This is because of the vanishing gradient problem, i.e., if your input is on a higher side (where sigmoid goes flat) then the gradient will be near zero.  The best function for hidden layers is thus ReLu."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Generally, we use softmax activation instead of sigmoid with the cross-entropy loss because softmax activation distributes the probability throughout each output node. But, since it is a binary classification, using sigmoid is same as softmax. For multi-class classification use sofmax with cross-entropy."}, {"text": "The rectifier is, as of 2017, the most popular activation function for deep neural networks. A unit employing the rectifier is also called a rectified linear unit (ReLU)."}, {"text": "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable."}, {"text": "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable."}, {"text": "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable."}, {"text": "So a CDF is a function whose output is a probability. The PDF is a function whose output is a nonnegative number. The PDF itself is not a probability (unlike the CDF), but it can be used to calculate probabilities."}, {"text": "We use binary cross-entropy loss for classification models which output a probability p. The range of the sigmoid function is [0, 1] which makes it suitable for calculating probability."}]}, {"question": "Do I need to do feature scaling for simple linear regression", "positive_ctxs": [{"text": "No, you don't. You'll get an equivalent solution whether you apply some kind of linear scaling or not.  Then linear scaling can change the results dramatically. That's actually another reason to do feature scaling, but since you asked about simple linear regression, I won't go into that."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The process of dividing each feature by its range is called feature scaling. The process feature scaling is used to standardize each variables individually. The term feature scaling when it comes to data processing is also known as data normalization."}, {"text": "Simple linear regression is commonly used in forecasting and financial analysis\u2014for a company to tell how a change in the GDP could affect sales, for example. Microsoft Excel and other software can do all the calculations, but it's good to know how the mechanics of simple linear regression work."}, {"text": "Logistic regression is known and used as a linear classifier. It is used to come up with a hyperplane in feature space to separate observations that belong to a class from all the other observations that do not belong to that class. The decision boundary is thus linear.13\u200f/03\u200f/2019"}, {"text": "Feature scaling is essential for machine learning algorithms that calculate distances between data.  Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance."}, {"text": "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  After fitting a linear regression model, you need to determine how well the model fits the data."}, {"text": "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  After fitting a linear regression model, you need to determine how well the model fits the data."}, {"text": "The general guideline is to use linear regression first to determine whether it can fit the particular type of curve in your data. If you can't obtain an adequate fit using linear regression, that's when you might need to choose nonlinear regression."}]}, {"question": "What is a deconvolution layer", "positive_ctxs": [{"text": "Deconvolution layer is a very unfortunate name and should rather be called a transposed convolutional layer. Visually, for a transposed convolution with stride one and no padding, we just pad the original input (blue entries) with zeroes (white entries) (Figure 1)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Convolutional Neural Networks have a different architecture than regular Neural Networks.  Every layer is made up of a set of neurons, where each layer is fully connected to all neurons in the layer before. Finally, there is a last fully-connected layer \u2014 the output layer \u2014 that represent the predictions."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "Forward propagation is how neural networks make predictions. Input data is \u201cforward propagated\u201d through the network layer by layer to the final layer which outputs a prediction."}, {"text": "Dense CNN is a type of Deep CNN in which each layer is connected with another layer deeper than itself."}]}, {"question": "Why is logistic regression better than random forest", "positive_ctxs": [{"text": "In general, logistic regression performs better when the number of noise variables is less than or equal to the number of explanatory variables and random forest has a higher true and false positive rate as the number of explanatory variables increases in a dataset."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}, {"text": "The fundamental reason to use a random forest instead of a decision tree is to combine the predictions of many decision trees into a single model. The logic is that a single even made up of many mediocre models will still be better than one good model."}, {"text": "We use many algorithms such as Na\u00efve Bayes, Decision trees, SVM, Random forest classifier, KNN, and logistic regression for classification."}, {"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}, {"text": "The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree."}]}, {"question": "What are the layers of CNN", "positive_ctxs": [{"text": "We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A CNN LSTM can be defined by adding CNN layers on the front end followed by LSTM layers with a Dense layer on the output. It is helpful to think of this architecture as defining two sub-models: the CNN Model for feature extraction and the LSTM Model for interpreting the features across time steps."}, {"text": "CNNs are trained to identify and extract the best features from the images for the problem at hand. That is their main strength. The latter layers of a CNN are fully connected because of their strength as a classifier."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers, and normalization layers. Here it simply means that instead of using the normal activation functions defined above, convolution and pooling functions are used as activation functions."}, {"text": "Improve your model accuracy by Transfer Learning.Loading data using python libraries.Preprocess of data which includes reshaping, one-hot encoding and splitting.Constructing the model layers of CNN followed by model compiling, model training.Evaluating the model on test data.Finally, predicting the correct and incorrect labels."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}]}, {"question": "What is truncated SVD", "positive_ctxs": [{"text": "Truncated SVD are the singular values of the matrix A with rank r. We can find truncated SVD to A by setting all but the first k largest singular values equal to zero and using only the first k columns of U and V."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "where Ua is size m \u00d7 n, Ub is size m \u00d7 (m - n), and \u03a3a is of size n \u00d7 n. Then A = Ua\u03a3aVH is called the reduced SVD of the matrix A. In this context the SVD defined in Equation (1) is sometimes referred to as the full SVD for contrast. Notice that Ua is not unitary, but it does have orthogonal columns."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal. Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA.  If the matrix A is a real matrix, then U and V are also real."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler. For the case of simplicity we will focus on the SVD for real-valued matrices and ignore the case for complex numbers."}]}, {"question": "What is a hazard ratio with confidence interval", "positive_ctxs": [{"text": "The hazard ratio is a clinical trial statistic that allows the physician to say with confidence that healing is faster with the new drug. The hazard ratio must be >1 and the lower limit of the 95% confidence interval of the hazard ratio must be >1, which was the case in this example."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If the hazard ratio is less than 1, then the predictor is protective (i.e., associated with improved survival) and if the hazard ratio is greater than 1, then the predictor is associated with increased risk (or decreased survival)."}, {"text": "However, people generally apply this probability to a single study. Consequently, an odds ratio of 5.2 with a confidence interval of 3.2 to 7.2 suggests that there is a 95% probability that the true odds ratio would be likely to lie in the range 3.2-7.2 assuming there is no bias or confounding."}, {"text": "The difference between a ratio scale and an interval scale is that the zero point on an interval scale is some arbitrarily agreed value, whereas on a ratio scale it is a true zero."}, {"text": "An odds ratio is a measure of association between the presence or absence of two properties.  The value of the odds ratio tells you how much more likely someone under 25 might be to make a claim, for example, and the associated confidence interval indicates the degree of uncertainty associated with that ratio."}, {"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}, {"text": "The hazard function is the instantaneous rate of failure at a given time. Characteristics of a hazard function are frequently associated with certain products and applications. Different hazard functions are modeled with different distribution models."}, {"text": "In the large-sample case, a 95% confidence interval estimate for the population mean is given by x\u0304 \u00b1 1.96\u03c3/ \u221an. When the population standard deviation, \u03c3, is unknown, the sample standard deviation is used to estimate \u03c3 in the confidence interval formula."}]}, {"question": "What is one vs all classification in machine learning", "positive_ctxs": [{"text": "One-vs-rest (OvR for short, also referred to as One-vs-All or OvA) is a heuristic method for using binary classification algorithms for multi-class classification. It involves splitting the multi-class dataset into multiple binary classification problems."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The most significant difference between regression vs classification is that while regression helps predict a continuous quantity, classification predicts discrete class labels. There are also some overlaps between the two types of machine learning algorithms."}, {"text": "XGboost is the most widely used algorithm in machine learning, whether the problem is a classification or a regression problem. It is known for its good performance as compared to all other machine learning algorithms."}, {"text": "For multi class classification using SVM; It is NOT (one vs one) and NOT (one vs REST). Instead learn a two-class classifier where the feature vector is (x, y) where x is data and y is the correct label associated with the data."}, {"text": "Random initialization refers to the practice of using random numbers to initialize the weights of a machine learning model. Random initialization is one way of performing symmetry breaking, which is the act of preventing all of the weights in the machine learning model from being the same."}, {"text": "Preference learning is a subfield in machine learning, which is a classification method based on observed preference information. In the view of supervised learning, preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items."}, {"text": "The Gaussian Processes Classifier is a classification machine learning algorithm. Gaussian Processes are a generalization of the Gaussian probability distribution and can be used as the basis for sophisticated non-parametric machine learning algorithms for classification and regression."}, {"text": "KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.  KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point."}]}, {"question": "What is global thresholding in image processing", "positive_ctxs": [{"text": "A global thresholding technique is one which makes use of a single threshold value for the whole image, whereas local thresholding technique makes use of unique threshold values for the partitioned subimages obtained from the whole image."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition: An image processing method that creates a bitonal (aka binary) image based on setting a threshold value on the pixel intensity of the original image.  The thresholding process is sometimes described as separating an image into foreground values (black) and background values (white)."}, {"text": "Definition: An image processing method that creates a bitonal (aka binary) image based on setting a threshold value on the pixel intensity of the original image.  The thresholding process is sometimes described as separating an image into foreground values (black) and background values (white)."}, {"text": "In image processing, thresholding is used to split an image into smaller segments, or junks, using at least one color or gray scale value to define their boundary. The advantage of obtaining first a binary image is that it reduces the complexityof the data and simplifies the process of recognition and classification."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}, {"text": "The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent."}, {"text": "The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent."}]}, {"question": "Is linear regression invariant to scaling", "positive_ctxs": [{"text": "By design, linear regression is, in some way, scale-invariant.  Some authors have developed rank-regression techniques to handle non-linear re-scaling, using the same approach as in the previous section on clustering."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them."}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them. Sometimes this is incorrect."}, {"text": "The process of dividing each feature by its range is called feature scaling. The process feature scaling is used to standardize each variables individually. The term feature scaling when it comes to data processing is also known as data normalization."}, {"text": "Linear time invariant (LTI) filters are linear applications that transform a signal into another signal, as such that the application commutes with time shifts."}, {"text": "They are basically equivalent: the linear time invariant systems refers to an analog system and shift-invariant system refers to a discrete-time system.  The shift-invariant is the same as time invariant: if we delay the input, the output that we get is the original input to the signal that wasn't delayed."}, {"text": "A Latent Class regression model: Is used to predict a dependent variable as a function of predictor variables (Regression model). Includes a K-category latent variable X to cluster cases (LC model)  Each case may contain multiple records (Regression with repeated measurements)."}, {"text": "The trace of a matrix is the sum of its (complex) eigenvalues, and it is invariant with respect to a change of basis. This characterization can be used to define the trace of a linear operator in general. The trace is only defined for a square matrix (n \u00d7 n)."}]}, {"question": "What is the least square estimate", "positive_ctxs": [{"text": "The method of least squares is about estimating parameters by minimizing the squared discrepancies between observed data, on the one hand, and their expected values on the other (see Optimization Methods)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Firstly, while the sample variance (using Bessel's correction) is an unbiased estimator of the population variance, its square root, the sample standard deviation, is a biased estimate of the population standard deviation; because the square root is a concave function, the bias is downward, by Jensen's inequality."}, {"text": "MSE is the average of the squared error that is used as the loss function for least squares regression: It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE."}, {"text": "The standard deviation of this set of mean values is the standard error. In lieu of taking many samples one can estimate the standard error from a single sample. This estimate is derived by dividing the standard deviation by the square root of the sample size."}, {"text": "First, it is a very quick estimate of the standard deviation. The standard deviation requires us to first find the mean, then subtract this mean from each data point, square the differences, add these, divide by one less than the number of data points, then (finally) take the square root."}]}, {"question": "What types of problems can machine learning solve", "positive_ctxs": [{"text": "Let's take a look at some of the important business problems solved by machine learning.Manual data entry.  Detecting Spam.  Product recommendation.  Medical Diagnosis.  Customer segmentation and Lifetime value prediction.  Financial analysis.  Predictive maintenance.  Image recognition (Computer Vision)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Supervised learning allows collecting data and produce data output from the previous experiences. Helps to optimize performance criteria with the help of experience. Supervised machine learning helps to solve various types of real-world computation problems."}, {"text": "Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data.  Deep learning allows machines to solve complex problems even when using a data set that is very diverse, unstructured and inter-connected."}, {"text": "Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data.  Deep learning allows machines to solve complex problems even when using a data set that is very diverse, unstructured and inter-connected."}, {"text": "Some common types of problems built on top of classification and regression include recommendation and time series prediction respectively. Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems. Random forest for classification and regression problems."}, {"text": "Reinforcement learning (RL) is a significant area of machine learning, with the potential to solve a lot of real world problems in various fields, like game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics."}, {"text": "If you want to solve some real-world problems and design a cool product or algorithm, then having machine learning skills is not enough. You would need good working knowledge of data structures.  So you've decided to move beyond canned algorithms and start to code your own machine learning methods."}]}, {"question": "What is the point of K means clustering", "positive_ctxs": [{"text": "k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.  The results of the K-means clustering algorithm are: The centroids of the K clusters, which can be used to label new data. Labels for the training data (each data point is assigned to a single cluster)"}, {"text": "A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical."}, {"text": "Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows\u2022"}, {"text": "K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups).  The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers."}]}, {"question": "What is the difference between an absolute minimum and a local minimum", "positive_ctxs": [{"text": "Similarly, an absolute minimum occurs at the x value where the function is the smallest, while a local minimum occurs at an x value if the function is smaller there than points around it (i.e. an open interval around it)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."}, {"text": "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."}, {"text": "A local minimum of a function (typically a cost function in machine learning, which is something we want to minimize based on empirical data) is a point in the domain of a function that has the following property: the function evaluates to a greater value at every other point in a neighborhood around the local minimum"}, {"text": "A local minimum of a function is a point where the function value is smaller than at nearby points, but possibly greater than at a distant point. A global minimum is a point where the function value is smaller than at all other feasible points."}, {"text": "Plot a symbol at the median and draw a box between the lower and upper quartiles. Calculate the interquartile range (the difference between the upper and lower quartile) and call it IQ. The line from the lower quartile to the minimum is now drawn from the lower quartile to the smallest point that is greater than L1."}, {"text": "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.  But if we instead take steps proportional to the positive of the gradient, we approach a local maximum of that function; the procedure is then known as gradient ascent."}, {"text": "Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent is simply used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible."}]}, {"question": "What properties make an image easy to recognize", "positive_ctxs": [{"text": "As a human can easily recognize the image by seeing its color, shape, texture or some other feature, the same way machine first extracts the features of the object and then it applies the classification algorithm to label a particular class of the recognized object according to the extracted features."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Unlikely to CNN, RNN learns to recognize image features across time. Although RNN can be used for image classification theoretically, only a few researches about RNN image classifier can be found."}, {"text": "The availability heuristic is a mental shortcut that helps us make a decision based on how easy it is to bring something to mind.  The representativeness heuristic is a mental shortcut that helps us make a decision by comparing information to our mental prototypes."}, {"text": "Deep neural networks. A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed."}, {"text": "Template matching is a technique for finding areas of an image that match (or are similar) to a template image which requires two images. Source image (I): The image in which we expect to find a match to the template image. Template image (T): The patch image which will be compared to the template image."}, {"text": "A high-pass filter can be used to make an image appear sharper. These filters emphasize fine details in the image \u2013 exactly the opposite of the low-pass filter. High-pass filtering works in exactly the same way as low-pass filtering; it just uses a different convolution kernel."}, {"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates."}]}, {"question": "How statistics can be misused", "positive_ctxs": [{"text": "Statistics, when used in a misleading fashion, can trick the casual observer into believing something other than what the data shows. That is, a misuse of statistics occurs when a statistical argument asserts a falsehood. In some cases, the misuse may be accidental."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Robust statistics are resistant to outliers. In other words, if your data set contains very high or very low values, then some statistics will be good estimators for population parameters, and some statistics will be poor estimators."}, {"text": "Moments in mathematical statistics involve a basic calculation. These calculations can be used to find a probability distribution's mean, variance, and skewness. Using this formula requires us to be careful with our order of operations."}, {"text": "Look up the normal distribution in a statistics table. Statistics tables can be found online or in statistics textbooks. Find the value for the intersection of the correct degrees of freedom and alpha. If this value is less than or equal to the chi-square value, the data is statistically significant."}, {"text": "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It can be used to estimate summary statistics such as the mean or standard deviation.  That when using the bootstrap you must choose the size of the sample and the number of repeats."}, {"text": "In statistics and machine learning, the bias\u2013variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters."}, {"text": "Univariate statistics summarize only one variable at a time. Bivariate statistics compare two variables. Multivariate statistics compare more than two variables."}, {"text": "Univariate statistics summarize only one variable at a time. Bivariate statistics compare two variables. Multivariate statistics compare more than two variables."}]}, {"question": "Why is TNF alpha test done", "positive_ctxs": [{"text": "Blood Test Helps Determine Patient Response To TNF-Alpha Inhibitors. Baseline levels of serum interferon in rheumatoid arthritis (RA) patients may help rheumatologists determine who may have a poor response to tumour necrosis factor-alpha inhibitor drugs."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Why is an alpha level of . 05 commonly used? Seeing as the alpha level is the probability of making a Type I error, it seems to make sense that we make this area as tiny as possible.  The smaller the alpha level, the smaller the area where you would reject the null hypothesis."}, {"text": "Mathematically test efficiency is calculated as a percentage of the number of alpha testing (in-house or on-site) defects divided by sum of a number of alpha testing and a number of beta testing (off-site) defects."}, {"text": "Medical Definition of alpha state : a state of wakeful relaxation that is associated with increased alpha wave activity When electroencephalograms show a brain wave pattern of 9 to 12 cycles per second, the subject is said to be in alpha state, usually described as relaxed, peaceful, or floating.\u2014"}, {"text": "The p-value is calculated using the sampling distribution of the test statistic under the null hypothesis, the sample data, and the type of test being done (lower-tailed test, upper-tailed test, or two-sided test).  an upper-tailed test is specified by: p-value = P(TS ts | H 0 is true) = 1 - cdf(ts)"}, {"text": "The most common threshold is p < 0.05, which means that the data is likely to occur less than 5% of the time under the null hypothesis. When the p-value falls below the chosen alpha value, then we say the result of the test is statistically significant."}, {"text": "In programming languages In Fortran, R, APL, J and Wolfram Language (Mathematica), it is done through simple multiplication operator * , whereas the matrix product is done through the function matmul , %*% , +."}, {"text": "Rejection Regions and Alpha Levels You, as a researcher, choose the alpha level you are willing to accept. For example, if you wanted to be 95% confident that your results are significant, you would choose a 5% alpha level (100% \u2013 95%). That 5% level is the rejection region."}]}, {"question": "Is it working hand in glove or hand and glove", "positive_ctxs": [{"text": "Both phrases are grammatically correct and meaningful, so the difference is a matter of style. \u201cIn\u201d is more specific than \u201cand\u201d. A glove with a hand in it functions in close conformance with the hand, while a glove on a shelf does not."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The idea behind bootstrap is to use the data of a sample study at hand as a \u201csurrogate population\u201d, for the purpose of approximating the sampling distribution of a statistic; i.e. to resample (with replacement) from the sample data at hand and create a large number of \u201cphantom samples\u201d known as bootstrap samples."}, {"text": "Body parts are not used as standard unit of measurement because length of palm and hand are different for different persons which causes error in measurement."}, {"text": "The ground-truth bounding boxes (i.e., the hand labeled bounding boxes from the testing set that specify where in the image our object is)."}, {"text": "Naive Bayes Classifier example by hand and how to do in Scikit-Learn, You can use any kind of predictor in a naive Bayes classifier, as long as you can specify a discriminative linear models take a mixture of categorical and continuous predictors."}, {"text": "\u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance."}, {"text": "\u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance."}, {"text": "\u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance."}]}, {"question": "What is a stratified sample in statistics", "positive_ctxs": [{"text": "Definition: Stratified sampling is a type of sampling method in which the total population is divided into smaller groups or strata to complete the sampling process. The strata is formed based on some common characteristics in the population data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Quota sampling is different from stratified sampling, because in a stratified sample individuals within each stratum are selected at random. Quota sampling achieves a representative age distribution, but it isn't a random sample, because the sampling frame is unknown."}, {"text": "Compared to simple random sampling, stratified sampling has two main disadvantages.Advantages and DisadvantagesA stratified sample can provide greater precision than a simple random sample of the same size.Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money.More items"}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "Stratified sampling offers several advantages over simple random sampling. A stratified sample can provide greater precision than a simple random sample of the same size. Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money."}, {"text": "Stratified sampling offers several advantages over simple random sampling. A stratified sample can provide greater precision than a simple random sample of the same size. Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money."}, {"text": "Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables."}, {"text": "A sample may be selected from a population through a number of ways, one of which is the stratified random sampling method. A stratified random sampling involves dividing the entire population into homogeneous groups called strata (plural for stratum). Random samples are then selected from each stratum."}]}, {"question": "How do you do a time series analysis", "positive_ctxs": [{"text": "Nevertheless, the same has been delineated briefly below:Step 1: Visualize the Time Series. It is essential to analyze the trends prior to building any kind of time series model.  Step 2: Stationarize the Series.  Step 3: Find Optimal Parameters.  Step 4: Build ARIMA Model.  Step 5: Make Predictions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A stationary time series is one whose properties do not depend on the time at which the series is observed. 14. Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times."}, {"text": "Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.  Interrupted time series analysis is the analysis of interventions on a single time series. Time series data have a natural temporal ordering."}, {"text": "Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components. Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Time series analysis is a statistical technique that deals with time series data, or trend analysis. Time series data means that data is in a series of particular time periods or intervals.  Time series data: A set of observations on the values that a variable takes at different times."}, {"text": "When Longitudinal data looks like a time series is when we measure the same thing over time. The big difference is that in a time series we can measure the overall change in the measurement over time (or by group) while in a longitudinal analysis you actually have the measurement of change at the individual level."}, {"text": "Time series analysis involves developing models that best capture or describe an observed time series in order to understand the underlying causes. This field of study seeks the \u201cwhy\u201d behind a time series dataset."}]}, {"question": "Why do we use Gaussian noise", "positive_ctxs": [{"text": "The reason why a Gaussian makes sense is because noise is often the result of summing a large number of different and independent factors, which allows us to apply an important result from probability and statistics, called the central limit theorem."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Gaussian Noise is a statistical noise having a probability density function equal to normal distribution, also known as Gaussian Distribution. Random Gaussian function is added to Image function to generate this noise. It is also called as electronic noise because it arises in amplifiers or detectors."}, {"text": "The Canny filter is a multi-stage edge detector. It uses a filter based on the derivative of a Gaussian in order to compute the intensity of the gradients. The Gaussian reduces the effect of noise present in the image."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise.  The Gaussian filter alone will blur edges and reduce contrast. The Median filter is a non-linear filter that is most commonly used as a simple way to reduce noise in an image."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}]}, {"question": "What is a cross sectional study quizlet statistics", "positive_ctxs": [{"text": "\u200bCross-sectional studies are observational studies that collect information about individuals at a specific point in time or over a very short period of time.  For the lung cancer\u200b study, it could be that individuals develop cancer after the data are\u200b collected, so the study will not give the full picture."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cross-sectional data, or a cross section of a study population, in statistics and econometrics is a type of data collected by observing many subjects (such as individuals, firms, countries, or regions) at the one point or period of time. The analysis might also have no regard to differences in time."}, {"text": "Statistics is the study of the collection, organization, analysis, and interpretation of data.  Mathematical statistics is the study of statistics from a mathematical standpoint, using probability theory as well as other branches of mathematics such as linear algebra and analysis."}, {"text": "The cross product is a calculation used in order to define the correlation coefficient between two variables. SP is the sum of all cross products between two variables."}, {"text": "Matrix theory is a branch of mathematics which is focused on study of matrices. Initially, it was a sub-branch of linear algebra, but soon it grew to cover subjects related to graph theory, algebra, combinatorics and statistics as well."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "Binary cross-entropy is for multi-label classifications, whereas categorical cross entropy is for multi-class classification where each example belongs to a single class."}]}, {"question": "Is a maximum likelihood estimator unbiased", "positive_ctxs": [{"text": "Therefore, the maximum likelihood estimator is an unbiased estimator of ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Cram\u00e9r-Rao Inequality provides a lower bound for the variance of an unbiased estimator of a parameter. It allows us to conclude that an unbiased estimator is a minimum variance unbiased estimator for a parameter."}, {"text": "The Cram\u00e9r-Rao Inequality provides a lower bound for the variance of an unbiased estimator of a parameter. It allows us to conclude that an unbiased estimator is a minimum variance unbiased estimator for a parameter."}, {"text": "An unbiased estimator is a statistics that has an expected value equal to the population parameter being estimated. Examples: The sample mean, is an unbiased estimator of the population mean, . The sample variance, is an unbiased estimator of the population variance, ."}, {"text": "Definition 1. A statistic d is called an unbiased estimator for a function of the parameter g(\u03b8) provided that for every choice of \u03b8, E\u03b8d(X) = g(\u03b8). Any estimator that not unbiased is called biased.  Note that the mean square error for an unbiased estimator is its variance."}, {"text": "Maximum likelihood, also called the maximum likelihood method, is the procedure of finding the value of one or more parameters for a given statistic which makes the known likelihood distribution a maximum. The maximum likelihood estimate for a parameter is denoted ."}, {"text": "Maximum likelihood, also called the maximum likelihood method, is the procedure of finding the value of one or more parameters for a given statistic which makes the known likelihood distribution a maximum. The maximum likelihood estimate for a parameter is denoted . For a Bernoulli distribution, (1)"}, {"text": "Maximum likelihood, also called the maximum likelihood method, is the procedure of finding the value of one or more parameters for a given statistic which makes the known likelihood distribution a maximum. The maximum likelihood estimate for a parameter is denoted . For a Bernoulli distribution, (1)"}]}, {"question": "What is a frequency distribution and how is it constructed", "positive_ctxs": [{"text": "One of the common methods for organizing data is to construct frequency distribution. Frequency distribution is an organized tabulation/graphical representation of the number of individuals in each category on the scale of measurement."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The only difference between a relative frequency distribution graph and a frequency distribution graph is that the vertical axis uses proportional or relative frequency rather than simple frequency. Cumulative relative frequency (also called an ogive) is the accumulation of the previous relative frequencies."}, {"text": "A probability frequency distribution is a way to show how often an event will happen. It also shows what the probability of each event happening is. A frequency distribution table can be created by hand, or you can make a frequency distribution table in Excel."}, {"text": "Frequency distribution in statistics is a representation that displays the number of observations within a given interval. The representation of a frequency distribution can be graphical or tabular so that it is easier to understand."}, {"text": "A curve that represents the cumulative frequency distribution of grouped data on a graph is called a Cumulative Frequency Curve or an Ogive. Representing cumulative frequency data on a graph is the most efficient way to understand the data and derive results."}, {"text": "To convert a frequency distribution to a probability distribution, divide area of the bar or interval of x by the total area of all the Bars. A simpler formula is: , N is the total Frequency and w is the interval of x. Example (From a frequency distribution table construct a probability plot)."}, {"text": "In statistics, normality tests are used to determine if a data set is well-modeled by a normal distribution and to compute how likely it is for a random variable underlying the data set to be normally distributed."}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}]}, {"question": "Are the samples dependent in the Central Limit Theorem", "positive_ctxs": [{"text": "According to the central limit theorem, the mean of a sampling distribution of means is an unbiased estimator of the population mean. Note that the larger the sample, the less variable the sample mean. The mean of many observations is less variable than the mean of few."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Because our sample size is greater than 30, the Central Limit Theorem tells us that the sampling distribution will approximate a normal distribution.  Because we know the population standard deviation and the sample size is large, we'll use the normal distribution to find probability."}, {"text": "The Finite Population Correction Factor (FPC) is used when you sample without replacement from more than 5% of a finite population. It's needed because under these circumstances, the Central Limit Theorem doesn't hold and the standard error of the estimate (e.g. the mean or proportion) will be too big."}, {"text": "The Finite Population Correction Factor (FPC) is used when you sample without replacement from more than 5% of a finite population. It's needed because under these circumstances, the Central Limit Theorem doesn't hold and the standard error of the estimate (e.g. the mean or proportion) will be too big."}, {"text": "The Central Limit Theorem and Means In other words, add up the means from all of your samples, find the average and that average will be your actual population mean. Similarly, if you find the average of all of the standard deviations in your sample, you'll find the actual standard deviation for your population."}, {"text": "The Central limit Theorem states that when sample size tends to infinity, the sample mean will be normally distributed. The Law of Large Number states that when sample size tends to infinity, the sample mean equals to population mean."}, {"text": "The Central limit Theorem states that when sample size tends to infinity, the sample mean will be normally distributed. The Law of Large Number states that when sample size tends to infinity, the sample mean equals to population mean."}, {"text": "Unlike Monte Carlo sampling methods that are able to draw independent samples from the distribution, Markov Chain Monte Carlo methods draw samples where the next sample is dependent on the existing sample, called a Markov Chain."}]}, {"question": "What is the difference between a relative frequency histogram and a regular histogram", "positive_ctxs": [{"text": "The only difference between a frequency histogram and a relative frequency histogram is that the vertical axis uses relative or proportional frequency instead of simple frequency (see Figure 1)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The only difference between a relative frequency distribution graph and a frequency distribution graph is that the vertical axis uses proportional or relative frequency rather than simple frequency. Cumulative relative frequency (also called an ogive) is the accumulation of the previous relative frequencies."}, {"text": "An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value. By looking at the histogram for a specific image a viewer will be able to judge the entire tonal distribution at a glance."}, {"text": "A continuous random variable is normally distributed or has a normal probability distribution if its relative frequency histogram has the shape of a normal curve.  We can extend this idea to the shape of other distributions. If \u03bc = 0 and \u03c3 = 1, almost all of the data should be between -3 and 3, with the center at 0."}, {"text": "An easy way to define the difference between frequency and relative frequency is that frequency relies on the actual values of each class in a statistical data set while relative frequency compares these individual values to the overall totals of all classes concerned in a data set."}, {"text": "A histogram looks like a bar chart , except the area of the bar, and not the height, shows the frequency of the data . Histograms are typically used when the data is in groups of unequal width.  This is called frequency density."}, {"text": "Histograms are generally used to show the results of a continuous data set such as height, weight, time, etc. A bar graph has spaces between the bars, while a histogram does not. A histogram often shows the frequency that an event occurs within the defined range. It shows you how many times that event happens."}, {"text": "A histogram is a graphical display of data using bars of different heights. In a histogram, each bar groups numbers into ranges. Taller bars show that more data falls in that range. A histogram displays the shape and spread of continuous sample data."}]}, {"question": "Where does the F distribution come from", "positive_ctxs": [{"text": "The F-distribution arises from inferential statistics concerning population variances. More specifically, we use an F-distribution when we are studying the ratio of the variances of two normally distributed populations."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The test statistic used in ANOVA is Student's t. One characteristic of the F distribution is that F cannot be negative. One characteristic of the F distribution is that the computed F can only range between -1 and +1."}, {"text": "There are two sets of degrees of freedom; one for the numerator and one for the denominator. For example, if F follows an F distribution and the number of degrees of freedom for the numerator is four, and the number of degrees of freedom for the denominator is ten, then F ~ F 4,10."}, {"text": "The F Distribution The distribution of all possible values of the f statistic is called an F distribution, with v1 = n1 - 1 and v2 = n2 - 1 degrees of freedom. The curve of the F distribution depends on the degrees of freedom, v1 and v2."}, {"text": "F statistic is a statistic that is determined by an ANOVA test. It determines the significance of the groups of variables. The F critical value is also known as the F \u2013statistic. The F \u2013 statistic value is obtained from the F-distribution table."}, {"text": "The inversion method relies on the principle that continuous cumulative distribution functions (cdfs) range uniformly over the open interval (0,1). If u is a uniform random number on (0,1), then x = F - 1 ( u ) generates a random number x from any continuous distribution with the specified cdf F ."}, {"text": "Find the F Statistic (the critical value for this test). The F statistic formula is: F Statistic = variance of the group means / mean of the within group variances. You can find the F Statistic in the F-Table."}, {"text": "The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time.  The P value is determined from the F ratio and the two values for degrees of freedom shown in the ANOVA table."}]}, {"question": "How do you write a regression model", "positive_ctxs": [{"text": "The Linear Regression Equation The equation has the form Y= a + bX, where Y is the dependent variable (that's the variable that goes on the Y axis), X is the independent variable (i.e. it is plotted on the X axis), b is the slope of the line and a is the y-intercept."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How Stepwise Regression WorksStart the test with all available predictor variables (the \u201cBackward: method), deleting one variable at a time as the regression model progresses.  Start the test with no predictor variables (the \u201cForward\u201d method), adding one at a time as the regression model progresses."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}, {"text": "A continuous sample space is based on the same principles, but it has an infinite number of items in the space.  In other words, you can't write out the space in the same way that you would write out the sample space for a die roll."}, {"text": "How to Formulate an Effective HypothesisState the problem that you are trying to solve. Make sure that the hypothesis clearly defines the topic and the focus of the experiment.Try to write the hypothesis as an if-then statement.  Define the variables."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How to train a Machine Learning model in 5 minutesModel Naming \u2014 Give Your Model a Name: Let's start with giving your model a name, describe your model and attach tags to your model.  Data Type Selection \u2014 Choose data type(Images/Text/CSV): It's time to tell us about the type of data you want to train your model.More items"}]}, {"question": "What is first order statistics", "positive_ctxs": [{"text": "The first order statistic is the smallest sample value (i.e. the minimum), once the values have been placed in order. For example, in the sample 9, 2, 11, 5, 7, 4 the first order statistic is 2. In notation, that's x(1) = 2. The second order statistic x(2) is the next smallest value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference is obviously that, in a first order reaction, the order of reaction is one by nature. A pseudo first-order reaction is second order reaction by nature but has been altered to make it a first order reaction."}, {"text": "In statistics, the kth order statistic of a statistical sample is equal to its kth-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The stress state is a second order tensor since it is a quantity associated with two directions. As a result, stress components have 2 subscripts. A surface traction is a first order tensor (i.e. vector) since it a quantity associated with only one direction. Vector components therefore require only 1 subscript."}, {"text": "If you have n observations and order them to and define and then a future observation is equally likely to be between and for all from to . That is independent of the distribution, and also of the ordering of your sample. That is the sense in which order statistics are independent."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "Why is stochastic gradient descent stochastic", "positive_ctxs": [{"text": "Stochastic gradient descent is, well, stochastic. Because you are no longer using your entire training set a once, and instead picking one or more examples at a time in some likely random fashion, each time you tun SGD you will obtain a different optimum and a unique cost vs."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "The coefficients used in simple linear regression can be found using stochastic gradient descent.  Linear regression does provide a useful exercise for learning stochastic gradient descent which is an important algorithm used for minimizing cost functions by machine learning algorithms."}, {"text": "Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data."}, {"text": "Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data."}, {"text": "Adam optimizer. Implements the Adam optimization algorithm. Adam is a stochastic gradient descent method that computes individual adaptive learning rates for different parameters from estimates of first- and second-order moments of the gradients."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}]}, {"question": "What is transfer learning and how is it useful", "positive_ctxs": [{"text": "Transfer learning is useful when you have insufficient data for a new domain you want handled by a neural network and there is a big pre-existing data pool that can be transferred to your problem."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Fine tuning is one approach to transfer learning, and it is very popular in computer vision and NLP. The most common example given is when a model is trained on ImageNet is fine-tuned on a second task.  Transfer learning is when a model developed for one task is reused for a model on a second task."}, {"text": "Neural style transfer is trained as a supervised learning task in which the goal is to input two images (x), and train a network to output a new, synthesized image (y)."}, {"text": "When two or more random variables are defined on a probability space, it is useful to describe how they vary together; that is, it is useful to measure the relationship between the variables. A common measure of the relationship between two random variables is the covariance."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "1 Answer. Transfer learning is when a model developed for one task is reused to work on a second task. Fine tuning is one approach to transfer learning."}, {"text": "1 Answer. Transfer learning is when a model developed for one task is reused to work on a second task. Fine tuning is one approach to transfer learning."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "What is Adaline in neural network", "positive_ctxs": [{"text": "ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network. The network uses memistors.  It is based on the McCulloch\u2013Pitts neuron. It consists of a weight, a bias and a summation function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.  Often the weights of a neural network are contained within the hidden layers of the network."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.  Often the weights of a neural network are contained within the hidden layers of the network."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value."}, {"text": "A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation."}]}, {"question": "What is a good way to understand tensors", "positive_ctxs": [{"text": "Rather than trying to define a number, instead define what a field of numbers is; instead of defining what a vector is, consider instead all the vectors that make up a vector space. So to understand tensors of a particular type, instead consider all those tensors of the same type together."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "A curve that represents the cumulative frequency distribution of grouped data on a graph is called a Cumulative Frequency Curve or an Ogive. Representing cumulative frequency data on a graph is the most efficient way to understand the data and derive results."}, {"text": "The simplest way to compare two distributions is via the Z-test. The error in the mean is calculated by dividing the dispersion by the square root of the number of data points.  This is one way you can use to determine, in fact, the likelihood that your sample means it a good indicator of the true population mean."}, {"text": "The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms. The bag-of-words model is simple to understand and implement and has seen great success in problems such as language modeling and document classification."}, {"text": "Tensors are simply mathematical objects that can be used to describe physical properties, just like scalars and vectors. In fact tensors are merely a generalisation of scalars and vectors; a scalar is a zero rank tensor, and a vector is a first rank tensor."}, {"text": "In mathematics, a tensor is an algebraic object that describes a (multilinear) relationship between sets of algebraic objects related to a vector space. Objects that tensors may map between include vectors and scalars, and even other tensors."}, {"text": "In mathematics, a tensor is an algebraic object that describes a (multilinear) relationship between sets of algebraic objects related to a vector space. Objects that tensors may map between include vectors and scalars, and even other tensors."}]}, {"question": "What do you mean by information theory", "positive_ctxs": [{"text": "Information theory studies the quantification, storage, and communication of information. It was originally proposed by Claude Shannon in 1948 to find fundamental limits on signal processing and communication operations such as data compression, in a landmark paper titled \"A Mathematical Theory of Communication\"."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "Coding theory is one of the most important and direct applications of information theory.  Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "To give you two ideas:A Kolmogorov-Smirnov test is a non-parametric test, that measures the \"distance\" between two cumulative/empirical distribution functions.The Kullback-Leibler divergence measures the \"distance\" between two distributions in the language of information theory as a change in entropy."}, {"text": "Structural information theory (SIT) is a theory about human perception and in particular about visual perceptual organization, which is the neuro-cognitive process that enables us to perceive scenes as structured wholes consisting of objects arranged in space."}]}, {"question": "What are the disadvantages of dimensional analysis", "positive_ctxs": [{"text": "The method cannot be considered to derive composite relations. Examples s = ut + 1/2 at2 and 2as = v2 \u2013 u2. A formula containing trigonometric function, exponential function, and logarithmic function can not derive from it. The method cannot be used to derive the relationship between more than three quantities."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "One of the major disadvantages of the backpropagation learning rule is its ability to get stuck in local minima. The error is a function of all the weights in a multidimensional space."}, {"text": "Disadvantages include its \"black box\" nature, greater computational burden, proneness to overfitting, and the empirical nature of model development. An overview of the features of neural networks and logistic regression is presented, and the advantages and disadvantages of using this modeling technique are discussed."}, {"text": "However there are disadvantages to the use of second order derivatives. (We should note that first derivative operators exaggerate the effects of noise.) Second derivatives will exaggerated noise twice as much. No directional information about the edge is given."}, {"text": "Dimensionality reduction refers to techniques for reducing the number of input variables in training data. When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the \u201cessence\u201d of the data."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Major advantages include its simplicity and lack of bias. Among the disadvantages are difficulty gaining access to a list of a larger population, time, costs, and that bias can still occur under certain circumstances."}, {"text": "The mathematics of factor analysis and principal component analysis (PCA) are different. Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables."}]}, {"question": "What is validation in machine learning", "positive_ctxs": [{"text": "Definition. In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived.  Model validation is carried out after model training."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A loss function is used to optimize a machine learning algorithm. The loss is calculated on training and validation and its interpretation is based on how well the model is doing in these two sets.  An accuracy metric is used to measure the algorithm's performance in an interpretable way."}, {"text": "Definition. In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived.  Model validation is carried out after model training."}, {"text": "1) Your model performs better on the training data than on the unknown validation data.  It can also happen when your training loss is calculated as a moving average over 1 epoch, whereas the validation loss is calculated after the learning phase of the same epoch."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The loss is calculated on training and validation and its interpretation is how well the model is doing for these two sets. Unlike accuracy, a loss is not a percentage. It is a sum of the errors made for each example in training or validation sets."}, {"text": "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived."}, {"text": "No. If the learning rate is too high, then the model can diverge.  If the validation error consistently goes up, that means the model could be diverging because of high learning rate."}]}, {"question": "What is an example of the normal approximation of the binomial distribution", "positive_ctxs": [{"text": "For example, if n = 100 and p = 0.25 then we are justified in using the normal approximation. This is because np = 25 and n(1 - p) = 75. Since both of these numbers are greater than 10, the appropriate normal distribution will do a fairly good job of estimating binomial probabilities."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The normal approximation to the binomial is when you use a continuous distribution (the normal distribution) to approximate a discrete distribution (the binomial distribution)."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The direct approximation of the binomial by the Poisson says that a binomial(n,p) random variable has approximately the same distribution as a Poisson(np) random variable when np is large."}, {"text": "The probability mass function of the negative binomial distribution is. where r is the number of successes, k is the number of failures, and p is the probability of success."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "The Poisson distribution is a limiting case of the binomial distribution which arises when the number of trials n increases indefinitely whilst the product \u03bc = np, which is the expected value of the number of successes from the trials, remains constant."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side.  The normal distribution is often called the bell curve because the graph of its probability density looks like a bell."}]}, {"question": "What is difference between Bag of Words and TF IDF", "positive_ctxs": [{"text": "Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "As far as I know, in Bag Of Words method, features are a set of words and their frequency counts in a document. In another hand, N-grams, for example unigrams does exactly the same, but it does not take into consideration the frequency of occurance of a word."}, {"text": "Both LSA and LDA have same input which is Bag of words in matrix format. LSA focus on reducing matrix dimension while LDA solves topic modeling problems."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}]}, {"question": "What is supervised and unsupervised data", "positive_ctxs": [{"text": "In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Counterintuitive as it may be, supervised algorithms (particularly logistic regression and random forest) tend to outperform unsupervised ones on discrete classification and categorization tasks, where data is relatively structured and well-labeled."}, {"text": "KNN represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points, while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters."}, {"text": "KNN represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points, while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters."}, {"text": "In the absence of a class label, clustering analysis is also called unsupervised learning, as opposed to supervised learning that includes classification and regression. Accordingly, approaches to clustering analysis are typically quite different from supervised learning."}, {"text": "These are three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning."}, {"text": "These are three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning."}, {"text": "You can use an unsupervised learning algorithm (like clustering) to create your training data for the supervised learning algorithm but you cannot simply convert an unsupervised learning algorithm into a supervised one."}]}, {"question": "Why is it called analysis of variance", "positive_ctxs": [{"text": "Analysis of Variance (ANOVA) is a statistical method used to test differences between two or more means. It may seem odd that the technique is called \"Analysis of Variance\" rather than \"Analysis of Means.\" As you will see, the name is appropriate because inferences about means are made by analyzing variance."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Analysis of variance (ANOVA) is an analysis tool used in statistics that splits an observed aggregate variability found inside a data set into two parts: systematic factors and random factors.  1\ufeff\ufeff2\ufeff ANOVA is also called the Fisher analysis of variance, and it is the extension of the t- and z-tests."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data. Due to this value of denominator in the formula for variance in case of sample data is 'n-1', and it is 'n' for population data."}, {"text": "The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups."}, {"text": "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  Factor analysis is related to principal component analysis (PCA), but the two are not identical."}, {"text": "The term that does not apply to cluster analysis is factorization. Cluster analysis is a way of grouping data, based on obvious similarities. It is also called as classification analysis or numerical taxonomy.  Hierarchical cluster analysis tends to build a hierarchy within clusters."}, {"text": "Factorial analysis of variance (ANOVA) is a statistical procedure that allows researchers to explore the influence of two or more independent variables (factors) on a single dependent variable."}, {"text": "A One Way ANOVA is an analysis of variance in which there is only one independent variable.  One way is through Analyze/Compare Means/One-Way ANOVA and the other is through Analyze/General Linear Model/Univariate."}]}, {"question": "What is area under precision recall curve", "positive_ctxs": [{"text": "The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. The area under the normal distribution curve represents probability and the total area under the curve sums to one."}, {"text": "Definition. Average precision is a measure that combines recall and precision for ranked retrieval results. For one information need, the average precision is the mean of the precision scores after each relevant document is retrieved."}, {"text": "Geometrical meaning of integration is a statement so it must be true. Another way of analysing this statement is area of a curve or the volume of a curve of revolution or area of an implicit equation of x and y.  The geometrical meaning of integration is to find the area under the corresponding curve."}, {"text": "Density values can be greater than 1. In the frequency histogram the y-axis was percentage, but in the density curve the y-axis is density and the area gives the percentage. When creating the density curve the values on the y-axis are calculated (scaled) so that the total area under the curve is 1."}, {"text": "A continuous random variable is not defined at specific values.  1: The curve has no negative values (p(x) > 0 for all x) 2: The total area under the curve is equal to 1. A curve meeting these requirements is known as a density curve."}]}, {"question": "What is the input data for multi dimensional scaling", "positive_ctxs": [{"text": "The input to multidimensional scaling is a distance matrix. The output is typically a two-dimensional scatterplot, where each of the objects is represented as a point."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Dimensionality reduction refers to techniques for reducing the number of input variables in training data. When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the \u201cessence\u201d of the data."}, {"text": "A relatively new method of dimensionality reduction is the autoencoder. Autoencoders are a branch of neural network which attempt to compress the information of the input variables into a reduced dimensional space and then recreate the input data set.  This is where the information from the input has been compressed."}, {"text": "For multi class classification using SVM; It is NOT (one vs one) and NOT (one vs REST). Instead learn a two-class classifier where the feature vector is (x, y) where x is data and y is the correct label associated with the data."}, {"text": "The process of dividing each feature by its range is called feature scaling. The process feature scaling is used to standardize each variables individually. The term feature scaling when it comes to data processing is also known as data normalization."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "MinMax scaling will not affect the values of dummy variables but Standardised scaling will."}]}, {"question": "What's Noise How can noise be reduced in a dataset", "positive_ctxs": [{"text": "The term is often called as corrupt data.  We can't avoid the Noise data, but we can reduce it by using noise filters."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Gaussian Noise is a statistical noise having a probability density function equal to normal distribution, also known as Gaussian Distribution. Random Gaussian function is added to Image function to generate this noise. It is also called as electronic noise because it arises in amplifiers or detectors."}, {"text": "Adding Noise into Neural Network Neural networks are capable of learning output functions that can change wildly with small changes in input. Adding noise to inputs randomly is like telling the network to not change the output in a ball around your exact input."}, {"text": "How to Calculate a Confusion MatrixYou need a test dataset or a validation dataset with expected outcome values.Make a prediction for each row in your test dataset.From the expected outcomes and predictions count: The number of correct predictions for each class."}, {"text": "A dataset can be created in three different ways:  As a copy of an existing dataset in the database or on your local computer. As a child dataset from an existing global dataset in the database or on your local computer. The time period and the dataset name cannot be changed in this case."}, {"text": "How to Use K-means Cluster Algorithms in Predictive AnalysisPick k random items from the dataset and label them as cluster representatives.Associate each remaining item in the dataset with the nearest cluster representative, using a Euclidean distance calculated by a similarity function.Recalculate the new clusters' representatives.More items"}, {"text": "How to Use K-means Cluster Algorithms in Predictive AnalysisPick k random items from the dataset and label them as cluster representatives.Associate each remaining item in the dataset with the nearest cluster representative, using a Euclidean distance calculated by a similarity function.Recalculate the new clusters' representatives.More items"}, {"text": "Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). These new reduced set of features should then be able to summarize most of the information contained in the original set of features."}]}, {"question": "How many layers does CNN have", "positive_ctxs": [{"text": "We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture. Example Architecture: Overview."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A CNN LSTM can be defined by adding CNN layers on the front end followed by LSTM layers with a Dense layer on the output. It is helpful to think of this architecture as defining two sub-models: the CNN Model for feature extraction and the LSTM Model for interpreting the features across time steps."}, {"text": "Adding more training data.Reducing parameters. We have too many neurons in our hidden layers or too many layers. Let's remove some layers, or reduce the number of hidden neurons.Increase regularization. Either by increasing our. for L1/L2 weight regularization. We can also use dropout the technique."}, {"text": "Introduction[edit] Shift Invariance simply refers to the 'invariance' that a CNN has to recognising images. It allows the CNN to detect features/objects even if it does not look exactly like the images in it's training period. Shift invariance covers 'small' differences, such as movements shifts of a couple of pixels."}, {"text": "CNNs are trained to identify and extract the best features from the images for the problem at hand. That is their main strength. The latter layers of a CNN are fully connected because of their strength as a classifier."}, {"text": "Improve your model accuracy by Transfer Learning.Loading data using python libraries.Preprocess of data which includes reshaping, one-hot encoding and splitting.Constructing the model layers of CNN followed by model compiling, model training.Evaluating the model on test data.Finally, predicting the correct and incorrect labels."}, {"text": "The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers, and normalization layers. Here it simply means that instead of using the normal activation functions defined above, convolution and pooling functions are used as activation functions."}, {"text": "An SVM possesses a number of parameters that increase linearly with the linear increase in the size of the input. A NN, on the other hand, doesn't. Even though here we focused especially on single-layer networks, a neural network can have as many layers as we want."}]}, {"question": "What is a Perceptron in machine learning", "positive_ctxs": [{"text": "In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks. Perceptron is a linear classifier (binary). Also, it is used in supervised learning. It helps to classify the given input data."}, {"text": "A perceptron is a simple model of a biological neuron in an artificial neural network. Perceptron is also the name of an early algorithm for supervised learning of binary classifiers."}, {"text": "A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features or business intelligence in the input data. Perceptron was introduced by Frank Rosenblatt in 1957.  A Perceptron is an algorithm for supervised learning of binary classifiers."}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "Multilayer Perceptron (MLP) MLP is a deep learning method. A multilayer perceptron is a neural network connecting multiple layers in a directed graph, which means that the signal path through the nodes only goes one way. Each node, apart from the input nodes, has a nonlinear activation function."}]}, {"question": "How do you fix a vanishing gradient problem", "positive_ctxs": [{"text": "The simplest solution is to use other activation functions, such as ReLU, which doesn't cause a small derivative. Residual networks are another solution, as they provide residual connections straight to earlier layers."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "The ReLU activation solves the problem of vanishing gradient that is due to sigmoid-like non-linearities (the gradient vanishes because of the flat regions of the sigmoid). The other kind of \"vanishing\" gradient seems to be related to the depth of the network (e.g. see this for example)."}, {"text": "LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs."}, {"text": "For the vanishing gradient problem, the further you go through the network, the lower your gradient is and the harder it is to train the weights, which has a domino effect on all of the further weights throughout the network. That was the main roadblock to using Recurrent Neural Networks."}, {"text": "Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron."}]}, {"question": "What are the two requirements of the one sample t test", "positive_ctxs": [{"text": "The dependent variable must be continuous (interval/ratio). The observations are independent of one another. The dependent variable should be approximately normally distributed. The dependent variable should not contain any outliers."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To apply the linear regression t-test to sample data, we require the standard error of the slope, the slope of the regression line, the degrees of freedom, the t statistic test statistic, and the P-value of the test statistic.  Therefore, the P-value is 0.0121 + 0.0121 or 0.0242."}, {"text": "To apply the linear regression t-test to sample data, we require the standard error of the slope, the slope of the regression line, the degrees of freedom, the t statistic test statistic, and the P-value of the test statistic.  Therefore, the P-value is 0.0121 + 0.0121 or 0.0242. Interpret results."}, {"text": "Hypothesis Tests of the Mean and MedianParametric tests (means)Nonparametric tests (medians)1-sample t test1-sample Sign, 1-sample Wilcoxon2-sample t testMann-Whitney testOne-Way ANOVAKruskal-Wallis, Mood's median testFactorial DOE with one factor and one blocking variableFriedman test"}, {"text": "Hypothesis Tests of the Mean and MedianParametric tests (means)Nonparametric tests (medians)1-sample t test1-sample Sign, 1-sample Wilcoxon2-sample t testMann-Whitney testOne-Way ANOVAKruskal-Wallis, Mood's median testFactorial DOE with one factor and one blocking variableFriedman test"}, {"text": "The four requirements are: each observation falls into one of two categories called a success or failure. there is a fixed number of observations. the observations are all independent. the probability of success (p) for each observation is the same - equally likely."}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}]}, {"question": "What is the difference between arithmetic mean geometric mean and harmonic mean", "positive_ctxs": [{"text": "The arithmetic mean is appropriate if the values have the same units, whereas the geometric mean is appropriate if the values have differing units. The harmonic mean is appropriate if the data values are ratios of two variables with different measures, called rates."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are different types of mean, viz. arithmetic mean, weighted mean, geometric mean (GM) and harmonic mean (HM). If mentioned without an adjective (as mean), it generally refers to the arithmetic mean."}, {"text": "The geometric mean must be used when working with percentages, which are derived from values, while the standard arithmetic mean works with the values themselves. The harmonic mean is best used for fractions such as rates or multiples."}, {"text": "The harmonic mean is a type of numerical average. It is calculated by dividing the number of observations by the reciprocal of each number in the series. Thus, the harmonic mean is the reciprocal of the arithmetic mean of the reciprocals."}, {"text": "It is technically defined as \"the nth root product of n numbers.\" The geometric mean must be used when working with percentages, which are derived from values, while the standard arithmetic mean works with the values themselves. The harmonic mean is best used for fractions such as rates or multiples."}, {"text": "In mathematics, the inequality of arithmetic and geometric means, or more briefly the AM\u2013GM inequality, states that the arithmetic mean of a list of non-negative real numbers is greater than or equal to the geometric mean of the same list; and further, that the two means are equal if and only if every number in the"}, {"text": "The geometric mean differs from the arithmetic average, or arithmetic mean, in how it is calculated because it takes into account the compounding that occurs from period to period. Because of this, investors usually consider the geometric mean a more accurate measure of returns than the arithmetic mean."}, {"text": "The geometric mean differs from the arithmetic average, or arithmetic mean, in how it is calculated because it takes into account the compounding that occurs from period to period. Because of this, investors usually consider the geometric mean a more accurate measure of returns than the arithmetic mean."}]}, {"question": "What is layered targeting", "positive_ctxs": [{"text": "If your goal is to target those with buying intent, you can do so by layering an in-market audience on top of your custom affinity audience. This will signal to Google that you want to show ads to people inside the affinity audience you've created who also have shown intent to buy in their search history."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is chain rule in artificial intelligence", "positive_ctxs": [{"text": "The chain rule, or general product rule, calculates any component of the joint distribution of a set of random variables using only conditional probabilities. This probability theory is used as a foundation for backpropagation and in creating Bayesian networks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The squared hinge loss is differentiable because the term from the chain rule forces the limits to converge to the same number from both sides."}, {"text": "An artificial neural network's learning rule or learning process is a method, mathematical logic or algorithm which improves the network's performance and/or training time. Usually, this rule is applied repeatedly over the network."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm. #"}, {"text": "Why is the derivative of the LSTM cell state w.r.t. to the previous cell state equal to the forget gate?  The chain rule would extend for longer and we'd have more derivative terms in there."}]}, {"question": "What is validity and reliability in statistics", "positive_ctxs": [{"text": "Reliability refers to the extent that the instrument yields the same results over multiple trials. Validity refers to the extent that the instrument measures what it was designed to measure.  Construct validity uses statistical analyses, such as correlations, to verify the relevance of the questions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Having good test re-test reliability signifies the internal validity of a test and ensures that the measurements obtained in one sitting are both representative and stable over time."}, {"text": "Face validity is only considered to be a superficial measure of validity, unlike construct validity and content validity because is not really about what the measurement procedure actually measures, but what it appears to measure. This appearance is only superficial."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Split-half reliability is a statistical method used to measure the consistency of the scores of a test. It is a form of internal consistency reliability and had been commonly used before the coefficient \u03b1 was invented."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Because it arises from consistency between parts of a test, split-half reliability is an \u201cinternal consistency\u201d approach to estimating reliability. This result is an estimate of the reliability of the test scores, and it provides some support for the quality of the test scores."}, {"text": "Statistical conclusion validity is the degree to which conclusions about the relationship among variables based on the data are correct or \"reasonable\".  Statistical conclusion validity involves ensuring the use of adequate sampling procedures, appropriate statistical tests, and reliable measurement procedures."}]}, {"question": "What is LRT in statistics", "positive_ctxs": [{"text": "The likelihood ratio test (LRT) is a statistical test of the goodness-of-fit between two models. A relatively more complex model is compared to a simpler model to see if it fits a particular dataset significantly better. If so, the additional parameters of the more complex model are often used in subsequent analyses."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables."}]}, {"question": "How data engineering is different from data science", "positive_ctxs": [{"text": "The main difference is the one of focus. Data Engineers are focused on building infrastructure and architecture for data generation. In contrast, data scientists are focused on advanced mathematics and statistical analysis on that generated data.  Simply put, data scientists depend on data engineers."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Similarity is a numerical measure of how alike two data objects are, and dissimilarity is a numerical measure of how different two data objects are.  We go into more data mining in our data science bootcamp, have a look."}, {"text": "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself."}, {"text": "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself."}, {"text": "Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data."}, {"text": "Because data science is a broad term for multiple disciplines, machine learning fits within data science. Machine learning uses various techniques, such as regression and supervised clustering. On the other hand, the data' in data science may or may not evolve from a machine or a mechanical process."}, {"text": "Unsupervised or undirected data science uncovers hidden patterns in unlabeled data. In unsupervised data science, there are no output variables to predict. The objective of this class of data science techniques, is to find patterns in data based on the relationship between data points themselves."}, {"text": "Machine learning, on the other hand, refers to a group of techniques used by data scientists that allow computers to learn from data. These techniques produce results that perform well without programming explicit rules.  Although data science includes machine learning, it is a vast field with many different tools."}]}, {"question": "Can random forest be used for time series", "positive_ctxs": [{"text": "Therefore, a Random Forest model does not scale very well for time-series data and might need to be constantly updated in Production or trained with some Random Data that lies outside our range of Training set."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Generally, we use linear regression for time series analysis, it is used for predicting the result for time series as its trends. For example, If we have a dataset of time series with the help of linear regression we can predict the sales with the time."}, {"text": "Some common types of problems built on top of classification and regression include recommendation and time series prediction respectively. Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems. Random forest for classification and regression problems."}, {"text": "A time series is a stochastic process that operates in continuous state space and discrete time set. A stochastic process is nothing but a set of random variables. It is a time dependent random phenomenon. Same is time series."}, {"text": "Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components. Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting."}, {"text": "D refers to the number of differencing transformations required by the time series to get stationary.  Differencing is a method of transforming a non-stationary time series into a stationary one. This is an important step in preparing data to be used in an ARIMA model."}, {"text": "A stationary time series is one whose properties do not depend on the time at which the series is observed. 14. Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times."}, {"text": "Time series forecasting is an important area of machine learning that is often neglected. It is important because there are so many prediction problems that involve a time component.  Standard definitions of time series, time series analysis, and time series forecasting."}]}, {"question": "What is an F vector space", "positive_ctxs": [{"text": "The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector space or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A vector is an element of a vector space. Assuming you're talking about an abstract vector space, which has an addition and scalar multiplication satisfying a number of properties, then a vector space is what we call a set which satisfies those properties."}, {"text": "A vector space is a space of vectors, ie. each element is a vector. A vector field is, at its core, a function between some space and some vector space, so every point in our base space has a vector assigned to it. A good example would be wind direction maps you see on weather reports."}, {"text": "A matrix is a linear operator acting on the vector space of column vectors. Per linear algebra and its isomorphism theorems, any vector space is isomorphic to any other vector space of the same dimension. As such, matrices can be seen as representations of linear operators subject to some basis of column vectors."}, {"text": "F statistic is a statistic that is determined by an ANOVA test. It determines the significance of the groups of variables. The F critical value is also known as the F \u2013statistic. The F \u2013 statistic value is obtained from the F-distribution table."}, {"text": "Linear operators, matrices, change of coordinates: a brief HOWTO. A function A: V \u2192 W from one vector space V (source space) to another vector. space W (target space) with the same set of scalars is called a linear operator (a linear. transformation), if for all vectors v,v1,v2 \u2208 V and for any c \u2208 F we have. 1."}, {"text": "The test statistic used in ANOVA is Student's t. One characteristic of the F distribution is that F cannot be negative. One characteristic of the F distribution is that the computed F can only range between -1 and +1."}, {"text": "There are two sets of degrees of freedom; one for the numerator and one for the denominator. For example, if F follows an F distribution and the number of degrees of freedom for the numerator is four, and the number of degrees of freedom for the denominator is ten, then F ~ F 4,10."}]}, {"question": "What is the example of uniform distribution", "positive_ctxs": [{"text": "A deck of cards has within it uniform distributions because the likelihood of drawing a heart, a club, a diamond or a spade is equally likely. A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, uniform distribution is a probability distribution where all outcomes are equally likely. Discrete uniform distributions have a finite number of outcomes. A continuous uniform distribution is a statistical distribution with an infinite number of equally likely measurable values."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The Dirichlet is the multivariate generalization of the beta distribution.  The Dirichlet equals the uniform distribution when all parameters (\u03b11\u2026 \u03b1k) are equal. The Dirichlet distribution is a conjugate prior to the categorical distribution and multinomial distributions. A compound variant is the Dirichlet-multinomial."}, {"text": "Mean of General discrete uniform distribution The expected value of discrete uniform random variable is E ( X ) = a + b 2 ."}, {"text": "It is called Laplace smoothing because the smoothing proceeds from a logic of slightly correcting the observed proportions (in the case of categorical variables) in the direction of a uniform distribution among the categories (i.e., injecting a bit of equi-probability among them)."}, {"text": "In statistics, a type of probability distribution in which all outcomes are equally likely.  A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same."}, {"text": "In statistics, a type of probability distribution in which all outcomes are equally likely.  A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same."}]}, {"question": "Are A and B independent events", "positive_ctxs": [{"text": "Events A and B are independent if: knowing whether A occured does not change the probability of B. Mathematically, can say in two equivalent ways: P(B|A) = P(B) P(A and B)  Important to distinguish independence from mutually exclusive which would say B \u2229 A is empty (cannot happen)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In the case where events A and B are independent (where event A has no effect on the probability of event B), the conditional probability of event B given event A is simply the probability of event B, that is P(B). P(A and B) = P(A)P(B|A)."}, {"text": "This probability is written P(B|A), notation for the probability of B given A. In the case where events A and B are independent (where event A has no effect on the probability of event B), the conditional probability of event B given event A is simply the probability of event B, that is P(B). P(A and B) = P(A)P(B|A)."}, {"text": "If A and B are two events in a sample space S, then the conditional probability of A given B is defined as P(A|B)=P(A\u2229B)P(B), when P(B)>0."}, {"text": "Definition 1. Suppose that events A and B are defined on the same probability space, and the event B is such that P(B) > 0. The conditional probability of A given that B has occurred is given by P(A|B) = P(A \u2229 B)/P(B)."}, {"text": "Events A and B are independent if: knowing whether A occured does not change the probability of B. Mathematically, can say in tw. Page 1. Events A and B are independent if: knowing whether A occured does not change the probability of B."}, {"text": "Events A and B are independent if the equation P(A\u2229B) = P(A) \u00b7 P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together."}, {"text": "Events A and B are independent if the equation P(A\u2229B) = P(A) \u00b7 P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together."}]}, {"question": "How do you find the uncertainty of a measurement", "positive_ctxs": [{"text": "To find the average, add them together and divide by the number of values (10 in this case). When repeated measurements give different results, we want to know how widely spread the readings are. The spread of values tells us something about the uncertainty of a measurement."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Scientific uncertainty generally means that there is a range of possible values within which the true value of the measurement lies. Further research on a topic or theory may reduce the level of uncertainty or the range of possible values."}, {"text": "Accuracy of a measured value refers to how close a measurement is to the correct value. The uncertainty in a measurement is an estimate of the amount by which the measurement result may differ from this value. Precision of measured values refers to how close the agreement is between repeated measurements."}, {"text": "The Kalman filter produces an estimate of the state of the system as an average of the system's predicted state and of the new measurement using a weighted average. The purpose of the weights is that values with better (i.e., smaller) estimated uncertainty are \"trusted\" more."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "At the foundation of quantum mechanics is the Heisenberg uncertainty principle. Simply put, the principle states that there is a fundamental limit to what one can know about a quantum system.  Heisenberg sometimes explained the uncertainty principle as a problem of making measurements."}]}, {"question": "Is a random walk a Markov chain", "positive_ctxs": [{"text": "A random walk on a graph is a very special case of a Markov chain. Unlike a general Markov chain, random walk on a graph enjoys a property called time symmetry or reversibility."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Among the trademarks of the Bayesian approach, Markov chain Monte Carlo methods are especially mysterious.  So, what are Markov chain Monte Carlo (MCMC) methods? The short answer is: MCMC methods are used to approximate the posterior distribution of a parameter of interest by random sampling in a probabilistic space."}, {"text": "In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain."}, {"text": "In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain."}, {"text": "Markov Chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its stationary distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution."}, {"text": "Statistics Definitions > A random walk is a sequence of discrete, fixed-length steps in random directions. Random walks may be 1-dimensional, 2-dimensional, or n-dimensional for any n. A random walk can also be confined to a lattice."}, {"text": "Statistics Definitions > A random walk is a sequence of discrete, fixed-length steps in random directions. Random walks may be 1-dimensional, 2-dimensional, or n-dimensional for any n. A random walk can also be confined to a lattice."}, {"text": "Markovian is an adjective that may describe: In probability theory and statistics, subjects named for Andrey Markov: A Markov chain or Markov process, a stochastic model describing a sequence of possible events. The Markov property, the memoryless property of a stochastic process."}]}, {"question": "What are the application of binomial distribution", "positive_ctxs": [{"text": "The binomial distribution model allows us to compute the probability of observing a specified number of \"successes\" when the process is repeated a specific number of times (e.g., in a set of patients) and the outcome for a given patient is either a success or a failure."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "Remember that the Poisson distribution assumes that the mean and variance are the same.  The negative binomial distribution has one parameter more than the Poisson regression that adjusts the variance independently from the mean. In fact, the Poisson distribution is a special case of the negative binomial distribution."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution.  Suppose we flip a coin repeatedly and count the number of heads (successes)."}, {"text": "The probability mass function of the negative binomial distribution is. where r is the number of successes, k is the number of failures, and p is the probability of success."}, {"text": "The Poisson distribution is a limiting case of the binomial distribution which arises when the number of trials n increases indefinitely whilst the product \u03bc = np, which is the expected value of the number of successes from the trials, remains constant."}, {"text": "The geometric distribution describes the probability of \"x trials are made before a success\", and the negative binomial distribution describes that of \"x trials are made before r successes are obtained\", where r is fixed. So you see that the latter is a particular case of the former, namely, when r=1."}, {"text": "A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix \u201cbi\u201d means two, or twice)."}]}, {"question": "What does hot encoding mean", "positive_ctxs": [{"text": "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multi hot encoding is one of such popular encoding technique in order to successfully convert categorical variables into numerical variables.  Now, both independent variables and dependent variable became encoded and converted to numerical values from categorical values."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "A one hot encoding is a representation of categorical variables as binary vectors.  Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1."}, {"text": "A one hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "There are two different ways to encoding categorical variables. One-hot encoding converts it into n variables, while dummy encoding converts it into n-1 variables.  If we have k categorical variables, each of which has n values."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}]}, {"question": "Which type of data is often Modelled using regression trees", "positive_ctxs": [{"text": "Regression trees are used in Statistics, Data Mining and Machine learning. It is a very important and powerful technique when it comes to predictive analysis [5] . The goal is to predict the value of target variable on the basis of several input attributes that act as nodes of the regression tree."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In the mathematical field of numerical analysis, interpolation is a type of estimation, a method of constructing new data points within the range of a discrete set of known data points.  It is often required to interpolate, i.e., estimate the value of that function for an intermediate value of the independent variable."}, {"text": "A box plot (also known as box and whisker plot) is a type of chart often used in explanatory data analysis to visually show the distribution of numerical data and skewness through displaying the data quartiles (or percentiles) and averages."}, {"text": "A box plot (also known as box and whisker plot) is a type of chart often used in explanatory data analysis to visually show the distribution of numerical data and skewness through displaying the data quartiles (or percentiles) and averages."}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "Undercoverage bias often occurs as a result of convenience sampling. To eliminate (or at least minimize) the effects of undercoverage bias, a better form of sampling is using a simple random sample. In this type of sample, every member of a population has an equal chance of being selected to be in the sample."}, {"text": "Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters)."}]}, {"question": "What are the units of a probability density function", "positive_ctxs": [{"text": "Technically, the probability density of variable X , means the probability per unit increment of X . The units of probability density are the reciprocal of the units of X \u2014 if the units of X are dollars, the units of probability density are probability per dollar increment."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable."}, {"text": "A probability density plot simply means a density plot of probability density function (Y-axis) vs data points of a variable (X-axis).  By showing probability density plots, we're only able to understand the distribution of data visually without knowing the exact probability for a certain range of values."}, {"text": "The Relationship Between a CDF and a PDF In technical terms, a probability density function (pdf) is the derivative of a cumulative density function (cdf). Futhermore, the area under the curve of a pdf between negative infinity and x is equal to the value of x on the cdf."}, {"text": "In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the"}, {"text": "In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the"}, {"text": "In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the"}, {"text": "The probability density function (PDF) is defined for probability distributions of continuous random variables. The probability at a certain point of a continuous variable is zero. The cumulative distribution function (CDF) is a non-decreasing function as the probabilities can never be less than 0."}]}, {"question": "What's a word for learning", "positive_ctxs": [{"text": "What is another word for learning?educationknowledgeexperienceintelligenceeruditionexpertiseintellectwisdomabilityaccomplishment227 more rows"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Machine learning algorithms can minimize forecasting error and do the forecast much faster and with the usage of more data. What's more, machine learning algorithms can analyze many alternative models at the same time, when in traditional econometrics you can analyze just one model at a time."}, {"text": "Text classification using word embeddings and deep learning in python \u2014 classifying tweets from twitterSplit the data into text (X) and labels (Y)Preprocess X.Create a word embedding matrix from X.Create a tensor input from X.Train a deep learning model using the tensor inputs and labels (Y)More items\u2022"}, {"text": "The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word. In word2vec, a distributed representation of a word is used."}, {"text": "In natural language processing, word sense disambiguation (WSD) is the problem of determining which \"sense\" (meaning) of a word is activated by the use of the word in a particular context, a process which appears to be largely unconscious in people."}, {"text": "POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition. This task is not straightforward, as a particular word may have a different part of speech based on the context in which the word is used."}, {"text": "Implementing Deep Learning Methods and Feature Engineering for Text Data: FastText. Overall, FastText is a framework for learning word representations and also performing robust, fast and accurate text classification. The framework is open-sourced by Facebook on GitHub."}, {"text": "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space."}]}, {"question": "What is the objective of Perceptron learning", "positive_ctxs": [{"text": "The objective is to reduce the error e, which is the difference between the neuron response a, and the target vector t. The perceptron learning rule learnp calculates desired changes to the perceptron's weights and biases given an input vector p, and the associated error e."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A perceptron is a simple model of a biological neuron in an artificial neural network. Perceptron is also the name of an early algorithm for supervised learning of binary classifiers."}, {"text": "A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features or business intelligence in the input data. Perceptron was introduced by Frank Rosenblatt in 1957.  A Perceptron is an algorithm for supervised learning of binary classifiers."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Train Loss is the value of the objective function that you are minimizing. This value could be a positive or negative number, depending on the specific objective function of your training data. The training loss is calculated over the entire training dataset."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Perceptron Learning Rule The Perceptron receives multiple input signals, and if the sum of the input signals exceeds a certain threshold, it either outputs a signal or does not return an output. In the context of supervised learning and classification, this can then be used to predict the class of a sample."}, {"text": "Perceptron Learning Rule The Perceptron receives multiple input signals, and if the sum of the input signals exceeds a certain threshold, it either outputs a signal or does not return an output. In the context of supervised learning and classification, this can then be used to predict the class of a sample."}]}, {"question": "Why use robust standard errors Stata", "positive_ctxs": [{"text": "Robust standard errors address the problem of errors that are not independent and identically distributed. The use of robust standard errors will not change the coefficient estimates provided by OLS, but they will change the standard errors and significance tests."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "We find the robust standard deviation estimate by multiplying the MAD by a factor that happens to have a value close to 1.5. This gives us a robust value ('sigma- hat') of B . . If we use this method on data without outliers, it provides estimates that are close to x and s, so no harm is done."}, {"text": "The median is a robust measure of central tendency.  The median absolute deviation and interquartile range are robust measures of statistical dispersion, while the standard deviation and range are not. Trimmed estimators and Winsorised estimators are general methods to make statistics more robust."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "robust is a programmer's command that computes a robust variance estimator based on a varlist of equation-level scores and a covariance matrix."}, {"text": "robust is a programmer's command that computes a robust variance estimator based on a varlist of equation-level scores and a covariance matrix.  The robust variance estimator goes by many names: Huber/White/sandwich are typically used in the context of robustness against heteroskedasticity."}, {"text": "One reason this is done is because the normal distribution often describes the actual distribution of the random errors in real-world processes reasonably well.  Some methods, like maximum likelihood, use the distribution of the random errors directly to obtain parameter estimates."}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}]}, {"question": "How do you perform convolution", "positive_ctxs": [{"text": "0:255:16Suggested clip \u00b7 118 secondsConvolution of Two Functions - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "In purely mathematical terms, convolution is a function derived from two given functions by integration which expresses how the shape of one is modified by the other.  Those of you who have practiced any field that entails signal processing are probably familiar with the convolution function."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "The larger the RAM the higher the amount of data it can handle hence faster processing. With larger RAM you can use your machine to perform other tasks as the model trains. Although a minimum of 8GB RAM can do the job, 16GB RAM and above is recommended for most deep learning tasks."}]}, {"question": "What is the meaning of active learning", "positive_ctxs": [{"text": "Active learning is generally defined as any instructional method that engages students in the learning process. In short, active learning requires students to do meaningful learning activities and think about what they are doing.  The students work individually on assignments, and cooperation is limited."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Connectionism theory is based on the principle of active learning and is the result of the work of the American psychologist Edward Thorndike. This work led to Thorndike's Laws. According to these Laws, learning is achieved when an individual is able to form associations between a particular stimulus and a response."}, {"text": "DEFINITION 1. Given a set of active nodes and an ordering on active nodes, amorphous data-parallelism is the parallelism that arises from simultaneously processing active nodes, subject to neighborhood and ordering constraints."}, {"text": "The learning algorithm of the Hopfield network is unsupervised, meaning that there is no \u201cteacher\u201d telling the network what is the correct output for a certain input."}, {"text": "Other examples of active learning techniques include role-playing, case studies, group projects, think-pair-share, peer teaching, debates, Just-in-Time Teaching, and short demonstrations followed by class discussion. There are two easy ways to promote active learning through the discussion."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Geometrical meaning of integration is a statement so it must be true. Another way of analysing this statement is area of a curve or the volume of a curve of revolution or area of an implicit equation of x and y.  The geometrical meaning of integration is to find the area under the corresponding curve."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is the false discovery rate FDR", "positive_ctxs": [{"text": "The false discovery rate (FDR) is a statistical approach used in multiple hypothesis testing to correct for multiple comparisons. It is typically used in high-throughput experiments in order to correct for random events that falsely appear significant."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The false discovery rate (FDR) is a statistical approach used in multiple hypothesis testing to correct for multiple comparisons.  The FDR is defined as the expected proportion of false discoveries, i.e., incorrectly rejected null hypothesis, among all discoveries (Benjamini and Hochberg 1995)."}, {"text": "The false discovery rate (FDR) is a method of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons.  Thus, FDR-controlling procedures have greater power, at the cost of increased numbers of Type I errors."}, {"text": "The false discovery rate is the ratio of the number of false positive results to the number of total positive test results. Out of 10,000 people given the test, there are 450 true positive results (box at top right) and 190 false positive results (box at bottom right) for a total of 640 positive results."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "A q-value threshold of 0.05 yields a FDR of 5% among all features called significant. The q-value is the expected proportion of false positives among all features as or more extreme than the observed one."}]}, {"question": "Why IIR filter and its realization is called recursive system", "positive_ctxs": [{"text": "Owing to the dependence of an IIR filter's result upon its previous results, an IIR filter is necessarily recursive. However, certain recursive filters have finite impulse response, so a recursive filter does not necessarily have infinite impulse response."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Any LTI filter with output and input can be represented by a difference equation in the form: If at least one of the is not null, the filter is recursive. If the are all zero, it is a non recursive filter usually called FIR (Finite Input Response) filter.  This happens both to recursive and non recursive filters."}, {"text": "Two classes of digital filters are Finite Impulse Response (FIR) and Infinite Impulse Response (IIR). The term 'Impulse Response' refers to the appearance of the filter in the time domain.  The mathematical difference between the IIR and FIR implementation is that the IIR filter uses some of the filter output as input."}, {"text": "The infinite impulse response (IIR) filter is a recursive filter in that the output from the filter is computed by using the current and previous inputs and previous outputs. Because the filter uses previous values of the output, there is feedback of the output in the filter structure."}, {"text": "Recursive and Nonrecursive Discrete-Time Systems This is a recursive system which means the output at time n depends on any number of a past output values. So, a recursive system has feed back output of the system into the input."}, {"text": "An adaptive filter is a system with a linear filter that has a transfer function controlled by variable parameters and a means to adjust those parameters according to an optimization algorithm.  The closed loop adaptive filter uses feedback in the form of an error signal to refine its transfer function."}, {"text": "Since this impulse response in infinitely long, recursive filters are often called infinite impulse response (IIR) filters. In effect, recursive filters convolve the input signal with a very long filter kernel, although only a few coefficients are involved."}, {"text": "Since this impulse response in infinitely long, recursive filters are often called infinite impulse response (IIR) filters. In effect, recursive filters convolve the input signal with a very long filter kernel, although only a few coefficients are involved."}]}, {"question": "What does bootstrapping mean in statistics", "positive_ctxs": [{"text": "Bootstrapping is any test or metric that uses random sampling with replacement, and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "Misleading statistics are simply the misusage - purposeful or not - of a numerical data. The results provide a misleading information to the receiver, who then believes something wrong if he or she does not notice the error or the does not have the full data picture."}]}, {"question": "What is endogenous variable", "positive_ctxs": [{"text": "An endogenous variable is a variable in a statistical model that's changed or determined by its relationship with other variables within the model. In other words, an endogenous variable is synonymous with a dependent variable, meaning it correlates with other factors within the system being studied."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An endogenous variable is a variable in a statistical model that's changed or determined by its relationship with other variables within the model. In other words, an endogenous variable is synonymous with a dependent variable, meaning it correlates with other factors within the system being studied."}, {"text": "According to Daniel Little, University of Michigan-Dearborn, an endogenous variable is defined in the following way: A variable xj is said to be endogenous within the causal model M if its value is determined or influenced by one or more of the independent variables X (excluding itself)."}, {"text": "A variable xj is said to be endogenous within the causal model M if its value is determined or influenced by one or more of the independent variables X (excluding itself). A purely endogenous variable is a factor that is entirely determined by the states of other variables in the system."}, {"text": "In an economic model, an exogenous variable is one whose value is determined outside the model and is imposed on the model, and an exogenous change is a change in an exogenous variable. In contrast, an endogenous variable is a variable whose value is determined by the model."}, {"text": "The definition of an endogenous variable, exogenous variable and parameter are as follows: An Endogenous Variable- is a variable whose value is determined within the model itself.  An Exogenous Variable \u2013 is a variable whose value is assumed to be determined outside the model."}, {"text": "An endogenous variable is a variable in a statistical model that's changed or determined by its relationship with other variables within the model.  Endogenous variables are the opposite of exogenous variables, which are independent variables or outside forces."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}]}, {"question": "What is the difference between random forest and bagging", "positive_ctxs": [{"text": "\" The fundamental difference between bagging and random forest is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node.\" Does"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Bootstrap aggregating (bagging) In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy."}, {"text": "The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree."}, {"text": "Random forest is a supervised learning algorithm. The \"forest\" it builds, is an ensemble of decision trees, usually trained with the \u201cbagging\u201d method. The general idea of the bagging method is that a combination of learning models increases the overall result."}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "Random forest improves on bagging because it decorrelates the trees with the introduction of splitting on a random subset of features. This means that at each split of the tree, the model considers only a small subset of features rather than all of the features of the model."}]}, {"question": "What is meant by Gaussian distribution", "positive_ctxs": [{"text": "Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The probability distribution for a random error that is as likely to move the value in either direction is called a Gaussian distribution. Such a distribution is characterized by two parameters, \u00b5 the mean or average value, and \u03c3 the standard deviation."}, {"text": "The Gaussian Processes Classifier is a classification machine learning algorithm. Gaussian Processes are a generalization of the Gaussian probability distribution and can be used as the basis for sophisticated non-parametric machine learning algorithms for classification and regression."}, {"text": "The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss.  It is often called the bell curve, because the graph of its probability density looks like a bell. Many values follow a normal distribution."}, {"text": "The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss.  It is often called the bell curve, because the graph of its probability density looks like a bell. Many values follow a normal distribution."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions."}, {"text": "There is a direct relationship between the coefficients produced by logit and the odds ratios produced by logistic. First, let's define what is meant by a logit: A logit is defined as the log base e (log) of the odds. : [1] logit(p) = log(odds) = log(p/q) The range is negative infinity to positive infinity."}]}, {"question": "Is keras a part of TensorFlow", "positive_ctxs": [{"text": "Keras is a high-level interface and uses Theano or Tensorflow for its backend. It runs smoothly on both CPU and GPU. Keras supports almost all the models of a neural network \u2013 fully connected, convolutional, pooling, recurrent, embedding, etc. Furthermore, these models can be combined to build more complex models."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "TensorFlow 2.0 is an updated version of TensorFlow that has been designed with a focus on simple execution, ease of use, and developer's productivity. TensorFlow 2.0 makes the development of machine learning applications even easier."}, {"text": "Train and serve a TensorFlow model with TensorFlow ServingTable of contents.Create your model. Import the Fashion MNIST dataset. Train and evaluate your model.Save your model.Examine your saved model.Serve your model with TensorFlow Serving. Add TensorFlow Serving distribution URI as a package source:  Make a request to your model in TensorFlow Serving. Make REST requests."}, {"text": "Train and serve a TensorFlow model with TensorFlow ServingTable of contents.Create your model. Import the Fashion MNIST dataset. Train and evaluate your model.Save your model.Examine your saved model.Serve your model with TensorFlow Serving. Add TensorFlow Serving distribution URI as a package source:  Make a request to your model in TensorFlow Serving. Make REST requests."}, {"text": "A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes. When writing a TensorFlow program, the main object you manipulate and pass around is the tf$Tensor ."}, {"text": "Content validity: Is the test fully representative of what it aims to measure?  Criterion validity: Do the results correspond to a different test of the same thing?"}, {"text": "keras is tightly integrated into the TensorFlow ecosystem, and also includes support for: tf. data, enabling you to build high performance input pipelines. If you prefer, you can train your models using data in NumPy format, or use tf."}, {"text": "Convolution is a general purpose filter effect for images. \u25a1 Is a matrix applied to an image and a mathematical operation. comprised of integers. \u25a1 It works by determining the value of a central pixel by adding the. weighted values of all its neighbors together."}]}, {"question": "What are the effective methods of dimension reduction", "positive_ctxs": [{"text": "Here is a brief review of our original seven techniques for dimensionality reduction:Missing Values Ratio.  Low Variance Filter.  High Correlation Filter.  Random Forests/Ensemble Trees.  Principal Component Analysis (PCA).  Backward Feature Elimination.  Forward Feature Construction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Kmeans clustering algorithm is applied to reduced datasets which is done by principal component analysis dimension reduction method. Cluster analysis is one of the major data analysis methods widely used for many practical applications in emerging areas[12]."}, {"text": "Abstract. The goal of statistical pattern feature extraction (SPFE) is 'low loss dimension reduction'. As the key link of pattern recognition, dimension reduction has become the research hot spot and difficulty in the fields of pattern recognition, machine learning, data mining and so on."}, {"text": "For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality."}, {"text": "Image compression with principal component analysis is a frequently occurring application of the dimension reduction technique.  As the number of principal components used to project the new data increases, the quality and representation compared to the original image improve."}, {"text": "Abstract: The k-Nearest Neighbors (kNN) classifier is one of the most effective methods in supervised learning problems. It classifies unseen cases comparing their similarity with the training data.  Fuzzy-kNN computes a fuzzy degree of membership of each instance to the classes of the problem."}, {"text": "The dimension of a data set is the number of columns. The rows are the number of samples, usually."}, {"text": "Feature extraction is a general term for methods of constructing combinations of the variables to get around these problems while still describing the data with sufficient accuracy. Many machine learning practitioners believe that properly optimized feature extraction is the key to effective model construction."}]}, {"question": "What is forward propagation in machine learning", "positive_ctxs": [{"text": "Well, if you break down the words, forward implies moving ahead and propagation is a term for saying spreading of anything. forward propagation means we are moving in only one direction, from input to the output, in a neural network."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What are the five steps in the backpropagation learning algorithm?Initialize weights with random values and set other parameters.Read in the input vector and the desired output.Compute the actual output via the calculations, working forward through the layers."}, {"text": "The three different ways of feature extraction are horizontal direction, vertical direction and diagonal direction. Recognition rate percentage for vertical, horizontal and diagonal based feature extraction using feed forward back propagation neural network as classification phase are 92.69, 93.68, 97.80 respectively."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Backpropagation AlgorithmSet a(1) = X; for the training examples.Perform forward propagation and compute a(l) for the other layers (l = 2\u2026  Use y and compute the delta value for the last layer \u03b4(L) = h(x) \u2014 y.Compute the \u03b4(l) values backwards for each layer (described in \u201cMath behind Backpropagation\u201d section)More items\u2022"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}]}, {"question": "What are the two layers of a restricted Boltzmann machine called in deep learning", "positive_ctxs": [{"text": "Restricted Boltzmann Machines are shallow, two-layer neural nets that constitute the building blocks of deep-belief networks. The first layer of the RBM is called the visible, or input layer, and the second is the hidden layer. Each circle represents a neuron-like unit called a node."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  Restricted Boltzmann machines can also be used in deep learning networks."}, {"text": "A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  Restricted Boltzmann machines can also be used in deep learning networks."}, {"text": "A deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables. It is a network of symmetrically coupled stochastic binary units. It comprises a set of visible units and layers of hidden units ."}, {"text": "In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (\"hidden units\"), with connections between the layers but not between units within each layer."}, {"text": "In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (\"hidden units\"), with connections between the layers but not between units within each layer."}, {"text": "At a very basic level, deep learning is a machine learning technique. It teaches a computer to filter inputs through layers to learn how to predict and classify information. Observations can be in the form of images, text, or sound. The inspiration for deep learning is the way that the human brain filters information."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}]}, {"question": "What is prior probability in statistics", "positive_ctxs": [{"text": "Prior probability, in Bayesian statistical inference, is the probability of an event before new data is collected. This is the best rational assessment of the probability of an outcome based on the current knowledge before an experiment is performed."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "It is closely related to prior probability, which is the probability an event will happen before you taken any new evidence into account. You can think of posterior probability as an adjustment on prior probability: Posterior probability = prior probability + new evidence (called likelihood)."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information. The posterior probability is calculated by updating the prior probability using Bayes' theorem."}, {"text": "A posterior probability value is a prior probability value that has been a | Course Hero. Study Resources. by Textbook. by Literature Title."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Bayesian decision making is the process in which a decision is made based on the probability of a successful outcome, where this probability is informed by both prior information and new evidence that the decision maker obtains."}, {"text": "Bayesian decision making is the process in which a decision is made based on the probability of a successful outcome, where this probability is informed by both prior information and new evidence that the decision maker obtains."}, {"text": "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis."}]}, {"question": "What part of AI is not machine learning", "positive_ctxs": [{"text": "\u201cExpert systems\u201d basically set a number of \u201cif this, then do that\u201d statements. It does not learn by itself (so it is not machine learning), and it still can be very useful for use cases like medical diagnosis and treatment."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Artificial intelligence (AI) is a branch of computer science.  Most AI programs are not used to control robots. Even when AI is used to control robots, the AI algorithms are only part of the larger robotic system, which also includes sensors, actuators, and non-AI programming."}, {"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "Natural Language Processing (NLP) is the part of AI that studies how machines interact with human language.  Combined with machine learning algorithms, NLP creates systems that learn to perform tasks on their own and get better through experience."}, {"text": "Artificial intelligence is a technology which enables a machine to simulate human behavior. Machine learning is a subset of AI which allows a machine to automatically learn from past data without programming explicitly. The goal of AI is to make a smart computer system like humans to solve complex problems."}, {"text": "AdaBoost. AdaBoost is an ensemble machine learning algorithm for classification problems. It is part of a group of ensemble methods called boosting, that add new machine learning models in a series where subsequent models attempt to fix the prediction errors made by prior models."}, {"text": "Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised."}, {"text": "AI is a bigger concept to create intelligent machines that can simulate human thinking capability and behavior, whereas, machine learning is an application or subset of AI that allows machines to learn from data without being programmed explicitly."}]}, {"question": "What is a false positive in research", "positive_ctxs": [{"text": "A false positive, also known as Type I error or alpha error, is an error that occurs when a researcher falsely concludes that an effect exists, or when a null hypothesis is rejected even though the null is true.  On the basis of these data, the researcher concludes that there is an effect."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "All medical tests can be resulted in false positive and false negative errors.  A false positive can lead to unnecessary treatment and a false negative can lead to a false diagnostic, which is very serious since a disease has been ignored."}, {"text": "Since medical tests can't be absolutely true, false positive and false negative are two problems we have to deal with. A false positive can lead to unnecessary treatment and a false negative can lead to a false diagnostic, which is very serious since a disease has been ignored."}, {"text": "Since medical tests can't be absolutely true, false positive and false negative are two problems we have to deal with. A false positive can lead to unnecessary treatment and a false negative can lead to a false diagnostic, which is very serious since a disease has been ignored."}, {"text": "A false positive state is when the IDS identifies an activity as an attack but the activity is acceptable behavior. A false positive is a false alarm.  This is when the IDS identifies an activity as acceptable when the activity is actually an attack. That is, a false negative is when the IDS fails to catch an attack."}, {"text": "A Type I is a false positive where a true null hypothesis that there is nothing going on is rejected. A Type II error is a false negative, where a false null hypothesis is not rejected \u2013 something is going on \u2013 but we decide to ignore it."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}]}, {"question": "How do you collect time series data", "positive_ctxs": [{"text": "Time series data is data that is collected at different points in time. This is opposed to cross-sectional data which observes individuals, companies, etc. at a single point in time. Because data points in time series are collected at adjacent time periods there is potential for correlation between observations."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A stationary time series is one whose properties do not depend on the time at which the series is observed. 14. Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times."}, {"text": "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data."}, {"text": "Time series analysis is a statistical technique that deals with time series data, or trend analysis. Time series data means that data is in a series of particular time periods or intervals.  Time series data: A set of observations on the values that a variable takes at different times."}, {"text": "Stationarity. A common assumption in many time series techniques is that the data are stationary. A stationary process has the property that the mean, variance and autocorrelation structure do not change over time."}, {"text": "Stationary Time Series Time series are stationary if they do not have trend or seasonal effects. Summary statistics calculated on the time series are consistent over time, like the mean or the variance of the observations."}, {"text": "A time series is a sequence of numerical data points in successive order. In investing, a time series tracks the movement of the chosen data points, such as a security's price, over a specified period of time with data points recorded at regular intervals."}, {"text": "The key difference between time series and panel data is that time series focuses on a single individual at multiple time intervals while panel data (or longitudinal data) focuses on multiple individuals at multiple time intervals.  Fields such as Econometrics and statistics relies on data."}]}, {"question": "What is N grams Python", "positive_ctxs": [{"text": "N-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.  This post describes several different ways to generate n-grams quickly from input sentences in Python."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Top N accuracy \u2014 Top N accuracy is when you measure how often your predicted class falls in the top N values of your softmax distribution."}, {"text": "A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "For the coin flip example, N = 2 and \u03c0 = 0.5. The formula for the binomial distribution is shown below: where P(x) is the probability of x successes out of N trials, N is the number of trials, and \u03c0 is the probability of success on a given trial.Number of HeadsProbability21/42 more rows"}, {"text": "Python is easy to learn and work with, and provides convenient ways to express how high-level abstractions can be coupled together. Nodes and tensors in TensorFlow are Python objects, and TensorFlow applications are themselves Python applications. The actual math operations, however, are not performed in Python."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model.  The rows represent the predicted values of the target variable."}]}, {"question": "What is the role of probability to statistic", "positive_ctxs": [{"text": "Probability Role of probability in statistics:  Use probability to predict results of experiment under assumptions. Compute probability of error larger than given amount. Compute probability of given departure between prediction and results under assumption."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "The Wald Chi-Square test statistic is the squared ratio of the Estimate to the Standard Error of the respective predictor. The probability that a particular Wald Chi-Square test statistic is as extreme as, or more so, than what has been observed under the null hypothesis is given by Pr > ChiSq."}, {"text": "Graphically, the p value is the area in the tail of a probability distribution. It's calculated when you run hypothesis test and is the area to the right of the test statistic (if you're running a two-tailed test, it's the area to the left and to the right)."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "You can use test statistics to determine whether to reject the null hypothesis. The test statistic compares your data with what is expected under the null hypothesis. The test statistic is used to calculate the p-value. A test statistic measures the degree of agreement between a sample of data and the null hypothesis."}, {"text": "So you can see that the ch-sq is the statistical measurement, while the P value is the level of probability that the result was due to chance alone. As the chi-sq statistic becomes larger, the P value becomes smaller."}]}, {"question": "What is receptive field size", "positive_ctxs": [{"text": "In the context of neural networks So, in a neural network context, the receptive field is defined as the size of the region in the input that produces the feature. Basically, it is a measure of association of an output feature (of any layer) to the input region (patch)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "The receptive field size of a unit can be increased in a number of ways. One option is to stack more layers to make the network deeper, which increases the receptive field size linearly by theory, as each extra layer increases the receptive field size by the kernel size."}, {"text": "Receptive fields are defined portion of space or spatial construct containing units that provide input to a set of units within a corresponding layer. The receptive field is defined by the filter size of a layer within a convolution neural network."}, {"text": "In the visual system, visual receptive fields are volumes in visual space.  The receptive field is often identified as the region of the retina where the action of light alters the firing of the neuron."}, {"text": "If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 5*5*3 = 75 weights (and +1 bias parameter)."}, {"text": "A CNN has multiple layers. Weight sharing happens across the receptive field of the neurons(filters) in a particular layer. Weights are the numbers within each filter.  These filters act on a certain receptive field/ small section of the image. When the filter moves through the image, the filter does not change."}, {"text": "The receptive field in Convolutional Neural Networks (CNN) is the region of the input space that affects a particular unit of the network.  The numbers inside the pixels on the left image represent how many times this pixel was part of a convolution step (each sliding step of the filter)."}]}, {"question": "Which is more accurate MBTI and Enneagram", "positive_ctxs": [{"text": "Enneagram test results are very accurate for determining your enneagram type and the MBTI test results are quite accurate for determining your MBTI type. Neither is in competition with the other. That being said, it can be very interesting to have the results for both of these uniquely different typologies."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Decomposition is a forecasting technique that separates or decomposes historical data into different components and uses them to create a forecast that is more accurate than a simple trend line."}, {"text": "The common wisdom is, Interpolation is likely to be more accurate than extrapolation. And the further you extrapolate from your data, the more inaccurate your predictions are likely to be.  The closer you are to a known data point, the more accurate your estimate is likely to be."}, {"text": "Data wrangling is the process of cleaning, structuring and enriching raw data into a desired format for better decision making in less time.  This self-service model with data wrangling tools allows analysts to tackle more complex data more quickly, produce more accurate results, and make better decisions."}, {"text": "If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier."}, {"text": "Results of a study can be made more accurate by controlling for the variation in the covariate. So, a covariate is in fact, a type of control variable.  A control variable is a nominal variable (not continuous) and although it has more than one value, the values are categorical and not infinite."}, {"text": "GRU use less training parameters and therefore use less memory, execute faster and train faster than LSTM's whereas LSTM is more accurate on dataset using longer sequence. In short, if sequence is large or accuracy is very critical, please go for LSTM whereas for less memory consumption and faster operation go for GRU."}, {"text": "A model is considered to be robust if its output and forecasts are consistently accurate even if one or more of the input variables or assumptions are drastically changed due to unforeseen circumstances."}]}, {"question": "What is the goodness of fit in regression", "positive_ctxs": [{"text": "\u201cGoodness of Fit\u201d of a linear regression model attempts to get at the perhaps sur- prisingly tricky issue of how well a model fits a given set of data, or how well it will predict a future set of observations."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution."}, {"text": "In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution."}, {"text": "The chi-square test is the most commonly used to test the goodness of fit tests and is used for discrete distributions like the binomial distribution and the Poisson distribution, whereas The Kolmogorov-Smirnov and Anderson-Darling goodness of fit tests are used for continuous distributions."}, {"text": "Definition. Pearson's chi-squared test is used to assess three types of comparison: goodness of fit, homogeneity, and independence. A test of goodness of fit establishes whether an observed frequency distribution differs from a theoretical distribution."}, {"text": "In statistics, the likelihood function (often simply called the likelihood) measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters."}, {"text": "In statistics, the likelihood function (often simply called the likelihood) measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters."}, {"text": "In statistics, the likelihood function (often simply called the likelihood) measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters."}]}, {"question": "What are Bayesian classifiers in data mining", "positive_ctxs": [{"text": "Advertisements. Bayesian classification is based on Bayes' Theorem. Bayesian classifiers are the statistical classifiers. Bayesian classifiers can predict class membership probabilities such as the probability that a given tuple belongs to a particular class."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bayesian networks (BN) and Bayesian classifiers (BC) are traditional probabilistic techniques that have been successfully used by various machine learning methods to help solving a variety of problems in many different domains."}, {"text": "Important Data mining techniques are Classification, clustering, Regression, Association rules, Outer detection, Sequential Patterns, and prediction. R-language and Oracle Data mining are prominent data mining tools. Data mining technique helps companies to get knowledge-based information."}, {"text": "Association rules mining is another key unsupervised data mining method, after clustering, that finds interesting associations (relationships, dependencies) in large sets of data items."}, {"text": "Big data analytics and data mining are not the same. Both of them involve the use of large data sets, handling the collection of the data or reporting of the data which is mostly used by businesses. However, both big data analytics and data mining are both used for two different operations."}, {"text": "Definition: In simple words, data mining is defined as a process used to extract usable data from a larger set of any raw data. It implies analysing data patterns in large batches of data using one or more software.  Data mining is also known as Knowledge Discovery in Data (KDD)."}, {"text": "Definition: In simple words, data mining is defined as a process used to extract usable data from a larger set of any raw data. It implies analysing data patterns in large batches of data using one or more software.  Data mining is also known as Knowledge Discovery in Data (KDD)."}, {"text": "The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or"}]}, {"question": "What is an example of a biased sample", "positive_ctxs": [{"text": "a survey of high school students to measure teenage use of illegal drugs will be a biased sample because it does not include home-schooled students or dropouts. A sample is also biased if certain members are underrepresented or overrepresented relative to others in the population."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Firstly, while the sample variance (using Bessel's correction) is an unbiased estimator of the population variance, its square root, the sample standard deviation, is a biased estimate of the population standard deviation; because the square root is a concave function, the bias is downward, by Jensen's inequality."}, {"text": "However, for a general population it is not true that the sample median is an unbiased estimator of the population median. The sample mean is a biased estimator of the population median when the population is not symmetric.  It only will be unbiased if the population is symmetric."}, {"text": "An biased estimator is one which delivers an estimate which is consistently different from the parameter to be estimated. In a more formal definition we can define that the expectation E of a biased estimator is not equal to the parameter of a population."}, {"text": "and is commonly used as an estimator for \u03c3. Nevertheless, S is a biased estimator of \u03c3."}, {"text": "A convenience sample is a type of non-probability sampling method where the sample is taken from a group of people easy to contact or to reach. For example, standing at a mall or a grocery store and asking people to answer questions would be an example of a convenience sample."}, {"text": "The definition of an ensemble is two or more people or things that function together as a whole. An example of an ensemble is a string quartet. An example of an ensemble is a group of actors in a play.  A small group of musicians playing or singing together."}, {"text": "Definition. A convenience sample is a type of non-probability sampling method where the sample is taken from a group of people easy to contact or to reach. For example, standing at a mall or a grocery store and asking people to answer questions would be an example of a convenience sample."}]}, {"question": "What is a linear model", "positive_ctxs": [{"text": "A linear model is an equation that describes a relationship between two quantities that show a constant rate of change."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Alternative procedures include: Different linear model: fitting a linear model with additional X variable(s) Nonlinear model: fitting a nonlinear model when the linear model is inappropriate.  Weighted least squares linear regression: dealing with unequal variances in Y by performing a weighted least squares fit."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "A log-linear model is a mathematical model that takes the form of a function whose logarithm equals a linear combination of the parameters of the model, which makes it possible to apply (possibly multivariate) linear regression."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "In statistics, Poisson regression is a generalized linear model form of regression analysis used to model count data and contingency tables.  This model is popular because it models the Poisson heterogeneity with a gamma distribution."}, {"text": "The term linear model implies that the model is specified as a linear combination of features. Based on training data, the learning process computes one weight for each feature to form a model that can predict or estimate the target value."}]}, {"question": "What's the impact of artificial intelligence and technology on society", "positive_ctxs": [{"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Explainable AI (XAI) refers to methods and techniques in the application of artificial intelligence technology (AI) such that the results of the solution can be understood by humans."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception."}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}]}, {"question": "What is Cluster Analysis example", "positive_ctxs": [{"text": "Cluster analysis is also used to group variables into homogeneous and distinct groups. This approach is used, for example, in revising a question- naire on the basis of responses received to a draft of the questionnaire."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cluster Analysis and Factor Analysis. Latent Class Analysis is similar to cluster analysis. Observed data is analyzed, connections are found, and the data is grouped into clusters.  Another difference is that LCA includes discrete latent categorical variables that have a multinomial distribution."}, {"text": "Here are five ways to identify segments.Cross-Tab. Cross-tabbing is the process of examining more than one variable in the same table or chart (\u201ccrossing\u201d them).  Cluster Analysis.  Factor Analysis.  Latent Class Analysis (LCA)  Multidimensional Scaling (MDS)"}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Linear Discriminant Analysis or Normal Discriminant Analysis or Discriminant Function Analysis is a dimensionality reduction technique which is commonly used for the supervised classification problems. It is used for modeling differences in groups i.e. separating two or more classes."}, {"text": "Linear Discriminant Analysis or Normal Discriminant Analysis or Discriminant Function Analysis is a dimensionality reduction technique which is commonly used for the supervised classification problems. It is used for modeling differences in groups i.e. separating two or more classes."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "Can statistics be manipulated", "positive_ctxs": [{"text": "There are several undeniable truths about statistics: First and foremost, they can be manipulated, massaged and misstated.  Second, if bogus statistical information is repeated often enough, it eventually is considered to be true."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Yes. We Can Always Use The Normal Distribution To Approximate The Binomial Distribution."}, {"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "Robust statistics are resistant to outliers. In other words, if your data set contains very high or very low values, then some statistics will be good estimators for population parameters, and some statistics will be poor estimators."}, {"text": "Advantages and disadvantagesAre simple to understand and interpret.  Have value even with little hard data.  Help determine worst, best and expected values for different scenarios.Use a white box model.  Can be combined with other decision techniques."}, {"text": "An independent variable, sometimes called an experimental or predictor variable, is a variable that is being manipulated in an experiment in order to observe the effect on a dependent variable, sometimes called an outcome variable."}, {"text": "Dual Booting Can Impact Disk Swap Space. In most cases there shouldn't be too much impact on your hardware from dual booting.  Both Linux and Windows use chunks of the hard disk drive to improve performance while the computer is running."}]}, {"question": "Does sample size matter in quantitative research", "positive_ctxs": [{"text": "That's your sample size--the number of participants needed to achieve valid conclusions or statistical significance in quantitative research.  When sample sizes are too small, you run the risk of not gathering enough data to support your hypotheses or expectations."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main aim of a sample size calculation is to determine the number of participants needed to detect a clinically relevant treatment effect. Pre-study calculation of the required sample size is warranted in the majority of quantitative studies."}, {"text": "The goal of observational research is to describe a variable or set of variables.  The data that are collected in observational research studies are often qualitative in nature but they may also be quantitative or both (mixed-methods)."}, {"text": "Pearson's correlation is utilized when you have two quantitative variables and you wish to see if there is a linear relationship between those variables. Your research hypothesis would represent that by stating that one score affects the other in a certain way. The correlation is affected by the size and sign of the r."}, {"text": "The effect size is the main finding of a quantitative study. While a P value can inform the reader whether an effect exists, the P value will not reveal the size of the effect."}, {"text": "Does not affect R2 or adjusted R2 (since these estimate the POPULATION variances which are not conditional on X)"}, {"text": "The consistency of the sampling distribution is dependent on the sample size not on the distribution of the population. As the sample size decreases the absolute value of the skewness and kurtosis of the sampling distribution increases. This sample size relationship is expressed in the central limit theorem."}, {"text": "The larger the sample size is the smaller the effect size that can be detected. The reverse is also true; small sample sizes can detect large effect sizes.  Thus an appropriate determination of the sample size used in a study is a crucial step in the design of a study."}]}, {"question": "Why are prospective studies better than retrospective", "positive_ctxs": [{"text": "Prospective studies usually have fewer potential sources of bias and confounding than retrospective studies. A retrospective study looks backwards and examines exposures to suspected risk or protection factors in relation to an outcome that is established at the start of the study."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Table 1Type of BiasHow to AvoidSelection bias\u2022 Select patients using rigorous criteria to avoid confounding results. Patients should originate from the same general population. Well designed, prospective studies help to avoid selection bias as outcome is unknown at time of enrollment.17 more rows"}, {"text": "Data structure and algorithms help in understanding the nature of the problem at a deeper level and thereby a better understanding of the world. If you want to know more about Why Data Structures and Algorithms then you must watch this video of Mr."}, {"text": "Why gradient clipping accelerates training: A theoretical justification for adaptivity.  These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption."}, {"text": "Natural Language Processing (NLP) is the part of AI that studies how machines interact with human language.  Combined with machine learning algorithms, NLP creates systems that learn to perform tasks on their own and get better through experience."}, {"text": "Not usually. SGD tends to perform better than using line search."}, {"text": "In statistical theory, the field of high-dimensional statistics studies data whose dimension is larger than dimensions considered in classical multivariate analysis.  In many applications, the dimension of the data vectors may be larger than the sample size."}, {"text": "The disadvantages are numerous. Cross-over studies are often of longer duration than parallel-group studies. There may be difficulty in incorporating multiple dosage arms and in dealing with drop-outs; patients who only complete the first evaluation phase contribute little to the analysis."}]}, {"question": "What is batch normalization CNN", "positive_ctxs": [{"text": "Batch normalization is a layer that allows every layer of the network to do learning more independently. It is used to normalize the output of the previous layers. The activations scale the input layer in normalization."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling.  Others sustain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks."}, {"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling."}, {"text": "To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation."}, {"text": "Dense CNN is a type of Deep CNN in which each layer is connected with another layer deeper than itself."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig. 2."}]}, {"question": "Where are data structures used", "positive_ctxs": [{"text": "In general, data structures are used to implement the physical forms of abstract data types. This can be translated into a variety of applications, such as displaying a relational database as a binary tree. In programming languages, data structures are used to organize code and information in a digital space."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Probabilistic data structures are a group of data structures that are extremely useful for big data and streaming applications. Generally speaking, these data structures use hash functions to randomize and compactly represent a set of items."}, {"text": "Unsupervised learning works by analyzing the data without its labels for the hidden structures within it, and through determining the correlations, and for features that actually correlate two data items. It is being used for clustering, dimensionality reduction, feature learning, density estimation, etc."}, {"text": "Neurons are organized into bundle fibers called nerves.  Dendrites are structures of neurons that conduct electrical impulses toward the cell body."}, {"text": "What is the best way to store data used for Natural Language Processing?stream data from disk when you can.  inverted indexes each get their own text file (more relevant for search, maybe not what you're doing)use sparse data structures (e.g. sparse matrix) as much as possible when you need to load stuff into memory."}, {"text": "Values range from 0 to 1, where 0 is perfect disagreement and 1 is perfect agreement. Krippendorff suggests: \u201c[I]t is customary to require \u03b1 \u2265 . 800. Where tentative conclusions are still acceptable, \u03b1 \u2265 ."}, {"text": "Chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval (IR). The second more subtle advantage of compression is faster transfer of data from disk to memory."}, {"text": "Hierarchical clustering is a powerful technique that allows you to build tree structures from data similarities. You can now see how different sub-clusters relate to each other, and how far apart data points are."}]}, {"question": "Where can I use TensorFlow", "positive_ctxs": [{"text": "TensorFlow applications can be run on most any target that's convenient: a local machine, a cluster in the cloud, iOS and Android devices, CPUs or GPUs. If you use Google's own cloud, you can run TensorFlow on Google's custom TensorFlow Processing Unit (TPU) silicon for further acceleration."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If you are a beginner, I can recommend you as below.Quickly learn Python first.Take a course of AI and Machine learning (several online courses are there). You can try MIT OCW also.Then start with Tutorial of TensorFlow website (https://www.tensorflow.org/versions/0.6.0/tutorials/index.html )"}, {"text": "TensorFlow is more of a low-level library; basically, we can think of TensorFlow as the Lego bricks (similar to NumPy and SciPy) that we can use to implement machine learning algorithms whereas scikit-learn comes with off-the-shelf algorithms, e.g., algorithms for classification such as SVMs, Random Forests, Logistic"}, {"text": "Tensorflow is the most famous library used in production for deep learning models.  However TensorFlow is not that easy to use. On the other hand, Keras is a high level API built on TensorFlow (and can be used on top of Theano too). It is more user-friendly and easy to use as compared to TF."}, {"text": "Google built the underlying TensorFlow software with the C++ programming language. But in developing applications for this AI engine, coders can use either C++ or Python, the most popular language among deep learning researchers."}, {"text": "Train and serve a TensorFlow model with TensorFlow ServingTable of contents.Create your model. Import the Fashion MNIST dataset. Train and evaluate your model.Save your model.Examine your saved model.Serve your model with TensorFlow Serving. Add TensorFlow Serving distribution URI as a package source:  Make a request to your model in TensorFlow Serving. Make REST requests."}, {"text": "Train and serve a TensorFlow model with TensorFlow ServingTable of contents.Create your model. Import the Fashion MNIST dataset. Train and evaluate your model.Save your model.Examine your saved model.Serve your model with TensorFlow Serving. Add TensorFlow Serving distribution URI as a package source:  Make a request to your model in TensorFlow Serving. Make REST requests."}, {"text": "Basically, we can think of TensorFlow as the Lego bricks (similar to NumPy and SciPy) that we can use to implement machine learning algorithms whereas Scikit-Learn comes with off-the-shelf algorithms, e.g., algorithms for classification such as SVMs, Random Forests, Logistic Regression, and many, many more."}]}, {"question": "How do you find strongly connected components", "positive_ctxs": [{"text": "Strongly Connected Components1) Create an empty stack 'S' and do DFS traversal of a graph. In DFS traversal, after calling recursive DFS for adjacent vertices of a vertex, push the vertex to stack.  2) Reverse directions of all arcs to obtain the transpose graph.3) One by one pop a vertex from S while S is not empty. Let the popped vertex be 'v'."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Kosaraju's algorithm finds the strongly connected components of a graph.  - For each vertex u of the graph do Visit(u), where Visit(u) is the recursive subroutine: - If u is unvisited then: - Mark u as visited. - For each out-neighbour v of u, do Visit(v)."}, {"text": "Connected components labeling scans an image and groups its pixels into components based on pixel connectivity, i.e. all pixels in a connected component share similar pixel intensity values and are in some way connected with each other."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "Connected components, in a 2D image, are clusters of pixels with the same value, which are connected to each other through either 4-pixel, or 8-pixel connectivity.  We offer several user-friendly ways to segment, and then rapidly calculate and display the connected components of 2D and 3D segmentations."}, {"text": "Connected components, in a 2D image, are clusters of pixels with the same value, which are connected to each other through either 4-pixel, or 8-pixel connectivity.  We offer several user-friendly ways to segment, and then rapidly calculate and display the connected components of 2D and 3D segmentations."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}]}, {"question": "What is posterior probability example", "positive_ctxs": [{"text": "Posterior probability = prior probability + new evidence (called likelihood). For example, historical data suggests that around 60% of students who start college will graduate within 6 years. This is the prior probability. However, you think that figure is actually much lower, so set out to collect new data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis."}, {"text": "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information. The posterior probability is calculated by updating the prior probability using Bayes' theorem."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred."}]}, {"question": "What is the difference between Type 1 and Type 2 error", "positive_ctxs": [{"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true."}, {"text": "Classification SVM Type 1 (also known as C-SVM classification); Classification SVM Type 2 (also known as nu-SVM classification); Regression SVM Type 1 (also known as epsilon-SVM regression); Regression SVM Type 2 (also known as nu-SVM regression)."}, {"text": "In a courtroom, a Type 2 error is acquitting a guilty person. A Type 1 error is when you incorrectly reject the null when it is true.  If the p -value is small, then you have observed something rare if the null is true. This then provides evidence against the truth of H0 ."}, {"text": "For binary classification problems, there are two primary types of errors. Type 1 errors (false positives) and Type 2 errors (false negatives). It's often possible through model selection and tuning to increase one while decreasing the other, and often one must choose which error type is more acceptable."}, {"text": "Type I error is equivalent to a False positive. Type II error is equivalent to a False negative. Type I error refers to non-acceptance of hypothesis which ought to be accepted. Type II error is the acceptance of hypothesis which ought to be rejected."}, {"text": "The comparison - wise error rate is the probability of a Type I error set by the experimentor for evaluating each comparison. The experiment - wise error rate is the probability of making at least one Type I error when performing the whole set of comparisons."}]}, {"question": "What is probability theory in research", "positive_ctxs": [{"text": "Probability theory is the mathematical study of phenomena characterized by randomness or uncertainty. More precisely, probability is used for modelling situations when the result of an experiment, realized under the same circumstances, produces different results (typically throwing a dice or a coin)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Bayesian analysis is a statistical paradigm that answers research questions about unknown parameters using probability statements."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment."}, {"text": "In statistics, a Bayesian is someone who tries to determine the probability that a theory is true given the observed data. This is in contrast to classical statisticians, who work with the probability of observing certain data assuming a theory."}, {"text": "In qualitative research no hypotheses or relationships of variables are tested. Because variables must be defined numerically in hypothesis-testing research, they cannot reflect subjective experience. This leads to hypothesis-generating research using the grounded theory method to study subjective experience directly."}, {"text": "In probability theory and statistics, the Bernoulli distribution, named after Swiss mathematician Jacob Bernoulli, is the discrete probability distribution of a random variable which takes the value 1 with probability and the value 0 with probability , and is sometimes denoted as ."}]}, {"question": "What is autoregressive coefficient", "positive_ctxs": [{"text": "An Autoregressive (AR) Process Remembers Where It Was An observation of an autoregressive process (the AR in ARIMA) consists of a linear function of the previous observation plus random noise.  Note that this is a linear regression model that predicts the current level (Y = Yt) from the previous level (X = Yt \u2212 1)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "A statistical model is autoregressive if it predicts future values based on past values. For example, an autoregressive model might seek to predict a stock's future prices based on its past performance."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "The t statistic is the coefficient divided by its standard error.  It can be thought of as a measure of the precision with which the regression coefficient is measured. If a coefficient is large compared to its standard error, then it is probably different from 0."}]}, {"question": "Can regression be used for prediction", "positive_ctxs": [{"text": "You can use regression equations to make predictions. Regression equations are a crucial part of the statistical output after you fit a model.  However, you can also enter values for the independent variables into the equation to predict the mean value of the dependent variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.  Explicit regression gradient boosting algorithms were subsequently developed by Jerome H."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.  Explicit regression gradient boosting algorithms were subsequently developed by Jerome H."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. ("}, {"text": "Classification and regression trees are machine-learning methods for constructing. prediction models from data. The models are obtained by recursively partitioning. the data space and fitting a simple prediction model within each partition."}]}, {"question": "What is Eigen analysis", "positive_ctxs": [{"text": "Eigenanalysis is a mathematical operation on a square, symmetric matrix. A square matrix has the same number of rows as columns. A symmetric matrix is the same if you switch rows and columns. Distance and similarity matrices are nearly always square and symmetric."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "How long will it take to learn data structures and algorithms", "positive_ctxs": [{"text": "If you are already a programmer and has basic knowledge of how it works. I would say 2 days to a month to learn it. Toby Thain, Started at around 10 years old. Still learning."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To recap the differences between the two: Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned. Deep learning structures algorithms in layers to create an \"artificial neural network\u201d that can learn and make intelligent decisions on its own."}, {"text": "To recap the differences between the two: Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned. Deep learning structures algorithms in layers to create an \"artificial neural network\u201d that can learn and make intelligent decisions on its own."}, {"text": "To recap the differences between the two: Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned. Deep learning structures algorithms in layers to create an \"artificial neural network\u201d that can learn and make intelligent decisions on its own."}, {"text": "To recap the differences between the two: Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned. Deep learning structures algorithms in layers to create an \"artificial neural network\u201d that can learn and make intelligent decisions on its own."}, {"text": "Neural Turing Machines can take input and output and learn algorithms that map from one to the other.  This means that once they have learned that algorithm, they can take a given input, and they can extrapolate based on that algorithm to any variable output."}, {"text": "Deep Learning does this by utilizing neural networks with many hidden layers, big data, and powerful computational resources.  In unsupervised learning, algorithms such as k-Means, hierarchical clustering, and Gaussian mixture models attempt to learn meaningful structures in the data."}, {"text": "Probabilistic data structures are a group of data structures that are extremely useful for big data and streaming applications. Generally speaking, these data structures use hash functions to randomize and compactly represent a set of items."}]}, {"question": "How long will it take to learn deep learning", "positive_ctxs": [{"text": "Each of the steps should take about 4\u20136 weeks' time. And in about 26 weeks since the time you started, and if you followed all of the above religiously, you will have a solid foundation in deep learning."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "deep learning - a name for an algorithm in machine learning (just like SVM, Regression etc.) transfer learning - as you may know, in order to train a Neural network it might take long time. So, we use a Neural Network that is already trained and in this way we can extract some features of new sample."}, {"text": "Deep reinforcement learning is a promising combination between two artificial intelligence techniques: reinforcement learning, which uses sequential trial and error to learn the best action to take in every situation, and deep learning, which can evaluate complex inputs and select the best response."}, {"text": "At a very basic level, deep learning is a machine learning technique. It teaches a computer to filter inputs through layers to learn how to predict and classify information. Observations can be in the form of images, text, or sound. The inspiration for deep learning is the way that the human brain filters information."}, {"text": "Generally, a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights. A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train."}, {"text": "Artificial intelligence (AI) makes it possible for machines to learn from experience, adjust to new inputs and perform human-like tasks. Most AI examples that you hear about today \u2013 from chess-playing computers to self-driving cars \u2013 rely heavily on deep learning and natural language processing."}, {"text": "If you don't have enough time to read through the entire post, the following hits on the key components: Bag-of-words: How to break up long text into individual words. Filtering: Different approaches to remove uninformative words. Bag of n-grams: Retain some context by breaking long text into sequences of words."}]}, {"question": "How does KNN classification work", "positive_ctxs": [{"text": "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "KNN algorithm is one of the simplest classification algorithm. Even with such simplicity, it can give highly competitive results. KNN algorithm can also be used for regression problems."}, {"text": "KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.  KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point."}, {"text": "KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.  KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point."}, {"text": "As we saw above, KNN algorithm can be used for both classification and regression problems. The KNN algorithm uses 'feature similarity' to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set."}, {"text": "As we saw above, KNN algorithm can be used for both classification and regression problems. The KNN algorithm uses 'feature similarity' to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "It uses data with several classes to predict the classification of the new sample point. KNN is non-parametric since it doesn't make any assumptions on the data being studied, i.e., the model is distributed from the data."}]}, {"question": "How do you find eigenvectors from eigenvalues", "positive_ctxs": [{"text": "Therefore, the eigenvalues of A are \u03bb = 4,\u22122. (\u03bb = \u22122 is a repeated root of the characteristic equation.) Once the eigenvalues of a matrix (A) have been found, we can find the eigenvectors by Gaussian Elimination. to row echelon form, and solve the resulting linear system by back substitution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA. The eigenvectors of ATA make up the columns of V , the eigenvectors of AAT make up the columns of U. Also, the singular values in S are square roots of eigenvalues from AAT or ATA."}, {"text": "Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA. The eigenvectors of ATA make up the columns of V , the eigenvectors of AAT make up the columns of U. Also, the singular values in S are square roots of eigenvalues from AAT or ATA."}, {"text": "The function scipy. linalg. eig computes eigenvalues and eigenvectors of a square matrix ."}, {"text": "0:005:03Suggested clip \u00b7 117 secondsPCA 5: finding eigenvalues and eigenvectors - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}, {"text": "0:001:38Suggested clip \u00b7 98 secondsFind the matrix A given the eigenvalues and eigenvectors - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}, {"text": "If two matrices are similar, they have the same eigenvalues and the same number of independent eigenvectors (but probably not the same eigenvectors)."}, {"text": "), while other elements may be complex. Hermitian matrices have real eigenvalues whose eigenvectors form a unitary basis. For real matrices, Hermitian is the same as symmetric."}]}, {"question": "What is mean by bootstrapping", "positive_ctxs": [{"text": "Bootstrapping is building a company from the ground up with nothing but personal savings, and with luck, the cash coming in from the first sales. The term is also used as a noun: A bootstrap is a business an entrepreneur with little or no outside cash or other support launches."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "\u201cThe advantages of bootstrapping are that it is a straightforward way to derive the estimates of standard errors and confidence intervals, and it is convenient since it avoids the cost of repeating the experiment to get other groups of sampled data."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "The harmonic mean is a type of numerical average. It is calculated by dividing the number of observations by the reciprocal of each number in the series. Thus, the harmonic mean is the reciprocal of the arithmetic mean of the reciprocals."}]}, {"question": "How do you handle an unbalanced data set", "positive_ctxs": [{"text": "7 Techniques to Handle Imbalanced DataUse the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Random forests perform well for multi-class object detection and bioinformatics, which tends to have a lot of statistical noise. Gradient Boosting performs well when you have unbalanced data such as in real time risk assessment."}, {"text": "The larger the RAM the higher the amount of data it can handle hence faster processing. With larger RAM you can use your machine to perform other tasks as the model trains. Although a minimum of 8GB RAM can do the job, 16GB RAM and above is recommended for most deep learning tasks."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Analysis of variance (ANOVA) is an analysis tool used in statistics that splits an observed aggregate variability found inside a data set into two parts: systematic factors and random factors. The systematic factors have a statistical influence on the given data set, while the random factors do not."}]}, {"question": "Why accuracy is not used as a preferred method for real world IR system evaluation", "positive_ctxs": [{"text": "There is a good reason why accuracy is not an appropriate measure for information retrieval problems. In almost all circumstances, the data is extremely skewed: normally over 99.9% of the documents are in the nonrelevant category."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Logarithmic Loss, or simply Log Loss, is a classification loss function often used as an evaluation metric in Kaggle competitions.  Log Loss quantifies the accuracy of a classifier by penalising false classifications."}, {"text": "Logarithmic Loss, or simply Log Loss, is a classification loss function often used as an evaluation metric in Kaggle competitions.  Log Loss quantifies the accuracy of a classifier by penalising false classifications."}, {"text": "Logarithmic Loss, or simply Log Loss, is a classification loss function often used as an evaluation metric in Kaggle competitions.  Log Loss quantifies the accuracy of a classifier by penalising false classifications."}, {"text": "> DCT is preferred over DFT in image compression algorithms like JPEG > because DCT is a real transform which results in a single real number per > data point. In contrast, a DFT results in a complex number (real and > imaginary parts) which requires double the memory for storage."}, {"text": "Offline evaluations test the effectiveness of recommender system algorithms on a certain dataset. Online evaluation attempts to evaluate recommender systems by a method called A/B testing where a part of users are served by recommender system A and the another part of users by recommender system B."}, {"text": "Image processing is often viewed as arbitrarily manipulating an image to achieve an aesthetic standard or to support a preferred reality. However, image processing is more accurately defined as a means of translation between the human visual system and digital imaging devices."}, {"text": "Image processing is often viewed as arbitrarily manipulating an image to achieve an aesthetic standard or to support a preferred reality. However, image processing is more accurately defined as a means of translation between the human visual system and digital imaging devices."}]}, {"question": "How do Expert Systems work", "positive_ctxs": [{"text": "An expert system (ES) is a knowledge-based system that employs knowledge about its application domain and uses an inferencing (reason) procedure to solve problems that would otherwise require human competence or expertise."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Expert System is an application using AI to build a knowledge base and use that knowledge base to solve such problems where human experts are needed to solve the problem. Artificial Intelligence targets to make machines intelligent.  Expert System is an application using Artificial Intelligence."}, {"text": "Expert System is an application using AI to build a knowledge base and use that knowledge base to solve such problems where human experts are needed to solve the problem. Artificial Intelligence targets to make machines intelligent.  Expert System is an application using Artificial Intelligence."}]}, {"question": "Why is R used for data science", "positive_ctxs": [{"text": "R is a highly extensible and easy to learn language and fosters an environment for statistical computing and graphics. All of this makes R an ideal choice for data science, big data analysis, and machine learning."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "R is a very dynamic and versatile programming language for data science. This article deals with classification in R. Generally classifiers in R are used to predict specific category related information like reviews or ratings such as good, best or worst. Various Classifiers are: Decision Trees."}, {"text": "Coefficient of correlation is \u201cR\u201d value which is given in the summary table in the Regression output. R square is also called coefficient of determination. Multiply R times R to get the R square value. In other words Coefficient of Determination is the square of Coefficeint of Correlation."}, {"text": "The answer is a big NO. Data science gets solutions and results to specific business problems using AI as a tool. If data science is to insights, machine learning is to predictions and artificial intelligence is to actions."}, {"text": "Because data science is a broad term for multiple disciplines, machine learning fits within data science. Machine learning uses various techniques, such as regression and supervised clustering. On the other hand, the data' in data science may or may not evolve from a machine or a mechanical process."}, {"text": "Why the Lognormal Distribution is used to Model Stock Prices Since the lognormal distribution is bound by zero on the lower side, it is therefore perfect for modeling asset prices which cannot take negative values. The normal distribution cannot be used for the same purpose because it has a negative side."}, {"text": "Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data."}, {"text": "Unsupervised or undirected data science uncovers hidden patterns in unlabeled data. In unsupervised data science, there are no output variables to predict. The objective of this class of data science techniques, is to find patterns in data based on the relationship between data points themselves."}]}, {"question": "What is a good false positive rate", "positive_ctxs": [{"text": "(Example: a test with 90% specificity will correctly return a negative result for 90% of people who don't have the disease, but will return a positive result \u2014 a false-positive \u2014 for 10% of the people who don't have the disease and should have tested negative.)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "False positive rate (FPR) is a measure of accuracy for a test: be it a medical diagnostic test, a machine learning model, or something else. In technical terms, the false positive rate is defined as the probability of falsely rejecting the null hypothesis."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false discovery rate is the ratio of the number of false positive results to the number of total positive test results. Out of 10,000 people given the test, there are 450 true positive results (box at top right) and 190 false positive results (box at bottom right) for a total of 640 positive results."}, {"text": "Recall and True Positive Rate (TPR) are exactly the same. So the difference is in the precision and the false positive rate.  While precision measures the probability of a sample classified as positive to actually be positive, the false positive rate measures the ratio of false positives within the negative samples."}, {"text": "The receiver operating characteristic (ROC) curve is a two dimensional graph in which the false positive rate is plotted on the X axis and the true positive rate is plotted on the Y axis. The ROC curves are useful to visualize and compare the performance of classifier methods (see Figure 1)."}]}, {"question": "What are the composition for agents in artificial intelligence", "positive_ctxs": [{"text": "Explanation: Simple reflex agent is based on the present condition and so it is condition action rule. 5. What are the composition for agents in artificial intelligence? Explanation: An agent program will implement function mapping percepts to actions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In the real world, knowledge plays a vital role in intelligence as well as creating artificial intelligence. It demonstrates the intelligent behavior in AI agents or systems. It is possible for an agent or system to act accurately on some input only when it has the knowledge or experience about the input."}, {"text": "Multi-agent reinforcement learning is the study of numerous artificial intelligence agents cohabitating in an environment, often collaborating toward some end goal. When focusing on collaboration, it derives inspiration from other social structures in the animal kingdom. It also draws heavily on game theory."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "Java, Python, Lisp, Prolog, and C++ are major AI programming language used for artificial intelligence capable of satisfying different needs in the development and designing of different software.  It answers the question, 'what is the language used for artificial intelligence?"}, {"text": "In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats\u2014artificial systems that exhibit complex behaviour in an agent environment.  The term is also sometimes used in ethology or animal behavior."}, {"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}, {"text": "Java, Python, Lisp, Prolog, and C++ are major AI programming language used for artificial intelligence capable of satisfying different needs in the development and designing of different software."}]}, {"question": "What are the advantage of wilcoxon sign rank test over sign test", "positive_ctxs": [{"text": "The advantage with Wilcoxon Signed Rank Test is that it neither depends on the form of the parent distribution nor on its parameters. It does not require any assumptions about the shape of the distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main nonparametric tests are:1-sample sign test.  1-sample Wilcoxon signed rank test.  Friedman test.  Goodman Kruska's Gamma: a test of association for ranked variables.Kruskal-Wallis test.  The Mann-Kendall Trend Test looks for trends in time-series data.Mann-Whitney test.  Mood's Median test.More items\u2022"}, {"text": "If we assume that there is some variation in our data, we will be able to disregard the possibility that either of these standard deviations is zero. Therefore the sign of the correlation coefficient will be the same as the sign of the slope of the regression line."}, {"text": "The Wilcoxon signed rank test is a nonparametric test that compares the median of a set of numbers against a hypothetical median. The Wilcoxon rank sum test is a nonparametric test to compare two unmatched groups. It is equivalent to the Mann-Whitney test."}, {"text": "The Wilcoxon signed rank test is a nonparametric test that compares the median of a set of numbers against a hypothetical median. The Wilcoxon rank sum test is a nonparametric test to compare two unmatched groups. It is equivalent to the Mann-Whitney test. The Gehan-Wilcoxon test is a method to compare survival curves."}, {"text": "Another sign of overfitting may be seen in the classification accuracy on the training data, If the training accuracy is out performing our test accuracy, it means that our model is learning details and noises of training data and specifically working of training data. Overfitting is a major problem in neural networks."}, {"text": "Correlation Test Between Two Variables in RR functions.Import your data into R.Visualize your data using scatter plots.Preleminary test to check the test assumptions.Pearson correlation test. Interpretation of the result. Access to the values returned by cor.test() function.Kendall rank correlation test.Spearman rank correlation coefficient."}, {"text": "The rank mean of one group is compared to the overall rank mean to determine a test statistic called a z-score. If the groups are evenly distributed, then the z-score will be closer to 0."}]}, {"question": "What are the limitations of neural networks", "positive_ctxs": [{"text": "Deep learning is getting a lot of hype right now, but neural networks aren't the answer to everything.Disadvantages of Neural NetworksBlack Box.  Duration of Development.  Amount of Data.  Computationally Expensive."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Neural networks are sets of algorithms intended to recognize patterns and interpret data through clustering or labeling. In other words, neural networks are algorithms. A training algorithm is the method you use to execute the neural network's learning process."}, {"text": "Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. In fact, it is the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three."}, {"text": "Neural network ensemble is a learning paradigm where many neural networks are jointly used to solve a problem.  Then it assigns random weights to those networks and employs genetic algorithm to evolve the weights so that they can characterize to some extent the fitness of the neural networks in constituting an ensemble."}, {"text": "Backpropagation, short for \"backward propagation of errors,\" is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights."}, {"text": "One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture."}, {"text": "One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture."}, {"text": "One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture."}]}, {"question": "What are biases in neural network", "positive_ctxs": [{"text": "Bias allows you to shift the activation function by adding a constant (i.e. the given bias) to the input. Bias in Neural Networks can be thought of as analogous to the role of a constant in a linear function, whereby the line is effectively transposed by the constant value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Neural network momentum is a simple technique that often improves both training speed and accuracy. Training a neural network is the process of finding values for the weights and biases so that for a given set of input values, the computed output values closely match the known, correct, target values."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "A recurrent neural network (RNN) is a type of neural network commonly used in speech recognition. RNNs are designed to recognize the sequential characteristics in data and use patterns to predict the next likely scenario."}, {"text": "Backpropagation is an algorithm commonly used to train neural networks. When the neural network is initialized, weights are set for its individual elements, called neurons. Inputs are loaded, they are passed through the network of neurons, and the network provides an output for each one, given the initial weights."}, {"text": "A feedforward neural network is an artificial neural network wherein connections between the units do not form a cycle.  The simplest kind of neural network is a single-layer perceptron network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights."}, {"text": "A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation."}]}, {"question": "What is a learning rate in machine learning", "positive_ctxs": [{"text": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.  In setting a learning rate, there is a trade-off between the rate of convergence and overshooting."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."}, {"text": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."}, {"text": "In machine learning, a hyperparameter is a parameter whose value is used to control the learning process.  An example of a model hyperparameter is the topology and size of a neural network. Examples of algorithm hyperparameters are learning rate and mini-batch size."}, {"text": "A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck.  The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate."}, {"text": "False positive rate (FPR) is a measure of accuracy for a test: be it a medical diagnostic test, a machine learning model, or something else. In technical terms, the false positive rate is defined as the probability of falsely rejecting the null hypothesis."}, {"text": "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}, {"text": "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}]}, {"question": "What are partitioning methods", "positive_ctxs": [{"text": "Partitioning methods: Given a set of n objects, a partitioning method constructs k partitions of the data, where each partition represents a cluster and k \u2264 n. That is, it divides the data into k groups such that each group must contain at least one object."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Partitioning methods Horizontal partitioning involves putting different rows into different tables.  Vertical partitioning involves creating tables with fewer columns and using additional tables to store the remaining columns."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "When we think of data structures, there are generally four forms:Linear: arrays, lists.Tree: binary, heaps, space partitioning etc.Hash: distributed hash table, hash tree etc.Graphs: decision, directed, acyclic etc."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "This clustering method classifies the information into multiple groups based on the characteristics and similarity of the data.  There are many algorithms that come under partitioning method some of the popular ones are K-Mean, PAM(K-Mediods), CLARA algorithm (Clustering Large Applications) etc."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "For a good regression model, you want to include the variables that you are specifically testing along with other variables that affect the response in order to avoid biased results.  Cross-validation determines how well your model generalizes to other data sets by partitioning your data."}]}, {"question": "How do you calculate the median percentage", "positive_ctxs": [{"text": "Once you have calculated the decimal values of each percentage for each given sample size, you then add these decimal values together and divide the total number by the total sum of both sample sizes. You then need to multiply this value by 100 to get the average percentage.5 days ago"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Weighted percentages allow you to account for this. All you have to do is convert the percentage the assignment is worth into a decimal and multiply that by your grade. To convert, just divide the percentage of your final grade the assignment represents by 100."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "To calculate how much weight you need, divide the known population percentage by the percent in the sample. For this example: Known population females (51) / Sample Females (41) = 51/41 = 1.24. Known population males (49) / Sample males (59) = 49/59 = ."}, {"text": "Arrange your set of numbers from smallest to largest. Determine which measure of central tendency you wish to calculate. The three types are mean, median and mode. To calculate the mean, add all your data and divide the result by the number of data."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "What is an example of a continuous probability distribution", "positive_ctxs": [{"text": "Because there are infinite values that X could assume, the probability of X taking on any one specific value is zero. Therefore we often speak in ranges of values (p(X>0) = . 50). The normal distribution is one example of a continuous distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable."}, {"text": "The Beta distribution is a continuous probability distribution having two parameters. One of its most common uses is to model one's uncertainty about the probability of success of an experiment."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The conversion of a frequency distribution to a probability distribution is also called an adjusted histogram. This is true for continuous random variables. To convert a frequency distribution to a probability distribution, divide area of the bar or interval of x by the total area of all the Bars."}, {"text": "The conversion of a frequency distribution to a probability distribution is also called an adjusted histogram. This is true for continuous random variables. To convert a frequency distribution to a probability distribution, divide area of the bar or interval of x by the total area of all the Bars."}, {"text": "A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function."}, {"text": "A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function."}]}, {"question": "Are generative models unsupervised", "positive_ctxs": [{"text": "Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Though originally proposed as a form of generative model for unsupervised learning, GANs have also proven useful for semi-supervised learning, fully supervised learning, and reinforcement learning."}, {"text": "Generative Adversarial Networks, or GANs, are a deep-learning-based generative model. More generally, GANs are a model architecture for training a generative model, and it is most common to use deep learning models in this architecture."}, {"text": "A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words."}, {"text": "A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words."}, {"text": "Generative model. A generative model can estimate the probability of the instance, and also the probability of a class label. Not enough information to tell. Both generative and discriminative models can estimate probabilities (but they don't have to)."}, {"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "In other words, discriminative models are used to specify outputs based on inputs (by models such as Logistic regression, Neural networks and Random forests), while generative models generate both inputs and outputs (for example, by Hidden Markov model, Bayesian Networks and Gaussian mixture model)."}]}, {"question": "What is distance metric learning", "positive_ctxs": [{"text": "Distance metric learning (or simply, metric learning) aims at automatically constructing task-specific distance metrics from (weakly) supervised data, in a machine learning manner. The learned distance metric can then be used to perform various tasks (e.g., k-NN classification, clustering, information retrieval)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep metric learning (DML) is an emerging field in metric learning by introducing deep neural network. Taking advantage of the nonlinear feature representation learning ability of deep learning and discrimination power of metric learning, DML is widely applied in various computer vision tasks."}, {"text": "Metric learning aims to measure the similarity among samples while using an optimal distance metric for learning tasks.  In recent years, deep metric learning, which provides a better solution for nonlinear data through activation functions, has attracted researchers' attention in many different areas."}, {"text": "Usually, people use the cosine similarity as a similarity metric between vectors. Now, the distance can be defined as 1-cos_similarity. The intuition behind this is that if 2 vectors are perfectly the same then similarity is 1 (angle=0) and thus, distance is 0 (1-1=0)."}, {"text": "While PCA is based on Euclidean distances, PCoA can handle (dis)similarity matrices calculated from quantitative, semi-quantitative, qualitative, and mixed variables.  When the distance metric is Euclidean, PCoA is equivalent to Principal Components Analysis."}, {"text": "The most commonly used metric for regression tasks is RMSE (Root Mean Square Error). This is defined as the square root of the average squared distance between the actual score and the predicted score: rmse=\u221a\u2211ni=1(yi\u2212^yi)2n."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "Is email structured or unstructured data", "positive_ctxs": [{"text": "Email messages are a good example. While the actual content is unstructured, it does contain structured data such as name and email address of sender and recipient, time sent, etc. Another example is a digital photograph."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Examples of semi-structured data include JSON and XML are forms of semi-structured data. The reason that this third category exists (between structured and unstructured data) is because semi-structured data is considerably easier to analyse than unstructured data."}, {"text": "Structured data is highly specific and is stored in a predefined format, where unstructured data is a conglomeration of many varied types of data that are stored in their native formats. This means that structured data takes advantage of schema-on-write and unstructured data employs schema-on-read."}, {"text": "NLP is short for natural language processing while NLU is the shorthand for natural language understanding.  They share a common goal of making sense of concepts represented in unstructured data, like language, as opposed to structured data like statistics, actions, etc."}, {"text": "Big data is a term that describes the large volume of data \u2013 both structured and unstructured \u2013 that inundates a business on a day-to-day basis. But it's not the amount of data that's important.  Big data can be analyzed for insights that lead to better decisions and strategic business moves."}, {"text": "Structured data is clearly defined and searchable types of data, while unstructured data is usually stored in its native format. Structured data is quantitative, while unstructured data is qualitative. Structured data is often stored in data warehouses, while unstructured data is stored in data lakes."}, {"text": "Hadoop Examples: 5 Real-World Use CasesFinancial services companies use analytics to assess risk, build investment models, and create trading algorithms; Hadoop has been used to help build and run those applications.Retailers use it to help analyze structured and unstructured data to better understand and serve their customers.More items\u2022"}, {"text": "Definition: Hadoop is a kind of framework that can handle the huge volume of Big Data and process it, whereas Big Data is just a large volume of the Data which can be in unstructured and structured data."}]}, {"question": "What does cross validation reduce", "positive_ctxs": [{"text": "To reduce variability we perform multiple rounds of cross-validation with different subsets from the same data. We combine the validation results from these multiple rounds to come up with an estimate of the model's predictive performance. Cross-validation will give us a more accurate estimate of a model's performance."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "10.2 - Discriminant Analysis ProcedureStep 1: Collect training data.  Step 2: Prior Probabilities.  Step 3: Bartlett's test.  Step 4: Estimate the parameters of the conditional probability density functions f ( X | \u03c0 i ) .  Step 5: Compute discriminant functions.  Step 6: Use cross validation to estimate misclassification probabilities.More items"}, {"text": "Random forest will reduce variance part of error rather than bias part, so on a given training data set decision tree may be more accurate than a random forest. But on an unexpected validation data set, Random forest always wins in terms of accuracy."}, {"text": "The cross product is a calculation used in order to define the correlation coefficient between two variables. SP is the sum of all cross products between two variables."}, {"text": "Jaccard similarity is good for cases where duplication does not matter, cosine similarity is good for cases where duplication matters while analyzing text similarity. For two product descriptions, it will be better to use Jaccard similarity as repetition of a word does not reduce their similarity."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}]}, {"question": "Do ReLU networks suffer from the exploding gradient problem", "positive_ctxs": [{"text": "In deep multilayer Perceptron networks, exploding gradients can result in an unstable network that at best cannot learn from the training data and at worst results in NaN weight values that can no longer be updated. \u2026 exploding gradients can make learning unstable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The ReLU activation solves the problem of vanishing gradient that is due to sigmoid-like non-linearities (the gradient vanishes because of the flat regions of the sigmoid). The other kind of \"vanishing\" gradient seems to be related to the depth of the network (e.g. see this for example)."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "However, RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences. The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done."}, {"text": "If exploding gradients are still occurring, you can check for and limit the size of gradients during the training of your network. This is called gradient clipping. Dealing with the exploding gradients has a simple but very effective solution: clipping gradients if their norm exceeds a given threshold."}, {"text": "ReLU is important because it does not saturate; the gradient is always high (equal to 1) if the neuron activates. As long as it is not a dead neuron, successive updates are fairly effective. ReLU is also very quick to evaluate."}]}, {"question": "How do you avoid local minima in neural networks", "positive_ctxs": [{"text": "To overcome the local minimum problems, many methods have been proposed. A widely used one is to train a neural network more than once, starting with a random set of weights [3,4]. An advantage of this approach lies in the simplicity of using and applying to other learning algorithms."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "K-Means clustering algorithm instead converses on local minima which might also correspond to the global minima in some cases but not always.  But that is done by simply making the algorithm choose the set of same random no. for each run."}, {"text": "In addition, another reason to not initialize everything to zero is so that you get different answers. Some optimization techniques are deterministic, so if you initialize randomly, you'll get different answers each time you run it. This helps you explore the space better and avoid (other) local optima."}, {"text": "The reason why Convolutional Neural Networks (CNNs) do so much better than classic neural networks on images and videos is that the convolutional layers take advantage of inherent properties of images. Simple feedforward neural networks don't see any order in their inputs."}, {"text": "The main problem of using adaptive learning rate optimizers including Adam, RMSProp, etc. is the difficulty of being stuck on local minima while not converging to the global minimum.  These can lead to bad decisions of the optimizer and being stuck on local optima instead of finding global minima."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Specifically, you learned: That a key approach is to use word embeddings and convolutional neural networks for text classification. That a single layer model can do well on moderate-sized problems, and ideas on how to configure it."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}]}, {"question": "How do you evaluate the mean and median", "positive_ctxs": [{"text": "To find the mean, add up the values in the data set and then divide by the number of values that you added. To find the median, list the values of the data set in numerical order and identify which value appears in the middle of the list."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In a normal distribution, the mean and the median are the same number while the mean and median in a skewed distribution become different numbers: A left-skewed, negative distribution will have the mean to the left of the median. A right-skewed distribution will have the mean to the right of the median."}, {"text": "In case of mean and median, it is not necessary. However, the accuracy of the mean would be higher if the class intervals are short. Similarly the median would be more accurate if the 'median class', class interval in which median falls, is of short length."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right.  In a positively skewed distribution, the mode is always less than the mean and median."}, {"text": "The median is usually preferred in these situations because the value of the mean can be distorted by the outliers. However, it will depend on how influential the outliers are. If they do not significantly distort the mean, using the mean as the measure of central tendency will usually be preferred."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}]}, {"question": "What does gamma distribution look like", "positive_ctxs": [{"text": "In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. With a shape parameter \u03b1 = k and an inverse scale parameter \u03b2 = 1/\u03b8, called a rate parameter.  With a shape parameter k and a mean parameter \u03bc = k\u03b8 = \u03b1/\u03b2."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution."}, {"text": "In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution."}, {"text": "For values of x > 0, the gamma function is defined using an integral formula as \u0393(x) = Integral on the interval [0, \u221e ] of \u222b 0\u221et x \u22121 e\u2212t dt. The probability density function for the gamma distribution is given by. The mean of the gamma distribution is \u03b1\u03b2 and the variance (square of the standard deviation) is \u03b1\u03b22."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Introduction[edit] Shift Invariance simply refers to the 'invariance' that a CNN has to recognising images. It allows the CNN to detect features/objects even if it does not look exactly like the images in it's training period. Shift invariance covers 'small' differences, such as movements shifts of a couple of pixels."}]}, {"question": "What is the good range of correlation values to include in the regression model", "positive_ctxs": [{"text": "Values between 0.7 and 1.0 (\u22120.7 and \u22121.0) indicate a strong positive (negative) linear relationship through a firm linear rule. It is the correlation coefficient between the observed and modelled (predicted) data values. It can increase as the number of predictor variables in the model increases; it does not decrease."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Simply put, R is the correlation between the predicted values and the observed values of Y. R square is the square of this coefficient and indicates the percentage of variation explained by your regression line out of the total variation. This value tends to increase as you include additional predictors in the model."}, {"text": "The correlation is the covariance divided by the product of the standard deviations. Therefore the correlation is the gradient of the regression line multiplied by the ratio of the standard deviations. If these standard deviations are equal the correlation is equal to the gradient."}, {"text": "A linear regression model extended to include more than one independent variable is called a multiple regression model. It is more accurate than to the simple regression.  The principal adventage of multiple regression model is that it gives us more of the information available to us who estimate the dependent variable."}, {"text": "The correlation coefficient is a statistical measure of the strength of the relationship between the relative movements of two variables. The values range between -1.0 and 1.0.  Since oil companies earn greater profits as oil prices rise, the correlation between the two variables is highly positive."}, {"text": "The correlation coefficient is a statistical measure of the strength of the relationship between the relative movements of two variables. The values range between -1.0 and 1.0.  A correlation of -1.0 shows a perfect negative correlation, while a correlation of 1.0 shows a perfect positive correlation."}, {"text": "The correlation coefficient is a statistical measure of the strength of the relationship between the relative movements of two variables. The values range between -1.0 and 1.0. A calculated number greater than 1.0 or less than -1.0 means that there was an error in the correlation measurement."}, {"text": "In contrast, if there are many values that have the same count, then mode can be meaningless. I did not include the range in the tabs above because it is not really a measure of central tendency. However, the concept of range is usually discussed alongside with Mean, Median, and Mode."}]}, {"question": "Is logistic regression mainly used for regression True or false", "positive_ctxs": [{"text": "Logistic regression is a classification algorithm, don't confuse with the name regression."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "Linear regression is used for predicting the continuous dependent variable using a given set of independent features whereas Logistic Regression is used to predict the categorical. Linear regression is used to solve regression problems whereas logistic regression is used to solve classification problems."}, {"text": "Linear regression is used for predicting the continuous dependent variable using a given set of independent features whereas Logistic Regression is used to predict the categorical. Linear regression is used to solve regression problems whereas logistic regression is used to solve classification problems."}, {"text": "Multinomial logistic regression (often just called 'multinomial regression') is used to predict a nominal dependent variable given one or more independent variables. It is sometimes considered an extension of binomial logistic regression to allow for a dependent variable with more than two categories."}, {"text": "Multinomial logistic regression (often just called 'multinomial regression') is used to predict a nominal dependent variable given one or more independent variables. It is sometimes considered an extension of binomial logistic regression to allow for a dependent variable with more than two categories."}, {"text": "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)\u2208{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits."}, {"text": "Introduction. Linear regression and logistic regression are two types of regression analysis techniques that are used to solve the regression problem using machine learning. They are the most prominent techniques of regression."}]}, {"question": "What are the methods of data analysis", "positive_ctxs": [{"text": "Data analysis has two prominent methods: qualitative research and quantitative research. Each method has their own techniques. Interviews and observations are forms of qualitative research, while experiments and surveys are quantitative research."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Kmeans clustering algorithm is applied to reduced datasets which is done by principal component analysis dimension reduction method. Cluster analysis is one of the major data analysis methods widely used for many practical applications in emerging areas[12]."}, {"text": "Another common example of univariate analysis is the mean of a population distribution. Tables, charts, polygons, and histograms are all popular methods for displaying univariate analysis of a specific variable (e.g. mean, median, mode, standard variation, range, etc)."}, {"text": "Adaptive learning rate methods are an optimization of gradient descent methods with the goal of minimizing the objective function of a network by using the gradient of the function and the parameters of the network."}, {"text": "Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values."}, {"text": "Several methods could be used to measure the performance of the classification model. Some of them are log-loss, AUC, confusion matrix, and precision-recall. Accuracy is the measure of correct prediction of the classifier compared to the overall data points."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is single label classification", "positive_ctxs": [{"text": "As seen Table 1, for the single label classification, labels (category) are mutually exclusive and each instance is assigned to only one category. On the other hand, in the multi-label classification, the labels are interrelated and each instance corresponds to multiple class labels ( Table 2)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Fundamentally, classification is about predicting a label and regression is about predicting a quantity.  That classification is the problem of predicting a discrete class label output for an example. That regression is the problem of predicting a continuous quantity output for an example."}, {"text": "Basically, there are three methods to solve a multi-label classification problem, namely: Problem Transformation. Adapted Algorithm.1 Binary Relevance. This is the simplest technique, which basically treats each label as a separate single class classification problem.  2 Classifier Chains.  3 Label Powerset."}, {"text": "Difference between multi-class classification & multi-label classification is that in multi-class problems the classes are mutually exclusive, whereas for multi-label problems each label represents a different classification task, but the tasks are somehow related."}, {"text": "Classification Algorithms in Data Mining. It is one of the Data Mining. That is used to analyze a given data set and takes each instance of it. It assigns this instance to a particular class.  So classification is the process to assign class label from a data set whose class label is unknown."}, {"text": "Imbalanced classification refers to a classification predictive modeling problem where the number of examples in the training dataset for each class label is not balanced. That is, where the class distribution is not equal or close to equal, and is instead biased or skewed."}, {"text": "Binary cross-entropy is for multi-label classifications, whereas categorical cross entropy is for multi-class classification where each example belongs to a single class."}, {"text": "\u201cThe distinction between white label and private label are subtle,\u201d he writes. \u201cThat's why these terms are so easily confused. Private label is a brand sold exclusively in one retailer, for example, Equate (WalMart). White label is a generic product, which is sold to multiple retailers like generic ibuprofen (Advil).\u201d"}]}, {"question": "What are the two layers of a restricted Boltzmann machine called", "positive_ctxs": [{"text": "Restricted Boltzmann Machines are shallow, two-layer neural nets that constitute the building blocks of deep-belief networks. The first layer of the RBM is called the visible, or input layer, and the second is the hidden layer. Each circle represents a neuron-like unit called a node."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  Restricted Boltzmann machines can also be used in deep learning networks."}, {"text": "A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  Restricted Boltzmann machines can also be used in deep learning networks."}, {"text": "RBMs were invented by Geoffrey Hinton and can be used for dimensionality reduction, classification, regression, collaborative filtering, feature learning, and topic modeling. RBMs are a special class of Boltzmann Machines and they are restricted in terms of the connections between the visible and the hidden units."}, {"text": "Word2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, word2vec trains words against other words that neighbor them in the input corpus."}, {"text": "A deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables. It is a network of symmetrically coupled stochastic binary units. It comprises a set of visible units and layers of hidden units ."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}]}, {"question": "What is the difference between the bias and variance of an estimator", "positive_ctxs": [{"text": "This implies that bias and variance of an estimator are complementary to each other i.e. an estimator with high bias will vary less(have low variance) and an estimator with high variance will have less bias(as it can vary more to fit/explain/estimate the data points)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}, {"text": "To use the more formal terms for bias and variance, assume we have a point estimator \u02c6\u03b8 of some parameter or function \u03b8. Then, the bias is commonly defined as the difference between the expected value of the estimator and the parameter that we want to estimate: Bias=E[\u02c6\u03b8]\u2212\u03b8."}, {"text": "To use the more formal terms for bias and variance, assume we have a point estimator \u02c6\u03b8 of some parameter or function \u03b8. Then, the bias is commonly defined as the difference between the expected value of the estimator and the parameter that we want to estimate: Bias=E[\u02c6\u03b8]\u2212\u03b8."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated.  Consistent estimators converge in probability to the true value of the parameter, but may be biased or unbiased; see bias versus consistency for more."}, {"text": "Try to see the difference between an estimator and an estimate. An estimator is a random variable and an estimate is a number (that is the computed value of the estimator).  Similarly, the sample median would be a natural point estimator for the population median."}]}, {"question": "Is mean a biased estimator", "positive_ctxs": [{"text": "A statistic is biased if the long-term average value of the statistic is not the parameter it is estimating. More formally, a statistic is biased if the mean of the sampling distribution of the statistic is not equal to the parameter.  Therefore the sample mean is an unbiased estimate of \u03bc."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "and is commonly used as an estimator for \u03c3. Nevertheless, S is a biased estimator of \u03c3."}, {"text": "An biased estimator is one which delivers an estimate which is consistently different from the parameter to be estimated. In a more formal definition we can define that the expectation E of a biased estimator is not equal to the parameter of a population."}, {"text": "However, for a general population it is not true that the sample median is an unbiased estimator of the population median. The sample mean is a biased estimator of the population median when the population is not symmetric.  It only will be unbiased if the population is symmetric."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}, {"text": "From Wikipedia, the free encyclopedia. In statistics and signal processing, a minimum mean square error (MMSE) estimator is an estimation method which minimizes the mean square error (MSE), which is a common measure of estimator quality, of the fitted values of a dependent variable."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}]}, {"question": "How do you find the Iqr with the mean and standard deviation", "positive_ctxs": [{"text": "When working with box plots, the IQR is computed by subtracting the first quartile from the third quartile. In a standard normal distribution (with mean 0 and standard deviation 1), the first and third quartiles are located at -0.67448 and +0.67448 respectively. Thus the interquartile range (IQR) is 1.34896."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean."}, {"text": "The standard deviation is proportional to the mean - , e.g. a mean with 20 may have a std.  When you have hug differences in means and want to compare their variation, it would be better to take the coefficient of variation, because it normalizes the standard deviation with respect to the mean."}, {"text": "Lets do this step by step:Step 1: find the mean.Step 2: fin the standard deviation of the mean (using the population SD)Step 3: find the Z score.Step 4: compare to the critical Z score. From the stated hypothesis, we know that we are dealing with a 1-tailed hypothesis test.  Step 4 : compare to the critical Z score."}, {"text": "Mean, variance, and standard deviation The mean of the sampling distribution of the sample mean will always be the same as the mean of the original non-normal distribution. In other words, the sample mean is equal to the population mean. where \u03c3 is population standard deviation and n is sample size."}, {"text": "How to Calculate a CorrelationFind the mean of all the x-values.Find the standard deviation of all the x-values (call it sx) and the standard deviation of all the y-values (call it sy).  For each of the n pairs (x, y) in the data set, take.Add up the n results from Step 3.Divide the sum by sx \u2217 sy.More items"}, {"text": "The Central Limit Theorem and Means In other words, add up the means from all of your samples, find the average and that average will be your actual population mean. Similarly, if you find the average of all of the standard deviations in your sample, you'll find the actual standard deviation for your population."}, {"text": "The coefficient of variation is a better risk measure than the standard deviation alone because the CV adjusts for the size of the project. The CV measures the standard deviation divided by the mean and therefore puts the standard deviation into context."}]}, {"question": "What does the D stand for in differentiation", "positive_ctxs": [{"text": "The 'd' means a \u0394 in the limit approaching zero. Basically the slope is approximately \u0394y/\u0394x but if you let \u0394x approach zero, you reach the exactly slope which is then dy/dx."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Typical examples are the linear operator of multiplication by and differentiation in ."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "formal parameter \u2014 the identifier used in a method to stand for the value that is passed into the method by a caller. actual parameter \u2014 the actual value that is passed into the method by a caller."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}]}, {"question": "Which indicator is Mohr's method", "positive_ctxs": [{"text": "chromate ions"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The indicator function 1[0,\u221e) is right differentiable at every real a, but discontinuous at zero (note that this indicator function is not left differentiable at zero)."}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "The expectation of Bernoulli random variable implies that since an indicator function of a random variable is a Bernoulli random variable, its expectation equals the probability. Formally, given a set A, an indicator function of a random variable X is defined as, 1A(X) = { 1 if X \u2208 A 0 otherwise ."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The Spearman correlation is the same as the Pearson correlation, but it is used on data from an ordinal scale. Which situation would be appropriate for obtaining a phi-coefficient with a Pearson test?"}, {"text": "An indicator random variable is a special kind of random variable associated with the occurence of an event. The indicator random variable IA associated with event A has value 1 if event A occurs and has value 0 otherwise. In other words, IA maps all outcomes in the set A to 1 and all outcomes outside A to 0."}, {"text": "An indicator random variable is a special kind of random variable associated with the occurence of an event. The indicator random variable IA associated with event A has value 1 if event A occurs and has value 0 otherwise. In other words, IA maps all outcomes in the set A to 1 and all outcomes outside A to 0."}]}, {"question": "Is class width the same as class interval", "positive_ctxs": [{"text": "A frequency distribution is a table that shows \u201cclasses\u201d or \u201cintervals\u201d of data entries with a count of the number of entries in each class. The frequency f of a class is the number of data entries in the class.  The \u201cclass width\u201d is the distance between the lower limits of consecutive classes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In class limit, the upper extreme value of the first class interval and the lower extreme value of the next class interval will not be equal. In class boundary, the upper extreme value of the first class interval and the lower extreme value of the next class interval will be equal."}, {"text": "This is answered by examining the meaning of each term in the phrase: modal means the one that occurs most often (averages: mode), a class interval is the width of one of your groups in the frequency table or, the class interval is what you use when grouping data together, e.g., if you counted the number of pencils in"}, {"text": "there are mainly five types of class interval such as exclusive class interval, inclusive class interval, less than class interval, more than class interval, mid value class interval , which has been discussed."}, {"text": "All the classes may have the same class size or they may have different classes sizes depending on how you group your data. The class interval is always a whole number."}, {"text": "YES. we calculate height of the class interval by dividing the frequency by that class width. That class which has the maximum height will be the modal class, containing the mode."}, {"text": "Definition. The class intervals are the subsets into which the data is grouped. The width of the class intervals will be a compromise between having intervals short enough so that not all of the observations fall in the same interval, but long enough so that you do not end up with only one observation per interval."}, {"text": "Definition. The class intervals are the subsets into which the data is grouped. The width of the class intervals will be a compromise between having intervals short enough so that not all of the observations fall in the same interval, but long enough so that you do not end up with only one observation per interval."}]}, {"question": "Is normalization required for neural networks", "positive_ctxs": [{"text": "Standardizing Neural Network Data.  In theory, it's not necessary to normalize numeric x-data (also called independent data). However, practice has shown that when numeric x-data values are normalized, neural network training is often more efficient, which leads to a better predictor."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks."}, {"text": "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks."}, {"text": "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks."}, {"text": "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks."}, {"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling.  Others sustain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks."}, {"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling."}, {"text": "Batch normalization enables the use of higher learning rates, greatly accelerating the learning process. It also enabled the training of deep neural networks with sigmoid activations that were previously deemed too difficult to train due to the vanishing gradient problem."}]}, {"question": "How do you reduce random error", "positive_ctxs": [{"text": "If you reduce the random error of a data set, you reduce the width (FULL WIDTH AT HALF MAXIMUM) of a distribution, or the counting noise (POISSON NOISE) of a measurement. Usually, you can reduce random error by simply taking more measurements."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use."}, {"text": "How to reduce False Positive and False Negative in binary classificationfirstly random forest overfits if the training data and testing data are not drawn from same distribution.check the data for linearity,multicollinearity ,outliers,etc.More items"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How many parity check bits must be included with the data word to achieve single-bit error correction and double error correction when data words are as follows: 16 bits."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Random forest will reduce variance part of error rather than bias part, so on a given training data set decision tree may be more accurate than a random forest. But on an unexpected validation data set, Random forest always wins in terms of accuracy."}]}, {"question": "What is data frequency table", "positive_ctxs": [{"text": "A data set can also be presented by means of a data frequency table, a table in which each distinct value is listed in the first row and its frequency, which is the number of times the value appears in the data set, is listed below it in the second row."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, a frequency distribution is a list, table or graph that displays the frequency of various outcomes in a sample. Each entry in the table contains the frequency or count of the occurrences of values within a particular group or interval."}, {"text": "A probability frequency distribution is a way to show how often an event will happen. It also shows what the probability of each event happening is. A frequency distribution table can be created by hand, or you can make a frequency distribution table in Excel."}, {"text": "When a table shows relative frequencies for different categories of a categorical variable, it is called a relative frequency table. The first table shows relative frequencies as a proportion, and the second table shows relative frequencies as a percentage."}, {"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}, {"text": "Step 1: Prepare a table containing less than type cumulative frequency with the help of given frequencies. belongs. Class-interval of this cumulative frequency is the median class-interval. Step 3 : Find out the frequency f and lower limit l of this median class."}, {"text": "The cumulative frequency is calculated by adding each frequency from a frequency distribution table to the sum of its predecessors. The last value will always be equal to the total for all observations, since all frequencies will already have been added to the previous total."}, {"text": "The cumulative frequency is calculated by adding each frequency from a frequency distribution table to the sum of its predecessors. The last value will always be equal to the total for all observations, since all frequencies will already have been added to the previous total."}]}, {"question": "Why is rectified linear unit a good activation function", "positive_ctxs": [{"text": "It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.  The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The rectifier is, as of 2017, the most popular activation function for deep neural networks. A unit employing the rectifier is also called a rectified linear unit (ReLU)."}, {"text": "It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.  The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better."}, {"text": "Yes a perceptron (one fully connected unit) can be used for regression. It will just be a linear regressor. If you use no activation function you get a regressor and if you put a sigmoid activation you get a classifier.  That's why the loss function for classification is called \"logistic regression\"."}, {"text": "Collinearity is a linear association between two predictors. Multicollinearity is a situation where two or more predictors are highly linearly related.  But, correlation 'among the predictors' is a problem to be rectified to be able to come up with a reliable model."}, {"text": "ReLU has become the darling activation function of the neural network world. Short for Rectified Linear Unit, it is a piecewise linear function that is defined to be 0 for all negative values of x and equal to a \u00d7 x otherwise, where a is a learnable parameter."}, {"text": "Delta learning does this using the difference between a target activation and an actual obtained activation. Using a linear activation function, network connections are adjusted. Another way to explain the Delta rule is that it uses an error function to perform gradient descent learning."}, {"text": "To predict a continuous value, you need to adjust your model (regardless whether it is Recurrent or Not) to the following conditions:Use a linear activation function for the final layer.Chose an appropriate cost function (square error loss is typically used to measure the error of predicting real values)"}]}, {"question": "How does pruning work in decision trees", "positive_ctxs": [{"text": "In Computer science (especially Machine learning) Pruning means simplifying/compressing and optimizing a Decision tree by removing sections of the tree that are uncritical and redundant to classify instances."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood."}, {"text": "There are two main types of decision trees that are based on the target variable, i.e., categorical variable decision trees and continuous variable decision trees.Categorical variable decision tree.  Continuous variable decision tree.  Assessing prospective growth opportunities.More items"}, {"text": "Decision trees can help organizations structure and automate (complex) information. Decision trees are decision models that answer a specific question based on a question structure and certain conditions."}, {"text": "A decision tree is a non-linear classifier. If your dataset contains consistent samples, namely you don't have the same input features and contradictory labels, decision trees can classify the data entirely and overfit it."}, {"text": "Decision trees are a classic machine learning technique. The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree. By Narendra Nath Joshi, Carnegie Mellon.  The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree."}, {"text": "A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results."}, {"text": "A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results."}]}, {"question": "What is the meaning of the word Quantised", "positive_ctxs": [{"text": "verb (used with object), quan\u00b7tized, quan\u00b7tiz\u00b7ing. Mathematics, Physics. to restrict (a variable quantity) to discrete values rather than to a continuous set of values."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "fastText is another word embedding method that is an extension of the word2vec model. Instead of learning vectors for words directly, fastText represents each word as an n-gram of characters.  This helps capture the meaning of shorter words and allows the embeddings to understand suffixes and prefixes."}, {"text": "fastText is another word embedding method that is an extension of the word2vec model. Instead of learning vectors for words directly, fastText represents each word as an n-gram of characters.  This helps capture the meaning of shorter words and allows the embeddings to understand suffixes and prefixes."}, {"text": "Word sense disambiguation, in natural language processing (NLP), may be defined as the ability to determine which meaning of word is activated by the use of word in a particular context.  Lexical ambiguity, syntactic or semantic, is one of the very first problem that any NLP system faces."}, {"text": "Word vectors are simply vectors of numbers that represent the meaning of a word.  In simpler terms, a word vector is a row of real-valued numbers (as opposed to dummy numbers) where each point captures a dimension of the word's meaning and where semantically similar words have similar vectors."}, {"text": "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word."}, {"text": "Etymologically speaking, it's my understanding that kernel is a modernization of cyrnel (Old English, meaning seed ; it's also the word that corn \"stems\" from, if you'll forgive the pun). A kernel in that context is something from which the rest grows."}, {"text": "In simple words, stemming technique only looks at the form of the word whereas lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word."}]}, {"question": "What is general linear model in SPSS", "positive_ctxs": [{"text": "General linear modeling in SPSS for Windows The general linear model (GLM) is a flexible statistical model that incorporates normally distributed dependent variables and categorical or continuous independent variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Analysis of covariance (ANCOVA) is a general linear model which blends ANOVA and regression.  Mathematically, ANCOVA decomposes the variance in the DV into variance explained by the CV(s), variance explained by the categorical IV, and residual variance."}]}, {"question": "What is the difference between T score and Z score", "positive_ctxs": [{"text": "Difference between Z score vs T score. Z score is a conversion of raw data to a standard score, when the conversion is based on the population mean and population standard deviation.  T score is a conversion of raw data to the standard score when the conversion is based on the sample mean and sample standard deviation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "Definition. A score that is derived from an individual's raw score within a distribution of scores. The standard score describes the difference of the raw score from a sample mean, expressed in standard deviations. Standard scores preserve the absolute differences between scores."}, {"text": "For example, let's say a child received a scaled score of 8, with a 95% confidence interval range of 7-9. This means that with high certainty, the child's true score lies between 7 and 9, even if the received score of 8 is not 100% accurate."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "How do you convert a stationary to a non stationary series", "positive_ctxs": [{"text": "A non-stationary process with a deterministic trend becomes stationary after removing the trend, or detrending. For example, Yt = \u03b1 + \u03b2t + \u03b5t is transformed into a stationary process by subtracting the trend \u03b2t: Yt - \u03b2t = \u03b1 + \u03b5t, as shown in the figure below."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A stationary time series is one whose properties do not depend on the time at which the series is observed. 14. Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times."}, {"text": "How to find accuracy of ARIMA model?Problem description: Prediction on CPU utilization.  Step 1: From Elasticsearch I collected 1000 observations and exported on Python.Step 2: Plotted the data and checked whether data is stationary or not.Step 3: Used log to convert the data into stationary form.Step 4: Done DF test, ACF and PACF.More items\u2022"}, {"text": "In mathematics and statistics, a stationary process (or a strict/strictly stationary process or strong/strongly stationary process) is a stochastic process whose unconditional joint probability distribution does not change when shifted in time.  For many applications strict-sense stationarity is too restrictive."}, {"text": "D refers to the number of differencing transformations required by the time series to get stationary.  Differencing is a method of transforming a non-stationary time series into a stationary one. This is an important step in preparing data to be used in an ARIMA model."}, {"text": "Stationary Time Series Time series are stationary if they do not have trend or seasonal effects. Summary statistics calculated on the time series are consistent over time, like the mean or the variance of the observations."}, {"text": "In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time."}, {"text": "In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time."}]}, {"question": "How are Convolutional Neural Networks revolutionising Computer Vision", "positive_ctxs": [{"text": "Most computer vision algorithms use something called a convolution neural network, or CNN. Like basic feedforward neural networks, CNNs learn from inputs, adjusting their parameters (weights and biases) to make an accurate prediction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep Neural Networks (DNN) have greater capabilities for image pattern recognition and are widely used in Computer Vision algorithms. And, Convolutional Neural Network (CNN, or ConvNet) is a class of DNN which is most commonly applied to analyzing visual imagery."}, {"text": "Convolutional Neural Networks"}, {"text": "Convolutional Neural Networks"}, {"text": "17. Deep Convolutional Network (DCN): Convolutional Neural Networks are neural networks used primarily for classification of images, clustering of images and object recognition."}, {"text": "To teach an algorithm how to recognise objects in images, we use a specific type of Artificial Neural Network: a Convolutional Neural Network (CNN). Their name stems from one of the most important operations in the network: convolution. Convolutional Neural Networks are inspired by the brain."}, {"text": "Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. ConvNets have been successful in identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars."}, {"text": "6 Types of Artificial Neural Networks Currently Being Used in Machine LearningFeedforward Neural Network \u2013 Artificial Neuron:  Radial basis function Neural Network:  Kohonen Self Organizing Neural Network:  Recurrent Neural Network(RNN) \u2013 Long Short Term Memory:  Convolutional Neural Network:  Modular Neural Network:"}]}, {"question": "What is ingroup derogation", "positive_ctxs": [{"text": "While the previous study (Wu et al., 2015) suggests that ingroup derogation is a specialized mechanism which disregards explicit disease-relevant information mediated by outgroup members, a different pattern was observed in Experiment 2."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is root mean square error", "positive_ctxs": [{"text": "Root mean squared error (RMSE) is the square root of the mean of the square of all of the error. RMSE is a good measure of accuracy, but only to compare prediction errors of different models or model configurations for a particular variable and not between variables, as it is scale-dependent."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A kind of average sometimes used in statistics and engineering, often abbreviated as RMS. To find the root mean square of a set of numbers, square all the numbers in the set and then find the arithmetic mean of the squares. Take the square root of the result. This is the root mean square."}, {"text": "From Wikipedia, the free encyclopedia. In statistics and signal processing, a minimum mean square error (MMSE) estimator is an estimation method which minimizes the mean square error (MSE), which is a common measure of estimator quality, of the fitted values of a dependent variable."}, {"text": "The standard deviation of this set of mean values is the standard error. In lieu of taking many samples one can estimate the standard error from a single sample. This estimate is derived by dividing the standard deviation by the square root of the sample size."}, {"text": "In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."}, {"text": "In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "Statistical Analysis The root mean square error (RMSE), which is the sample standard deviation of the differences between predicted and observed values, with results in the same unit of measure of observed values.  the correlation coefficient (r) as a measure of the degree of association among data."}]}, {"question": "What is w2v", "positive_ctxs": [{"text": "Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is quartile deviation with example", "positive_ctxs": [{"text": "When one takes half of the difference or variance between the 3rd and the 1st quartiles of a simple distribution or frequency distribution it is quartile deviation. The quartile deviation formula is. Q.D. = Q3-Q1/ 2. Example \u2013 Quartiles are values that divide a list of numbers into quarters."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Standard deviation is the deviation from the mean, and a standard deviation is nothing but the square root of the variance. Mean is an average of all set of data available with an investor or company. Standard deviation used for measuring the volatility of a stock.  Standard deviation is easier to picture and apply."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Definition. Multi-label learning is an extension of the standard supervised learning setting. In contrast to standard supervised learning where one training example is associated with a single class label, in multi-label learning, one training example is associated with multiple class labels simultaneously."}, {"text": "The median divides the data into a lower half and an upper half. The lower quartile is the middle value of the lower half. The upper quartile is the middle value of the upper half. The following figure shows the median, quartiles and interquartile range."}, {"text": "The interquartile range is the difference between the third quartile and the first quartile in a data set, giving the middle 50%. The interquartile range is a measure of spread; it's used to build box plots, determine normal distributions and as a way to determine outliers."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "What is common sense in artificial intelligence", "positive_ctxs": [{"text": "In artificial intelligence research, commonsense knowledge consists of facts about the everyday world, such as \"Lemons are sour\", that all humans are expected to know.  Common sense knowledge also helps to solve problems in the face of incomplete information."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action. Some academic sources reserve the term \"strong AI\" for machines that can experience consciousness."}, {"text": "It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action. Some academic sources reserve the term \"strong AI\" for machines that can experience consciousness."}, {"text": "Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subtopic of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem."}, {"text": "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception."}]}, {"question": "What is motivation short note", "positive_ctxs": [{"text": "Motivation is the reason for people's actions, willingness and goals. Motivation is derived from the word motive which is defined as a need that requires satisfaction. These needs could be wants or desires that are acquired through influence of culture, society, lifestyle, etc. or generally innate."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In stratified sampling, a random sample is drawn from each of the strata, whereas in cluster sampling only the selected clusters are sampled. A common motivation of cluster sampling is to reduce costs by increasing sampling efficiency."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "The mean of the negative binomial distribution with parameters r and p is rq / p, where q = 1 \u2013 p. The variance is rq / p2. The simplest motivation for the negative binomial is the case of successive random trials, each having a constant probability P of success."}]}, {"question": "What is the trigger for causal analysis and resolution", "positive_ctxs": [{"text": "The Causal Analysis and Resolution process area involves the following activities: Identifying and analyzing causes of selected outcomes. The selected outcomes can represent defects and problems that can be prevented from happening in the future or successes that can be implemented in projects or the organization."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Criteria for CausalityStrength: A relationship is more likely to be causal if the correlation coefficient is large and statistically significant.Consistency: A relationship is more likely to be causal if it can be replicated.More items\u2022"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Spectral analysis is the process of breaking down a signal into its components at various frequencies, and in the context of acoustics there are two very different ways of doing this, depending on whether the result is desired on a linear frequency scale with constant resolution (in Hz) or on a logarithmic frequency"}, {"text": "Augmented reality uses existing reality and physical objects to trigger computer-generated enhancements over the top of reality, in real time. Essentially, AR is a technology that lays computer-generated images over a user's view of the real world. These images typically take shape as 3D models, videos and information."}, {"text": "While the multivariable model is used for the analysis with one outcome (dependent) and multiple independent (a.k.a., predictor or explanatory) variables,2,3 multivariate is used for the analysis with more than 1 outcomes (eg, repeated measures) and multiple independent variables."}, {"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}, {"text": "Coreference resolution is the task of finding all expressions that refer to the same entity in a text. It is an important step for a lot of higher level NLP tasks that involve natural language understanding such as document summarization, question answering, and information extraction."}]}, {"question": "How do you calculate K means clustering", "positive_ctxs": [{"text": "Essentially, the process goes as follows:Select k centroids. These will be the center point for each segment.Assign data points to nearest centroid.Reassign centroid value to be the calculated mean value for each cluster.Reassign data points to nearest centroid.Repeat until data points stay in the same cluster."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows\u2022"}, {"text": "A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical."}, {"text": "K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups).  The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided."}, {"text": "The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.  The results of the K-means clustering algorithm are: The centroids of the K clusters, which can be used to label new data. Labels for the training data (each data point is assigned to a single cluster)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}]}, {"question": "Whats the difference between a Markov chain and a random walk", "positive_ctxs": [{"text": "Markov chains and random walks are examples of random processes i.e. an indexed collection of random variables.  Markov chains and random walks are examples of random processes i.e. an indexed collection of random variables. A random walk is a specific kind of random process made up of a sum of iid random variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "Among the trademarks of the Bayesian approach, Markov chain Monte Carlo methods are especially mysterious.  So, what are Markov chain Monte Carlo (MCMC) methods? The short answer is: MCMC methods are used to approximate the posterior distribution of a parameter of interest by random sampling in a probabilistic space."}, {"text": "In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain."}, {"text": "In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain."}, {"text": "Markov Chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its stationary distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution."}, {"text": "Markovian is an adjective that may describe: In probability theory and statistics, subjects named for Andrey Markov: A Markov chain or Markov process, a stochastic model describing a sequence of possible events. The Markov property, the memoryless property of a stochastic process."}]}, {"question": "What is decision boundary in logistic regression", "positive_ctxs": [{"text": "The decision boundary Let's suppose we define a line that is equal to zero along this decision boundary.  For example, in the following graph, z=6\u2212x1 represents a decision boundary for which any values of x1>6 will return a negative value for z and any values of x1<6 will return a positive value for z."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Naive Bayes is a linear classifier Naive Bayes leads to a linear decision boundary in many common cases.  The red decision line indicates the decision boundary where P(y=1|x)=P(y=2|x)."}, {"text": "First, Cross-entropy (or softmax loss, but cross-entropy works better) is a better measure than MSE for classification, because the decision boundary in a classification task is large (in comparison with regression).  For regression problems, you would almost always use the MSE."}, {"text": "First, Cross-entropy (or softmax loss, but cross-entropy works better) is a better measure than MSE for classification, because the decision boundary in a classification task is large (in comparison with regression).  For regression problems, you would almost always use the MSE."}, {"text": "The most basic way to use a SVC is with a linear kernel, which means the decision boundary is a straight line (or hyperplane in higher dimensions)."}, {"text": "Non-linearity is needed in activation functions because its aim in a neural network is to produce a nonlinear decision boundary via non-linear combinations of the weight and inputs."}, {"text": "Logistic regression is known and used as a linear classifier. It is used to come up with a hyperplane in feature space to separate observations that belong to a class from all the other observations that do not belong to that class. The decision boundary is thus linear.13\u200f/03\u200f/2019"}, {"text": "A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous. If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable. Decision boundaries are not always clear cut."}]}, {"question": "Why do we use weighted least squares", "positive_ctxs": [{"text": "Like all of the least squares methods discussed so far, weighted least squares is an efficient method that makes good use of small data sets. It also shares the ability to provide different types of easily interpretable statistical intervals for estimation, prediction, calibration and optimization."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Weighted least squares (WLS), also known as weighted linear regression, is a generalization of ordinary least squares and linear regression in which the errors covariance matrix is allowed to be different from an identity matrix."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Alternative procedures include: Different linear model: fitting a linear model with additional X variable(s) Nonlinear model: fitting a nonlinear model when the linear model is inappropriate.  Weighted least squares linear regression: dealing with unequal variances in Y by performing a weighted least squares fit."}, {"text": "Ordinary least squares assumes things like equal variance of the noise at every x location. Generalized least squares does not assume a diagonal co-variance matrix."}, {"text": "The least squares approach limits the distance between a function and the data points that the function explains. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data. Mathematicians use the least squares method to arrive at a maximum-likelihood estimate."}, {"text": "The least squares approach limits the distance between a function and the data points that the function explains. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data. Mathematicians use the least squares method to arrive at a maximum-likelihood estimate."}, {"text": "linear_model . LinearRegression. Ordinary least squares Linear Regression."}]}, {"question": "What is the difference between machine learning and neural networks", "positive_ctxs": [{"text": "Machine Learning is a set of algorithms that parse data and learns from the parsed data and use those learnings to discover patterns of interest. Neural Network or Artificial Neural Network is one set of algorithms used in machine learning for modeling the data using graphs of Neurons."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}, {"text": "Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. In fact, it is the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three."}, {"text": "The ways in which they function Another fundamental difference between traditional computers and artificial neural networks is the way in which they function. While computers function logically with a set of rules and calculations, artificial neural networks can function via images, pictures, and concepts."}, {"text": "Learning Rate and Gradient Descent Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}, {"text": "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}, {"text": "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}]}, {"question": "What is the dependent and outcome variable", "positive_ctxs": [{"text": "A dependent variable is a variable whose value depends upon independent variable s. The dependent variable is what is being measured in an experiment or evaluated in a mathematical equation. The dependent variable is sometimes called \"the outcome variable.\""}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "You can think of independent and dependent variables in terms of cause and effect: an independent variable is the variable you think is the cause, while a dependent variable is the effect. In an experiment, you manipulate the independent variable and measure the outcome in the dependent variable."}, {"text": "The outcome variable and dependent variable are used synonymously. However, they are not exactly the same: the outcome variable is defined as the presumed effect in a non-experimental study, where the dependent variable is the presumed effect in an experimental study1."}, {"text": "The outcome variable is also called the response or dependent variable, and the risk factors and confounders are called the predictors, or explanatory or independent variables. In regression analysis, the dependent variable is denoted \"Y\" and the independent variables are denoted by \"X\"."}, {"text": "The outcome variable is also called the response or dependent variable, and the risk factors and confounders are called the predictors, or explanatory or independent variables. In regression analysis, the dependent variable is denoted \"Y\" and the independent variables are denoted by \"X\"."}, {"text": "The outcome variable is also called the response or dependent variable, and the risk factors and confounders are called the predictors, or explanatory or independent variables. In regression analysis, the dependent variable is denoted \"Y\" and the independent variables are denoted by \"X\"."}, {"text": "The outcome variable is also called the response or dependent variable, and the risk factors and confounders are called the predictors, or explanatory or independent variables. In regression analysis, the dependent variable is denoted \"Y\" and the independent variables are denoted by \"X\"."}, {"text": "The independent variable is the variable the experimenter changes or controls and is assumed to have a direct effect on the dependent variable.  The dependent variable is the variable being tested and measured in an experiment, and is 'dependent' on the independent variable."}]}, {"question": "What is the difference between an odds ratio and a hazard ratio", "positive_ctxs": [{"text": "In logistic regression, an odds ratio of 2 means that the event is 2 time more probable given a one-unit increase in the predictor. In Cox regression, a hazard ratio of 2 means the event will occur twice as often at each time point given a one-unit increase in the predictor."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To conclude, the important thing to remember about the odds ratio is that an odds ratio greater than 1 is a positive association (i.e., higher number for the predictor means group 1 in the outcome), and an odds ratio less than 1 is negative association (i.e., higher number for the predictor means group 0 in the outcome"}, {"text": "To conclude, the important thing to remember about the odds ratio is that an odds ratio greater than 1 is a positive association (i.e., higher number for the predictor means group 1 in the outcome), and an odds ratio less than 1 is negative association (i.e., higher number for the predictor means group 0 in the outcome"}, {"text": "The difference between a ratio scale and an interval scale is that the zero point on an interval scale is some arbitrarily agreed value, whereas on a ratio scale it is a true zero."}, {"text": "If the hazard ratio is less than 1, then the predictor is protective (i.e., associated with improved survival) and if the hazard ratio is greater than 1, then the predictor is associated with increased risk (or decreased survival)."}, {"text": "An odds ratio (OR) is a measure of association between an exposure and an outcome. The OR represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure."}, {"text": "From Wikipedia, the free encyclopedia. An odds ratio (OR) is a statistic that quantifies the strength of the association between two events, A and B."}, {"text": "The odds ratio is the measure of association for a case-control study. It tells us how much higher the odds of exposure is among cases of a disease compared with controls. The odds ratio compares the odds of exposure to the factor of interest among cases to the odds of exposure to the factor among controls."}]}, {"question": "What is machine learning how does it fit into predictive analytics", "positive_ctxs": [{"text": "Machine learning is an AI technique where the algorithms are given data and are asked to process without a predetermined set of rules and regulations whereas Predictive analysis is the analysis of historical data as well as existing external data to find patterns and behaviors."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Predictive analytics is the process of using data analytics to make predictions based on data. This process uses data along with analysis, statistics, and machine learning techniques to create a predictive model for forecasting future events."}, {"text": "According to SAS, predictive analytics is \u201cthe use of data, statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data.  In short, predictive intelligence drives marketing decisions.\u201d"}, {"text": "Predictive analytics uses predictors or known features to create predictive models that will be used in obtaining an output. A predictive model is able to learn how different points of data connect with each other. Two of the most widely used predictive modeling techniques are regression and neural networks."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "It's more of an approach than a process. Predictive analytics and machine learning go hand-in-hand, as predictive models typically include a machine learning algorithm. These models can be trained over time to respond to new data or values, delivering the results the business needs."}, {"text": "Definition. Predictive analytics is an area of statistics that deals with extracting information from data and using it to predict trends and behavior patterns.  Predictive analytics statistical techniques include data modeling, machine learning, AI, deep learning algorithms and data mining."}, {"text": "It's more of an approach than a process. Predictive analytics and machine learning go hand-in-hand, as predictive models typically include a machine learning algorithm.  These models are then made up of algorithms. The algorithms perform the data mining and statistical analysis, determining trends and patterns in data."}]}, {"question": "How AI will change the future", "positive_ctxs": [{"text": "In the future, artificial intelligence (AI) is likely to substantially change both marketing strategies and customer behaviors.  Finally, the authors suggest AI will be more effective if it augments (rather than replaces) human managers. AI is going to make our lives better in the future."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A regression model will have unit changes between the x and y variables, where a single unit change in x will coincide with a constant change in y. Taking the log of one or both variables will effectively change the case from a unit change to a percent change."}, {"text": "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use."}, {"text": "Change Detection means updating the DOM whenever data is changed.  In its default strategy, whenever any data is mutated or changed, Angular will run the change detector to update the DOM. In the onPush strategy, Angular will only run the change detector when a new reference is passed to @Input() data."}, {"text": "On a far grander scale, AI is poised to have a major effect on sustainability, climate change and environmental issues. Ideally and partly through the use of sophisticated sensors, cities will become less congested, less polluted and generally more livable. Inroads are already being made."}, {"text": "On a far grander scale, AI is poised to have a major effect on sustainability, climate change and environmental issues. Ideally and partly through the use of sophisticated sensors, cities will become less congested, less polluted and generally more livable. Inroads are already being made."}, {"text": "How to Handle Imbalanced DatasetChange the evaluation matrix. If we apply the wrong evaluation matrix on the imbalanced dataset, it can give us misleading results.  Resample the dataset. Resample means to change the distribution of the imbalance classes in the dataset.  Change the algorithm and approach to the problem."}, {"text": "Some business analysts at claim that AI is a game changer for the personal device market. By 2020, about 60 percent of personal-device technology vendors will depend on AI-enabled Cloud platforms to deliver enhanced functionality and personalized services. AI technology will deliver an \u201cemotional user experience.\u201d"}]}, {"question": "What is the difference between NLU and NLP", "positive_ctxs": [{"text": "NLP is short for natural language processing while NLU is the shorthand for natural language understanding.  They share a common goal of making sense of concepts represented in unstructured data, like language, as opposed to structured data like statistics, actions, etc."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "NLP is short for natural language processing while NLU is the shorthand for natural language understanding. Similarly named, the concepts both deal with the relationship between natural language (as in, what we as humans speak, not what computers understand) and artificial intelligence."}, {"text": "NLP is short for natural language processing while NLU is the shorthand for natural language understanding. Similarly named, the concepts both deal with the relationship between natural language (as in, what we as humans speak, not what computers understand) and artificial intelligence."}, {"text": "Let's Start with NLP and NLG Setting aside NLU for the moment, we can draw a really simple distinction: Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language."}, {"text": "Let's Start with NLP and NLG Setting aside NLU for the moment, we can draw a really simple distinction: Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language."}, {"text": "NLP is short for natural language processing while NLU is the shorthand for natural language understanding.  They share a common goal of making sense of concepts represented in unstructured data, like language, as opposed to structured data like statistics, actions, etc."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}]}, {"question": "What is a grid search and why do we use it in machine learning", "positive_ctxs": [{"text": "Grid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In this blog we will learn what is calibration and why and when we should use it. We calibrate our model when the probability estimate of a data point belonging to a class is very important. Calibration is comparison of the actual output and the expected output given by a system."}, {"text": "The experiment results show that the accuracy of the model performance has a significant improvement by using hyperparameter optimization algorithms. Both Bayesian optimization and grid search perform almost equally well. However, Bayesian optimization runs faster than grid search."}, {"text": "deep learning - a name for an algorithm in machine learning (just like SVM, Regression etc.) transfer learning - as you may know, in order to train a Neural network it might take long time. So, we use a Neural Network that is already trained and in this way we can extract some features of new sample."}, {"text": "Grid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.  In grid searching, you first define the range of values for each of the hyperparameters a1, a2 and a3."}, {"text": "A turing machine is a theoretical machine that computes a function. It is theoretical because it has unlimited tape and time. A human brain is a limited object in space and time, hence we can ignore the unlimited tape requirement."}]}, {"question": "What is the difference between statistics and machine learning", "positive_ctxs": [{"text": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The main difference between these two approaches is the goals (not the methods used). Therefore, Image processing is related to enhancing the image and play with features like colors. While computer vision is related to \"Image Understanding\" and it can use machine learning as well."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Last Updated on Decem. Cross-entropy is commonly used in machine learning as a loss function. Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions."}, {"text": "Last Updated on Decem. Cross-entropy is commonly used in machine learning as a loss function. Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions."}, {"text": "The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or"}]}, {"question": "What is the Matrix Morpheus quote", "positive_ctxs": [{"text": "Morpheus: If real is what you can feel, smell, taste and see, then 'real' is simply electrical signals interpreted by your brain."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Fans believe \"What if I told you\" was said by Morpheus when he was explaining the Matrix to Neo (Keanu Reeves).  Per KnowYourMeme, chances are good the \"What if I told you\" line was merely a reworded take on Morpheus' actual dialogue in the scene: \"Do you want to know what 'it' is?\"."}, {"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "If we want to indicate the uncertainty around the estimate of the mean measurement, we quote the standard error of the mean. The standard error is most useful as a means of calculating a confidence interval. For a large sample, a 95% confidence interval is obtained as the values 1.96\u00d7SE either side of the mean."}, {"text": "The significance of Matrix is - they represent Linear transformations like rotation/scaling. A Matrix is just a stack of numbers - but very special - you can add them and subtract them and multiply them [restrictions]. The significance of Matrix is - they represent Linear transformations like rotation/scaling."}]}, {"question": "How do you convert probability to Z score", "positive_ctxs": [{"text": "The first thing you do is use the z-score formula to figure out what the z-score is. In this case, it is the difference between 30 and 21, which is 9, divided by the standard deviation of 5, which gives you a z-score of 1.8. If you look at the z-table below, that gives you a probability value of 0.9641."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}]}, {"question": "Why is batch size 32", "positive_ctxs": [{"text": "A batch size of 32 means that 32 samples from the training dataset will be used to estimate the error gradient before the model weights are updated."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration. The batch size can be one of three options: batch mode: where the batch size is equal to the total dataset thus making the iteration and epoch values equivalent."}, {"text": "Given that very large datasets are often used to train deep learning neural networks, the batch size is rarely set to the size of the training dataset. Smaller batch sizes are used for two main reasons: Smaller batch sizes are noisy, offering a regularizing effect and lower generalization error."}, {"text": "The batch size limits the number of samples to be shown to the network before a weight update can be performed. This same limitation is then imposed when making predictions with the fit model. Specifically, the batch size used when fitting your model controls how many predictions you must make at a time."}, {"text": "Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration.  Usually, a number that can be divided into the total dataset size. stochastic mode: where the batch size is equal to one."}, {"text": "This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration. The sample is randomly shuffled and selected for performing the iteration."}, {"text": "Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process."}, {"text": "Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process."}]}, {"question": "How does sentiment analysis work generally", "positive_ctxs": [{"text": "Sentiment analysis \u2013 otherwise known as opinion mining \u2013 is a much bandied about but often misunderstood term. In essence, it is the process of determining the emotional tone behind a series of words, used to gain an understanding of the the attitudes, opinions and emotions expressed within an online mention."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Sentiment analysis is the automated process of analyzing text data and sorting it into sentiments positive, negative, or neutral. Using sentiment analysis tools to analyze opinions in Twitter data can help companies understand how people are talking about their brand."}, {"text": "Feature extraction identifies those product aspects which are being commented by customers, sentiment prediction identifies the text containing sentiment or opinion by deciding sentiment polarity as positive, negative or neutral and finally summarization module aggregates the results obtained from previous two steps."}, {"text": "It depends on the data you want and the project you're doing. You could use even your twitter data for sentiment analysis. Request your archive in twitter -> download -> analyse sentiment through supervised learning techniques."}, {"text": "The term that does not apply to cluster analysis is factorization. Cluster analysis is a way of grouping data, based on obvious similarities. It is also called as classification analysis or numerical taxonomy.  Hierarchical cluster analysis tends to build a hierarchy within clusters."}, {"text": "How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression."}, {"text": "How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression."}, {"text": "How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression."}]}, {"question": "What is data augmentation in deep learning", "positive_ctxs": [{"text": "Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The performance of deep learning neural networks often improves with the amount of data available. Data augmentation is a technique to artificially create new training data from existing training data. This means, variations of the training set images that are likely to be seen by the model."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "Others tried to use deep learning to solve problems that were beyond its scope.  But according to famous data scientist and deep learning researcher Jeremy Howard, the \u201cdeep learning is overhyped\u201d argument is a bit\u2014 well\u2014overhyped."}, {"text": "Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks."}, {"text": "Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks."}]}, {"question": "What is computer vision and image processing", "positive_ctxs": [{"text": "Computer Vision. Image processing is mainly focused on processing the raw input images to enhance them or preparing them to do other tasks. Computer vision is focused on extracting information from the input images or videos to have a proper understanding of them to predict the visual input like human brain."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts occurrences of gradient orientation in localized portions of an image."}, {"text": "The main difference between these two approaches is the goals (not the methods used). Therefore, Image processing is related to enhancing the image and play with features like colors. While computer vision is related to \"Image Understanding\" and it can use machine learning as well."}, {"text": "In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects).  Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images."}, {"text": "Image annotation is the process of manually defining regions in an image and creating text-based descriptions of those regions.  You can use the following image annotation tools to quickly and accurately build the ground truth for your computer vision models."}, {"text": "Edge detection is an image processing technique for finding the boundaries of objects within images. It works by detecting discontinuities in brightness. Edge detection is used for image segmentation and data extraction in areas such as image processing, computer vision, and machine vision."}, {"text": "The Sobel operator, sometimes called the Sobel\u2013Feldman operator or Sobel filter, is used in image processing and computer vision, particularly within edge detection algorithms where it creates an image emphasising edges."}, {"text": "Object recognition is a computer vision technique for identifying objects in images or videos. Object recognition is a key output of deep learning and machine learning algorithms.  The goal is to teach a computer to do what comes naturally to humans: to gain a level of understanding of what an image contains."}]}, {"question": "How do you calculate lift in association rules", "positive_ctxs": [{"text": "Lift can be found by dividing the confidence by the unconditional probability of the consequent, or by dividing the support by the probability of the antecedent times the probability of the consequent, so: The lift for Rule 1 is (3/4)/(4/7) = (3*7)/(4 * 4) = 21/16 \u2248 1.31."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "In data mining, association rules are useful for analyzing and predicting customer behavior. They play an important part in customer analytics, market basket analysis, product clustering, catalog design and store layout. Programmers use association rules to build programs capable of machine learning."}, {"text": "In data science, association rules are used to find correlations and co-occurrences between data sets. They are ideally used to explain patterns in data from seemingly independent information repositories, such as relational databases and transactional databases."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Definition: To give a helpful lift up to someone, either physically or emotionally. This phrasal verb means to lift someone up to reach a higher point. This can be physically, if someone cannot reach something, or emotionally, if someone needs a boost, or increase, in confidence or morale."}, {"text": "Association Rule Mining, as the name suggests, association rules are simple If/Then statements that help discover relationships between seemingly independent relational databases or other data repositories. Most machine learning algorithms work with numeric datasets and hence tend to be mathematical."}, {"text": "The Apriori algorithm is used for mining frequent itemsets and devising association rules from a transactional database. The parameters \u201csupport\u201d and \u201cconfidence\u201d are used. Support refers to items' frequency of occurrence; confidence is a conditional probability. Items in a transaction form an item set."}]}, {"question": "What is the difference between chi square goodness of fit and homogeneity", "positive_ctxs": [{"text": "Summary: Goodness of Fit: used to compare a single sample proportion against a publicized model. Homogeneity: used to examine whether things have changed or stayed the same or whether the proportions that exist between two populations are the same, or when comparing data from MULTIPLE samples."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The chi-square test is the most commonly used to test the goodness of fit tests and is used for discrete distributions like the binomial distribution and the Poisson distribution, whereas The Kolmogorov-Smirnov and Anderson-Darling goodness of fit tests are used for continuous distributions."}, {"text": "In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution."}, {"text": "In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution."}, {"text": "There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "MSE is the average of the squared error that is used as the loss function for least squares regression: It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE."}]}, {"question": "How long do neural networks take to train", "positive_ctxs": [{"text": "It might take about 2-4 hours of coding and 1-2 hours of training if done in Python and Numpy (assuming sensible parameter initialization and a good set of hyperparameters). No GPU required, your old but gold CPU on a laptop will do the job. Longer training time is expected if the net is deeper than 2 hidden layers."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The reason why Convolutional Neural Networks (CNNs) do so much better than classic neural networks on images and videos is that the convolutional layers take advantage of inherent properties of images. Simple feedforward neural networks don't see any order in their inputs."}, {"text": "LSTM stands for long short term memory. It is a model or architecture that extends the memory of recurrent neural networks. Typically, recurrent neural networks have 'short term memory' in that they use persistent previous information to be used in the current neural network."}, {"text": "Naive Bayes Classifier example by hand and how to do in Scikit-Learn, You can use any kind of predictor in a naive Bayes classifier, as long as you can specify a discriminative linear models take a mixture of categorical and continuous predictors."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Nucleus is a library of Python and C++ code designed to make it easy to read, write and analyze data in common genomics file formats like SAM and VCF. A library from DeepMind for constructing neural networks. A learning framework to train neural networks by leveraging structured signals in addition to feature inputs."}, {"text": "If you don't have enough time to read through the entire post, the following hits on the key components: Bag-of-words: How to break up long text into individual words. Filtering: Different approaches to remove uninformative words. Bag of n-grams: Retain some context by breaking long text into sequences of words."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}]}, {"question": "How does a neural network function", "positive_ctxs": [{"text": "The basic idea behind a neural network is to simulate (copy in a simplified but reasonably faithful way) lots of densely interconnected brain cells inside a computer so you can get it to learn things, recognize patterns, and make decisions in a humanlike way."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "8 Radial Basis Function Networks. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems.  An RBF network is a type of feed forward neural network composed of three layers, namely the input layer, the hidden layer and the output layer."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "7.2. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems. Radial basis function networks are distinguished from other neural networks due to their universal approximation and faster learning speed."}, {"text": "Hidden layers allow for the function of a neural network to be broken down into specific transformations of the data. Each hidden layer function is specialized to produce a defined output."}, {"text": "The main downside was that it was a pretty large network in terms of the number of parameters to be trained. VGG-19 neural network which is bigger then VGG-16, but because VGG-16 does almost as well as the VGG-19 a lot of people will use VGG-16."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}]}, {"question": "What is a good log loss score", "positive_ctxs": [{"text": "The bolder the probabilities, the better will be your Log Loss \u2014 closer to zero. It is a measure of uncertainty (you may call it entropy), so a low Log Loss means a low uncertainty/entropy of your model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1.  So predicting a probability of . 012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0."}, {"text": "For a binary classification like our example, the typical loss function is the binary cross-entropy / log loss."}, {"text": "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.  As the predicted probability decreases, however, the log loss increases rapidly."}, {"text": "Answer. The low value of loss function determines whether a model is a good fit for the datasets."}, {"text": "In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs). For an intended output t = \u00b11 and a classifier score y, the hinge loss of the prediction y is defined as."}, {"text": "An IQ (Intelligence Quotient) score from a standardized test of intelligences is a good example of an interval scale score.  IQ scores are created so that a score of 100 represents the average IQ of the population and the standard deviation (or average variability) of scores is 15."}, {"text": "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label."}]}, {"question": "Which is a more acceptable term binary variable or dummy variable", "positive_ctxs": [{"text": "The terms dummy variable and binary variable are sometimes used interchangeably. However, they are not exactly the same thing.  If your dummy variable has only two options, like 1=Male and 2=female, then that dummy variable is also a binary variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The definition of a dummy dependent variable model is quite simple: If the dependent, response, left-hand side, or Y variable is a dummy variable, you have a dummy dependent variable model. The reason dummy dependent variable models are important is that they are everywhere."}, {"text": "A dummy variable is a numerical variable used in regression analysis to represent subgroups of the sample in your study. In research design, a dummy variable is often used to distinguish different treatment groups."}, {"text": "A dummy variable (aka, an indicator variable) is a numeric variable that represents categorical data, such as gender, race, political affiliation, etc.  For example, suppose we are interested in political affiliation, a categorical variable that might assume three values - Republican, Democrat, or Independent."}, {"text": "Binary Variables A simple version of a categorical variable is called a binary variable. This type of variable lists two distinct, mutually exclusive choices. True-or-false and yes-or-no questions are examples of binary variables."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}]}, {"question": "What is Sobel edge detection in image processing", "positive_ctxs": [{"text": "Sobel Filter. The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image. It finds the direction of the largest increase from light to dark and the rate of change in that direction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Sobel operator, sometimes called the Sobel\u2013Feldman operator or Sobel filter, is used in image processing and computer vision, particularly within edge detection algorithms where it creates an image emphasising edges."}, {"text": "Image processing techniques use filters to enhance an image. Their main applications are to transform the contrast, brightness, resolution and noise level of an image. Contouring, image sharpening, blurring, embossing and edge detection are typical image processing functions (see Table 4.1)."}, {"text": "The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image.  The result of applying it to a pixel on an edge is a vector that points across the edge from darker to brighter values."}, {"text": "The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image)."}, {"text": "The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image)."}, {"text": "The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image)."}, {"text": "The Laplacian of an image highlights regions of rapid intensity change and is therefore often used for edge detection (see zero crossing edge detectors).  The operator normally takes a single graylevel image as input and produces another graylevel image as output."}]}, {"question": "What is cluster example", "positive_ctxs": [{"text": "An example of cluster sampling is area sampling or geographical cluster sampling. Each cluster is a geographical area. Because a geographically dispersed population can be expensive to survey, greater economy than simple random sampling can be achieved by grouping several respondents within a local area into a cluster."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other."}, {"text": "Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other."}]}, {"question": "What is pruning in decision trees Why is it important", "positive_ctxs": [{"text": "Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood."}, {"text": "SVMs and decision trees are discriminative models because they learn explicit boundaties between classes. SVM is a maximal margin classifier, meaning that it learns a decision boundary that maximizes the distance between samples of the two classes, given a kernel."}, {"text": "Decision trees are a classic machine learning technique. The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree. By Narendra Nath Joshi, Carnegie Mellon.  The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree."}, {"text": "A decision tree is a non-linear classifier. If your dataset contains consistent samples, namely you don't have the same input features and contradictory labels, decision trees can classify the data entirely and overfit it."}, {"text": "The fundamental reason to use a random forest instead of a decision tree is to combine the predictions of many decision trees into a single model. The logic is that a single even made up of many mediocre models will still be better than one good model."}, {"text": "Decision trees can handle both categorical and numerical variables at the same time as features, there is not any problem in doing that. Edit: Every split in a decision tree is based on a feature. If the feature is categorical, the split is done with the elements belonging to a particular class."}, {"text": "A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results."}]}, {"question": "How does Bayesian inference work", "positive_ctxs": [{"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.  Bayesian updating is particularly important in the dynamic analysis of a sequence of data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.  Bayesian updating is particularly important in the dynamic analysis of a sequence of data."}, {"text": "The difference between MLE/MAP and Bayesian inference MLE gives you the value which maximises the Likelihood P(D|\u03b8). And MAP gives you the value which maximises the posterior probability P(\u03b8|D).  MLE and MAP returns a single fixed value, but Bayesian inference returns probability density (or mass) function."}, {"text": "Bayesian inference is a machine learning model not as widely used as deep learning or regression models."}, {"text": "Bayesian Model Averaging (BMA) is an application of Bayesian inference to the problems of model selection, combined estimation and prediction that produces a straightforward model choice criteria and less risky predictions."}]}, {"question": "What is discriminator loss", "positive_ctxs": [{"text": "Critic Loss: D(x) - D(G(z)) The discriminator tries to maximize this function. In other words, it tries to maximize the difference between its output on real instances and its output on fake instances."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Minimax GAN loss refers to the minimax simultaneous optimization of the discriminator and generator models. Minimax refers to an optimization strategy in two-player turn-based games for minimizing the loss or cost for the worst case of the other player."}, {"text": "The discriminator in a GAN is simply a classifier. It tries to distinguish real data from the data created by the generator. It could use any network architecture appropriate to the type of data it's classifying. Figure 1: Backpropagation in discriminator training."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The Generative Adversarial Network, or GAN, is an architecture that makes effective use of large, unlabeled datasets to train an image generator model via an image discriminator model. The discriminator model can be used as a starting point for developing a classifier model in some cases."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The second reason you may see validation loss lower than training loss is due to how the loss value are measured and reported: Training loss is measured during each epoch. While validation loss is measured after each epoch."}]}, {"question": "What is the difference between gradient descent and stochastic gradient descent", "positive_ctxs": [{"text": "In Gradient Descent or Batch Gradient Descent, we use the whole training data per epoch whereas, in Stochastic Gradient Descent, we use only single training example per epoch and Mini-batch Gradient Descent lies in between of these two extremes, in which we can use a mini-batch(small portion) of training data per epoch"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning."}, {"text": "According to a senior data scientist, one of the distinct advantages of using Stochastic Gradient Descent is that it does the calculations faster than gradient descent and batch gradient descent. However, gradient descent is the best approach if one wants a speedier result."}, {"text": "Adam optimizer. Implements the Adam optimization algorithm. Adam is a stochastic gradient descent method that computes individual adaptive learning rates for different parameters from estimates of first- and second-order moments of the gradients."}, {"text": "Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data."}, {"text": "Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data."}, {"text": "Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process."}]}, {"question": "How do you find the probability of a histogram", "positive_ctxs": [{"text": "0:003:50Suggested clip \u00b7 105 secondsHow to determine outcomes from a probability histogram - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A probability histogram is a graph that shows the probability of each outcome on the y -axis."}, {"text": "An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value. By looking at the histogram for a specific image a viewer will be able to judge the entire tonal distribution at a glance."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}]}, {"question": "What is greedy agent does a greedy agent always find an optimal policy explain it", "positive_ctxs": [{"text": "This is referred to as a greedy method. Taking the action which the agent estimates to be the best at the current moment is an example of exploitation: the agent is exploiting its current knowledge about the reward structure of the environment to act. There is always at least one such optimal policy[8]."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There is always at least one such optimal policy[8]. The so called greedy policy is following the currently best path of actions. During learning however, for the values to converge into good estimates it is required that the agent visits all available states to gain information about them."}, {"text": "A greedy algorithm is used to construct a Huffman tree during Huffman coding where it finds an optimal solution. In decision tree learning, greedy algorithms are commonly used, however they are not guaranteed to find the optimal solution. One popular such algorithm is the ID3 algorithm for decision tree construction."}, {"text": "It's greedy because you always mark the closest vertex. It's dynamic because distances are updated using previously calculated values. I would say it's definitely closer to dynamic programming than to a greedy algorithm. To find the shortest distance from A to B, it does not decide which way to go step by step."}, {"text": "An environment is everything in the world which surrounds the agent, but it is not a part of an agent itself. An environment can be described as a situation in which an agent is present. The environment is where agent lives, operate and provide the agent with something to sense and act upon it."}, {"text": "Reinforcement learning is an area of Machine Learning.  In the absence of a training dataset, it is bound to learn from its experience. Example: The problem is as follows: We have an agent and a reward, with many hurdles in between. The agent is supposed to find the best possible path to reach the reward."}, {"text": "Epsilon greedy policy is a way of selecting random actions with uniform distribution from a set of available actions.  This policy selects random actions in twice if the value of epsilon is 0.2. Consider a following example, There is a robot with capability to move in 4 direction. Up,down,left,right."}, {"text": "6 Answers. Yes, this is the only difference. On-policy SARSA learns action values relative to the policy it follows, while off-policy Q-Learning does it relative to the greedy policy. Under some common conditions, they both converge to the real value function, but at different rates."}]}, {"question": "What is BFS and DFS", "positive_ctxs": [{"text": "BFS stands for Breadth First Search. DFS stands for Depth First Search. 2. BFS(Breadth First Search) uses Queue data structure for finding the shortest path. DFS(Depth First Search) uses Stack data structure."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "BFS is slower than DFS. DFS is faster than BFS. Time Complexity of BFS = O(V+E) where V is vertices and E is edges. Time Complexity of DFS is also O(V+E) where V is vertices and E is edges."}, {"text": "BFS vs DFS BFS stands for Breadth First Search. DFS stands for Depth First Search.  DFS(Depth First Search) uses Stack data structure. 3. BFS can be used to find single source shortest path in an unweighted graph, because in BFS, we reach a vertex with minimum number of edges from a source vertex."}, {"text": "The only difference between Greedy BFS and A* BFS is in the evaluation function. For Greedy BFS the evaluation function is f(n) = h(n) while for A* the evaluation function is f(n) = g(n) + h(n)."}, {"text": "The data structure which is being used in DFS is stack. The process is similar to BFS algorithm. In DFS, the edges that leads to an unvisited node are called discovery edges while the edges that leads to an already visited node are called block edges."}, {"text": "BFS stands for Breadth First Search. DFS stands for Depth First Search. 2. BFS(Breadth First Search) uses Queue data structure for finding the shortest path. DFS(Depth First Search) uses Stack data structure."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Strongly Connected Components1) Create an empty stack 'S' and do DFS traversal of a graph. In DFS traversal, after calling recursive DFS for adjacent vertices of a vertex, push the vertex to stack.  2) Reverse directions of all arcs to obtain the transpose graph.3) One by one pop a vertex from S while S is not empty. Let the popped vertex be 'v'."}]}, {"question": "What is a linear model example", "positive_ctxs": [{"text": "A linear model is a comparison of two values, usually x and y, and the consistent change between the values. In the opening story, Jill was analyzing two values: the amount of electricity used and the total cost of her bill. The change between these two values is the cost of each kilowatt hour."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An example of a nonlinear classifier is kNN.  If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A blurring filter where you move over the image with a box filter (all the same values in the window) is an example of a linear filter. A non-linear filter is one that cannot be done with convolution or Fourier multiplication. A sliding median filter is a simple example of a non-linear filter."}, {"text": "Fine tuning is one approach to transfer learning, and it is very popular in computer vision and NLP. The most common example given is when a model is trained on ImageNet is fine-tuned on a second task.  Transfer learning is when a model developed for one task is reused for a model on a second task."}, {"text": "Alternative procedures include: Different linear model: fitting a linear model with additional X variable(s) Nonlinear model: fitting a nonlinear model when the linear model is inappropriate.  Weighted least squares linear regression: dealing with unequal variances in Y by performing a weighted least squares fit."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Even if a model-fitting procedure has been used, R2 may still be negative, for example when linear regression is conducted without including an intercept, or when a non-linear function is used to fit the data."}]}, {"question": "What is the difference between K means clustering and hierarchical clustering", "positive_ctxs": [{"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows\u2022"}, {"text": "A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical."}, {"text": "Centroid-based clustering organizes the data into non-hierarchical clusters, in contrast to hierarchical clustering defined below. k-means is the most widely-used centroid-based clustering algorithm. Centroid-based algorithms are efficient but sensitive to initial conditions and outliers."}, {"text": "The hclust function in R uses the complete linkage method for hierarchical clustering by default. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components."}, {"text": "Average Linkage is a type of hierarchical clustering in which the distance between one cluster and another cluster is considered to be equal to the average distance from any member of one cluster to any member of the other cluster."}, {"text": "Cluster analysis is applied in many fields such as the natural sciences, the medical sciences, economics, marketing, etc. There are essentially two types of clustering methods: hierarchical algorithms and partioning algorithms. The hierarchical algorithms can be divided into agglomerative and splitting procedures."}]}, {"question": "Where do we use linear regression explain linear regression", "positive_ctxs": [{"text": "Linear regression is commonly used for predictive analysis and modeling. For example, it can be used to quantify the relative impacts of age, gender, and diet (the predictor variables) on height (the outcome variable)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Generally, we use linear regression for time series analysis, it is used for predicting the result for time series as its trends. For example, If we have a dataset of time series with the help of linear regression we can predict the sales with the time."}, {"text": "The principal advantage of linear regression is its simplicity, interpretability, scientific acceptance, and widespread availability. Linear regression is the first method to use for many problems. Analysts can use linear regression together with techniques such as variable recoding, transformation, or segmentation."}, {"text": "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  After fitting a linear regression model, you need to determine how well the model fits the data."}, {"text": "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  After fitting a linear regression model, you need to determine how well the model fits the data."}, {"text": "Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables. For instance, when we predict rent based on square feet alone that is simple linear regression."}, {"text": "Quantile regression is an extension of linear regression used when the conditions of linear regression are not met."}, {"text": "3 Answers. Since your response is ordinal then you should use ordinal regression. At a very high level, the main difference ordinal regression and linear regression is that with linear regression the dependent variable is continuous and ordinal the dependent variable is ordinal."}]}, {"question": "What is covariance matrix example", "positive_ctxs": [{"text": "Covariance Matrix is a measure of how much two random variables gets change together.  The Covariance Matrix is also known as dispersion matrix and variance-covariance matrix. The covariance between two jointly distributed real-valued random variables X and Y with finite second moments is defined as."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A covariance matrix is a square matrix which gives two types of information. If you are looking at the population covariance matrix then. each diagonal element is the variance of the corresponding random variable. each off-diagonal element is the covariance of the corresponding pair of random variables."}, {"text": "Now, three variable case it is less clear for me. An intuitive definition for covariance function would be Cov(X,Y,Z)=E[(x\u2212E[X])(y\u2212E[Y])(z\u2212E[Z])], but instead the literature suggests using covariance matrix that is defined as two variable covariance for each pair of variables."}, {"text": "In probability theory and statistics, a covariance matrix (also known as auto-covariance matrix, dispersion matrix, variance matrix, or variance\u2013covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector."}, {"text": "The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of observed values of two of the variables and whose i, i element is the sample variance of the observed values of one of the variables."}, {"text": "When the population contains higher dimensions or more random variables, a matrix is used to describe the relationship between different dimensions. In a more easy-to-understand way, covariance matrix is to define the relationship in the entire dimensions as the relationships between every two random variables."}, {"text": "The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal. Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA.  If the matrix A is a real matrix, then U and V are also real."}, {"text": "Weighted least squares (WLS), also known as weighted linear regression, is a generalization of ordinary least squares and linear regression in which the errors covariance matrix is allowed to be different from an identity matrix."}]}, {"question": "What is the difference between a prior and a posterior probability", "positive_ctxs": [{"text": "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis."}, {"text": "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis."}, {"text": "The output, y is generated from a normal (Gaussian) Distribution characterized by a mean and variance. In contrast to OLS, we have a posterior distribution for the model parameters that is proportional to the likelihood of the data multiplied by the prior probability of the parameters."}, {"text": "The prior probability of an event will be revised as new data or information becomes available, to produce a more accurate measure of a potential outcome. That revised probability becomes the posterior probability and is calculated using Bayes' theorem."}, {"text": "A posterior probability value is a prior probability value that has been a | Course Hero. Study Resources. by Textbook. by Literature Title."}, {"text": "The Dirichlet distribution is a conjugate prior for the multinomial distribution. This means that if the prior distribution of the multinomial parameters is Dirichlet then the posterior distribution is also a Dirichlet distribution (with parameters different from those of the prior)."}, {"text": "The Dirichlet distribution is a conjugate prior for the multinomial distribution. This means that if the prior distribution of the multinomial parameters is Dirichlet then the posterior distribution is also a Dirichlet distribution (with parameters different from those of the prior)."}]}, {"question": "What is a sign test in statistics", "positive_ctxs": [{"text": "The sign test is a statistical method to test for consistent differences between pairs of observations, such as the weight of subjects before and after treatment.  The sign test can also test if the median of a collection of numbers is significantly greater than or less than a specified value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Descriptive statistics summarize the characteristics of a data set. Inferential statistics allow you to test a hypothesis or assess whether your data is generalizable to the broader population."}, {"text": "Descriptive statistics summarize the characteristics of a data set. Inferential statistics allow you to test a hypothesis or assess whether your data is generalizable to the broader population."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "There is a wide rangeof statistical tests.  There are many different types of tests in statistics like t-test,Z-test,chi-square test, anova test ,binomial test, one sample median test etc. Choosing a Statistical test- Parametric tests are used if the data is normally distributed ."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Another sign of overfitting may be seen in the classification accuracy on the training data, If the training accuracy is out performing our test accuracy, it means that our model is learning details and noises of training data and specifically working of training data. Overfitting is a major problem in neural networks."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}]}, {"question": "What is the probability distribution of a continuous random variable", "positive_ctxs": [{"text": "The probability distribution of a continuous random variable X is an assignment of probabilities to intervals of decimal numbers using a function f(x), called a density function, in the following way: the probability that X assumes a value in the interval [a,b] is equal to the area of the region that is bounded above"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed.  A log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive."}, {"text": "The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable."}, {"text": "A continuous random variable is a random variable whose statistical distribution is continuous. Formally: A continuous random variable is a function X X X on the outcomes of some probabilistic experiment which takes values in a continuous set V V V."}, {"text": "The probability density function (PDF) is defined for probability distributions of continuous random variables. The probability at a certain point of a continuous variable is zero. The cumulative distribution function (CDF) is a non-decreasing function as the probabilities can never be less than 0."}, {"text": "For example, a random variable could be the outcome of the roll of a die or the flip of a coin. A probability distribution is a list of all of the possible outcomes of a random variable along with their corresponding probability values."}, {"text": "The probability of a specific value of a continuous random variable will be zero because the area under a point is zero."}, {"text": "A random variable is a numerical description of the outcome of a statistical experiment.  For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x). This function provides the probability for each value of the random variable."}]}, {"question": "What is random in probability", "positive_ctxs": [{"text": "A random variable is a numerical description of the outcome of a statistical experiment.  For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "A random variable is a numerical description of the outcome of a statistical experiment.  For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x). This function provides the probability for each value of the random variable."}, {"text": "In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed.  A log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive."}, {"text": "The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable."}]}, {"question": "What is univariate and bivariate analysis with examples", "positive_ctxs": [{"text": "When you conduct a study that looks at a single variable, that study involves univariate data. For example, you might study a group of college students to find out their average SAT scores or you might study a group of diabetic patients to find their weights. Bivariate data is when you are studying two variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.  Like univariate analysis, bivariate analysis can be descriptive or inferential."}, {"text": "Another common example of univariate analysis is the mean of a population distribution. Tables, charts, polygons, and histograms are all popular methods for displaying univariate analysis of a specific variable (e.g. mean, median, mode, standard variation, range, etc)."}, {"text": "7 Practical Guidelines for Accurate Statistical Model BuildingRemember that regression coefficients are marginal results.  Start with univariate descriptives and graphs.  Next, run bivariate descriptives, again including graphs.  Think about predictors in sets.  Model building and interpreting results go hand-in-hand.More items"}, {"text": "Bivariate analysis means the analysis of bivariate data. It is one of the simplest forms of statistical analysis, used to find out if there is a relationship between two sets of values. It usually involves the variables X and Y. Univariate analysis is the analysis of one (\u201cuni\u201d) variable."}, {"text": "Bivariate analysis means the analysis of bivariate data. It is one of the simplest forms of statistical analysis, used to find out if there is a relationship between two sets of values. It usually involves the variables X and Y."}, {"text": "Graphs that are appropriate for bivariate analysis depend on the type of variable. For two continuous variables, a scatterplot is a common graph. When one variable is categorical and the other continuous, a box plot is common and when both are categorical a mosaic plot is common."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What should the Gini coefficient be", "positive_ctxs": [{"text": "The Gini index, or Gini coefficient, is a measure of the distribution of income across a population developed by the Italian statistician Corrado Gini in 1912.  The coefficient ranges from 0 (or 0%) to 1 (or 100%), with 0 representing perfect equality and 1 representing perfect inequality."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Gini coefficient for the entire world has been estimated by various parties to be between 0.61 and 0.68."}, {"text": "Cowell says that the Gini coefficient is useful, particularly because it allows negative values for income and wealth, unlike some other measures of inequality. (If some amount of the population has negative wealth (owes money), the Lorenz curve will dip below the x-axis.) But the Gini coefficient also has limitations."}, {"text": "In the chapter on Human Development Indicators, there should be a table that includes the Gini coefficient. For example, in the 2004 edition, they are in table number 14. See also the \u201cGet Indicators\u201d portion of their web site, where you can download an Excel table with the Gini index."}, {"text": "The Gini coefficient is equal to the area below the line of perfect equality (0.5 by definition) minus the area below the Lorenz curve, divided by the area below the line of perfect equality."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Gini Index, also known as Gini impurity, calculates the amount of probability of a specific feature that is classified incorrectly when selected randomly. If all the elements are linked with a single class then it can be called pure."}, {"text": "Receiver Operating Characteristics (ROC) Curve For classification models, there are many other evaluation methods like Gain and Lift charts, Gini coefficient etc. But the in depth knowledge about the confusion matrix can help to evaluate any classification model very effectively."}]}, {"question": "What are the assumptions of a generalized linear mixed model", "positive_ctxs": [{"text": "Mixed models add at least one random variable to a linear or generalized linear model. The random variables of a mixed model add the assumption that observations within a level, the random variable groups, are correlated."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, a generalized linear mixed model (GLMM) is an extension to the generalized linear model (GLM) in which the linear predictor contains random effects in addition to the usual fixed effects. They also inherit from GLMs the idea of extending linear mixed models to non-normal data."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution."}]}, {"question": "Is the Implicit Association Test having poor test retest reliability", "positive_ctxs": [{"text": "IAT is a popular measure in social psychology to measure the relative strength of association between pairs of concepts (Greenwald, McGhee, & Schwartz, 1998).  Studies have found that racial bias IAT studies have a test-retest reliability score of only 0.44, while the IAT overall is just around 0.5."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Test-retest reliability example You administer the test two months apart to the same group of people, but the results are significantly different, so the test-retest reliability of the IQ questionnaire is low."}, {"text": "Alternate-form reliability is the consistency of test results between two different \u2013 but equivalent \u2013 forms of a test. Alternate-form reliability is used when it is necessary to have two forms of the same tests.  \u2013 Alternative-form reliability is needed whenever two test forms are being used to measure the same thing."}, {"text": "Because it arises from consistency between parts of a test, split-half reliability is an \u201cinternal consistency\u201d approach to estimating reliability. This result is an estimate of the reliability of the test scores, and it provides some support for the quality of the test scores."}, {"text": "Content validity: Is the test fully representative of what it aims to measure?  Criterion validity: Do the results correspond to a different test of the same thing?"}, {"text": "Association between two variables means the values of one variable relate in some way to the values of the other. Association is usually measured by correlation for two continuous variables and by cross tabulation and a Chi-square test for two categorical variables."}, {"text": "Association between two variables means the values of one variable relate in some way to the values of the other. Association is usually measured by correlation for two continuous variables and by cross tabulation and a Chi-square test for two categorical variables."}]}, {"question": "Why are algorithms so important", "positive_ctxs": [{"text": "The use of computer algorithms plays an essential role in space search programs.  We are in the age of algorithms because they solve our everyday tasks and we won't be able to live with them. They make our life more comfortable and, in the future, they will be able to predict our behavior."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Genetic algorithms are important in machine learning for three reasons. First, they act on discrete spaces, where gradient-based methods cannot be used. They can be used to search rule sets, neural network architectures, cellular automata computers, and so forth."}, {"text": "Time series forecasting is an important area of machine learning that is often neglected. It is important because there are so many prediction problems that involve a time component.  Standard definitions of time series, time series analysis, and time series forecasting."}, {"text": "Data structure and algorithms help in understanding the nature of the problem at a deeper level and thereby a better understanding of the world. If you want to know more about Why Data Structures and Algorithms then you must watch this video of Mr."}, {"text": "Role of Scaling is mostly important in algorithms that are distance based and require Euclidean Distance. Random Forest is a tree-based model and hence does not require feature scaling."}, {"text": "Supervised learning algorithms are trained using labeled data. Unsupervised learning algorithms are trained using unlabeled data.  In unsupervised learning, only input data is provided to the model. The goal of supervised learning is to train the model so that it can predict the output when it is given new data."}, {"text": "The important limitations of statistics are: (1) Statistics laws are true on average. Statistics are aggregates of facts, so a single observation is not a statistic.  (2) Statistical methods are best applicable to quantitative data. (3) Statistics cannot be applied to heterogeneous data."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}]}, {"question": "What is production system and its types", "positive_ctxs": [{"text": "There are three common types of basic production systems: the batch system, the continuous system, and the project system. In the batch system, general-purpose equipment and methods are used to produce small quantities of output (goods or services) with specifications that vary greatly from one batch to the next.3 days ago"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Caffe2 is was intended as a framework for production edge deployment whereas TensorFlow is more suited towards server production and research. TensorFlow is aimed for researchers and servers while Caffe2 is aimed towards mobile phones and other (relatively) computationally constrained platforms."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Operating system is a system software. Kernel is a part of operating system. Operating system acts as an interface between user and hardware. Kernel acts as an interface between applications and hardware."}, {"text": "Structured data is clearly defined and searchable types of data, while unstructured data is usually stored in its native format.  Structured data is often stored in data warehouses, while unstructured data is stored in data lakes."}, {"text": "We input the data in the learning algorithm as a set of inputs, which is called as Features, denoted by X along with the corresponding outputs, which is indicated by Y, and the algorithm learns by comparing its actual production with correct outputs to find errors. It then modifies the model accordingly."}, {"text": "A rule-based system (e.g., production system, expert system) uses rules as the knowledge representation. These rules are coded into the system in the form of if-then-else statements.  So, let's regard rule-based systems as the simplest form of AI."}]}, {"question": "What is meant by hypothesis in statistics", "positive_ctxs": [{"text": "Statistical hypothesis: A statement about the nature of a population. It is often stated in terms of a population parameter. Null hypothesis: A statistical hypothesis that is to be tested. Alternative hypothesis: The alternative to the null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A null hypothesis is a type of hypothesis used in statistics that proposes that there is no difference between certain characteristics of a population (or data-generating process)."}, {"text": "A null hypothesis is a type of conjecture used in statistics that proposes that there is no difference between certain characteristics of a population or data-generating process. The alternative hypothesis proposes that there is a difference."}, {"text": "A null hypothesis is a type of conjecture used in statistics that proposes that there is no difference between certain characteristics of a population or data-generating process. The alternative hypothesis proposes that there is a difference."}, {"text": "Classical statistics uses techniques such as Ordinary Least Squares and Maximum Likelihood \u2013 this is the conventional type of statistics that you see in most textbooks covering estimation, regression, hypothesis testing, confidence intervals, etc.  In fact Bayesian statistics is all about probability calculations!"}, {"text": "A null hypothesis is a type of hypothesis used in statistics that proposes that there is no difference between certain characteristics of a population (or data-generating process). For example, a gambler may be interested in whether a game of chance is fair."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Descriptive statistics summarize the characteristics of a data set. Inferential statistics allow you to test a hypothesis or assess whether your data is generalizable to the broader population."}]}, {"question": "What is cluster sampling and its example", "positive_ctxs": [{"text": "Cluster sampling refers to a type of sampling method . With cluster sampling, the researcher divides the population into separate groups, called clusters.  For example, given equal sample sizes, cluster sampling usually provides less precision than either simple random sampling or stratified sampling."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "Cluster Sampling: Advantages and Disadvantages Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "In stratified sampling, a random sample is drawn from each of the strata, whereas in cluster sampling only the selected clusters are sampled. A common motivation of cluster sampling is to reduce costs by increasing sampling efficiency."}, {"text": "Cluster sampling is best used when the clusters occur naturally in a population, when you don't have access to the entire population, and when the clusters are geographically convenient. However, cluster sampling is not as precise as simple random sampling or stratified random sampling."}]}, {"question": "What is sentiment analysis example", "positive_ctxs": [{"text": "Examples of Sentiment Analysis For instance, sentiment analysis may be performed on Twitter to determine overall opinion on a particular trending topic. Companies and brands often utilize sentiment analysis to monitor brand reputation across social media platforms or across the web as a whole."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Sentiment analysis is the automated process of analyzing text data and sorting it into sentiments positive, negative, or neutral. Using sentiment analysis tools to analyze opinions in Twitter data can help companies understand how people are talking about their brand."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Feature extraction identifies those product aspects which are being commented by customers, sentiment prediction identifies the text containing sentiment or opinion by deciding sentiment polarity as positive, negative or neutral and finally summarization module aggregates the results obtained from previous two steps."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Another common example of univariate analysis is the mean of a population distribution. Tables, charts, polygons, and histograms are all popular methods for displaying univariate analysis of a specific variable (e.g. mean, median, mode, standard variation, range, etc)."}]}, {"question": "How do I find the best machine learning algorithm", "positive_ctxs": [{"text": "Here are some important considerations while choosing an algorithm.Size of the training data. It is usually recommended to gather a good amount of data to get reliable predictions.  Accuracy and/or Interpretability of the output.  Speed or Training time.  Linearity.  Number of features."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Hypothesis Space (H): Hypothesis space is the set of all the possible legal hypothesis. This is the set from which the machine learning algorithm would determine the best possible (only one) which would best describe the target function or the outputs."}, {"text": "Q-Learning is a value-based reinforcement learning algorithm which is used to find the optimal action-selection policy using a Q function. Our goal is to maximize the value function Q. The Q table helps us to find the best action for each state.  Initially we explore the environment and update the Q-Table."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Q-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It's considered off-policy because the q-learning function learns from actions that are outside the current policy, like taking random actions, and therefore a policy isn't needed."}]}, {"question": "What is converse inverse and Contrapositive", "positive_ctxs": [{"text": "The converse of the conditional statement is \u201cIf Q then P.\u201d The contrapositive of the conditional statement is \u201cIf not Q then not P.\u201d The inverse of the conditional statement is \u201cIf not P then not Q.\u201d"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The converse of the conditional statement is \u201cIf Q then P.\u201d The contrapositive of the conditional statement is \u201cIf not Q then not P.\u201d The inverse of the conditional statement is \u201cIf not P then not Q.\u201d"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "where 'In' denotes the n-by-n identity matrix. The matrix B is called the inverse matrix of A. A square matrix is Invertible if and only if its determinant is non-zero."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}]}, {"question": "How do you test if your data is normally distributed", "positive_ctxs": [{"text": "For quick and visual identification of a normal distribution, use a QQ plot if you have only one variable to look at and a Box Plot if you have many. Use a histogram if you need to present your results to a non-statistical public. As a statistical test to confirm your hypothesis, use the Shapiro Wilk test."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Non parametric tests are used when your data isn't normal. Therefore the key is to figure out if you have normally distributed data. For example, you could look at the distribution of your data. If your data is approximately normal, then you can use parametric statistical tests."}, {"text": "There is a wide rangeof statistical tests.  There are many different types of tests in statistics like t-test,Z-test,chi-square test, anova test ,binomial test, one sample median test etc. Choosing a Statistical test- Parametric tests are used if the data is normally distributed ."}, {"text": "A normality test is used to determine whether sample data has been drawn from a normally distributed population (within some tolerance). A number of statistical tests, such as the Student's t-test and the one-way and two-way ANOVA require a normally distributed sample population."}, {"text": "A multivariate normal distribution is a vector in multiple normally distributed variables, such that any linear combination of the variables is also normally distributed."}, {"text": "Nonparametric tests are also called distribution-free tests because they don't assume that your data follow a specific distribution. You may have heard that you should use nonparametric tests when your data don't meet the assumptions of the parametric test, especially the assumption about normally distributed data."}, {"text": "Nonparametric tests are also called distribution-free tests because they don't assume that your data follow a specific distribution. You may have heard that you should use nonparametric tests when your data don't meet the assumptions of the parametric test, especially the assumption about normally distributed data."}, {"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}]}, {"question": "What are the requirements of a hypergeometric distribution", "positive_ctxs": [{"text": "The hypergeometric distribution is discrete. It is similar to the binomial distribution.The hypergeometric distribution is used under these conditions:Total number of items (population) is fixed.Sample size (number of trials) is a portion of the population.Probability of success changes after each trial."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Use the hypergeometric distribution with populations that are so small that the outcome of a trial has a large effect on the probability that the next outcome is an event or non-event. For example, in a population of 10 people, 7 people have O+ blood."}, {"text": "Use the hypergeometric distribution with populations that are so small that the outcome of a trial has a large effect on the probability that the next outcome is an event or non-event. For example, in a population of 10 people, 7 people have O+ blood."}, {"text": "In probability theory and statistics, the hypergeometric distribution is a discrete probability distribution that describes the probability of successes (random draws for which the object drawn has a specified feature) in draws, without replacement, from a finite population of size that contains exactly objects with"}, {"text": "In probability theory and statistics, the hypergeometric distribution is a discrete probability distribution that describes the probability of successes (random draws for which the object drawn has a specified feature) in draws, without replacement, from a finite population of size that contains exactly objects with"}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "In this paper we describe a multiagent Q-learning tech- nique, called Sparse Cooperative Q-learning, that al- lows a group of agents to learn how to jointly solve a task when the global coordination requirements of the system (but not the particular action choices of the agents) are known beforehand."}, {"text": "The four requirements are: each observation falls into one of two categories called a success or failure. there is a fixed number of observations. the observations are all independent. the probability of success (p) for each observation is the same - equally likely."}]}, {"question": "What is the relationship between language and thought", "positive_ctxs": [{"text": "The bits of linguistic information that enter into one person's mind, from another, cause people to entertain a new thought with profound effects on his world knowledge, inferencing, and subsequent behavior. Language neither creates nor distorts conceptual life. Thought comes first, while language is an expression."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "NLP is short for natural language processing while NLU is the shorthand for natural language understanding. Similarly named, the concepts both deal with the relationship between natural language (as in, what we as humans speak, not what computers understand) and artificial intelligence."}, {"text": "NLP is short for natural language processing while NLU is the shorthand for natural language understanding. Similarly named, the concepts both deal with the relationship between natural language (as in, what we as humans speak, not what computers understand) and artificial intelligence."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A linear regression model attempts to explain the relationship between two or more variables using a straight line. Consider the data obtained from a chemical process where the yield of the process is thought to be related to the reaction temperature (see the table below)."}, {"text": "\u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance."}, {"text": "\u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance."}, {"text": "\u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance."}]}, {"question": "What are marginal probability and conditional probability", "positive_ctxs": [{"text": "Joint probability is the probability of two events occurring simultaneously. Marginal probability is the probability of an event irrespective of the outcome of another variable. Conditional probability is the probability of one event occurring in the presence of a second event."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A marginal distribution is the percentages out of totals, and conditional distribution is the percentages out of some column."}, {"text": "A conditional probability estimate is a probability estimate that we make given or assuming the occurrence of some other event. In this case we might start with an estimate that the probability of rain is 30% and then make a conditional probability estimate that the probability of rain given a cloudy sky is 65%."}, {"text": "The posterior probability is one of the quantities involved in Bayes' rule. It is the conditional probability of a given event, computed after observing a second event whose conditional and unconditional probabilities were known in advance."}, {"text": "A marginal distribution is the percentages out of totals, and conditional distribution is the percentages out of some column.  Conditional distribution, on the other hand, is the probability distribution of certain values in the table expressed as percentages out of sums (or local totals) of certain rows or columns."}, {"text": "Statistical independence is a concept in probability theory. Two events A and B are statistical independent if and only if their joint probability can be factorized into their marginal probabilities, i.e., P(A \u2229 B) = P(A)P(B).  The concept can be generalized to more than two events."}, {"text": "their joint probability distribution at (x,y), the functions given by: g(x) = \u03a3y f (x,y) and h(y) = \u03a3x f (x,y) are the marginal distributions of X and Y , respectively. If you're great with equations, that's probably all you need to know. It tells you how to find a marginal distribution."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "How does latent semantic analysis work", "positive_ctxs": [{"text": "Latent Semantic Analysis is an efficient way of analysing the text and finding the hidden topics by understanding the context of the text. Latent Semantic Analysis(LSA) is used to find the hidden topics represented by the document or text. This hidden topics then are used for clustering the similar documents together."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Latent classes divide the cases into their respective dimensions in relation to the variable. For example, cluster analysis groups similar cases and puts them into one group. The numbers of clusters in the cluster analysis are called the latent classes. In SEM, the number of constructs is called the latent classed."}, {"text": "It does this by using a means of representing knowledge called, semantic networks. These use graphical methods to describe relationships between concepts and events to describe common sense activities."}, {"text": "The mathematics of factor analysis and principal component analysis (PCA) are different. Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables."}, {"text": "Latent semantic analysis (LSA) is a mathematical method for computer modeling and simulation of the meaning of words and passages by analysis of representative corpora of natural text. LSA closely approximates many aspects of human language learning and understanding."}, {"text": "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  Factor analysis aims to find independent latent variables."}, {"text": "The difference between factor analysis and principal component analysis.  Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables."}, {"text": "Structural equation modeling is a multivariate statistical analysis technique that is used to analyze structural relationships. This technique is the combination of factor analysis and multiple regression analysis, and it is used to analyze the structural relationship between measured variables and latent constructs."}]}, {"question": "What is the formula for a mid range in statistics", "positive_ctxs": [{"text": "The midrange is a type of average, or mean. Electronic gadgets are sometimes classified as \u201cmidrange\u201d, meaning they're in the middle-price bracket. The formula to find the midrange = (high + low) / 2."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In case of continous series, a mid point is computed as lower\u2212limit+upper\u2212limit2 and Mean Deviation is computed using following formula.FormulaN = Number of observations.f = Different values of frequency f.x = Different values of mid points for ranges.Me = Median."}, {"text": "The least squares criterion is a formula used to measure the accuracy of a straight line in depicting the data that was used to generate it. That is, the formula determines the line of best fit. This mathematical formula is used to predict the behavior of the dependent variables."}, {"text": "The formula for calculating a z-score is is z = (x-\u03bc)/\u03c3, where x is the raw score, \u03bc is the population mean, and \u03c3 is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The formula for calculating a z-score is is z = (x-\u03bc)/\u03c3, where x is the raw score, \u03bc is the population mean, and \u03c3 is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation. Figure 2."}, {"text": "In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one fewer quantile than the number of groups created."}, {"text": "In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one fewer quantile than the number of groups created."}]}, {"question": "What are the rules of testing hypothesis", "positive_ctxs": [{"text": "The decision rule is: Reject H0 if Z > 1.645. The decision rule is: Reject H0 if Z < 1.645. The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960. The complete table of critical values of Z for upper, lower and two-tailed tests can be found in the table of Z values to the right in \"Other Resources.\""}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Hypothesis testing is used to assess the plausibility of a hypothesis by using sample data. The test provides evidence concerning the plausibility of the hypothesis, given the data. Statistical analysts test a hypothesis by measuring and examining a random sample of the population being analyzed."}, {"text": "Degrees of Freedom refers to the maximum number of logically independent values, which are values that have the freedom to vary, in the data sample. Degrees of Freedom are commonly discussed in relation to various forms of hypothesis testing in statistics, such as a Chi-Square."}, {"text": "Type I and type II errors are instrumental for the understanding of hypothesis testing in a clinical research scenario.  A type II error can be thought of as the opposite of a type I error and is when a researcher fails to reject the null hypothesis that is actually false in reality."}, {"text": "Abstract: The generalized likelihood ratio test (GLRT), which is commonly used in composite hypothesis testing problems, is investigated. Conditions for asymptotic optimality of the GLRT in the Neyman-Pearson sense are studied and discussed."}, {"text": "A rule-based system (e.g., production system, expert system) uses rules as the knowledge representation. These rules are coded into the system in the form of if-then-else statements.  So, let's regard rule-based systems as the simplest form of AI."}, {"text": "The false discovery rate (FDR) is a method of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons.  Thus, FDR-controlling procedures have greater power, at the cost of increased numbers of Type I errors."}, {"text": "If the sample being tested falls into either of the critical areas, the alternative hypothesis is accepted instead of the null hypothesis. The two-tailed test gets its name from testing the area under both tails of a normal distribution, although the test can be used in other non-normal distributions."}]}, {"question": "How does a deep neural network learn", "positive_ctxs": [{"text": "In simple terms, deep learning is when ANNs learn from large amounts of data. Similar to how humans learn from experience, a deep learning algorithm performs a task repeatedly, each time tweaking it slightly to improve the outcome."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery."}, {"text": "A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  Restricted Boltzmann machines can also be used in deep learning networks."}, {"text": "A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  Restricted Boltzmann machines can also be used in deep learning networks."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "In representation learning, features are extracted from unlabeled data by training a neural network on a secondary, supervised learning task.  When applying deep learning to natural language processing (NLP) tasks, the model must simultaneously learn several language concepts: the meanings of words."}, {"text": "Deep NN is just a deep neural network, with a lot of layers. It can be CNN, or just a plain multilayer perceptron. CNN, or convolutional neural network, is a neural network using convolution layer and pooling layer."}, {"text": "Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. In fact, it is the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three."}]}, {"question": "How are artificial neural networks similar to the brain", "positive_ctxs": [{"text": "The majority of neural networks are fully connected from one layer to another. These connexions are weighted; the higher the number the greater influence one unit has on another, similar to a human brain. As the data goes through each unit the network is learning more about the data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Neural Networks are networks used in Machine Learning that work similar to the human nervous system. It is designed to function like the human brain where many things are connected in various ways.  There are many kinds of artificial neural networks used for the computational model."}, {"text": "A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature."}, {"text": "A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature."}, {"text": "An artificial neural network is an attempt to simulate the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a humanlike manner. ANNs are created by programming regular computers to behave as though they are interconnected brain cells."}, {"text": "An artificial neural network is an attempt to simulate the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a humanlike manner. ANNs are created by programming regular computers to behave as though they are interconnected brain cells."}, {"text": "Neural networks are designed to work just like the human brain does. In the case of recognizing handwriting or facial recognition, the brain very quickly makes some decisions. For example, in the case of facial recognition, the brain might start with \u201cIt is female or male?"}, {"text": "Google uses artificial neural networks to power voice search."}]}, {"question": "Why is sampling important", "positive_ctxs": [{"text": "The idea behind importance sampling is that certain values of the input random variables in a simulation have more impact on the parameter being estimated than others. If these \"important\" values are emphasized by sampling more frequently, then the estimator variance can be reduced."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition: Random sampling is a part of the sampling technique in which each sample has an equal probability of being chosen. A sample chosen randomly is meant to be an unbiased representation of the total population.  An unbiased random sample is important for drawing conclusions."}, {"text": "One of the most important aspects of convenience sampling is its cost effectiveness. This method allows for funds to be distributed to other aspects of the project. Oftentimes this method of sampling is used to gain funding for a larger, more thorough research project."}, {"text": "The difference between quota sampling and stratified sampling is: although both \"group\" participants by an important characteristic, stratified sampling relies on random selection within each group, while quota sampling relies on convenience sampling within each group."}, {"text": "Data is the currency of applied machine learning. Therefore, it is important that it is both collected and used effectively. Data sampling refers to statistical methods for selecting observations from the domain with the objective of estimating a population parameter."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}]}, {"question": "What are the applications of F test", "positive_ctxs": [{"text": "F-test is used either for testing the hypothesis about the equality of two population variances or the equality of two or more population means. The equality of two population means was dealt with t-test. Besides a t-test, we can also apply F-test for testing equality of two population means."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The test statistic used in ANOVA is Student's t. One characteristic of the F distribution is that F cannot be negative. One characteristic of the F distribution is that the computed F can only range between -1 and +1."}, {"text": "There are two sets of degrees of freedom; one for the numerator and one for the denominator. For example, if F follows an F distribution and the number of degrees of freedom for the numerator is four, and the number of degrees of freedom for the denominator is ten, then F ~ F 4,10."}, {"text": "Find the F Statistic (the critical value for this test). The F statistic formula is: F Statistic = variance of the group means / mean of the within group variances. You can find the F Statistic in the F-Table."}, {"text": "T - test is used to if the means of two populations are equal (assuming similar variance) whereas F-test is used to test if the variances of two populations are equal. F - test can also be extended to check whether the means of three or more groups are different or not (ANOVA F-test)."}, {"text": "F statistic is a statistic that is determined by an ANOVA test. It determines the significance of the groups of variables. The F critical value is also known as the F \u2013statistic. The F \u2013 statistic value is obtained from the F-distribution table."}, {"text": "The F Distribution The distribution of all possible values of the f statistic is called an F distribution, with v1 = n1 - 1 and v2 = n2 - 1 degrees of freedom. The curve of the F distribution depends on the degrees of freedom, v1 and v2."}, {"text": "The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time.  The P value is determined from the F ratio and the two values for degrees of freedom shown in the ANOVA table."}]}, {"question": "What is linkage in agglomerative clustering", "positive_ctxs": [{"text": "In statistics, single-linkage clustering is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It's also known as AGNES (Agglomerative Nesting). The algorithm starts by treating each object as a singleton cluster."}, {"text": "The agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It's also known as AGNES (Agglomerative Nesting). The algorithm starts by treating each object as a singleton cluster."}, {"text": "Hierarchical clustering is an instance of the agglomerative or bottom-up approach, where we start with each data point as its own cluster and then combine clusters based on some similarity measure."}, {"text": "The hclust function in R uses the complete linkage method for hierarchical clustering by default. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components."}, {"text": "Cluster analysis is applied in many fields such as the natural sciences, the medical sciences, economics, marketing, etc. There are essentially two types of clustering methods: hierarchical algorithms and partioning algorithms. The hierarchical algorithms can be divided into agglomerative and splitting procedures."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What exactly is deep learning", "positive_ctxs": [{"text": "Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data.  Deep learning allows machines to solve complex problems even when using a data set that is very diverse, unstructured and inter-connected."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}, {"text": "Bayesian deep learning is a field at the intersection between deep learning and Bayesian probability theory.  Bayesian deep learning models typically form uncertainty estimates by either placing distributions over model weights, or by learning a direct mapping to probabilistic outputs."}]}, {"question": "What does it mean to hash something", "positive_ctxs": [{"text": "US, informal. 1 or hash over : to talk about (something) : discuss (something) The detectives hashed out their theories about who committed the murder. They've spent quite a bit of time hashing over the problem."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "A Hash Collision Attack is an attempt to find two input strings of a hash function that produce the same hash result. If two separate inputs produce the same hash output, it is called a collision."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Hashing is the practice of using an algorithm to map data of any size to a fixed length. This is called a hash value (or sometimes hash code or hash sums or even a hash digest if you're feeling fancy). Whereas encryption is a two-way function, hashing is a one-way function.  Every hash value is unique."}, {"text": "Rabin-Karp is another pattern searching algorithm to find the pattern in a more efficient way. It also checks the pattern by moving window one by one, but without checking all characters for all cases, it finds the hash value. When the hash value is matched, then only it tries to check each character."}, {"text": "The Rabin-Karp algorithm makes use of hash functions and the rolling hash technique. A hash function is essentially a function that maps one thing to a value. In particular, hashing can map data of arbitrary size to a value of fixed size."}]}, {"question": "What is AlexNet and GoogLeNet", "positive_ctxs": [{"text": "We use two well-known trained CNNs, GoogLeNet (Szegedy et al.  GoogLeNet has Inception Modules, which perform different sizes of convolutions and concatenate the filters for the next layer. AlexNet, on the other hand, has layers input provided by one previous layer instead of a filter concatenation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "A clinical trial is a randomized controlled trial only when participants are randomly allocated to the group receiving the treatment and a control group. What participants are allocated among groups receiving different treatments the clinical trial is simply called a randomized trial."}]}, {"question": "How do you tell the difference between correlation and causation", "positive_ctxs": [{"text": "Causation explicitly applies to cases where action A {quote:right}Causation explicitly applies to cases where action A causes outcome B. {/quote} causes outcome B. On the other hand, correlation is simply a relationship. Action A relates to Action B\u2014but one event doesn't necessarily cause the other event to happen."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}, {"text": "Once you find a correlation, you can test for causation by running experiments that \u201ccontrol the other variables and measure the difference.\u201d Two such experiments or analyses you can use to identify causation with your product are: Hypothesis testing. A/B/n experiments."}, {"text": "Once you find a correlation, you can test for causation by running experiments that \u201ccontrol the other variables and measure the difference.\u201d Two such experiments or analyses you can use to identify causation with your product are: Hypothesis testing. A/B/n experiments."}, {"text": "The product moment correlation coefficient (pmcc) can be used to tell us how strong the correlation between two variables is. A positive value indicates a positive correlation and the higher the value, the stronger the correlation.  If there is a perfect negative correlation, then r = -1."}, {"text": "Use Fisher's exact test when you have two nominal variables.  Fisher's exact test will tell you whether this difference between 81 and 31% is statistically significant. A data set like this is often called an \"R\u00d7C table,\" where R is the number of rows and C is the number of columns."}, {"text": "The range can only tell you basic details about the spread of a set of data. By giving the difference between the lowest and highest scores of a set of data it gives a rough idea of how widely spread out the most extreme observations are, but gives no information as to where any of the other data points lie."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}]}, {"question": "How random is pseudo random", "positive_ctxs": [{"text": "As the word 'pseudo' suggests, pseudo-random numbers are not random in the way you might expect, at least not if you're used to dice rolls or lottery tickets. Essentially, PRNGs are algorithms that use mathematical formulae or simply precalculated tables to produce sequences of numbers that appear random."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Stochastic vs. In general, stochastic is a synonym for random. For example, a stochastic variable is a random variable. A stochastic process is a random process. Typically, random is used to refer to a lack of dependence between observations in a sequence."}, {"text": "The main difference is obviously that, in a first order reaction, the order of reaction is one by nature. A pseudo first-order reaction is second order reaction by nature but has been altered to make it a first order reaction."}, {"text": "In particular, a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space."}, {"text": "In particular, a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space."}, {"text": "A continuous random variable is a random variable whose statistical distribution is continuous. Formally: A continuous random variable is a function X X X on the outcomes of some probabilistic experiment which takes values in a continuous set V V V."}, {"text": "\u2022 A random process is a time-varying function that assigns the outcome of a random experiment to each time instant: X(t). \u2022 For a fixed (sample path): a random process is a time varying function, e.g., a signal."}, {"text": "Definition: The range of a random variable is the smallest interval that contains all the values of the random variable. A variation of the last definition says that the range of a random variable is the smallest interval that contains all the values of the random variable with probability 1."}]}, {"question": "How do you assess your own level of intelligence", "positive_ctxs": [{"text": "While there are a number of different methods for measuring intelligence, the standard and most widely accepted method is by measuring a person's 'intelligence quotient' or IQ. Based on a series of tests which assess various types of abilities such a mathematical, spatial, verbal, logic and memory."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Descriptive statistics summarize the characteristics of a data set. Inferential statistics allow you to test a hypothesis or assess whether your data is generalizable to the broader population."}, {"text": "Descriptive statistics summarize the characteristics of a data set. Inferential statistics allow you to test a hypothesis or assess whether your data is generalizable to the broader population."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use."}, {"text": "\"Describe what works for you.Explain your time management strategies.Demonstrate your level of organization.Give past examples.Be honest."}, {"text": "An SLI (service level indicator) measures compliance with an SLO (service level objective). So, for example, if your SLA specifies that your systems will be available 99.95% of the time, your SLO is likely 99.95% uptime and your SLI is the actual measurement of your uptime. Maybe it's 99.96%. Maybe 99.99%."}]}, {"question": "Why do we convert normal distribution into standard normal distribution", "positive_ctxs": [{"text": "It also makes life easier because we only need one table (the Standard Normal Distribution Table), rather than doing calculations individually for each value of mean and standard deviation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Box-Cox Transformation is a type of power transformation to convert non-normal data to normal data by raising the distribution to a power of lambda (\u03bb). The algorithm can automatically decide the lambda (\u03bb) parameter that best transforms the distribution into normal distribution."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "You can start with a bimodal distribution of data and turn it into a standard normal distribution if you want."}, {"text": "A normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution. Areas of the normal distribution are often represented by tables of the standard normal distribution.  For example, a Z of -2.5 represents a value 2.5 standard deviations below the mean."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean."}]}, {"question": "How do you use structural equation modeling", "positive_ctxs": [{"text": "17:1525:32Suggested clip \u00b7 110 secondsStructural Equation Modeling: what is it and what can we use it for YouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Structural equation modeling is a multivariate statistical analysis technique that is used to analyze structural relationships. This technique is the combination of factor analysis and multiple regression analysis, and it is used to analyze the structural relationship between measured variables and latent constructs."}, {"text": "There are two main differences between regression and structural equation modelling. The first is that SEM allows us to develop complex path models with direct and indirect effects. This allows us to more accurately model causal mechanisms we are interested in. The second key difference is to do with measurement."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables."}, {"text": "Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "A latent variable is a variable that is inferred using models from observed data.  Approaches to inferring latent variables from data include: using a single observed variable, multi-item scales, predictive models, dimension reduction techniques such as factor analysis, structural equation models, and mixture models."}]}, {"question": "What is meant by predictive analytics", "positive_ctxs": [{"text": "Predictive analytics is the use of data, statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data. The goal is to go beyond knowing what has happened to providing a best assessment of what will happen in the future."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Predictive analytics is the process of using data analytics to make predictions based on data. This process uses data along with analysis, statistics, and machine learning techniques to create a predictive model for forecasting future events."}, {"text": "According to SAS, predictive analytics is \u201cthe use of data, statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data.  In short, predictive intelligence drives marketing decisions.\u201d"}, {"text": "Predictive analytics uses predictors or known features to create predictive models that will be used in obtaining an output. A predictive model is able to learn how different points of data connect with each other. Two of the most widely used predictive modeling techniques are regression and neural networks."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Predictive analytics uses historical data to predict future events. Typically, historical data is used to build a mathematical model that captures important trends. That predictive model is then used on current data to predict what will happen next, or to suggest actions to take for optimal outcomes."}, {"text": "There is a direct relationship between the coefficients produced by logit and the odds ratios produced by logistic. First, let's define what is meant by a logit: A logit is defined as the log base e (log) of the odds. : [1] logit(p) = log(odds) = log(p/q) The range is negative infinity to positive infinity."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is candidate sampling in machine learning", "positive_ctxs": [{"text": "\u201cCandidate Sampling\u201d training methods involve constructing a training task in which for each. training example. , we only need to evaluate. for a small set of candidate classes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset.  Model selection is the process of choosing one of the models as the final model that addresses the problem."}, {"text": "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data."}, {"text": "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data."}, {"text": "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Gibbs sampling is commonly used for statistical inference (e.g. determining the best value of a parameter, such as determining the number of people likely to shop at a particular store on a given day, the candidate a voter will most likely vote for, etc.)."}, {"text": "Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons a candidate (\"backtracks\") as soon as it determines that the candidate cannot possibly be completed to a"}]}, {"question": "What is the advantage of Sobel operator over Prewitt operator", "positive_ctxs": [{"text": "The resulting image after applying Canny operator (b). The primary advantages of the Sobel operator lie in its simplicity. The Sobel method provides a approximation to the gradient magnitude. Another advantage of the Sobel operator is it can detect edges and their orientations."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Prewitt operator is similar to the Sobel operator and is used for detecting vertical and horizontal edges in images. However, unlike the Sobel, this operator does not place any emphasis on the pixels that are closer to the center of the mask."}, {"text": "The Sobel operator, sometimes called the Sobel\u2013Feldman operator or Sobel filter, is used in image processing and computer vision, particularly within edge detection algorithms where it creates an image emphasising edges."}, {"text": "The integral operator is a linear operator because it preserves two operations; the addition between functions and the multiplication of a function"}, {"text": "Depending on the alternative hypothesis operator, greater than operator will be a right tailed test, less than operator is a left tailed test, and not equal operator is a two tailed test."}, {"text": "The sobel operator is very similar to Prewitt operator. It is also a derivate mask and is used for edge detection. It also calculates edges in both horizontal and vertical direction."}, {"text": "A matrix with rows and columns over a field is a function from the set of all ordered pairs of integers in range to .  A linear operator is a linear function from a Vector space to itself. In notations, given a vector space , a linear operator is a function which satisfies for all in the underlying Field and vectors ."}, {"text": "Given a linear operator it can have associated eigenvectors / functions.  For example, given the differential operator the exponential function is an eigenfunction of it. This is why we can solve linear homogeneous differential equations by solving a characteristic equation."}]}, {"question": "What is statistical learning in machine learning", "positive_ctxs": [{"text": "Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A GLM is absolutely a statistical model, but statistical models and machine learning techniques are not mutually exclusive. In general, statistics is more concerned with inferring parameters, whereas in machine learning, prediction is the ultimate goal."}, {"text": "In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data."}, {"text": "Logistic regression is one of the statistical techniques in machine learning used to form prediction models.  In short, Logistic Regression is used when the dependent variable(target) is categorical."}, {"text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."}, {"text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."}, {"text": "Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}]}, {"question": "What is rule based system in artificial intelligence", "positive_ctxs": [{"text": "In computer science, a rule-based system is used to store and manipulate knowledge to interpret information in a useful way. It is often used in artificial intelligence applications and research. Normally, the term rule-based system is applied to systems involving human-crafted or curated rule sets."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert.  The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software."}, {"text": "An artificial neural network (ANN) is the piece of a computing system designed to simulate the way the human brain analyzes and processes information. It is the foundation of artificial intelligence (AI) and solves problems that would prove impossible or difficult by human or statistical standards."}, {"text": "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception."}, {"text": "\"AI is a computer system able to perform tasks that ordinarily require human intelligence Many of these artificial intelligence systems are powered by machine learning, some of them are powered by deep learning and some of them are powered by very boring things like rules.\""}, {"text": "Image recognition is the ability of a system or software to identify objects, people, places, and actions in images. It uses machine vision technologies with artificial intelligence and trained algorithms to recognize images through a camera system."}, {"text": "An artificial neural network's learning rule or learning process is a method, mathematical logic or algorithm which improves the network's performance and/or training time. Usually, this rule is applied repeatedly over the network."}]}, {"question": "What is dropout in Lstm", "positive_ctxs": [{"text": "Dropout is a regularization method where input and recurrent connections to LSTM units are probabilistically excluded from activation and weight updates while training a network. This has the effect of reducing overfitting and improving model performance."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Inverted dropout is a variant of the original dropout technique developed by Hinton et al. Just like traditional dropout, inverted dropout randomly keeps some weights and sets others to zero. In contrast, traditional dropout requires scaling to be implemented during the test phase."}, {"text": "Inverted dropout is a variant of the original dropout technique developed by Hinton et al. Just like traditional dropout, inverted dropout randomly keeps some weights and sets others to zero. In contrast, traditional dropout requires scaling to be implemented during the test phase."}, {"text": "With dropout (dropout rate less than some small value), the accuracy will gradually increase and loss will gradually decrease first(That is what is happening in your case). When you increase dropout beyond a certain threshold, it results in the model not being able to fit properly."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "What is aperiodic in Markov chain", "positive_ctxs": [{"text": "If we have an irreducible Markov chain, this means that the chain is aperiodic. Since the number 1 is co-prime to every integer, any state with a self-transition is aperiodic. Consider a finite irreducible Markov chain Xn: If there is a self-transition in the chain (pii>0 for some i), then the chain is aperiodic."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A Markov chain in which every state can be reached from every other state is called an irreducible Markov chain. If a Markov chain is not irreducible, but absorbable, the sequences of microscopic states may be trapped into some independent closed states and never escape from such undesirable states."}, {"text": "Regular Markov Chains. \u25cb A transition matrix P is regular if some power of P has only positive entries. A Markov chain is a regular Markov chain if its transition matrix is regular. For example, if you take successive powers of the matrix D, the entries of D will always be positive (or so it appears)."}, {"text": "A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed."}, {"text": "A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "A Markov chain is ergodic if it is both irreducible and aperiodic. This condition is equivalent to the transition matrix being a primitive nonnegative matrix."}]}, {"question": "Is Word2Vec deep learning", "positive_ctxs": [{"text": "The Word2Vec Model This model was created by Google in 2013 and is a predictive deep learning based model to compute and generate high quality, distributed and continuous dense vector representations of words, which capture contextual and semantic similarity."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Bayesian deep learning is a field at the intersection between deep learning and Bayesian probability theory.  Bayesian deep learning models typically form uncertainty estimates by either placing distributions over model weights, or by learning a direct mapping to probabilistic outputs."}, {"text": "Fine-tuning, in general, means making small adjustments to a process to achieve the desired output or performance. Fine-tuning deep learning involves using weights of a previous deep learning algorithm for programming another similar deep learning process."}, {"text": "Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}, {"text": "Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2."}, {"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "Fine-tuning deep learning involves using weights of a previous deep learning algorithm for programming another similar deep learning process. Weights are used to connect each neuron in one layer to every neuron in the next layer in the neural network."}]}, {"question": "How is covariance matrix calculated", "positive_ctxs": [{"text": "where our data set is expressed by the matrix X\u2208Rn\u00d7d X \u2208 R n \u00d7 d . Following from this equation, the covariance matrix can be computed for a data set with zero mean with C=XXTn\u22121 C = X X T n \u2212 1 by using the semi-definite matrix XXT X X T ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A covariance matrix is a square matrix which gives two types of information. If you are looking at the population covariance matrix then. each diagonal element is the variance of the corresponding random variable. each off-diagonal element is the covariance of the corresponding pair of random variables."}, {"text": "Now, three variable case it is less clear for me. An intuitive definition for covariance function would be Cov(X,Y,Z)=E[(x\u2212E[X])(y\u2212E[Y])(z\u2212E[Z])], but instead the literature suggests using covariance matrix that is defined as two variable covariance for each pair of variables."}, {"text": "In probability theory and statistics, a covariance matrix (also known as auto-covariance matrix, dispersion matrix, variance matrix, or variance\u2013covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector."}, {"text": "The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of observed values of two of the variables and whose i, i element is the sample variance of the observed values of one of the variables."}, {"text": "The Pearson's correlation coefficient is calculated as the covariance of the two variables divided by the product of the standard deviation of each data sample. It is the normalization of the covariance between the two variables to give an interpretable score."}, {"text": "The Pearson's correlation coefficient is calculated as the covariance of the two variables divided by the product of the standard deviation of each data sample. It is the normalization of the covariance between the two variables to give an interpretable score."}, {"text": "When the population contains higher dimensions or more random variables, a matrix is used to describe the relationship between different dimensions. In a more easy-to-understand way, covariance matrix is to define the relationship in the entire dimensions as the relationships between every two random variables."}]}, {"question": "Is null hypothesis good or bad", "positive_ctxs": [{"text": "Not including the null hypothesis in your research is considered very bad practice by the scientific community. If you set out to prove an alternate hypothesis without considering it, you are likely setting yourself up for failure. At a minimum, your experiment will likely not be taken seriously."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Since p < 0.05 is enough to reject the null hypothesis (no association), p = 0.002 reinforce that rejection only. If the significance value that is p-value associated with chi-square statistics is 0.002, there is very strong evidence of rejecting the null hypothesis of no fit. It means good fit."}, {"text": "A t-test tests a null hypothesis about two means; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero.  A chi-square test tests a null hypothesis about the relationship between two variables."}, {"text": "A t-test tests a null hypothesis about two means; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero.  A chi-square test tests a null hypothesis about the relationship between two variables."}, {"text": "A t-test tests a null hypothesis about two means; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero.  A chi-square test tests a null hypothesis about the relationship between two variables."}, {"text": "Rejecting or failing to reject the null hypothesis If our statistical analysis shows that the significance level is below the cut-off value we have set (e.g., either 0.05 or 0.01), we reject the null hypothesis and accept the alternative hypothesis."}, {"text": "In statistical hypothesis testing, a type I error is the rejection of a true null hypothesis (also known as a \"false positive\" finding or conclusion; example: \"an innocent person is convicted\"), while a type II error is the non-rejection of a false null hypothesis (also known as a \"false negative\" finding or conclusion"}, {"text": "In statistical hypothesis testing, a type I error is the rejection of a true null hypothesis (also known as a \"false positive\" finding or conclusion; example: \"an innocent person is convicted\"), while a type II error is the non-rejection of a false null hypothesis (also known as a \"false negative\" finding or conclusion"}]}, {"question": "What is probability calibration", "positive_ctxs": [{"text": "The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction.  Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A vital aspect of the model construction process is the calibration phase.  In fact, a model's predictive uncertainty will only be reduced by calibration if the information content of the calibration data set is able to constrain those parameters that have a significant bearing on that prediction."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In this blog we will learn what is calibration and why and when we should use it. We calibrate our model when the probability estimate of a data point belonging to a class is very important. Calibration is comparison of the actual output and the expected output given by a system."}, {"text": "SVMs don't output probabilities natively, but probability calibration methods can be used to convert the output to class probabilities.  For many problems, it is convenient to get a probability P(y=1\u2223x), i.e. a classification that not only gives an answer, but also a degree of certainty about the answer."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "What is the difference between LSTM and GRU", "positive_ctxs": [{"text": "The key difference between a GRU and an LSTM is that a GRU has two gates (reset and update gates) whereas an LSTM has three gates (namely input, output and forget gates). Why do we make use of GRU when we clearly have more control on the network through the LSTM model (as we have three gates)?"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "GRU use less training parameters and therefore use less memory, execute faster and train faster than LSTM's whereas LSTM is more accurate on dataset using longer sequence. In short, if sequence is large or accuracy is very critical, please go for LSTM whereas for less memory consumption and faster operation go for GRU."}, {"text": "LSTMs control the exposure of memory content (cell state) while GRUs expose the entire cell state to other units in the network. The LSTM unit has separate input and forget gates, while the GRU performs both of these operations together via its reset gate."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "A CNN LSTM can be defined by adding CNN layers on the front end followed by LSTM layers with a Dense layer on the output. It is helpful to think of this architecture as defining two sub-models: the CNN Model for feature extraction and the LSTM Model for interpreting the features across time steps."}, {"text": "The Range is the difference between the lowest and highest values. Example: In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9. So the range is 9 \u2212 3 = 6."}]}, {"question": "What is invariance in statistics", "positive_ctxs": [{"text": "Any object, function, or statistic that doesn't change when scales are multiplied by a common factor is scale invariant. In statistics, it can also mean a statistic that tends not to change (i.e. 99% of the time, it will stay the same). Some specific statistics are scale invariant."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "In physics, mathematics and statistics, scale invariance is a feature of objects or laws that do not change if scales of length, energy, or other variables, are multiplied by a common factor, and thus represent a universality."}]}, {"question": "What is lag sequential analysis", "positive_ctxs": [{"text": "Lag sequential analysis is a method for analyzing the sequential dependency in a serially sequenced series of dichotomous codes representing different system states.  The analysis assumes that the events are sequenced in time (a time series) but does not assume equal time intervals between events."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Lag sequential analysis is a method for analyzing the sequential dependency in a serially sequenced series of dichotomous codes representing different system states.  The analysis assumes that the events are sequenced in time (a time series) but does not assume equal time intervals between events."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "jobs. deep learning performs better when sequential processing is used."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations as a function of the time lag between them."}]}, {"question": "What does the determinant of the correlation matrix represent", "positive_ctxs": [{"text": "The determinant is related to the volume of the space occupied by the swarm of data points represented by standard scores on the measures involved.  When the measures are correlated, the space occupied becomes an ellipsoid whose volume is less than 1."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An invertible matrix is a square matrix that has an inverse. We say that a square matrix is invertible if and only if the determinant is not equal to zero. In other words, a 2 x 2 matrix is only invertible if the determinant of the matrix is not 0."}, {"text": "The determinant is a unique number associated with a square matrix. If the determinant of a matrix is equal to zero: The matrix is less than full rank. The matrix is singular."}, {"text": "where 'In' denotes the n-by-n identity matrix. The matrix B is called the inverse matrix of A. A square matrix is Invertible if and only if its determinant is non-zero."}, {"text": "A term document matrix is a way of representing the words in the text as a table (or matrix) of numbers. The rows of the matrix represent the text responses to be analysed, and the columns of the matrix represent the words from the text that are to be used in the analysis."}, {"text": "A term document matrix is a way of representing the words in the text as a table (or matrix) of numbers. The rows of the matrix represent the text responses to be analysed, and the columns of the matrix represent the words from the text that are to be used in the analysis. The most basic version is binary."}, {"text": "A term document matrix is a way of representing the words in the text as a table (or matrix) of numbers. The rows of the matrix represent the text responses to be analysed, and the columns of the matrix represent the words from the text that are to be used in the analysis. The most basic version is binary."}, {"text": "To perform principal component analysis using the correlation matrix using the prcomp() function, set the scale argument to TRUE . Plot the first two PCs of the correlation matrix using the autoplot() function."}]}, {"question": "How are word Embeddings usually evaluated", "positive_ctxs": [{"text": "Word embeddings are widely used nowadays in Distributional Semantics and for a variety of tasks in NLP. Embeddings can be evaluated using ex- trinsic evaluation methods, i.e. the trained em- beddings are evaluated on a specific task such as part-of-speech tagging or named-entity recogni- tion (Schnabel et al., 2015)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Word Embeddings or Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics. The process of converting words into numbers are called Vectorization."}, {"text": "How many parity check bits must be included with the data word to achieve single-bit error correction and double error correction when data words are as follows: 16 bits."}, {"text": "1 Answer. In order to come up with a split point, the values are sorted, and the mid-points between adjacent values are evaluated in terms of some metric, usually information gain or gini impurity. For your example, lets say we have four examples and the values of the age variable are (20,29,40,50)."}, {"text": "Word2Vec, Doc2Vec and Glove are semi-supervised learning algorithms and they are Neural Word Embeddings for the sole purpose of Natural Language Processing. Specifically Word2vec is a two-layer neural net that processes text."}, {"text": "The skip-gram model. Both the input vector x and the output y are one-hot encoded word representations. The hidden layer is the word embedding of size N."}, {"text": "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word). Thus the model tries to predict the context_window words based on the target_word."}, {"text": "Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph. How sent_tokenize works ? The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk."}]}, {"question": "What does it mean to normalize a histogram", "positive_ctxs": [{"text": "Histogram normalization is a common technique that is used to enhance fine detail within an image.  Each column in the cumulative histogram is computed as the sum of all the image intensity histogram values up to and including that grey level, and then it is scaled so that the final value is 1.0."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Among the best practices for training a Neural Network is to normalize your data to obtain a mean close to 0. Normalizing the data generally speeds up learning and leads to faster convergence."}, {"text": "Histograms are generally used to show the results of a continuous data set such as height, weight, time, etc. A bar graph has spaces between the bars, while a histogram does not. A histogram often shows the frequency that an event occurs within the defined range. It shows you how many times that event happens."}, {"text": "When we do further analysis, like multivariate linear regression, for example, the attributed income will intrinsically influence the result more due to its larger value. But this doesn't necessarily mean it is more important as a predictor. So we normalize the data to bring all the variables to the same range."}, {"text": "In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time."}, {"text": "In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time."}, {"text": "An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value. By looking at the histogram for a specific image a viewer will be able to judge the entire tonal distribution at a glance."}]}, {"question": "What is the difference between Bernoulli distribution and binomial distribution", "positive_ctxs": [{"text": "The Bernoulli distribution represents the success or failure of a single Bernoulli trial. The Binomial Distribution represents the number of successes and failures in n independent Bernoulli trials for some given value of n.  Another example is the number of heads obtained in tossing a coin n times."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Remember that the Poisson distribution assumes that the mean and variance are the same.  The negative binomial distribution has one parameter more than the Poisson regression that adjusts the variance independently from the mean. In fact, the Poisson distribution is a special case of the negative binomial distribution."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "The Bernoulli distribution represents the success or failure of a single Bernoulli trial. The Binomial Distribution represents the number of successes and failures in n independent Bernoulli trials for some given value of n.  Another example is the number of heads obtained in tossing a coin n times."}, {"text": "The difference between the hypergeometric and the binomial distributions.  For the binomial distribution, the probability is the same for every trial. For the hypergeometric distribution, each trial changes the probability for each subsequent trial because there is no replacement."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}]}, {"question": "What is an N dimensional vector", "positive_ctxs": [{"text": "An -dimensional vector, i.e., a vector ( , , , ) with components. In dimensions greater than or equal to two, vectors are sometimes considered synonymous with points and so n-tuples ( , , , ) are sometimes called points in n-space."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The answer is whitening. If x is an n dimensional column vector of zero mean and has an n by n covariance R then x' inv(R) x is chi squared, that is x' inv(R) x is the sum of n unit variance zero mean random variables."}, {"text": "A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model."}, {"text": "Eigenvectors can be used to represent a large dimensional matrix. This means that a matrix M and a vector o can be replaced by a scalar n and a vector o. In this instance, o is the eigenvector and n is the eigenvalue and our target is to find o and n."}, {"text": "A vector is an element of a vector space. Assuming you're talking about an abstract vector space, which has an addition and scalar multiplication satisfying a number of properties, then a vector space is what we call a set which satisfies those properties."}, {"text": "4. A size of 100 means the vector representing each document will contain 100 elements - 100 values. The vector maps the document to a point in 100 dimensional space. A size of 200 would map a document to a point in 200 dimensional space. The more dimensions, the more differentiation between documents."}, {"text": "A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model.  The rows represent the predicted values of the target variable."}, {"text": "Top N accuracy \u2014 Top N accuracy is when you measure how often your predicted class falls in the top N values of your softmax distribution."}]}, {"question": "How do you feel when you learn something new", "positive_ctxs": [{"text": "Learning a new skill is often an extremely rewarding experience. If it's something you like, you'll quickly notice yourself improving, which can give you a great confidence boost. In most cases, trying something new is often about overcoming fear."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In Reinforcement Learning, this type of decision is called exploitation when you keep doing what you were doing, and exploration when you try something new.  In Reinforcement Learning on the other hand, it is not possible to do that, but there are some techniques that will help figuring out the best strategy."}, {"text": "So you are model-free. This is when you apply Q learning.  With value iteration, you learn the expected cost when you are given a state x. With q-learning, you get the expected discounted cost when you are in state x and apply action a."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "In Reinforcement Learning, this type of decision is called exploitation when you keep doing what you were doing, and exploration when you try something new. Naturally this raises a question about how much to exploit and how much to explore."}, {"text": "Your explanation, for example, could be, \u201cAn observation is something you sense: taste, touch, smell, see, or hear. An inference is something you decide or think about a thing or event after you observe it.\u201d"}, {"text": "False negatives \u2014 that is, a test that says you don't have the virus when you actually do have the virus \u2014 may occur."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}]}, {"question": "Why do we use bagging", "positive_ctxs": [{"text": "Definition: Bagging is used when the goal is to reduce the variance of a decision tree classifier. Here the objective is to create several subsets of data from training sample chosen randomly with replacement. Each collection of subset data is used to train their decision trees."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "An algorithm that uses random numbers to decide what to do next anywhere in its logic is called a Randomized Algorithm. For example, in Randomized Quick Sort, we use a random number to pick the next pivot (or we randomly shuffle the array). And in Karger's algorithm, we randomly pick an edge."}, {"text": "Overview of stacking. Stacking mainly differ from bagging and boosting on two points.  Second, stacking learns to combine the base models using a meta-model whereas bagging and boosting combine weak learners following deterministic algorithms."}, {"text": "Bootstrap aggregating (bagging) In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy."}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}, {"text": "A model is a simplified representation of a system. over some time period or spatial extent intended to promote understanding of the real system. Why Build a Model? Building models helps us understand the problem. (and its surrounding system) we are investigating solutions for."}, {"text": "In Gradient Descent or Batch Gradient Descent, we use the whole training data per epoch whereas, in Stochastic Gradient Descent, we use only single training example per epoch and Mini-batch Gradient Descent lies in between of these two extremes, in which we can use a mini-batch(small portion) of training data per epoch"}]}, {"question": "What is statistical significance in AB testing", "positive_ctxs": [{"text": "In the context of AB testing experiments, statistical significance is how likely it is that the difference between your experiment's control version and test version isn't due to error or random chance.  It's commonly used in business to observe how your experiments affect your business's conversion rates."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "This is why it is important to distinguish between the statistical significance of a result and the practical significance of that result.  Null hypothesis testing is a formal approach to deciding whether a statistical relationship in a sample reflects a real relationship in the population or is just due to chance."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "An AB test is an example of statistical hypothesis testing, a process whereby a hypothesis is made about the relationship between two data sets and those data sets are then compared against each other to determine if there is a statistically significant relationship or not."}, {"text": "A t-value is the relative error difference in contrast to the null hypothesis. A p-value, is the statistical significance of a measurement in how correct a statistical evidence part, is."}, {"text": "Statistical significance is a determination that a relationship between two or more variables is caused by something other than chance.  Statistical hypothesis testing is used to determine whether the result of a data set is statistically significant."}, {"text": "In statistics, a two-tailed test is a method in which the critical area of a distribution is two-sided and tests whether a sample is greater than or less than a certain range of values. It is used in null-hypothesis testing and testing for statistical significance."}, {"text": "In statistics, a two-tailed test is a method in which the critical area of a distribution is two-sided and tests whether a sample is greater than or less than a certain range of values. It is used in null-hypothesis testing and testing for statistical significance."}]}, {"question": "What is demeaning data", "positive_ctxs": [{"text": "Demeaning data means subtracting the sample mean from each observation so that they are mean zero. Given a simple linear regression Y = alpha + beta X + u, OLS estimation yields Y^ = ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What are the various areas where AI Artificial intelligence can be used", "positive_ctxs": [{"text": "Use of AI in Following Things/Fields/Areas:Virtual Assistant or Chatbots.Agriculture and Farming.Autonomous Flying.Retail, Shopping and Fashion.Security and Surveillance.Sports Analytics and Activities.Manufacturing and Production.Live Stock and Inventory Management.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Artificial intelligence (AI) is a branch of computer science.  Most AI programs are not used to control robots. Even when AI is used to control robots, the AI algorithms are only part of the larger robotic system, which also includes sensors, actuators, and non-AI programming."}, {"text": "Conclusion. Human intelligence revolves around adapting to the environment using a combination of several cognitive processes. The field of Artificial intelligence focuses on designing machines that can mimic human behavior. However, AI researchers are able to go as far as implementing Weak AI, but not the Strong AI."}, {"text": "Conclusion. Human intelligence revolves around adapting to the environment using a combination of several cognitive processes. The field of Artificial intelligence focuses on designing machines that can mimic human behavior. However, AI researchers are able to go as far as implementing Weak AI, but not the Strong AI."}, {"text": "Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing (NLP), speech recognition and machine vision."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}]}, {"question": "How is accuracy calculated in machine learning", "positive_ctxs": [{"text": "Accuracy in Machine Learning Accuracy is the number of correctly predicted data points out of all the data points. More formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A loss function is used to optimize a machine learning algorithm. The loss is calculated on training and validation and its interpretation is based on how well the model is doing in these two sets.  An accuracy metric is used to measure the algorithm's performance in an interpretable way."}, {"text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."}, {"text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."}, {"text": "Loss value implies how poorly or well a model behaves after each iteration of optimization. An accuracy metric is used to measure the algorithm's performance in an interpretable way. The accuracy of a model is usually determined after the model parameters and is calculated in the form of a percentage."}, {"text": "The clear sign of a machine learning overfitting is if its error on testing set is much greater than the error on training set.  For instance if the model accuracy for train data is 85% and the accuracy for test/validation data is 65% then its very obvious that the model has overlearned and you should check that."}, {"text": "In machine learning, the term \"ground truth\" refers to the accuracy of the training set's classification for supervised learning techniques. This is used in statistical models to prove or disprove research hypotheses."}, {"text": "False positive rate (FPR) is a measure of accuracy for a test: be it a medical diagnostic test, a machine learning model, or something else. In technical terms, the false positive rate is defined as the probability of falsely rejecting the null hypothesis."}]}, {"question": "What is the difference between a sample and a sampling frame", "positive_ctxs": [{"text": "Sampling Frame vs. A sampling frame is a list of things that you draw a sample from. A sample space is a list of all possible outcomes for an experiment. For example, you might have a sampling frame of names of people in a certain town for a survey you're going to be conducting on family size."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "In statistics, a sampling frame is the source material or device from which a sample is drawn. It is a list of all those within a population who can be sampled, and may include individuals, households or institutions. Importance of the sampling frame is stressed by Jessen and Salant and Dillman."}, {"text": "In statistics, a sampling frame is the source material or device from which a sample is drawn. It is a list of all those within a population who can be sampled, and may include individuals, households or institutions. Importance of the sampling frame is stressed by Jessen and Salant and Dillman."}, {"text": "A sampling frame is a list or other device used to define a researcher's population of interest. The sampling frame defines a set of elements from which a researcher can select a sample of the target population."}, {"text": "A sampling frame is a list or other device used to define a researcher's population of interest. The sampling frame defines a set of elements from which a researcher can select a sample of the target population."}]}, {"question": "Is collaborative filtering machine learning", "positive_ctxs": [{"text": "Explanation of Collaborative Filtering vs Content Based Filtering. Recommender systems help users select similar items when something is being chosen online. The method is based on content and collaborative filtering approach that captures correlation between user preferences and item features."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Model-based collaborative filtering algorithms provide item recommendation by first developing a model of user ratings. Algorithms in this category take a probabilistic approach and envision the collaborative filtering process as computing the expected value of a user prediction, given his/her ratings on other items."}, {"text": "Collaborative filtering (CF) is a technique used by recommender systems.  In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating)."}, {"text": "Collaborative filtering (CF) is a technique used by recommender systems.  In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating)."}, {"text": "Collaborative filtering (CF) is a technique used by recommender systems.  In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating)."}, {"text": "Collaborative filtering (CF) is a technique used by recommender systems.  For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes)."}, {"text": "Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them."}, {"text": "Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them."}]}, {"question": "How do you find the constant in a probability density function", "positive_ctxs": [{"text": "0:254:04Suggested clip \u00b7 117 secondsProbability density functions - Finding the constant k (example to try YouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition. In probability theory, a normalizing constant is a constant by which an everywhere non-negative function must be multiplied so the area under its graph is 1, e.g., to make it a probability density function or a probability mass function."}, {"text": "The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable."}, {"text": "A probability density plot simply means a density plot of probability density function (Y-axis) vs data points of a variable (X-axis).  By showing probability density plots, we're only able to understand the distribution of data visually without knowing the exact probability for a certain range of values."}, {"text": "The cumulative density function gives you the probability of a random variable being on or below a certain value. The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value.  A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75."}, {"text": "Bias allows you to shift the activation function by adding a constant (i.e. the given bias) to the input. Bias in Neural Networks can be thought of as analogous to the role of a constant in a linear function, whereby the line is effectively transposed by the constant value."}, {"text": "Bias allows you to shift the activation function by adding a constant (i.e. the given bias) to the input. Bias in Neural Networks can be thought of as analogous to the role of a constant in a linear function, whereby the line is effectively transposed by the constant value."}, {"text": "In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the"}]}, {"question": "What is the difference between accuracy and uncertainty and precision and accuracy", "positive_ctxs": [{"text": "Precision is the closeness of agreement between independent measurements. Precession is largely affected by random error. Accuracy is an expression of the lack of error. Uncertainty characterizes the range of values within which the true value is asserted to lie with some level of confidence."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In other words, accuracy describes the difference between the measurement and the part's actual value, while precision describes the variation you see when you measure the same part repeatedly with the same device."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The loss and accuracy of all three models is comparable but the Neocognitron and Coward model have a higher processing time than the Convolutional Neural Network. It is also evident that the Neocognitron requires more training steps than the Convolutional Neural Network to reach the same accuracy and loss."}, {"text": "Data is the currency of applied machine learning.  Resampling is a methodology of economically using a data sample to improve the accuracy and quantify the uncertainty of a population parameter. Resampling methods, in fact, make use of a nested resampling method."}, {"text": "Data is the currency of applied machine learning.  Resampling is a methodology of economically using a data sample to improve the accuracy and quantify the uncertainty of a population parameter. Resampling methods, in fact, make use of a nested resampling method."}, {"text": "According to my POV model accuracy is more important and its all depends on the training data.  Model performance can be improved using distributed computing and parallelizing over the scored assets, whereas accuracy has to be carefully built during the model training process."}, {"text": "Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process."}]}, {"question": "Why do we use ReLU in CNN", "positive_ctxs": [{"text": "ReLU is important because it does not saturate; the gradient is always high (equal to 1) if the neuron activates. As long as it is not a dead neuron, successive updates are fairly effective. ReLU is also very quick to evaluate."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "It is a Softmax activation plus a Cross-Entropy loss.  If we use this loss, we will train a CNN to output a probability over the C classes for each image. It is used for multi-class classification."}, {"text": "Leaky ReLU & Parametric ReLU (PReLU) Leaky ReLU has two benefits: It fixes the \u201cdying ReLU\u201d problem, as it doesn't have zero-slope parts. It speeds up training. There is evidence that having the \u201cmean activation\u201d be close to 0 makes training faster."}, {"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "Translational Invariance makes the CNN invariant to translation. Invariance to translation means that if we translate the inputs the CNN will still be able to detect the class to which the input belongs. Translational Invariance is a result of the pooling operation."}, {"text": "Translational Invariance makes the CNN invariant to translation. Invariance to translation means that if we translate the inputs the CNN will still be able to detect the class to which the input belongs. Translational Invariance is a result of the pooling operation."}, {"text": "An algorithm that uses random numbers to decide what to do next anywhere in its logic is called a Randomized Algorithm. For example, in Randomized Quick Sort, we use a random number to pick the next pivot (or we randomly shuffle the array). And in Karger's algorithm, we randomly pick an edge."}]}, {"question": "Why do we use Gaussian blur", "positive_ctxs": [{"text": "In image processing, a Gaussian blur (also known as Gaussian smoothing) is the result of blurring an image by a Gaussian function (named after mathematician and scientist Carl Friedrich Gauss). It is a widely used effect in graphics software, typically to reduce image noise and reduce detail."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "A Gaussian blur effect is typically generated by convolving an image with an FIR kernel of Gaussian values.  In the first pass, a one-dimensional kernel is used to blur the image in only the horizontal or vertical direction. In the second pass, the same one-dimensional kernel is used to blur in the remaining direction."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise.  The Gaussian filter alone will blur edges and reduce contrast. The Median filter is a non-linear filter that is most commonly used as a simple way to reduce noise in an image."}, {"text": "1. The Canny edge detector is a linear filter because it uses the Gaussian filter to blur the image and then uses the linear filter to compute the gradient. Solution False. Though it does those things, it also has non-linear operations: thresholding, hysteresis, non-maximum suppression."}]}, {"question": "What is Hyperparameter tuning in machine learning", "positive_ctxs": [{"text": "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."}, {"text": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."}, {"text": "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned."}, {"text": "1 Answer. Transfer learning is when a model developed for one task is reused to work on a second task. Fine tuning is one approach to transfer learning."}, {"text": "1 Answer. Transfer learning is when a model developed for one task is reused to work on a second task. Fine tuning is one approach to transfer learning."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Validation set is used for tuning the parameters of a model. Test set is used for performance evaluation. 2."}]}, {"question": "What is the difference between Anova and one way Anova", "positive_ctxs": [{"text": "A one-way ANOVA only involves one factor or independent variable, whereas there are two independent variables in a two-way ANOVA.  In a one-way ANOVA, the one factor or independent variable analyzed has three or more categorical groups. A two-way ANOVA instead compares multiple groups of two factors. 4."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The main difference between quota and stratified sampling can be explained in a way that in quota sampling researchers use non-random sampling methods to gather data from one stratum until the required quota fixed by the researcher is fulfilled."}, {"text": "The interquartile range is the difference between the third quartile and the first quartile in a data set, giving the middle 50%. The interquartile range is a measure of spread; it's used to build box plots, determine normal distributions and as a way to determine outliers."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "It is calculated in the same way - by running the network forward over inputs xi and comparing the network outputs \u02c6yi with the ground truth values yi using a loss function e.g. J=1N\u2211Ni=1L(\u02c6yi,yi) where L is the individual loss function based somehow on the difference between predicted value and target."}, {"text": "The difference between these two statistical measurements is that correlation measures the degree of a relationship between two variables (x and y), whereas regression is how one variable affects another."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}]}, {"question": "What the difference between standard deviation and mean", "positive_ctxs": [{"text": "Standard deviation is the deviation from the mean, and a standard deviation is nothing but the square root of the variance. Mean is an average of all set of data available with an investor or company. Standard deviation used for measuring the volatility of a stock."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "The difference between the two norms is that the standard deviation is calculating the square of the difference whereas the mean absolute deviation is only looking at the absolute difference. Hence large outliers will create a higher dispersion when using the standard deviation instead of the other method."}, {"text": "Definition of 'average deviation' 1. the difference between an observed value of a variable and its mean. 2. Also: mean deviation from the mean, mean deviation from the median, average deviation."}, {"text": "Mean, variance, and standard deviation The mean of the sampling distribution of the sample mean will always be the same as the mean of the original non-normal distribution. In other words, the sample mean is equal to the population mean. where \u03c3 is population standard deviation and n is sample size."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean."}, {"text": "The coefficient of variation is a better risk measure than the standard deviation alone because the CV adjusts for the size of the project. The CV measures the standard deviation divided by the mean and therefore puts the standard deviation into context."}, {"text": "The central limit theorem states that the sampling distribution of the mean approaches a normal distribution, as the sample size increases.  Therefore, as a sample size increases, the sample mean and standard deviation will be closer in value to the population mean \u03bc and standard deviation \u03c3 ."}]}, {"question": "What is accuracy in machine learning", "positive_ctxs": [{"text": "Accuracy in Machine Learning Accuracy is the number of correctly predicted data points out of all the data points.  Often, accuracy is used along with precision and recall, which are other metrics that use various ratios of true/false positives/negatives."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."}, {"text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."}, {"text": "The clear sign of a machine learning overfitting is if its error on testing set is much greater than the error on training set.  For instance if the model accuracy for train data is 85% and the accuracy for test/validation data is 65% then its very obvious that the model has overlearned and you should check that."}, {"text": "In machine learning, the term \"ground truth\" refers to the accuracy of the training set's classification for supervised learning techniques. This is used in statistical models to prove or disprove research hypotheses."}, {"text": "A loss function is used to optimize a machine learning algorithm. The loss is calculated on training and validation and its interpretation is based on how well the model is doing in these two sets.  An accuracy metric is used to measure the algorithm's performance in an interpretable way."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "False positive rate (FPR) is a measure of accuracy for a test: be it a medical diagnostic test, a machine learning model, or something else. In technical terms, the false positive rate is defined as the probability of falsely rejecting the null hypothesis."}]}, {"question": "Why is the probability of a continuous random variable 0", "positive_ctxs": [{"text": "The probability of a specific value of a continuous random variable will be zero because the area under a point is zero."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "The probability of each value of the discrete random variable is between 0 and\u200b 1, inclusive, and the sum of all the probabilities is 1.  It is the expected value of a discrete random variable."}, {"text": "The probability of a specific value of a continuous random variable will be zero because the area under a point is zero."}, {"text": "In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed.  A log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive."}, {"text": "The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable."}, {"text": "A continuous random variable is a random variable whose statistical distribution is continuous. Formally: A continuous random variable is a function X X X on the outcomes of some probabilistic experiment which takes values in a continuous set V V V."}]}, {"question": "How can we prevent Overfitting in transfer learning", "positive_ctxs": [{"text": "Secondly, there is more than one way to reduce overfitting: Enlarge your data set by using augmentation techniques such as flip, scale, Using regularization techniques like dropout (you already did it), but you can play with dropout rate, try more than or less than 0.5.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "deep learning - a name for an algorithm in machine learning (just like SVM, Regression etc.) transfer learning - as you may know, in order to train a Neural network it might take long time. So, we use a Neural Network that is already trained and in this way we can extract some features of new sample."}, {"text": "The basic premise of transfer learning is simple: take a model trained on a large dataset and transfer its knowledge to a smaller dataset. For object recognition with a CNN, we freeze the early convolutional layers of the network and only train the last few layers which make a prediction."}, {"text": "To convert a transfer function into state equations in phase variable form, we first convert the transfer function to a differential equation by cross-multiplying and taking the inverse Laplace transform, assuming zero initial conditions."}, {"text": "Overfitting is a significant practical difficulty for decision tree models and many other predictive models. Overfitting happens when the learning algorithm continues to develop hypotheses that reduce training set error at the cost of an. increased test set error."}, {"text": "Transfer learning without any labeled data from the target domain is referred to as unsupervised transfer learning."}, {"text": "Overfitting in Machine Learning Overfitting refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data."}, {"text": "When we have a high degree linear polynomial that is used to fit a set of points in a linear regression setup, to prevent overfitting, we use regularization, and we include a lambda parameter in the cost function. This lambda is then used to update the theta parameters in the gradient descent algorithm."}]}, {"question": "Why do we use log loss in logistic regression", "positive_ctxs": [{"text": "Log loss is used when we have {0,1} response. This is usually because when we have {0,1} response, the best models give us values in terms of probabilities. In simple words, log loss measures the UNCERTAINTY of the probabilities of your model by comparing them to the true labels."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Loss function for Logistic Regression The loss function for linear regression is squared loss. The loss function for logistic regression is Log Loss, which is defined as follows: Log Loss = \u2211 ( x , y ) \u2208 D \u2212 y log \u2061 ( y \u2032 ) \u2212 ( 1 \u2212 y ) log \u2061 where: ( x , y ) \u2208 D."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true ."}, {"text": "Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true ."}, {"text": "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)\u2208{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits."}]}, {"question": "How can I improve my GAN performance", "positive_ctxs": [{"text": "As part of the GAN series, this article looks into ways on how to improve GAN.In particular,Change the cost function for a better optimization goal.Add additional penalties to the cost function to enforce constraints.Avoid overconfidence and overfitting.Better ways of optimizing the model.Add labels."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "This post is about various evaluation metrics and how and when to use them.Accuracy, Precision, and Recall: A.  F1 Score: This is my favorite evaluation metric and I tend to use this a lot in my classification projects.  Log Loss/Binary Crossentropy.  Categorical Crossentropy.  AUC."}, {"text": "From the network operations perspective, streaming telemetry can improve efficiency in many use cases, including: Detecting problems by setting up network monitors and alerts based on pre-configured thresholds or network performance baselines. Troubleshooting connectivity and performance issues."}, {"text": "According to my POV model accuracy is more important and its all depends on the training data.  Model performance can be improved using distributed computing and parallelizing over the scored assets, whereas accuracy has to be carefully built during the model training process."}, {"text": "Gradient boosting is a greedy algorithm and can overfit a training dataset quickly. It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting."}, {"text": "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself."}, {"text": "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself."}, {"text": "Ensemble learning helps improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model."}]}, {"question": "Can we use Lstm for classification", "positive_ctxs": [{"text": "To train a deep neural network to classify sequence data, you can use an LSTM network. An LSTM network enables you to input sequence data into a network, and make predictions based on the individual time steps of the sequence data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "There are two reasons why Mean Squared Error(MSE) is a bad choice for binary classification problems:  If we use maximum likelihood estimation(MLE), assuming that the data is from a normal distribution(a wrong assumption, by the way), we get the MSE as a Cost function for optimizing our model."}, {"text": "Basically, we can think of TensorFlow as the Lego bricks (similar to NumPy and SciPy) that we can use to implement machine learning algorithms whereas Scikit-Learn comes with off-the-shelf algorithms, e.g., algorithms for classification such as SVMs, Random Forests, Logistic Regression, and many, many more."}, {"text": "TensorFlow is more of a low-level library; basically, we can think of TensorFlow as the Lego bricks (similar to NumPy and SciPy) that we can use to implement machine learning algorithms whereas scikit-learn comes with off-the-shelf algorithms, e.g., algorithms for classification such as SVMs, Random Forests, Logistic"}, {"text": "Different performance metrics are used to evaluate different Machine Learning Algorithms. For now, we will be focusing on the ones used for Classification problems. We can use classification performance metrics such as Log-Loss, Accuracy, AUC(Area under Curve) etc."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "How do you convert categorical data to numerical data", "positive_ctxs": [{"text": "Below are the methods to convert a categorical (string) input to numerical nature:Label Encoder: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables).  Convert numeric bins to number: Let's say, bins of a continuous variable are available in the data set (shown below)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Below are the methods to convert a categorical (string) input to numerical nature:Label Encoder: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables).  Convert numeric bins to number: Let's say, bins of a continuous variable are available in the data set (shown below)."}, {"text": "Below are the methods to convert a categorical (string) input to numerical nature:Label Encoder: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables).  Convert numeric bins to number: Let's say, bins of a continuous variable are available in the data set (shown below)."}, {"text": "Multi hot encoding is one of such popular encoding technique in order to successfully convert categorical variables into numerical variables.  Now, both independent variables and dependent variable became encoded and converted to numerical values from categorical values."}, {"text": "It is simply not possible to use the k-means clustering over categorical data because you need a distance between elements and that is not clear with categorical data as it is with the numerical part of your data."}, {"text": "Categorical data clustering refers to the case where the data objects are defined over categorical attributes.  That is, there is no single ordering or inherent distance function for the categorical values, and there is no mapping from categorical to numerical values that is semantically sensible."}, {"text": "So when you perform t-test for comparison of two means or ANOVA forr comparison of multiple means. You need dummy variables. In your case if the data is categorical you'll definitely need to convert them so simultaneously they are becoming dummy by themselves. Hence YES, you can use these tests for categorical data."}, {"text": "How to find accuracy of ARIMA model?Problem description: Prediction on CPU utilization.  Step 1: From Elasticsearch I collected 1000 observations and exported on Python.Step 2: Plotted the data and checked whether data is stationary or not.Step 3: Used log to convert the data into stationary form.Step 4: Done DF test, ACF and PACF.More items\u2022"}]}, {"question": "How do you deal with categorical variables in machine learning", "positive_ctxs": [{"text": "Below are the methods to convert a categorical (string) input to numerical nature:Label Encoder: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables).  Convert numeric bins to number: Let's say, bins of a continuous variable are available in the data set (shown below)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To deal with categorical variables that have more than two levels, the solution is one-hot encoding. This takes every level of the category (e.g., Dutch, German, Belgian, and other), and turns it into a variable with two levels (yes/no)."}, {"text": "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data."}, {"text": "A one hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical."}, {"text": "How to Get Started with AIPick a topic you are interested in.Find a quick solution.Improve your simple solution.Share your solution.Repeat steps 1-4 for different problems.Complete a Kaggle competition.Use machine learning professionally."}, {"text": "You do not need to learn linear algebra before you get started in machine learning, but at some time you may wish to dive deeper.  It will give you the tools to help you with the other areas of mathematics required to understand and build better intuitions for machine learning algorithms."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "In linear regression the independent variables can be categorical and/or continuous. But, when you fit the model if you have more than two category in the categorical independent variable make sure you are creating dummy variables."}]}, {"question": "Why is the pooling layer used in a convolution neural network image sensing", "positive_ctxs": [{"text": "pooling layers are used to down sample the volume of convolution neural network by reducing the small translation of the features. pooling layer also provides a parameter reduction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep NN is just a deep neural network, with a lot of layers. It can be CNN, or just a plain multilayer perceptron. CNN, or convolutional neural network, is a neural network using convolution layer and pooling layer."}, {"text": "A pooling or subsampling layer often immediately follows a convolution layer in CNN. Its role is to downsample the output of a convolution layer along both the spatial dimensions of height and width."}, {"text": "A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input."}, {"text": "A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input."}, {"text": "Convolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input. A pooling layer is a new layer added after the convolutional layer."}, {"text": "A pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling."}, {"text": "A pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling."}]}, {"question": "What is mean by weight in machine learning", "positive_ctxs": [{"text": "Weights and biases (commonly referred to as w and b) are the learnable parameters of a machine learning model.  When the inputs are transmitted between neurons, the weights are applied to the inputs along with the bias. A neuron. Weights control the signal (or the strength of the connection) between two neurons."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Explanation: The objective of perceptron learning is to adjust weight along with class identification."}, {"text": "Explanation: The objective of perceptron learning is to adjust weight along with class identification."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "So instead of updating the weight by taking in the output of a neuron in the previous layer, multiplying it by the learning rate and delta value, then subtracting that final value from the current weight, it will multiply the delta value and learning rate by 1, then subtract that final value from the bias weight in"}, {"text": "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus."}]}, {"question": "What is the purpose of analysis of variance", "positive_ctxs": [{"text": "The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.  Like univariate analysis, bivariate analysis can be descriptive or inferential."}, {"text": "The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models."}, {"text": "Univariate analysis has the purpose to describe a single variable distribution in one sample. It is the first important step of every clinical trial."}, {"text": "The variance of the sampling distribution of the mean is computed as follows: That is, the variance of the sampling distribution of the mean is the population variance divided by N, the sample size (the number of scores used to compute a mean)."}, {"text": "The variance of a set of numbers is the mean squared deviation from the mean. It is a measure of how spread out the set of numbers is.  The estimation variance is the variance of that large set of values. It measures how much, well, variance there is in an estimator from sample to sample."}]}, {"question": "What is Z score in blood test", "positive_ctxs": [{"text": "A Z score is the number of standard deviations a given result is above (positive score) or below (negative score) the age- and sex-adjusted population mean. Results that are within the IGF-1 reference interval will have a Z score between -2.0 and +2.0."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "A t score is one form of a standardized test statistic (the other you'll come across in elementary statistics is the z-score). The t score formula enables you to take an individual score and transform it into a standardized form>one which helps you to compare scores."}]}, {"question": "What is the difference between gamma distribution and exponential distribution", "positive_ctxs": [{"text": "The exponential distribution predicts the wait time until the *very first* event. The gamma distribution, on the other hand, predicts the wait time until the *k-th* event occurs."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution."}, {"text": "In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution."}, {"text": "The exponential distribution is one of the widely used continuous distributions. It is often used to model the time elapsed between events. We will now mathematically define the exponential distribution, and derive its mean and expected value."}, {"text": "For values of x > 0, the gamma function is defined using an integral formula as \u0393(x) = Integral on the interval [0, \u221e ] of \u222b 0\u221et x \u22121 e\u2212t dt. The probability density function for the gamma distribution is given by. The mean of the gamma distribution is \u03b1\u03b2 and the variance (square of the standard deviation) is \u03b1\u03b22."}, {"text": "It is very much like the exponential distribution, with \u03bb corresponding to 1/p, except that the geometric distribution is discrete while the exponential distribution is continuous."}, {"text": "In probability theory and statistics, the exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate."}, {"text": "In probability theory and statistics, the exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate."}]}, {"question": "Why do we use principal component analysis", "positive_ctxs": [{"text": "Principal Component Analysis (PCA) is used to explain the variance-covariance structure of a set of variables through linear combinations. It is often used as a dimensionality-reduction technique."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors."}, {"text": "Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Kmeans clustering algorithm is applied to reduced datasets which is done by principal component analysis dimension reduction method. Cluster analysis is one of the major data analysis methods widely used for many practical applications in emerging areas[12]."}, {"text": "Image compression with principal component analysis is a frequently occurring application of the dimension reduction technique.  As the number of principal components used to project the new data increases, the quality and representation compared to the original image improve."}, {"text": "The mathematics of factor analysis and principal component analysis (PCA) are different. Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables."}, {"text": "If your data contains both numeric and categorical variables, the best way to carry out clustering on the dataset is to create principal components of the dataset and use the principal component scores as input into the clustering."}]}, {"question": "How are statistics and data science related", "positive_ctxs": [{"text": "Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data."}, {"text": "Data science is the field of study that combines domain expertise, programming skills, and knowledge of mathematics and statistics to extract meaningful insights from data."}, {"text": "Statistical machine learning merges statistics with the computational sciences---computer science, systems science and optimization.  Moreover, by its interdisciplinary nature, statistical machine learning helps to forge new links among these fields."}, {"text": "While many people use the terms interchangeably, data science and big data analytics are unique fields, with the major difference being the scope.  Data science produces broader insights that concentrate on which questions should be asked, while big data analytics emphasizes discovering answers to questions being asked."}, {"text": "While many people use the terms interchangeably, data science and big data analytics are unique fields, with the major difference being the scope.  Data science produces broader insights that concentrate on which questions should be asked, while big data analytics emphasizes discovering answers to questions being asked."}, {"text": "The answer is a big NO. Data science gets solutions and results to specific business problems using AI as a tool. If data science is to insights, machine learning is to predictions and artificial intelligence is to actions."}, {"text": "Because data science is a broad term for multiple disciplines, machine learning fits within data science. Machine learning uses various techniques, such as regression and supervised clustering. On the other hand, the data' in data science may or may not evolve from a machine or a mechanical process."}]}, {"question": "What is ensemble in machine learning", "positive_ctxs": [{"text": "Ensemble methods"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "AdaBoost. AdaBoost is an ensemble machine learning algorithm for classification problems. It is part of a group of ensemble methods called boosting, that add new machine learning models in a series where subsequent models attempt to fix the prediction errors made by prior models."}, {"text": "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging."}, {"text": "Stacked Generalization or \u201cStacking\u201d for short is an ensemble machine learning algorithm. It involves combining the predictions from multiple machine learning models on the same dataset, like bagging and boosting."}, {"text": "Stacking, also known as stacked generalization, is an ensemble method where the models are combined using another machine learning algorithm. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models."}, {"text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."}, {"text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."}, {"text": "Ensemble methods helps improve machine learning results by combining multiple models. Using ensemble methods allows to produce better predictions compared to a single model. Therefore, the ensemble methods placed first in many prestigious machine learning competitions, such as Netflix Competition, KDD 2009, and Kaggle."}]}, {"question": "How do you determine the accuracy of a classifier", "positive_ctxs": [{"text": "You simply measure the number of correct decisions your classifier makes, divide by the total number of test examples, and the result is the accuracy of your classifier. It's that simple. The vast majority of research results report accuracy, and many practical projects do too."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "n essence, the kappa statistic is a measure of how closely the instances classified by the machine learning classifier matched the data labeled as ground truth, controlling for the accuracy of a random classifier as measured by the expected accuracy."}, {"text": "You simply measure the number of correct decisions your classifier makes, divide by the total number of test examples, and the result is the accuracy of your classifier. It's that simple. The vast majority of research results report accuracy, and many practical projects do too."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}, {"text": "Confusion matrix not only gives you insight into the errors being made by your classifier but also types of errors that are being made. This breakdown helps you to overcomes the limitation of using classification accuracy alone. Every column of the confusion matrix represents the instances of that predicted class."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "In order to label some more of the data my idea is to do the following:Build a classifier on the whole data set separating the class 'A from the unlabelled data.Run the classifier on the unlabelled data.Add the unlabelled items classified as being in class 'A' to class 'A'.Repeat."}]}, {"question": "What is cross sectional data with example", "positive_ctxs": [{"text": "Cross-sectional data, or a cross section of a study population, in statistics and econometrics is a type of data collected by observing many subjects (such as individuals, firms, countries, or regions) at the one point or period of time. The analysis might also have no regard to differences in time."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Binary cross-entropy is for multi-label classifications, whereas categorical cross entropy is for multi-class classification where each example belongs to a single class."}, {"text": "The cross product is a calculation used in order to define the correlation coefficient between two variables. SP is the sum of all cross products between two variables."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}, {"text": "Interval data is like ordinal except we can say the intervals between each value are equally split. The most common example is temperature in degrees Fahrenheit.  Ratio data is interval data with a natural zero point. For example, time is ratio since 0 time is meaningful."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Definition. Multi-label learning is an extension of the standard supervised learning setting. In contrast to standard supervised learning where one training example is associated with a single class label, in multi-label learning, one training example is associated with multiple class labels simultaneously."}]}, {"question": "How do I create a TensorFlow dataset", "positive_ctxs": [{"text": "2:537:37Suggested clip \u00b7 108 secondsPrepare your dataset for machine learning (Coding TensorFlow YouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How TensorFlow works. TensorFlow allows developers to create dataflow graphs\u2014structures that describe how data moves through a graph, or a series of processing nodes. Each node in the graph represents a mathematical operation, and each connection or edge between nodes is a multidimensional data array, or tensor."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How to Calculate a Confusion MatrixYou need a test dataset or a validation dataset with expected outcome values.Make a prediction for each row in your test dataset.From the expected outcomes and predictions count: The number of correct predictions for each class."}, {"text": "Well labeled dataset can be used to train a custom model.In the Data Labeling Service UI, you create a dataset and import items into it from the same page.Open the Data Labeling Service UI.  Click the Create button in the title bar.On the Add a dataset page, enter a name and description for the dataset.More items"}, {"text": "If you are a beginner, I can recommend you as below.Quickly learn Python first.Take a course of AI and Machine learning (several online courses are there). You can try MIT OCW also.Then start with Tutorial of TensorFlow website (https://www.tensorflow.org/versions/0.6.0/tutorials/index.html )"}]}, {"question": "What is false positive and true positive", "positive_ctxs": [{"text": "A true positive is an outcome where the model correctly predicts the positive class. Similarly, a true negative is an outcome where the model correctly predicts the negative class. A false positive is an outcome where the model incorrectly predicts the positive class."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The false discovery rate is the ratio of the number of false positive results to the number of total positive test results. Out of 10,000 people given the test, there are 450 true positive results (box at top right) and 190 false positive results (box at bottom right) for a total of 640 positive results."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "A true positive is an outcome where the model correctly predicts the positive class. Similarly, a true negative is an outcome where the model correctly predicts the negative class. A false positive is an outcome where the model incorrectly predicts the positive class."}, {"text": "A true positive is an outcome where the model correctly predicts the positive class. Similarly, a true negative is an outcome where the model correctly predicts the negative class. A false positive is an outcome where the model incorrectly predicts the positive class."}, {"text": "AUC and accuracy are fairly different things.  For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set. AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else."}]}, {"question": "Should I use R Squared or adjusted R squared", "positive_ctxs": [{"text": "3 Answers. Adjusted R2 is the better model when you compare models that have a different amount of variables. The logic behind it is, that R2 always increases when the number of variables increases. Meaning that even if you add a useless variable to you model, your R2 will still increase."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Coefficient of correlation is \u201cR\u201d value which is given in the summary table in the Regression output. R square is also called coefficient of determination. Multiply R times R to get the R square value. In other words Coefficient of Determination is the square of Coefficeint of Correlation."}, {"text": "10:1614:33Suggested clip \u00b7 106 secondsPermutation Hypothesis Test in R with Examples | R Tutorial 4.6 YouTubeStart of suggested clipEnd of suggested clip"}, {"text": "10:1614:33Suggested clip \u00b7 106 secondsPermutation Hypothesis Test in R with Examples | R Tutorial 4.6 YouTubeStart of suggested clipEnd of suggested clip"}, {"text": "AS a general thumb rule if adjusted R 2 increases when a new variables is added to the model, the variable should remain in the model. If the adjusted R2 decreases when the new variable is added then the variable should not remain in the model."}, {"text": "R is a very dynamic and versatile programming language for data science. This article deals with classification in R. Generally classifiers in R are used to predict specific category related information like reviews or ratings such as good, best or worst. Various Classifiers are: Decision Trees."}, {"text": "When a data set has a negative value, the axis will be shifted upward by \u2013MIN(R) where R is the data range containing the data. Thus if R ranges from -10 to 20, the range in the chart will range from 0 to 30."}]}, {"question": "What tests use categorical data", "positive_ctxs": [{"text": "Chi-squared test"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "So when you perform t-test for comparison of two means or ANOVA forr comparison of multiple means. You need dummy variables. In your case if the data is categorical you'll definitely need to convert them so simultaneously they are becoming dummy by themselves. Hence YES, you can use these tests for categorical data."}, {"text": "Confidence intervals and hypothesis tests are similar in that they are both inferential methods that rely on an approximated sampling distribution. Confidence intervals use data from a sample to estimate a population parameter. Hypothesis tests use data from a sample to test a specified hypothesis."}, {"text": "Nonparametric tests are also called distribution-free tests because they don't assume that your data follow a specific distribution. You may have heard that you should use nonparametric tests when your data don't meet the assumptions of the parametric test, especially the assumption about normally distributed data."}, {"text": "Nonparametric tests are also called distribution-free tests because they don't assume that your data follow a specific distribution. You may have heard that you should use nonparametric tests when your data don't meet the assumptions of the parametric test, especially the assumption about normally distributed data."}, {"text": "It is simply not possible to use the k-means clustering over categorical data because you need a distance between elements and that is not clear with categorical data as it is with the numerical part of your data."}, {"text": "Logistic regression is a pretty flexible method. It can readily use as independent variables categorical variables. Most software that use Logistic regression should let you use categorical variables.  A single column in your model can handle as many categories as needed for a single categorical variable."}, {"text": "The decision of which statistical test to use depends on the research design, the distribution of the data, and the type of variable.  In general, if the data is normally distributed, parametric tests should be used. If the data is non-normal, non-parametric tests should be used."}]}, {"question": "Can the normal distribution be used to approximate this probability", "positive_ctxs": [{"text": "The normal distribution can be used as an approximation to the binomial distribution, under certain circumstances, namely: If X ~ B(n, p) and if n is large and/or p is close to \u00bd, then X is approximately N(np, npq)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A continuity correction factor is used when you use a continuous probability distribution to approximate a discrete probability distribution. For example, when you want to use the normal to approximate a binomial.  p = probability of an event (e.g. 60%), q = probability the event doesn't happen (100% \u2013 p)."}, {"text": "These are generally used when direct sampling from the probability distribution would be difficult. Some of the use cases of MCMC methods are to approximate a target probability distribution or to compute an integral."}, {"text": "The normal approximation to the binomial is when you use a continuous distribution (the normal distribution) to approximate a discrete distribution (the binomial distribution)."}, {"text": "On the other hand, when the normal approximation is used to approximate a discrete distribution, a continuity correction can be employed so that we can approximate the probability of a specific value of the discrete distribution. The continuity correction requires adding or subtracting ."}, {"text": "Because the standard normal distribution is used to calculate critical values for the test, this test is often called the one-sample z-test."}, {"text": "Because our sample size is greater than 30, the Central Limit Theorem tells us that the sampling distribution will approximate a normal distribution.  Because we know the population standard deviation and the sample size is large, we'll use the normal distribution to find probability."}, {"text": "In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions."}]}, {"question": "What does log normally distributed mean", "positive_ctxs": [{"text": "In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed.  A log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}, {"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}, {"text": "The standard normal distribution table provides the probability that a normally distributed random variable Z, with mean equal to 0 and variance equal to 1, is less than or equal to z. It does this for positive values of z only (i.e., z-values on the right-hand side of the mean)."}, {"text": "OLS (linear regression, linear model) assumes normally distributed residuals.  Ordinary least squares assumes things like equal variance of the noise at every x location. Generalized least squares does not assume a diagonal co-variance matrix."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The t-distribution describes the standardized distances of sample means to the population mean when the population standard deviation is not known, and the observations come from a normally distributed population."}, {"text": "A multivariate normal distribution is a vector in multiple normally distributed variables, such that any linear combination of the variables is also normally distributed."}]}, {"question": "What is the difference between task function parallelism and data parallelism", "positive_ctxs": [{"text": "Task parallelism is the simultaneous execution on multiple cores of many different functions across the same or different datasets. Data parallelism (aka SIMD) is the simultaneous execution on multiple cores of the same function across the elements of a dataset."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In data parallel model, tasks are assigned to processes and each task performs similar types of operations on different data. Data parallelism is a consequence of single operations that is being applied on multiple data items. Data-parallel model can be applied on shared-address spaces and message-passing paradigms."}, {"text": "DEFINITION 1. Given a set of active nodes and an ordering on active nodes, amorphous data-parallelism is the parallelism that arises from simultaneously processing active nodes, subject to neighborhood and ordering constraints."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A machine learning task is the type of prediction or inference being made, based on the problem or question that is being asked, and the available data. For example, the classification task assigns data to categories, and the clustering task groups data according to similarity."}, {"text": "The only difference between Greedy BFS and A* BFS is in the evaluation function. For Greedy BFS the evaluation function is f(n) = h(n) while for A* the evaluation function is f(n) = g(n) + h(n)."}, {"text": "MSE is the average of the squared error that is used as the loss function for least squares regression: It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE."}, {"text": "It is calculated in the same way - by running the network forward over inputs xi and comparing the network outputs \u02c6yi with the ground truth values yi using a loss function e.g. J=1N\u2211Ni=1L(\u02c6yi,yi) where L is the individual loss function based somehow on the difference between predicted value and target."}]}, {"question": "What are the differences between Cohens Kappa and weighted Kappa", "positive_ctxs": [{"text": "To address this issue, there is a modification to Cohen's kappa called weighted Cohen's kappa.  The weighted kappa is calculated using a predefined table of weights which measure the degree of disagreement between the two raters, the higher the disagreement the higher the weight."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To analyze this data follow these steps:Open the file KAPPA.SAV.  Select Analyze/Descriptive Statistics/Crosstabs.Select Rater A as Row, Rater B as Col.Click on the Statistics button, select Kappa and Continue.Click OK to display the results for the Kappa test shown here:"}, {"text": "Cohen came up with a mechanism to calculate a value which represents the level of agreement between judges negating the agreement by chance.  You can see that balls which are agreed on by chance are removed both from agreed and total number of balls. And that is the whole intuition of Kappa value aka Kappa coefficient."}, {"text": "Accuracy is the percentage of correctly classifies instances out of all instances.  Kappa or Cohen's Kappa is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset."}, {"text": "The equation used to calculate kappa is: \u039a = PR(e), where Pr(a) is the observed agreement among the raters and Pr(e) is the hypothetical probability of the raters indicating a chance agreement. The formula was entered into Microsoft Excel and it was used to calculate the Kappa coefficient."}, {"text": "The equation used to calculate kappa is: \u039a = PR(e), where Pr(a) is the observed agreement among the raters and Pr(e) is the hypothetical probability of the raters indicating a chance agreement. The formula was entered into Microsoft Excel and it was used to calculate the Kappa coefficient."}, {"text": "Cohen suggested the Kappa result be interpreted as follows: values \u2264 0 as indicating no agreement and 0.01\u20130.20 as none to slight, 0.21\u20130.40 as fair, 0.41\u2013 0.60 as moderate, 0.61\u20130.80 as substantial, and 0.81\u20131.00 as almost perfect agreement."}, {"text": "Cohen suggested the Kappa result be interpreted as follows: values \u2264 0 as indicating no agreement and 0.01\u20130.20 as none to slight, 0.21\u20130.40 as fair, 0.41\u2013 0.60 as moderate, 0.61\u20130.80 as substantial, and 0.81\u20131.00 as almost perfect agreement."}]}, {"question": "What is feature encoding", "positive_ctxs": [{"text": "It is the process of transforming a categorical variable into a continuous variable and using them in the model. Lets start with basic and go to advanced methods. One Hot Encoding & Label Encoding."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Multi hot encoding is one of such popular encoding technique in order to successfully convert categorical variables into numerical variables.  Now, both independent variables and dependent variable became encoded and converted to numerical values from categorical values."}, {"text": "2 Answers. Simply put because one level of your categorical feature (here location) become the reference group during dummy encoding for regression and is redundant. I am quoting form here \"A categorical variable of K categories, or levels, usually enters a regression as a sequence of K-1 dummy variables."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "There are two different ways to encoding categorical variables. One-hot encoding converts it into n variables, while dummy encoding converts it into n-1 variables.  If we have k categorical variables, each of which has n values."}, {"text": "The process of dividing each feature by its range is called feature scaling. The process feature scaling is used to standardize each variables individually. The term feature scaling when it comes to data processing is also known as data normalization."}]}, {"question": "Which is an example of off policy method in reinforcement learning", "positive_ctxs": [{"text": "For example, Q-learning is an off-policy learner.  Q-learning is called off-policy because the updated policy is different from the behavior policy, so Q-Learning is off-policy. In other words, it estimates the reward for future actions and appends a value to the new state without actually following any greedy policy."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The example of reinforcement learning is your cat is an agent that is exposed to the environment. The biggest characteristic of this method is that there is no supervisor, only a real number or reward signal. Two types of reinforcement learning are 1) Positive 2) Negative."}, {"text": "Q-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It's considered off-policy because the q-learning function learns from actions that are outside the current policy, like taking random actions, and therefore a policy isn't needed."}, {"text": "Bellman equation is the basic block of solving reinforcement learning and is omnipresent in RL. It helps us to solve MDP. To solve means finding the optimal policy and value functions. The optimal value function V*(S) is one that yields maximum value."}, {"text": "From Wikipedia, the free encyclopedia. Error-driven learning is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback. It is a type of reinforcement learning."}, {"text": "Beyond the agent and the environment, there are four main elements of a reinforcement learning system: a policy, a reward, a value function, and, optionally, a model of the environment. A policy defines the way the agent behaves in a given time."}, {"text": "Definition. Multi-label learning is an extension of the standard supervised learning setting. In contrast to standard supervised learning where one training example is associated with a single class label, in multi-label learning, one training example is associated with multiple class labels simultaneously."}, {"text": "State\u2013action\u2013reward\u2013state\u2013action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning.  The acronym for the quintuple (st, at, rt, st+1, at+1) is SARSA."}]}, {"question": "What is the difference between test set and validation set", "positive_ctxs": [{"text": "\u2013 Validation set: A set of examples used to tune the parameters of a classifier, for example to choose the number of hidden units in a neural network. \u2013 Test set: A set of examples used only to assess the performance of a fully-specified classifier."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Training loss is the error on the training set of data. Validation loss is the error after running the validation set of data through the trained network. Train/valid is the ratio between the two. Unexpectedly, as the epochs increase both validation and training error drop."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Definition. In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived.  Model validation is carried out after model training."}, {"text": "Use Fisher's exact test when you have two nominal variables.  Fisher's exact test will tell you whether this difference between 81 and 31% is statistically significant. A data set like this is often called an \"R\u00d7C table,\" where R is the number of rows and C is the number of columns."}, {"text": "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived."}, {"text": "The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.  The \u201ctraining\u201d data set is the general term for the samples used to create the model, while the \u201ctest\u201d or \u201cvalidation\u201d data set is used to qualify performance."}, {"text": "The essential difference between the set and the multiset is that in a set the keys must be unique, while a multiset permits duplicate keys.  In both sets and multisets, the sort order of components is the sort order of the keys, so the components in a multiset that have duplicate keys may appear in any order."}]}, {"question": "What makes a matrix symmetric", "positive_ctxs": [{"text": "A matrix A is symmetric if it is equal to its transpose, i.e., A=AT. A matrix A is symmetric if and only if swapping indices doesn't change its components, i.e., aij=aji."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An invertible matrix is a square matrix that has an inverse. We say that a square matrix is invertible if and only if the determinant is not equal to zero. In other words, a 2 x 2 matrix is only invertible if the determinant of the matrix is not 0."}, {"text": "The determinant is a unique number associated with a square matrix. If the determinant of a matrix is equal to zero: The matrix is less than full rank. The matrix is singular."}, {"text": "Clustering starts by computing a distance between every pair of units that you want to cluster. A distance matrix will be symmetric (because the distance between x and y is the same as the distance between y and x) and will have zeroes on the diagonal (because every item is distance zero from itself)."}, {"text": "The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler."}, {"text": "In mathematics, a nonnegative matrix, written. is a matrix in which all the elements are equal to or greater than zero, that is, A positive matrix is a matrix in which all the elements are strictly greater than zero."}, {"text": "When you multiply a matrix by a number, you multiply every element in the matrix by the same number. This operation produces a new matrix, which is called a scalar multiple. For example, if x is 5, and the matrix A is: A ="}, {"text": "In mathematics, low-rank approximation is a minimization problem, in which the cost function measures the fit between a given matrix (the data) and an approximating matrix (the optimization variable), subject to a constraint that the approximating matrix has reduced rank."}]}, {"question": "What is the difference between P value and confidence interval", "positive_ctxs": [{"text": "In exploratory studies, p-values enable the recognition of any statistically noteworthy findings. Confidence intervals provide information about a range in which the true value lies with a certain degree of probability, as well as about the direction and strength of the demonstrated effect."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}, {"text": "So, if your significance level is 0.05, the corresponding confidence level is 95%. If the P value is less than your significance (alpha) level, the hypothesis test is statistically significant. If the confidence interval does not contain the null hypothesis value, the results are statistically significant."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The correct interpretation of a 95% confidence interval is that \"we are 95% confident that the population parameter is between X and X.\""}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "So the standard error of a mean provides a statement of probability about the difference between the mean of the population and the mean of the sample.  This is called the 95% confidence interval , and we can say that there is only a 5% chance that the range 86.96 to 89.04 mmHg excludes the mean of the population."}]}, {"question": "What is the difference between supervised and unsupervised learning algorithms", "positive_ctxs": [{"text": "In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Supervised learning algorithms are trained using labeled data. Unsupervised learning algorithms are trained using unlabeled data.  In unsupervised learning, only input data is provided to the model. The goal of supervised learning is to train the model so that it can predict the output when it is given new data."}, {"text": "Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Both PLS and PCA are used for dimension reduction. Partial Least Squares, use the annotated label to maximize inter-class variance.  Principal components are focus on maximize correlation. The main difference is that the PCA is unsupervised method and PLS is supervised method."}, {"text": "In unsupervised learning, an AI system is presented with unlabeled, uncategorized data and the system's algorithms act on the data without prior training. The output is dependent upon the coded algorithms. Subjecting a system to unsupervised learning is an established way of testing the capabilities of that system."}, {"text": "If the biggest problem with supervised learning is the expense of labeling the training data, the biggest problem with unsupervised learning (where the data is not labeled) is that it often doesn't work very well."}]}, {"question": "How does data augmentation work", "positive_ctxs": [{"text": "Data augmentation is a technique to artificially create new training data from existing training data. This is done by applying domain-specific techniques to examples from the training data that create new and different training examples.  The intent is to expand the training dataset with new, plausible examples."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks."}, {"text": "Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks."}, {"text": "How to Prevent OverfittingCross-validation. Cross-validation is a powerful preventative measure against overfitting.  Train with more data. It won't work every time, but training with more data can help algorithms detect the signal better.  Remove features.  Early stopping.  Regularization.  Ensembling."}, {"text": "Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model."}, {"text": "The performance of deep learning neural networks often improves with the amount of data available. Data augmentation is a technique to artificially create new training data from existing training data. This means, variations of the training set images that are likely to be seen by the model."}, {"text": "Increase Training Dataset Size Leaning on the law of large numbers, perhaps the simplest approach to reduce the model variance is to fit the model on more training data. In those cases where more data is not readily available, perhaps data augmentation methods can be used instead."}, {"text": "During the experiment, they found that one of the useful way to do text augmentation is replacing words or phrases with their synonyms . Leverage existing thesaurus help to generate lots of data in a short time. Zhang et al. select a word and replace it by synonyms according to geometric distribution."}]}, {"question": "What is precision recall and f1 score", "positive_ctxs": [{"text": "Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.  F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition. Average precision is a measure that combines recall and precision for ranked retrieval results. For one information need, the average precision is the mean of the precision scores after each relevant document is retrieved."}, {"text": "The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate."}, {"text": "Recall quantifies the number of positive class predictions made out of all positive examples in the dataset. F-Measure provides a single score that balances both the concerns of precision and recall in one number."}, {"text": "Calculate precision and recall for all objects present in the image. You also need to consider the confidence score for each object detected by the model in the image. Consider all of the predicted bounding boxes with a confidence score above a certain threshold."}, {"text": "For example, a perfect precision and recall score would result in a perfect F-Measure score:F-Measure = (2 * Precision * Recall) / (Precision + Recall)F-Measure = (2 * 1.0 * 1.0) / (1.0 + 1.0)F-Measure = (2 * 1.0) / 2.0.F-Measure = 1.0."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "In information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how"}]}, {"question": "What is the difference between class interval and class boundary", "positive_ctxs": [{"text": "In class limit, the upper extreme value of the first class interval and the lower extreme value of the next class interval will not be equal. In class boundary, the upper extreme value of the first class interval and the lower extreme value of the next class interval will be equal."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The lower class boundary is found by subtracting 0.5 units from the lower class limit and the upper class boundary is found by adding 0.5 units to the upper class limit. The difference between the upper and lower boundaries of any class."}, {"text": "In class limit, the upper extreme value of the first class interval and the lower extreme value of the next class interval will not be equal. In class boundary, the upper extreme value of the first class interval and the lower extreme value of the next class interval will be equal."}, {"text": "Class Boundaries. Separate one class in a grouped frequency distribution from another. The boundaries have one more decimal place than the raw data and therefore do not appear in the data. There is no gap between the upper boundary of one class and the lower boundary of the next class."}, {"text": "The main difference is the behavior concerning inheritance: class variables are shared between a class and all its subclasses, while class instance variables only belong to one specific class."}, {"text": "LDA (Linear Discriminant Analysis) is used when a linear boundary is required between classifiers and QDA (Quadratic Discriminant Analysis) is used to find a non-linear boundary between classifiers. LDA and QDA work better when the response classes are separable and distribution of X=x for all class is normal."}, {"text": "Class boundaries are the data values which separate classes. They are not part of the classes or the dataset. The lower class boundary of a class is defined as the average of the lower limit of the class in question and the upper limit of the previous class."}, {"text": "Class boundaries are the data values which separate classes. They are not part of the classes or the dataset. The lower class boundary of a class is defined as the average of the lower limit of the class in question and the upper limit of the previous class."}]}, {"question": "How can you distinguish between supervised and unsupervised learning", "positive_ctxs": [{"text": "In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "You can use an unsupervised learning algorithm (like clustering) to create your training data for the supervised learning algorithm but you cannot simply convert an unsupervised learning algorithm into a supervised one."}, {"text": "A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN).  MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable."}, {"text": "Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data)."}, {"text": "Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data)."}, {"text": "Machine learning uses two types of techniques: supervised learning, which trains a model on known input and output data so that it can predict future outputs, and unsupervised learning, which finds hidden patterns or intrinsic structures in input data."}, {"text": "Machine learning uses two types of techniques: supervised learning, which trains a model on known input and output data so that it can predict future outputs, and unsupervised learning, which finds hidden patterns or intrinsic structures in input data."}, {"text": "In the absence of a class label, clustering analysis is also called unsupervised learning, as opposed to supervised learning that includes classification and regression. Accordingly, approaches to clustering analysis are typically quite different from supervised learning."}]}, {"question": "How do you make a predictive model in R", "positive_ctxs": [{"text": "Clean, augment, and preprocess the data into a convenient form, if needed. Conduct an exploratory analysis of the data to get a better sense of it. Using what you find as a guide, construct a model of some aspect of the data. Use the model to answer the question you started with, and validate your results."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Overfitting is a modeling error that occurs when a function is too closely fit to a limited set of data points.  Thus, attempting to make the model conform too closely to slightly inaccurate data can infect the model with substantial errors and reduce its predictive power."}, {"text": "Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point.  Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood)."}, {"text": "A kNN algorithm is an extreme form of instance-based methods because all training observations are retained as a part of the model. It is a competitive learning algorithm because it internally uses competition between model elements (data instances) to make a predictive decision."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Predictive analytics is the process of using data analytics to make predictions based on data. This process uses data along with analysis, statistics, and machine learning techniques to create a predictive model for forecasting future events."}, {"text": "Lift is a measure of the effectiveness of a predictive model calculated as the ratio between the results obtained with and without the predictive model. Cumulative gains and lift charts are visual aids for measuring model performance."}, {"text": "Use imputation for the missing values. When the response is missing, we can use a predictive model to predict the missing response, then create a new fully-observed dataset containing the predictions instead of the missing values, and finally re-estimate the predictive model in this expanded dataset."}]}, {"question": "How do you compare two proportions in statistical testing", "positive_ctxs": [{"text": "This tests for a difference in proportions. A two proportion z-test allows you to compare two proportions to see if they are the same. The null hypothesis (H0) for the test is that the proportions are the same. The alternate hypothesis (H1) is that the proportions are not the same."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A/B testing is a way to compare two versions of something to figure out which performs better."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Benefits of Usability TestingUsability testing provides an unbiased, accurate, and direct examination of your product or website's user experience.  Usability testing is convenient.  Usability testing can tell you what your users do on your site or product and why they take these actions.More items\u2022"}, {"text": "Univariate statistics summarize only one variable at a time. Bivariate statistics compare two variables. Multivariate statistics compare more than two variables."}, {"text": "Univariate statistics summarize only one variable at a time. Bivariate statistics compare two variables. Multivariate statistics compare more than two variables."}, {"text": "Univariate statistics summarize only one variable at a time. Bivariate statistics compare two variables. Multivariate statistics compare more than two variables."}, {"text": "In your case, with three groups, you'd run ANOVA. If you need to compare the 5-point scales one at a time, then non-parametric statistics are more appropriate. To compare two groups use the Mann-Whitney U test. To compare three or more groups use the Kruskal\u2013Wallis H test."}]}, {"question": "What does regularization mean", "positive_ctxs": [{"text": "In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "So while L2 regularization does not perform feature selection the same way as L1 does, it is more useful for feature *interpretation*: a predictive feature will get a non-zero coefficient, which is often not the case with L1."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}]}, {"question": "How do deep learning algorithms use ReLU if it is not differentiable at 0", "positive_ctxs": [{"text": "At the point of non-differentiability, you can assign the derivative of the function at the point \u201cright next\u201d to the singularity and the algorithm will work fine. For example, in ReLU we can give the derivative of the function at zero as 0."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "ReLU is important because it does not saturate; the gradient is always high (equal to 1) if the neuron activates. As long as it is not a dead neuron, successive updates are fairly effective. ReLU is also very quick to evaluate."}, {"text": "The indicator function 1[0,\u221e) is right differentiable at every real a, but discontinuous at zero (note that this indicator function is not left differentiable at zero)."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Leaky ReLU & Parametric ReLU (PReLU) Leaky ReLU has two benefits: It fixes the \u201cdying ReLU\u201d problem, as it doesn't have zero-slope parts. It speeds up training. There is evidence that having the \u201cmean activation\u201d be close to 0 makes training faster."}, {"text": "ReLu refers to the Rectifier Unit, the most commonly deployed activation function for the outputs of the CNN neurons. Mathematically, it's described as: Unfortunately, the ReLu function is not differentiable at the origin, which makes it hard to use with backpropagation training."}]}, {"question": "What's the difference between a generative and discriminative model", "positive_ctxs": [{"text": "In General, A Discriminative model \u200cmodels the decision boundary between the classes. A Generative Model \u200cexplicitly models the actual distribution of each class.  A Discriminative model \u200clearns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Generative model. A generative model can estimate the probability of the instance, and also the probability of a class label. Not enough information to tell. Both generative and discriminative models can estimate probabilities (but they don't have to)."}, {"text": "Minibatch Discrimination is a discriminative technique for generative adversarial networks where we discriminate between whole minibatches of samples rather than between individual samples. This is intended to avoid collapse of the generator."}, {"text": "A generative model on the other hand will be able to produce a new picture of a either class. Typical discriminative models include logistic regression (LR), support vector machines (SVM), conditional random fields (CRFs) (specified over an undirected graph), decision trees, neural networks, and many others."}, {"text": "Machine learning algorithms can minimize forecasting error and do the forecast much faster and with the usage of more data. What's more, machine learning algorithms can analyze many alternative models at the same time, when in traditional econometrics you can analyze just one model at a time."}, {"text": "A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words."}, {"text": "A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words."}, {"text": "The convolutional neural networks (CNNs) have proven to be a powerful tool for discriminative learning. Recently researchers have also started to show interest in the generative aspects of CNNs in order to gain a deeper understanding of what they have learned and how to further improve them."}]}, {"question": "When should you use ensemble methods", "positive_ctxs": [{"text": "Ensemble learning is usually used to average the predictions of different models to get a better prediction. Ensemble methods is like using the predictions of small expert models in different parts of the input space."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "2 Answers. If you have two classes (i.e. binary classification), you should use a binary crossentropy loss. If you have more than two you should use a categorical crossentropy loss."}, {"text": "Ensemble methods helps improve machine learning results by combining multiple models. Using ensemble methods allows to produce better predictions compared to a single model. Therefore, the ensemble methods placed first in many prestigious machine learning competitions, such as Netflix Competition, KDD 2009, and Kaggle."}, {"text": "These models, when used as inputs of ensemble methods, are called \u201dbase models\u201d. In this blog post I will cover ensemble methods for classification and describe some widely known methods of ensemble: voting, stacking, bagging and boosting."}, {"text": "AdaBoost. AdaBoost is an ensemble machine learning algorithm for classification problems. It is part of a group of ensemble methods called boosting, that add new machine learning models in a series where subsequent models attempt to fix the prediction errors made by prior models."}, {"text": "You should put it after the non-linearity (eg. relu layer). If you are using dropout remember to use it before."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}]}, {"question": "Can the central limit theorem be applied to both discrete and continuous random variables", "positive_ctxs": [{"text": "The central limit theorem can be applied to both discrete and continuous random variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "So by the definition of discrete and continuous random variables, a random variable cannot be both discrete and continuous. No. For a random variable to be discrete, there must a countable sequence such that ."}, {"text": "normal approximation: The process of using the normal curve to estimate the shape of the distribution of a data set. central limit theorem: The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed."}, {"text": "The central limit theorem states that the CDF of Zn converges to the standard normal CDF. converges in distribution to the standard normal random variable as n goes to infinity, that is limn\u2192\u221eP(Zn\u2264x)=\u03a6(x), for all x\u2208R,  The Xi's can be discrete, continuous, or mixed random variables."}, {"text": "Key Terms. normal approximation: The process of using the normal curve to estimate the shape of the distribution of a data set. central limit theorem: The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed."}, {"text": "The central limit theorem has been extended to the case of dependent random variables by several authors (Bruns, Markoff, S.  The conditions under which these theorems are stated either are very restrictive or involve conditional distributions, which makes them difficult to apply."}, {"text": "The law of large numbers states that the sample mean of independent and identically distributed observations converges to a certain value. The central limit theorem describes the distribution of the difference between the sample mean and that value."}, {"text": "The central limit theorem states that the sampling distribution of the mean approaches a normal distribution, as the sample size increases.  Therefore, as a sample size increases, the sample mean and standard deviation will be closer in value to the population mean \u03bc and standard deviation \u03c3 ."}]}, {"question": "What is posterior belief", "positive_ctxs": [{"text": "1. It refers to the probability distribution of the robot pose estimate conditioned upon information such as control and sensor measurement data. The extended Kalman filter and particle filter are two different methods for computing the posterior belief."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "From Example 20.2, the posterior distribution of P is Beta(s+\u03b1, n\u2212s+\u03b1). The posterior mean is then (s+\u03b1)/(n+2\u03b1), and the posterior mode is (s+\u03b1\u22121)/(n+2\u03b1\u22122). Both of these may be taken as a point estimate p for p."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis."}, {"text": "Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information. The posterior probability is calculated by updating the prior probability using Bayes' theorem."}]}, {"question": "How is cross entropy loss calculated", "positive_ctxs": [{"text": "Cross-entropy can be calculated using the probabilities of the events from P and Q, as follows: H(P, Q) = \u2013 sum x in X P(x) * log(Q(x))"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cross Entropy is definitely a good loss function for Classification Problems, because it minimizes the distance between two probability distributions - predicted and actual.  So cross entropy make sure we are minimizing the difference between the two probability. This is the reason."}, {"text": "Cross Entropy is definitely a good loss function for Classification Problems, because it minimizes the distance between two probability distributions - predicted and actual.  So cross entropy make sure we are minimizing the difference between the two probability. This is the reason."}, {"text": "Binary cross-entropy is for multi-label classifications, whereas categorical cross entropy is for multi-class classification where each example belongs to a single class."}, {"text": "One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector."}, {"text": "MSE loss is used for regression tasks. As the name suggests, this loss is calculated by taking the mean of squared differences between actual(target) and predicted values."}, {"text": "1) Your model performs better on the training data than on the unknown validation data.  It can also happen when your training loss is calculated as a moving average over 1 epoch, whereas the validation loss is calculated after the learning phase of the same epoch."}, {"text": "The loss is calculated on training and validation and its interpretation is how well the model is doing for these two sets. Unlike accuracy, a loss is not a percentage. It is a sum of the errors made for each example in training or validation sets."}]}, {"question": "Why is it important for data to be normally distributed", "positive_ctxs": [{"text": "The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Normally distributed data The normal distribution is symmetric, so it has no skew (the mean is equal to the median). On a Q-Q plot normally distributed data appears as roughly a straight line (although the ends of the Q-Q plot often start to deviate from the straight line)."}, {"text": "A normality test is used to determine whether sample data has been drawn from a normally distributed population (within some tolerance). A number of statistical tests, such as the Student's t-test and the one-way and two-way ANOVA require a normally distributed sample population."}, {"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}, {"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}, {"text": "In statistics, normality tests are used to determine if a data set is well-modeled by a normal distribution and to compute how likely it is for a random variable underlying the data set to be normally distributed."}, {"text": "One of the most important aspects of convenience sampling is its cost effectiveness. This method allows for funds to be distributed to other aspects of the project. Oftentimes this method of sampling is used to gain funding for a larger, more thorough research project."}, {"text": "A multivariate normal distribution is a vector in multiple normally distributed variables, such that any linear combination of the variables is also normally distributed."}]}, {"question": "Why is the discount factor important in reinforcement learning", "positive_ctxs": [{"text": "The discount factor essentially determines how much the reinforcement learning agents cares about rewards in the distant future relative to those in the immediate future. If \u03b3=0, the agent will be completely myopic and only learn about actions that produce an immediate reward."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The example of reinforcement learning is your cat is an agent that is exposed to the environment. The biggest characteristic of this method is that there is no supervisor, only a real number or reward signal. Two types of reinforcement learning are 1) Positive 2) Negative."}, {"text": "State\u2013action\u2013reward\u2013state\u2013action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning.  The acronym for the quintuple (st, at, rt, st+1, at+1) is SARSA."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "Another most important role of training data for machine learning is classifying the data sets into various categorized which is very much important for supervised machine learning.  It helps them to recognize and classify the similar objects in future, thus training data is very important for such classification."}, {"text": "There are two types of factor analyses, exploratory and confirmatory. Exploratory factor analysis (EFA) is method to explore the underlying structure of a set of observed variables, and is a crucial step in the scale development process. The first step in EFA is factor extraction."}, {"text": "Deep reinforcement learning is a promising combination between two artificial intelligence techniques: reinforcement learning, which uses sequential trial and error to learn the best action to take in every situation, and deep learning, which can evaluate complex inputs and select the best response."}, {"text": "There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. For a robot that is learning to walk, the state is the position of its two legs. For a Go program, the state is the positions of all the pieces on the board."}]}, {"question": "Do I use sample or population standard deviation", "positive_ctxs": [{"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "On this site, we use the normal distribution when the population standard deviation is known and the sample size is large. We might use either distribution when standard deviation is unknown and the sample size is very large."}, {"text": "SOLUTION: sample siae =400; sample mean = 44; sample standard deviation =16. what is the margin of error? I have: 44-400/16=356/16=22.30 E=2.16/400=32/400=. 2E-4 margin or error."}, {"text": "The standard deviation of the sample mean \u02c9X that we have just computed is the standard deviation of the population divided by the square root of the sample size: \u221a10=\u221a20/\u221a2."}, {"text": "The population mean of the distribution of sample means is the same as the population mean of the distribution being sampled from.  Thus as the sample size increases, the standard deviation of the means decreases; and as the sample size decreases, the standard deviation of the sample means increases."}, {"text": "Introduction. The standard deviation is a measure of the spread of scores within a set of data. Usually, we are interested in the standard deviation of a population. However, as we are often presented with data from a sample only, we can estimate the population standard deviation from a sample standard deviation."}]}, {"question": "What is pre pruning and post pruning in decision tree", "positive_ctxs": [{"text": "Decision Tree - Overfitting There are several approaches to avoiding overfitting in building decision trees. Pre-pruning that stop growing the tree earlier, before it perfectly classifies the training set. Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "As the names suggest, pre-pruning or early stopping involves stopping the tree before it has completed classifying the training set and post-pruning refers to pruning the tree after it has finished."}, {"text": "Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood."}, {"text": "A common strategy is to grow the tree until each node contains a small number of instances then use pruning to remove nodes that do not provide additional information. Pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross-validation set."}, {"text": "A common strategy is to grow the tree until each node contains a small number of instances then use pruning to remove nodes that do not provide additional information. Pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross-validation set."}, {"text": "There are several approaches to avoiding overfitting in building decision trees.Pre-pruning that stop growing the tree earlier, before it perfectly classifies the training set.Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree."}, {"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}]}, {"question": "What is Hadoop and Big Data", "positive_ctxs": [{"text": "Hadoop is an open source, Java based framework used for storing and processing big data. The data is stored on inexpensive commodity servers that run as clusters.  Cafarella, Hadoop uses the MapReduce programming model for faster storage and retrieval of data from its nodes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition: Hadoop is a kind of framework that can handle the huge volume of Big Data and process it, whereas Big Data is just a large volume of the Data which can be in unstructured and structured data."}, {"text": "Big Data is defined as data that is huge in size. Bigdata is a term used to describe a collection of data that is huge in size and yet growing exponentially with time. Examples of Big Data generation includes stock exchanges, social media sites, jet engines, etc."}, {"text": "Big data analytics as the name suggest is the analysis of big data by discovering hidden patterns or extracting information from it.  Big data has got more to do with High-Performance Computing, while Machine Learning is a part of Data Science. Machine learning performs tasks where human interaction doesn't matter."}, {"text": "Hadoop Examples: 5 Real-World Use CasesFinancial services companies use analytics to assess risk, build investment models, and create trading algorithms; Hadoop has been used to help build and run those applications.Retailers use it to help analyze structured and unstructured data to better understand and serve their customers.More items\u2022"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "The reality is that stepwise regression is less effective the larger the number of potential explanatory variables. Stepwise regression does not solve the Big-Data problem of too many explanatory variables. Big Data exacerbates the failings of stepwise regression."}]}, {"question": "How do you know what t test to use", "positive_ctxs": [{"text": "If you are studying one group, use a paired t-test to compare the group mean over time or after an intervention, or use a one-sample t-test to compare the group mean to a standard value. If you are studying two groups, use a two-sample t-test. If you want to know only whether a difference exists, use a two-tailed test."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How to Avoid Confirmation Bias. Look for ways to challenge what you think you see. Seek out information from a range of sources, and use an approach such as the Six Thinking Hats technique to consider situations from multiple perspectives. Alternatively, discuss your thoughts with others."}, {"text": "A t score is one form of a standardized test statistic (the other you'll come across in elementary statistics is the z-score). The t score formula enables you to take an individual score and transform it into a standardized form>one which helps you to compare scores."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Here are 25 phases that you can use to increase confidence and self-esteem in your children.\u201cYou are capable.\"  \u201cThat was brave.\"  \u201cYou've got this.\"  \u201cI believe in you.\"  \u201cYou can do hard things.\"  \u201cNo matter what happens, I love you.\"  \u201cLet's try it together.\"  \u201cHow'd you do that?\"More items"}, {"text": "Face validity refers to the extent to which a test appears to measure what it is intended to measure. A test in which most people would agree that the test items appear to measure what the test is intended to measure would have strong face validity."}, {"text": "The fact is almost all big data sets, generated by systems powered by ML/AI based models, are known to be biased. However, most ML modelers are not aware of these biases and even if they are, they do not know what to do about it.  Most (almost all) big datasets generated by ML powered systems are biased."}, {"text": "\u201cThe decision of whether to use a one\u2010 or a two\u2010tailed test is important because a test statistic that falls in the region of rejection in a one\u2010tailed test may not do so in a two\u2010tailed test, even though both tests use the same probability level.\u201d"}]}, {"question": "How are the F statistic and t statistic related", "positive_ctxs": [{"text": "If you have two independent groups, and the variances are equal, F = t^2.  The value of \u201ct\u201d is then calculated as the difference between the two sample means divided by the estimated pooled sample standard deviation (in the case of two independent samples, drawn from populations of equal variance)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "F statistic is a statistic that is determined by an ANOVA test. It determines the significance of the groups of variables. The F critical value is also known as the F \u2013statistic. The F \u2013 statistic value is obtained from the F-distribution table."}, {"text": "The test statistic used in ANOVA is Student's t. One characteristic of the F distribution is that F cannot be negative. One characteristic of the F distribution is that the computed F can only range between -1 and +1."}, {"text": "The F distribution is the probability distribution associated with the f statistic. In this lesson, we show how to compute an f statistic and how to find probabilities associated with specific f statistic values."}, {"text": "Find the F Statistic (the critical value for this test). The F statistic formula is: F Statistic = variance of the group means / mean of the within group variances. You can find the F Statistic in the F-Table."}, {"text": "The F Distribution The distribution of all possible values of the f statistic is called an F distribution, with v1 = n1 - 1 and v2 = n2 - 1 degrees of freedom. The curve of the F distribution depends on the degrees of freedom, v1 and v2."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "The F Distribution The distribution of all possible values of the f statistic is called an F distribution, with v1 = n1 - 1 and v2 = n2 - 1 degrees of freedom.  The mean of the distribution is equal to v2 / ( v2 - 2 ) for v2 > 2."}]}, {"question": "What can you do with image recognition", "positive_ctxs": [{"text": "The Top 5 Uses of Image Recognition#1. Automated Image Organization \u2013 from Cloud Apps to Telecoms.#2. Stock Photography and Video Websites.#3. Visual Search for Improved Product Discoverability.#4. Image Classification for Websites with Large Visual Databases.#5.  #6.  Celebrating the Power of Image Recognition."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Object recognition is a computer vision technique for identifying objects in images or videos. Object recognition is a key output of deep learning and machine learning algorithms.  The goal is to teach a computer to do what comes naturally to humans: to gain a level of understanding of what an image contains."}, {"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "Image recognition is the process of identifying and detecting an object or a feature in a digital image or video. This concept is used in many applications like systems for factory automation, toll booth monitoring, and security surveillance. Typical image recognition algorithms include: Optical character recognition."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The most effective tool found for the task for image recognition is a deep neural network, specifically a Convolutional Neural Network (CNN)."}, {"text": "Image recognition is used to perform a large number of machine-based visual tasks, such as labeling the content of images with meta-tags, performing image content search and guiding autonomous robots, self-driving cars and accident avoidance systems."}, {"text": "categorization have not been convincingly shown. In this work we demonstrated that image segmentation can in fact improve object recognition and categorization and it also adds object localization and multi-class categorization ca- pabilities to an off-the-shelf categorization system."}]}, {"question": "How do you stop confirmation bias", "positive_ctxs": [{"text": "How To Overcome Confirmation Bias And Expand Your MindDon't Be Afraid.  Know That Your Ego Doesn't Want You To Expand Your Mind.  Think For Yourself.  If You Want To Expand Your Mind, You Must Be OK With Disagreements.  Ask Good Questions.  Keep Information Channels Open."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Confirmation bias can make people less likely to engage with information which challenges their views.  Even when people do get exposed to challenging information, confirmation bias can cause them to reject it and, perversely, become even more certain that their own beliefs are correct."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "Five tips to prevent confirmation bias Encourage and carefully consider critical views on the working hypothesis. Ensure that all stakeholders examine the primary data. Do not rely on analysis and summary from a single individual. Design experiments to actually test the hypothesis."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost."}]}, {"question": "What is the difference between continuous and discrete variables", "positive_ctxs": [{"text": "A discrete variable is a variable whose value is obtained by counting. A continuous variable is a variable whose value is obtained by measuring.  A discrete random variable X has a countable number of possible values. Example: Let X represent the sum of two dice."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If a variable can take on any value between two specified values, it is called a continuous variable; otherwise, it is called a discrete variable. Some examples will clarify the difference between discrete and continuous variables.  The number of heads could be any integer value between 0 and plus infinity."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A discrete distribution is a statistical distribution that shows the probabilities of discrete (countable) outcomes, such as 1, 2, 3  Overall, the concepts of discrete and continuous probability distributions and the random variables they describe are the underpinnings of probability theory and statistical analysis."}, {"text": "They are continuous vs discrete distributions. A first difference is that multinomial distribution M(N,p) is discrete (it generalises binomial disrtibution) whereas Dirichlet distribution is continuous (it generalizes Beta distribution)."}, {"text": "The most significant difference between regression vs classification is that while regression helps predict a continuous quantity, classification predicts discrete class labels. There are also some overlaps between the two types of machine learning algorithms."}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}]}, {"question": "What is a field in QFT", "positive_ctxs": [{"text": "Quantum fields are matter.  The simplest \u201cpractical\u201d quantum field theory is quantum electromagnetism. In it, two fields exist: the electromagnetic field and the \u201celectron field\u201d. These two fields continuously interact with each other, energy and momentum are transferred, and excitations are created or destroyed."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms."}, {"text": "Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms."}, {"text": "Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "In vector calculus and physics, a vector field is an assignment of a vector to each point in a subset of space. For instance, a vector field in the plane can be visualised as a collection of arrows with a given magnitude and direction, each attached to a point in the plane."}]}, {"question": "What is the chi square test used for and what does it tell you", "positive_ctxs": [{"text": "The Chi-square test is intended to test how likely it is that an observed distribution is due to chance. It is also called a \"goodness of fit\" statistic, because it measures how well the observed distribution of data fits with the distribution that is expected if the variables are independent."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "Use Fisher's exact test when you have two nominal variables.  Fisher's exact test will tell you whether this difference between 81 and 31% is statistically significant. A data set like this is often called an \"R\u00d7C table,\" where R is the number of rows and C is the number of columns."}, {"text": "Hold-out is when you split up your dataset into a 'train' and 'test' set. The training set is what the model is trained on, and the test set is used to see how well that model performs on unseen data."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "A dependent variable is what you measure in the experiment and what is affected during the experiment. The dependent variable responds to the independent variable. It is called dependent because it \"depends\" on the independent variable."}]}, {"question": "What is nonparametric statistics why and when is it used", "positive_ctxs": [{"text": "Nonparametric statistics is the branch of statistics that is not based solely on parametrized families of probability distributions (common examples of parameters are the mean and variance).  Nonparametric tests are often used when the assumptions of parametric tests are violated."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Abstract. Dunn's test is the appropriate nonparametric pairwise multiple- comparison. procedure when a Kruskal\u2013Wallis test is rejected, and it is now im- plemented for Stata in the dunntest command. dunntest produces multiple com- parisons following a Kruskal\u2013Wallis k-way test by using Stata's built-in kwallis command."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "In this blog we will learn what is calibration and why and when we should use it. We calibrate our model when the probability estimate of a data point belonging to a class is very important. Calibration is comparison of the actual output and the expected output given by a system."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A chi-square test is used when you want to see if there is a relationship between two categorical variables. In SPSS, the chisq option is used on the statistics subcommand of the crosstabs command to obtain the test statistic and its associated p-value."}, {"text": "A chi-square test is used when you want to see if there is a relationship between two categorical variables. In SPSS, the chisq option is used on the statistics subcommand of the crosstabs command to obtain the test statistic and its associated p-value."}, {"text": "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable."}]}, {"question": "What does positively skewed mean in statistics", "positive_ctxs": [{"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right.  In a positively skewed distribution, the mode is always less than the mean and median."}, {"text": "In positively skewed distributions, the mean is usually greater than the median, which is always greater than the mode. In negatively skewed distributions, the mean is usually less than the median, which is always less than the mode."}, {"text": "In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right. In a negatively skewed distribution, the mean is usually less than the median because the few low scores tend to shift the mean to the left."}, {"text": "If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "When p is greater than 0.5, the distribution will be positively skewed (the peak will be on the left side of the distribution, with relatively fewer observations on the right)."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}]}, {"question": "What does SVM optimize", "positive_ctxs": [{"text": "As already discussed, SVM aims at maximizing the geometric margin and returns the corresponding hyperplane.  Such points are called as support vectors (fig. - 1). Therefore, the optimization problem as defined above is equivalent to the problem of maximizing the margin value (not geometric/functional margin values)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Another common model for classification is the support vector machine (SVM). An SVM works by projecting the data into a higher dimensional space and separating it into different classes by using a single (or set of) hyperplanes. A single SVM does binary classification and can differentiate between two classes."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Classification SVM Type 1 (also known as C-SVM classification); Classification SVM Type 2 (also known as nu-SVM classification); Regression SVM Type 1 (also known as epsilon-SVM regression); Regression SVM Type 2 (also known as nu-SVM regression)."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}]}, {"question": "How does Facebook use artificial intelligence", "positive_ctxs": [{"text": "AI can signal posts of people who might be in need and/or perhaps driven by suicidal tendencies. The AI uses machine learning to flag key phrases in posts and concerned comments from friends or family members to help identify users who may be at risk."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features or business intelligence in the input data. Perceptron was introduced by Frank Rosenblatt in 1957.  A Perceptron is an algorithm for supervised learning of binary classifiers."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}, {"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}, {"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}, {"text": "Sudharsan also noted that deep meta reinforcement learning will be the future of artificial intelligence where we will implement artificial general intelligence (AGI) to build a single model to master a wide variety of tasks. Thus each model will be capable to perform a wide range of complex tasks."}, {"text": "Artificial intelligence has close connections with philosophy because both use concepts that have the same names and these include intelligence, action, consciousness, epistemology, and even free will.  These factors contributed to the emergence of the philosophy of artificial intelligence."}]}, {"question": "What is the real life example of Poisson distribution", "positive_ctxs": [{"text": "Example: One nanogram of Plutonium-239 will have an average of 2.3 radioactive decays per second, and the number of decays will follow a Poisson distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The goal of a company should be to achieve the target performance with minimal variation. That will minimize the customer dissatisfaction. A real life example of the Taguchi Loss Function would be the quality of food compared to expiration dates.  That is when the orange will taste the best (customer satisfaction)."}, {"text": "Introduction to Poisson Regression Poisson regression is also a type of GLM model where the random component is specified by the Poisson distribution of the response variable which is a count. When all explanatory variables are discrete, log-linear model is equivalent to poisson regression model."}, {"text": "A Poisson process is a non-deterministic process where events occur continuously and independently of each other.  A Poisson distribution is a discrete probability distribution that represents the probability of events (having a Poisson process) occurring in a certain period of time."}, {"text": "1 Answer. A probability distribution is the theoretical outcome of an experiment whereas a sampling distribution is the real outcome of an experiment."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}]}, {"question": "What is AlphaGo AI", "positive_ctxs": [{"text": "AlphaGo is a computer program that plays the board game Go.  In October 2015, in a match against Fan Hui, the original AlphaGo became the first computer Go program to beat a human professional Go player without handicap on a full-sized 19\u00d719 board."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "AlphaGo Zero is a version of DeepMind's Go software AlphaGo.  By playing games against itself, AlphaGo Zero surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0, reached the level of AlphaGo Master in 21 days, and exceeded all the old versions in 40 days."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Training. AlphaGo Zero's neural network was trained using TensorFlow, with 64 GPU workers and 19 CPU parameter servers.  In the first three days AlphaGo Zero played 4.9 million games against itself in quick succession."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}]}, {"question": "What is vector autoregression used for", "positive_ctxs": [{"text": "Vector autoregression (VAR) is a statistical model used to capture the relationship between multiple quantities as they change over time. VAR is a type of stochastic process model. VAR models generalize the single-variable (univariate) autoregressive model by allowing for multivariate time series."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Vector autoregression (VAR) is a statistical model used to capture the relationship between multiple quantities as they change over time.  VAR models generalize the single-variable (univariate) autoregressive model by allowing for multivariate time series. VAR models are often used in economics and the natural sciences."}, {"text": "Vector autoregression (VAR) is a statistical model used to capture the relationship between multiple quantities as they change over time.  VAR models generalize the single-variable (univariate) autoregressive model by allowing for multivariate time series. VAR models are often used in economics and the natural sciences."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "An autoregressive model is when a value from a time series is regressed on previous values from that same time series.  The order of an autoregression is the number of immediately preceding values in the series that are used to predict the value at the present time."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs). For an intended output t = \u00b11 and a classifier score y, the hinge loss of the prediction y is defined as."}]}, {"question": "Is Dual booting a good idea", "positive_ctxs": [{"text": "Dual boot is completely safe if the operating systems are installed properly with correct GRUB configuration. The main advantage of having multiple operating systems is that, you get the best performance for your work if you are working on the particular operating system's native platforms, tools, etc."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Dual booting has multiple decision impacting disadvantages, below are some of the notable ones.Restart required to access the other OS. Every time you need to switch between the OS, you will have to restart the PC.  Setup process is rather complicated.  Not very secure."}, {"text": "Genetic Algorithms are a type of learning algorithm, that uses the idea that crossing over the weights of two good neural networks, would result in a better neural network."}, {"text": "Convolutional Neural Networks (CNNs) is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough."}, {"text": "Convolutional Neural Networks (CNNs) is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough."}, {"text": "Some researchers say that it is a good idea to mean center variables prior to computing a product term (to serve as a moderator term) because doing so will help reduce multicollinearity in a regression model. Other researchers say that mean centering has no effect on multicollinearity."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2."}]}, {"question": "What is Bayesian decision theory", "positive_ctxs": [{"text": "Bayesian decision theory refers to a decision theory which is informed by Bayesian probability. It is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bayesian decision theory refers to a decision theory which is informed by Bayesian probability. It is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs.  We can then pick the option whose expected value is the highest, given the probability of rain."}, {"text": "Bayesian decision theory refers to a decision theory which is informed by Bayesian probability. It is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs.  We can then pick the option whose expected value is the highest, given the probability of rain."}, {"text": "Bayesian decision theory is a fundamental statistical approach to the problem of pattern classification.  This approach is based on quantifying the tradeoffs between various classification decisions using probability and the costs that accompany such decisions."}, {"text": "is that maximin is in decision theory and game theory etc, a rule to identify the worst outcome of each possible option to find one's best (maximum payoff) play while minimax is in decision theory, game theory, etc a decision rule used for minimizing the maximum possible loss, or maximizing the minimum gain."}, {"text": "Bayesian decision making is the process in which a decision is made based on the probability of a successful outcome, where this probability is informed by both prior information and new evidence that the decision maker obtains."}, {"text": "Bayesian decision making is the process in which a decision is made based on the probability of a successful outcome, where this probability is informed by both prior information and new evidence that the decision maker obtains."}, {"text": "Decision theory is the science of making optimal decisions in the face of uncertainty. Statistical decision theory is concerned with the making of decisions when in the presence of statistical knowledge (data) which sheds light on some of the uncertainties involved in the decision problem."}]}, {"question": "What is the decision rule for rejecting null hypothesis", "positive_ctxs": [{"text": "In an upper-tailed test the decision rule has investigators reject H0 if the test statistic is larger than the critical value. In a lower-tailed test the decision rule has investigators reject H0 if the test statistic is smaller than the critical value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive."}, {"text": "Alpha levels and beta levels are related: An alpha level is the probability of a type I error, or rejecting the null hypothesis when it is true. A beta level, usually just called beta(\u03b2), is the opposite; the probability of of accepting the null hypothesis when it's false."}, {"text": "Since p < 0.05 is enough to reject the null hypothesis (no association), p = 0.002 reinforce that rejection only. If the significance value that is p-value associated with chi-square statistics is 0.002, there is very strong evidence of rejecting the null hypothesis of no fit. It means good fit."}, {"text": "The power of a test is the probability of rejecting the null hypothesis when it is false; in other words, it is the probability of avoiding a type II error. The power may also be thought of as the likelihood that a particular study will detect a deviation from the null hypothesis given that one exists."}, {"text": "The probability of making a type I error is \u03b1, which is the level of significance you set for your hypothesis test. An \u03b1 of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis.  The probability of rejecting the null hypothesis when it is false is equal to 1\u2013\u03b2."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}]}, {"question": "How do you validate a model", "positive_ctxs": [{"text": "Using proper validation techniques helps you understand your model, but most importantly, estimate an unbiased generalization performance.Splitting your data.  k-Fold Cross-Validation (k-Fold CV)  Leave-one-out Cross-Validation (LOOCV)  Nested Cross-Validation.  Time Series CV.  Comparing Models."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In a dataset a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}, {"text": "In a dataset, a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How to train a Machine Learning model in 5 minutesModel Naming \u2014 Give Your Model a Name: Let's start with giving your model a name, describe your model and attach tags to your model.  Data Type Selection \u2014 Choose data type(Images/Text/CSV): It's time to tell us about the type of data you want to train your model.More items"}, {"text": "Probability sampling gives you the best chance to create a sample that is truly representative of the population. Using probability sampling for finding sample sizes means that you can employ statistical techniques like confidence intervals and margins of error to validate your results."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "What are the dependent events", "positive_ctxs": [{"text": "When two events are dependent events, one event influences the probability of another event. A dependent event is an event that relies on another event to happen first."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Dependent events: Two events are dependent when the outcome of the first event influences the outcome of the second event. The probability of two dependent events is the product of the probability of X and the probability of Y AFTER X occurs."}, {"text": "Two events are dependent if the outcome of the first event affects the outcome of the second event, so that the probability is changed."}, {"text": "When two events are dependent events, one event influences the probability of another event. A dependent event is an event that relies on another event to happen first."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "No. You can have dependent events that are not mutually exclusive."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What are the five steps in the backpropagation learning algorithm?Initialize weights with random values and set other parameters.Read in the input vector and the desired output.Compute the actual output via the calculations, working forward through the layers."}]}, {"question": "What is sigmoid in CNN", "positive_ctxs": [{"text": "Apply the sigmoid function as the final activation function of CNN network which is as below. The train and validation data set is little bit different, it has additional images that has multiple classes in a given images."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Dense CNN is a type of Deep CNN in which each layer is connected with another layer deeper than itself."}, {"text": "The derivative of the sigmoid function is the sigmoid function times one minus itself."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The derivative of the sigmoid is ddx\u03c3(x)=\u03c3(x)(1\u2212\u03c3(x))."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig. 2."}, {"text": "Definition. A sigmoid function is a bounded, differentiable, real function that is defined for all real input values and has a non-negative derivative at each point and exactly one inflection point. A sigmoid \"function\" and a sigmoid \"curve\" refer to the same object."}]}, {"question": "What is an intuitive explanation of singular value decomposition SVD", "positive_ctxs": [{"text": "Singular value decomposition is essentially trying to reduce a rank matrix to a rank K matrix. But what does this mean? It means that we can take a list of unique vectors, and approximate them as a linear combination of unique vectors. Take this example, the image below is an image made of 400 unique row vectors."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values.  The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning."}, {"text": "SVD is the decomposition of a matrix A into 3 matrices \u2013 U, S, and V. S is the diagonal matrix of singular values. Think of singular values as the importance values of different features in the matrix. The rank of a matrix is a measure of the unique information stored in a matrix."}, {"text": "Rank-reduced singular value decomposition T is a computed m by r matrix of term vectors where r is the rank of A\u2014a measure of its unique dimensions \u2264 min(m,n). S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors."}, {"text": "Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text."}, {"text": "The purpose of singular value decomposition is to reduce a dataset containing a large number of values to a dataset containing significantly fewer values, but which still contains a large fraction of the variability present in the original data."}, {"text": "The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler. For the case of simplicity we will focus on the SVD for real-valued matrices and ignore the case for complex numbers."}, {"text": "Non negative matrix factorization only takes positive values as input while SVD can take both positive and negative values.  SVD and NMF are both matrix decomposition techniques but they are very different and are generally used for different purposes. SVD helps in giving Eigen vectors of the input matrix."}]}, {"question": "What is an acceptable mean square error", "positive_ctxs": [{"text": "There are no acceptable limits for MSE except that the lower the MSE the higher the accuracy of prediction as there would be excellent match between the actual and predicted data set. This is as exemplified by improvement in correlation as MSE approaches zero."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "From Wikipedia, the free encyclopedia. In statistics and signal processing, a minimum mean square error (MMSE) estimator is an estimation method which minimizes the mean square error (MSE), which is a common measure of estimator quality, of the fitted values of a dependent variable."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "While the variance and the standard error of the mean are different estimates of variability, one can be derived from the other. Multiply the standard error of the mean by itself to square it. This step assumes that the standard error is a known quantity."}, {"text": "A false positive state is when the IDS identifies an activity as an attack but the activity is acceptable behavior. A false positive is a false alarm.  This is when the IDS identifies an activity as acceptable when the activity is actually an attack. That is, a false negative is when the IDS fails to catch an attack."}, {"text": "Definition 1. A statistic d is called an unbiased estimator for a function of the parameter g(\u03b8) provided that for every choice of \u03b8, E\u03b8d(X) = g(\u03b8). Any estimator that not unbiased is called biased.  Note that the mean square error for an unbiased estimator is its variance."}, {"text": "A kind of average sometimes used in statistics and engineering, often abbreviated as RMS. To find the root mean square of a set of numbers, square all the numbers in the set and then find the arithmetic mean of the squares. Take the square root of the result. This is the root mean square."}, {"text": "The standard deviation of this set of mean values is the standard error. In lieu of taking many samples one can estimate the standard error from a single sample. This estimate is derived by dividing the standard deviation by the square root of the sample size."}]}, {"question": "What is Hogarth s S Curve", "positive_ctxs": [{"text": "According to his theory, S-Shaped curved lines signify liveliness and activity and excite the attention of the viewer as contrasted with straight lines, parallel lines, or right-angled intersecting lines which signify stasis, death, or inanimate objects. He goes on to say that the S curve is the basis of all great art."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A (real-valued) random variable, often denoted by X (or some other capital letter), is a function mapping a probability space (S, P) into the real line R. This is shown in Figure 1. Associated with each point s in the domain S the function X assigns one and only one value X(s) in the range R."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "and is commonly used as an estimator for \u03c3. Nevertheless, S is a biased estimator of \u03c3."}, {"text": "The value of the step size s depends on the fauntion. If it is too small the algorithm will be too slow. If it is too large the algrithm may over shoot the global minimum and behave eratically. Usually we set s to something like 0.01 and then adjust according to the results."}, {"text": "For a spontaneous reaction, the sign on Delta G must be negative. Gibbs free energy relates enthalpy, entropy and temperature. A spontaneous reaction will always occur when Delta H is negative and Delta S is positive, and a reaction will always be non-spontaneous when Delta H is positive and Delta S is negative."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "s2 (sample variance) is the best point estimate for population variance o2. s (sample standard deviation) is the best point estimate for the population standard deviation o."}]}, {"question": "What is the significance of covariance and correlation and in what cases can we not use correlation", "positive_ctxs": [{"text": "When comparing data samples from different populations, covariance is used to determine how much two random variables vary together, whereas correlation is used to determine when a change in one variable can result in a change in another. Both covariance and correlation measure linear relationships between variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "The correlation of X and Y is the normalized covariance: Corr(X,Y) = Cov(X,Y) / \u03c3X\u03c3Y .  (Notice that the covariance of X with itself is Var(X), and therefore the correlation of X with itself is 1.) Correlation is a measure of the strength of the linear relationship between two variables."}, {"text": "It is possible to find the correlation between a categorical variable and a continuous variable using the analysis of covariance technique."}, {"text": "The correlation coefficient is the specific measure that quantifies the strength of the linear relationship between two variables in a correlation analysis. The coefficient is what we symbolize with the r in a correlation report."}, {"text": "A sample is a randomly chosen selection of elements from an underlying population. Sample covariance measures the strength and the direction of the relationship between the elements of two samples, and the sample correlation is derived from the covariance."}, {"text": "The correlation is the covariance divided by the product of the standard deviations. Therefore the correlation is the gradient of the regression line multiplied by the ratio of the standard deviations. If these standard deviations are equal the correlation is equal to the gradient."}, {"text": "Pearson correlation (r) is used to measure strength and direction of a linear relationship between two variables. Mathematically this can be done by dividing the covariance of the two variables by the product of their standard deviations. The value of r ranges between -1 and 1."}]}, {"question": "Is expected value the same as the mean", "positive_ctxs": [{"text": "There's no difference. They are two names for the same thing. They tend to be used in different contexts, though. You talk about the expected value of a random variable and the mean of a sample, population or probability distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The intercept (often labeled the constant) is the expected mean value of Y when all X=0. Start with a regression equation with one predictor, X. If X sometimes equals 0, the intercept is simply the expected mean value of Y at that value."}, {"text": "In statistics, the mode is the most commonly observed value in a set of data. For the normal distribution, the mode is also the same value as the mean and median. In many cases, the modal value will differ from the average value in the data."}, {"text": "The expected value of the sample mean is equal to the population mean \u00b5. Therefore, the sample mean is an unbiased estimator of the population mean.  Since only a sample of observations is available, the estimate of the mean can be either less than or greater than the true population mean."}, {"text": "Content validity: Is the test fully representative of what it aims to measure?  Criterion validity: Do the results correspond to a different test of the same thing?"}, {"text": "Covariance measures the total variation of two random variables from their expected values.  Obtain the data.Calculate the mean (average) prices for each asset.For each security, find the difference between each value and mean price.Multiply the results obtained in the previous step.More items"}, {"text": "Covariance measures the total variation of two random variables from their expected values.  Obtain the data.Calculate the mean (average) prices for each asset.For each security, find the difference between each value and mean price.Multiply the results obtained in the previous step.More items"}, {"text": "For a random variable yt, the unconditional mean is simply the expected value, E ( y t ) . In contrast, the conditional mean of yt is the expected value of yt given a conditioning set of variables, \u03a9t. A conditional mean model specifies a functional form for E ( y t | \u03a9 t ) . ."}]}, {"question": "What does statistical inference take into account", "positive_ctxs": [{"text": "Statistical inference involves hypothesis testing (evaluating some idea about a population using a sample) and estimation (estimating the value or potential range of values of some characteristic of the population based on that of a sample)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "POS tags make it possible for automatic text processing tools to take into account which part of speech each word is. This facilitates the use of linguistic criteria in addition to statistics."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "Bayesian analysis, a method of statistical inference (named for English mathematician Thomas Bayes) that allows one to combine prior information about a population parameter with evidence from information contained in a sample to guide the statistical inference process."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics."}]}, {"question": "How do you create a term Matrix", "positive_ctxs": [{"text": "What is a term document matrix?Clean your text responses using Insert > More > Text Analysis > Setup Text Analysis.  Add your term-document matrix using Insert > More > Text Analysis > Techniques > Create Term Document Matrix."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "The significance of Matrix is - they represent Linear transformations like rotation/scaling. A Matrix is just a stack of numbers - but very special - you can add them and subtract them and multiply them [restrictions]. The significance of Matrix is - they represent Linear transformations like rotation/scaling."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How TensorFlow works. TensorFlow allows developers to create dataflow graphs\u2014structures that describe how data moves through a graph, or a series of processing nodes. Each node in the graph represents a mathematical operation, and each connection or edge between nodes is a multidimensional data array, or tensor."}, {"text": "Latent semantic indexing (LSI) is a concept used by search engines to discover how a term and content work together to mean the same thing, even if they do not share keywords or synonyms.  Basically, though, you often need specific keywords on your pages to boost your website traffic."}, {"text": "Latent semantic indexing (LSI) is a concept used by search engines to discover how a term and content work together to mean the same thing, even if they do not share keywords or synonyms.  Basically, though, you often need specific keywords on your pages to boost your website traffic."}]}, {"question": "What is evolution of artificial intelligence", "positive_ctxs": [{"text": "Artificial intelligence (AI) is evolving\u2014literally. Researchers have created software that borrows concepts from Darwinian evolution, including \u201csurvival of the fittest,\u201d to build AI programs that improve generation after generation without human input."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}, {"text": "An artificial neural network (ANN) is the piece of a computing system designed to simulate the way the human brain analyzes and processes information. It is the foundation of artificial intelligence (AI) and solves problems that would prove impossible or difficult by human or statistical standards."}, {"text": "In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms."}]}, {"question": "Where is TensorFlow used", "positive_ctxs": [{"text": "It is an open source artificial intelligence library, using data flow graphs to build models. It allows developers to create large-scale neural networks with many layers. TensorFlow is mainly used for: Classification, Perception, Understanding, Discovering, Prediction and Creation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Tensorflow is the most famous library used in production for deep learning models.  However TensorFlow is not that easy to use. On the other hand, Keras is a high level API built on TensorFlow (and can be used on top of Theano too). It is more user-friendly and easy to use as compared to TF."}, {"text": "TensorFlow 2.0 is an updated version of TensorFlow that has been designed with a focus on simple execution, ease of use, and developer's productivity. TensorFlow 2.0 makes the development of machine learning applications even easier."}, {"text": "Caffe2 is was intended as a framework for production edge deployment whereas TensorFlow is more suited towards server production and research. TensorFlow is aimed for researchers and servers while Caffe2 is aimed towards mobile phones and other (relatively) computationally constrained platforms."}, {"text": "Values range from 0 to 1, where 0 is perfect disagreement and 1 is perfect agreement. Krippendorff suggests: \u201c[I]t is customary to require \u03b1 \u2265 . 800. Where tentative conclusions are still acceptable, \u03b1 \u2265 ."}, {"text": "A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes. When writing a TensorFlow program, the main object you manipulate and pass around is the tf$Tensor ."}, {"text": "An autoregressive (AR) model predicts future behavior based on past behavior. It's used for forecasting when there is some correlation between values in a time series and the values that precede and succeed them.  Where simple linear regression and AR models differ is that Y is dependent on X and previous values for Y."}, {"text": "Theoretically, yes. TensorFlow is designed with flexibility in mind, so that should be possible."}]}, {"question": "Is IID normal distribution", "positive_ctxs": [{"text": "If they are independent and identically distributed (IID), then they must meet the first two criteria (since differing variances constitute non-identical distributions). However, IID data need not be normally distributed.  Thus, whether or not a set of data is IID is unrelated to whether they are normal."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2."}, {"text": "The multivariate normal distribution has two or more random variables \u2014 so the bivariate normal distribution is actually a special case of the multivariate normal distribution."}, {"text": "normal distribution"}, {"text": "The \u201cregular\u201d normal distribution has one random variable; A bivariate normal distribution is made up of two independent random variables. The two variables in a bivariate normal are both are normally distributed, and they have a normal distribution when both are added together."}, {"text": "The \u201cregular\u201d normal distribution has one random variable; A bivariate normal distribution is made up of two independent random variables. The two variables in a bivariate normal are both are normally distributed, and they have a normal distribution when both are added together."}, {"text": "In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions."}]}, {"question": "What are the conditions for conducting a chi square goodness of fit test", "positive_ctxs": [{"text": "The chi-square goodness of fit test is appropriate when the following conditions are met: The sampling method is simple random sampling. The variable under study is categorical. The expected value of the number of sample observations in each level of the variable is at least 5."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship."}, {"text": "The chi-square test is the most commonly used to test the goodness of fit tests and is used for discrete distributions like the binomial distribution and the Poisson distribution, whereas The Kolmogorov-Smirnov and Anderson-Darling goodness of fit tests are used for continuous distributions."}, {"text": "In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution."}, {"text": "In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution."}, {"text": "Definition. Pearson's chi-squared test is used to assess three types of comparison: goodness of fit, homogeneity, and independence. A test of goodness of fit establishes whether an observed frequency distribution differs from a theoretical distribution."}, {"text": "In statistics, the likelihood function (often simply called the likelihood) measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters."}, {"text": "In statistics, the likelihood function (often simply called the likelihood) measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters."}]}, {"question": "What is similarity in machine learning", "positive_ctxs": [{"text": "Similarity is a machine learning method that uses a nearest neighbor approach to identify the similarity of two or more objects to each other based on algorithmic distance functions.  As a method, similarity is different than: Neural Networks which create vector nodes to predict an outcome."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In machine learning, the polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models."}, {"text": "Usually, people use the cosine similarity as a similarity metric between vectors. Now, the distance can be defined as 1-cos_similarity. The intuition behind this is that if 2 vectors are perfectly the same then similarity is 1 (angle=0) and thus, distance is 0 (1-1=0)."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis."}]}, {"question": "How do you calculate bias", "positive_ctxs": [{"text": "Calculate bias by finding the difference between an estimate and the actual value. To find the bias of a method, perform many estimates, and add up the errors in each estimate compared to the real value. Dividing by the number of estimates gives the bias of the method."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "To calculate the learnable parameters here, all we have to do is just multiply the by the shape of width m, height n, previous layer's filters d and account for all such filters k in the current layer. Don't forget the bias term for each of the filter."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Confirmation bias can make people less likely to engage with information which challenges their views.  Even when people do get exposed to challenging information, confirmation bias can cause them to reject it and, perversely, become even more certain that their own beliefs are correct."}]}, {"question": "What is the difference between normal distribution and binomial", "positive_ctxs": [{"text": "Normal distribution describes continuous data which have a symmetric distribution, with a characteristic 'bell' shape. Binomial distribution describes the distribution of binary data from a finite sample. Thus it gives the probability of getting r events out of n trials."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The normal approximation to the binomial is when you use a continuous distribution (the normal distribution) to approximate a discrete distribution (the binomial distribution)."}, {"text": "The difference between the hypergeometric and the binomial distributions.  For the binomial distribution, the probability is the same for every trial. For the hypergeometric distribution, each trial changes the probability for each subsequent trial because there is no replacement."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A gaussian and normal distribution is the same in statistics theory.  The normal distribution contains the curve between the x values and corresponding to the y values but the gaussian distribution made the curve with the x random variables and corresponding the PDF values."}, {"text": "A gaussian and normal distribution is the same in statistics theory.  The normal distribution contains the curve between the x values and corresponding to the y values but the gaussian distribution made the curve with the x random variables and corresponding the PDF values."}]}, {"question": "Where can cluster analysis be applied", "positive_ctxs": [{"text": "Clustering analysis is broadly used in many applications such as market research, pattern recognition, data analysis, and image processing. Clustering can also help marketers discover distinct groups in their customer base. And they can characterize their customer groups based on the purchasing patterns."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The data used in cluster analysis can be interval, ordinal or categorical. However, having a mixture of different types of variable will make the analysis more complicated."}, {"text": "The data used in cluster analysis can be interval, ordinal or categorical. However, having a mixture of different types of variable will make the analysis more complicated."}, {"text": "Cluster analysis is a statistical method used to group similar objects into respective categories. It can also be referred to as segmentation analysis, taxonomy analysis, or clustering.  For example, when cluster analysis is performed as part of market research, specific groups can be identified within a population."}, {"text": "The term that does not apply to cluster analysis is factorization. Cluster analysis is a way of grouping data, based on obvious similarities. It is also called as classification analysis or numerical taxonomy.  Hierarchical cluster analysis tends to build a hierarchy within clusters."}, {"text": "In Average linkage clustering, the distance between two clusters is defined as the average of distances between all pairs of objects, where each pair is made up of one object from each group. D(r,s) = Trs / ( Nr * Ns) Where Trs is the sum of all pairwise distances between cluster r and cluster s."}, {"text": "In Average linkage clustering, the distance between two clusters is defined as the average of distances between all pairs of objects, where each pair is made up of one object from each group. D(r,s) = Trs / ( Nr * Ns) Where Trs is the sum of all pairwise distances between cluster r and cluster s."}, {"text": "Cluster analysis can be a powerful data-mining tool for any organisation that needs to identify discrete groups of customers, sales transactions, or other types of behaviors and things. For example, insurance providers use cluster analysis to detect fraudulent claims, and banks use it for credit scoring."}]}, {"question": "What is knowledge representation in neural network", "positive_ctxs": [{"text": "A knowledge representation is an encoding of this information or understanding in a particular substrate, such as a set of if-then rules, a semantic network, conditional probability tables, a Venn diagram, a mind map, or the axioms of formal logic."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d."}, {"text": "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d."}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "A semantic network is a graphic notation for representing knowledge in patterns of interconnected nodes. Semantic networks became popular in artificial intelligence and natural language processing only because it represents knowledge or supports reasoning."}, {"text": "In neural image captioning systems, a recurrent neural network (RNN) is typically viewed as the primary `generation' component.  This view suggests that the RNN should only be used to encode linguistic features and that only the final representation should be `merged' with the image features at a later stage."}, {"text": "Probabilistic reasoning is a method of representation of knowledge where the concept of probability is applied to indicate the uncertainty in knowledge. Probabilistic reasoning is used in AI: When we are unsure of the predicates.  When it is known that an error occurs during an experiment."}]}, {"question": "Can you do multiclass classification with logistic regression", "positive_ctxs": [{"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class. For multiclass problem you will need to reduce it into multiple binary classification problems. Random Forest works well with a mixture of numerical and categorical features."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "This binary classifier for multiclass can be used with one-vs-all or all-vs-all reduction method. Here you can go with logistic regression, decision tree algorithms. You can go with algorithms like Naive Bayes, Neural Networks and SVM to solve multi class problem."}]}, {"question": "What are random errors", "positive_ctxs": [{"text": "Random errors are statistical fluctuations (in either direction) in the measured data due to the precision limitations of the measurement device. Random errors usually result from the experimenter's inability to take the same measurement in exactly the same way to get exact the same number."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "One reason this is done is because the normal distribution often describes the actual distribution of the random errors in real-world processes reasonably well.  Some methods, like maximum likelihood, use the distribution of the random errors directly to obtain parameter estimates."}, {"text": "What are the five steps in the backpropagation learning algorithm?Initialize weights with random values and set other parameters.Read in the input vector and the desired output.Compute the actual output via the calculations, working forward through the layers."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable."}, {"text": "Errors are normally classified in three categories: systematic errors, random errors, and blunders. Systematic errors are due to identified causes and can, in principle, be eliminated. Errors of this type result in measured values that are consistently too high or consistently too low."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is the difference between class intervals and class boundaries", "positive_ctxs": [{"text": "A symbol defining a class, such as 56 to 65 in Table (1), is called a class interval. The end numbers, 56 and 65, are called class limits; the smaller number (56) is the lower class limit, and the larger number (65) is the upper class limit."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The lower class boundary is found by subtracting 0.5 units from the lower class limit and the upper class boundary is found by adding 0.5 units to the upper class limit. The difference between the upper and lower boundaries of any class."}, {"text": "Class limits specify the span of data values that fall within a class. Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next."}, {"text": "(Select all that apply.) Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next. Class limits specify the span of data values that fall within a class."}, {"text": "The main difference is the behavior concerning inheritance: class variables are shared between a class and all its subclasses, while class instance variables only belong to one specific class."}, {"text": "Class limits specify the span of data values that fall within a class. Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next.  Class limits are not possible data values. Class boundaries specify the span of data values that fall within a class."}, {"text": "Class Boundaries. Separate one class in a grouped frequency distribution from another. The boundaries have one more decimal place than the raw data and therefore do not appear in the data. There is no gap between the upper boundary of one class and the lower boundary of the next class."}, {"text": "Class boundaries are the data values which separate classes. They are not part of the classes or the dataset. The lower class boundary of a class is defined as the average of the lower limit of the class in question and the upper limit of the previous class."}]}, {"question": "When should I use robust regression", "positive_ctxs": [{"text": "Robust regression is an alternative to least squares regression when data is contaminated with outliers or influential observations and it can also be used for the purpose of detecting influential observations. Please note: The purpose of this page is to show how to use various data analysis commands."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "When I calculate population variance, I then divide the sum of squared deviations from the mean by the number of items in the population (in example 1 I was dividing by 12). When I calculate sample variance, I divide it by the number of items in the sample less one. In our example 2, I divide by 99 (100 less 1)."}, {"text": "In robust statistics, robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Regression analysis seeks to find the relationship between one or more independent variables and a dependent variable."}, {"text": "Generally speaking, gradient boosted trees are more robust in multicollinearity situations than OLS regression.  When two independent variables are highly correlated, applying OLS regression could create problems. For example, p-values may not be reliable or even worse the OLS solution can't even be calculated."}, {"text": "Logistic regression is a pretty flexible method. It can readily use as independent variables categorical variables. Most software that use Logistic regression should let you use categorical variables.  A single column in your model can handle as many categories as needed for a single categorical variable."}, {"text": "3 Answers. Since your response is ordinal then you should use ordinal regression. At a very high level, the main difference ordinal regression and linear regression is that with linear regression the dependent variable is continuous and ordinal the dependent variable is ordinal."}, {"text": "Weighted regression The idea is to give small weights to observations associated with higher variances to shrink their squared residuals. Weighted regression minimizes the sum of the weighted squared residuals. When you use the correct weights, heteroscedasticity is replaced by homoscedasticity."}, {"text": "Unlike R-squared, you can use the standard error of the regression to assess the precision of the predictions. Approximately 95% of the observations should fall within plus/minus 2*standard error of the regression from the regression line, which is also a quick approximation of a 95% prediction interval."}]}, {"question": "How do you avoid participant bias", "positive_ctxs": [{"text": "One of the ways to help deal with this bias is to avoid shaping participants' ideas or experiences before they are faced with the experimental material. Even stating seemingly innocuous details might prime an individual to form theories or thoughts that could bias their answers or behavior."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Use Simple Random Sampling One of the most effective methods that can be used by researchers to avoid sampling bias is simple random sampling, in which samples are chosen strictly by chance. This provides equal odds for every member of the population to be chosen as a participant in the study at hand."}, {"text": "Use Simple Random Sampling One of the most effective methods that can be used by researchers to avoid sampling bias is simple random sampling, in which samples are chosen strictly by chance. This provides equal odds for every member of the population to be chosen as a participant in the study at hand."}, {"text": "Use Simple Random Sampling One of the most effective methods that can be used by researchers to avoid sampling bias is simple random sampling, in which samples are chosen strictly by chance. This provides equal odds for every member of the population to be chosen as a participant in the study at hand."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "And here are seven things you can do about that missing data:Listwise Deletion: Delete all data from any participant with missing values.  Recover the Values: You can sometimes contact the participants and ask them to fill out the missing values."}, {"text": "Table 1Type of BiasHow to AvoidSelection bias\u2022 Select patients using rigorous criteria to avoid confounding results. Patients should originate from the same general population. Well designed, prospective studies help to avoid selection bias as outcome is unknown at time of enrollment.17 more rows"}]}, {"question": "Is it possible to determine the statistical significance of a correlation coefficient", "positive_ctxs": [{"text": "If r is not between the positive and negative critical values, then the correlation coefficient is significant. If r is significant, then you may want to use the line for prediction. Suppose you computed r=0.801 using n=10 data points. df=n\u22122=10\u22122=8."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To determine whether the correlation between variables is significant, compare the p-value to your significance level. Usually, a significance level (denoted as \u03b1 or alpha) of 0.05 works well. An \u03b1 of 0.05 indicates that the risk of concluding that a correlation exists\u2014when, actually, no correlation exists\u2014is 5%."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "This is why it is important to distinguish between the statistical significance of a result and the practical significance of that result.  Null hypothesis testing is a formal approach to deciding whether a statistical relationship in a sample reflects a real relationship in the population or is just due to chance."}, {"text": "Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant."}, {"text": "Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant."}, {"text": "Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant."}, {"text": "Because the coefficient of determination is the result of squaring the correlation coefficient, the coefficient of determination cannot be negative. (Even if the correlation is negative, squaring it will result in a positive number.)"}]}, {"question": "What is target in neural network", "positive_ctxs": [{"text": "Target is the \"correct\" or desidered value for the respose associate to one input. Usually, this value will be compared with the output (the response of the neural network) to guide the learning process involving the weight changes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Neural network momentum is a simple technique that often improves both training speed and accuracy. Training a neural network is the process of finding values for the weights and biases so that for a given set of input values, the computed output values closely match the known, correct, target values."}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.  Often the weights of a neural network are contained within the hidden layers of the network."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.  Often the weights of a neural network are contained within the hidden layers of the network."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value."}]}, {"question": "What is a probability distribution example", "positive_ctxs": [{"text": "The probability distribution of a discrete random variable can always be represented by a table. For example, suppose you flip a coin two times.  The probability of getting 0 heads is 0.25; 1 head, 0.50; and 2 heads, 0.25. Thus, the table is an example of a probability distribution for a discrete random variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Discrete Probability Distributions If a random variable is a discrete variable, its probability distribution is called a discrete probability distribution. An example will make this clear. Suppose you flip a coin two times."}, {"text": "The probability distribution of a discrete random variable can always be represented by a table. For example, suppose you flip a coin two times.  The probability of getting 0 heads is 0.25; 1 head, 0.50; and 2 heads, 0.25. Thus, the table is an example of a probability distribution for a discrete random variable."}, {"text": "A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function."}, {"text": "A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function."}, {"text": "Discrete Probability Distributions If a random variable is a discrete variable, its probability distribution is called a discrete probability distribution. An example will make this clear. Suppose you flip a coin two times. This simple statistical experiment can have four possible outcomes: HH, HT, TH, and TT."}, {"text": "Continuous probability distribution: A probability distribution in which the random variable X can take on any value (is continuous). Because there are infinite values that X could assume, the probability of X taking on any one specific value is zero.  The normal distribution is one example of a continuous distribution."}]}, {"question": "How do you run a machine learning model", "positive_ctxs": [{"text": "How To Develop a Machine Learning Model From ScratchDefine adequately our problem (objective, desired outputs\u2026).Gather data.Choose a measure of success.Set an evaluation protocol and the different protocols available.Prepare the data (dealing with missing values, with categorial values\u2026).Spilit correctly the data.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Model fitting is a measure of how well a machine learning model generalizes to similar data to that on which it was trained.  During the fitting process, you run an algorithm on data for which you know the target variable, known as \u201clabeled\u201d data, and produce a machine learning model."}, {"text": "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How to Get Started with AIPick a topic you are interested in.Find a quick solution.Improve your simple solution.Share your solution.Repeat steps 1-4 for different problems.Complete a Kaggle competition.Use machine learning professionally."}, {"text": "Specifically, you learned: Machine learning algorithms are procedures that are implemented in code and are run on data. Machine learning models are output by algorithms and are comprised of model data and a prediction algorithm."}, {"text": "Machine learning algorithms can minimize forecasting error and do the forecast much faster and with the usage of more data. What's more, machine learning algorithms can analyze many alternative models at the same time, when in traditional econometrics you can analyze just one model at a time."}, {"text": "If you run a hypothesis test, there's a small chance (usually about 5%) that you'll get a bogus significant result. If you run thousands of tests, then the number of false alarms increases dramatically."}]}, {"question": "What is random error and how can it be reduced", "positive_ctxs": [{"text": "Random error can be reduced by: Using an average measurement from a set of measurements, or. Increasing sample size."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Sampling errors can be reduced by the following methods: (1) by increasing the size of the sample (2) by stratification. Increasing the size of the sample: The sampling error can be reduced by increasing the sample size. If the sample size n is equal to the population size N, then the sampling error is zero."}, {"text": "In statistics and machine learning, the bias\u2013variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters.  The bias error is an error from erroneous assumptions in the learning algorithm."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "In-group bias is notoriously difficult to avoid completely, but research shows it can be reduced through interaction with other groups, and by giving people an incentive to act in an unbiased manner."}, {"text": "In-group bias is notoriously difficult to avoid completely, but research shows it can be reduced through interaction with other groups, and by giving people an incentive to act in an unbiased manner."}, {"text": "Random error is always present in a measurement. It is caused by inherently unpredictable fluctuations in the readings of a measurement apparatus or in the experimenter's interpretation of the instrumental reading.  They can be estimated by comparing multiple measurements, and reduced by averaging multiple measurements."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "Which types of data are normally used with nonparametric statistics", "positive_ctxs": [{"text": "Parametric statistics generally require interval or ratio data. An example of this type of data is age, income, height, and weight in which the values are continuous and the intervals between values have meaning. In contrast, nonparametric statistics are typically used on data that nominal or ordinal."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population."}, {"text": "There is a wide rangeof statistical tests.  There are many different types of tests in statistics like t-test,Z-test,chi-square test, anova test ,binomial test, one sample median test etc. Choosing a Statistical test- Parametric tests are used if the data is normally distributed ."}, {"text": "The common assumptions in nonparametric tests are randomness and independence. The chi\u2010square test is one of the nonparametric tests for testing three types of statistical tests: the goodness of fit, independence, and homogeneity."}, {"text": "The common assumptions in nonparametric tests are randomness and independence. The chi\u2010square test is one of the nonparametric tests for testing three types of statistical tests: the goodness of fit, independence, and homogeneity."}, {"text": "The Wilcoxon rank-sum test is commonly used for the comparison of two groups of nonparametric (interval or not normally distributed) data, such as those which are not measured exactly but rather as falling within certain limits (e.g., how many animals died during each hour of an acute study)."}, {"text": "The following types of inferential statistics are extensively used and relatively easy to interpret: One sample test of difference/One sample hypothesis test. Confidence Interval. Contingency Tables and Chi Square Statistic."}, {"text": "Nonparametric tests are also called distribution-free tests because they don't assume that your data follow a specific distribution. You may have heard that you should use nonparametric tests when your data don't meet the assumptions of the parametric test, especially the assumption about normally distributed data."}]}, {"question": "What does normal distribution mean in statistics", "positive_ctxs": [{"text": "Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution."}, {"text": "The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution."}, {"text": "The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution."}, {"text": "The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution."}]}, {"question": "What is a continuous quantitative variable", "positive_ctxs": [{"text": "A second type of quantitative variable is called a continuous variable . This is a variable where the scale is continuous and not made up of discrete steps. For example, if playing a game of trivia, the length of time it takes a player to give an answer might be represented by a continuous variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Examples of continuous variables are body mass, height, blood pressure and cholesterol. A discrete quantitative variable is one that can only take specific numeric values (rather than any value in an interval), but those numeric values have a clear quantitative interpretation."}, {"text": "A continuous random variable is a random variable whose statistical distribution is continuous. Formally: A continuous random variable is a function X X X on the outcomes of some probabilistic experiment which takes values in a continuous set V V V."}, {"text": "A discrete variable is a variable whose value is obtained by counting. A continuous variable is a variable whose value is obtained by measuring.  A discrete random variable X has a countable number of possible values."}, {"text": "It is possible to find the correlation between a categorical variable and a continuous variable using the analysis of covariance technique."}, {"text": "Also known as a parallel boxplot or comparative boxplot, a side-by-side boxplot is a visual display comparing the levels (the possible values) of one categorical variable by means of a quantitative variable."}, {"text": "A discrete quantitative variable is one that can only take specific numeric values (rather than any value in an interval), but those numeric values have a clear quantitative interpretation. Examples of discrete quantitative variables are number of needle punctures, number of pregnancies and number of hospitalizations."}, {"text": "Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used."}]}, {"question": "How do you find the maximum random error", "positive_ctxs": [{"text": "The random (or precision) error for this data point is defined as the reading minus the average of readings, or -1.20 - (-1.42) = 0.22oC. Thus, the maximum absolute value of random error is 0.22oC. You can verify that the magnitude of the random error for any of the other data points is less than this."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How to calculate the absolute error and relative errorTo find out the absolute error, subtract the approximated value from the real one: |1.41421356237 - 1.41| = 0.00421356237.Divide this value by the real value to obtain the relative error: |0.00421356237 / 1.41421356237| = 0.298%"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Maximum likelihood estimation is a method that will find the values of \u03bc and \u03c3 that result in the curve that best fits the data.  The goal of maximum likelihood is to find the parameter values that give the distribution that maximise the probability of observing the data."}]}, {"question": "Can we use decision tree for regression", "positive_ctxs": [{"text": "Decision Tree algorithm has become one of the most used machine learning algorithm both in competitions like Kaggle as well as in business environment. Decision Tree can be used both in classification and regression problem."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "The general regression tree building methodology allows input variables to be a mixture of continuous and categorical variables. A decision tree is generated when each decision node in the tree contains a test on some input variable's value. The terminal nodes of the tree contain the predicted output variable values."}, {"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes"}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}]}, {"question": "What is the Lorenz curve and what does it suggest", "positive_ctxs": [{"text": "The Lorenz Curve is a graph that illustrates the distribution of income in the economy. It suggests that the distribution of income in the United States is unequal."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "A Lorenz curve is a graphical representation of income inequality or wealth inequality developed by American economist Max Lorenz in 1905. The graph plots percentiles of the population on the horizontal axis according to income or wealth."}, {"text": "Cowell says that the Gini coefficient is useful, particularly because it allows negative values for income and wealth, unlike some other measures of inequality. (If some amount of the population has negative wealth (owes money), the Lorenz curve will dip below the x-axis.) But the Gini coefficient also has limitations."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A dependent variable is what you measure in the experiment and what is affected during the experiment. The dependent variable responds to the independent variable. It is called dependent because it \"depends\" on the independent variable."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}]}, {"question": "What is statistical fairness", "positive_ctxs": [{"text": "At a high level, there are two families of fairness definitions.  \"Statistical\" definitions of fairness ask for equality of some error metric (like false positive rate) evaluated over \"protected\" populations. These are easy to check and satisfy, but don't provide guarantees to individuals."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}]}, {"question": "How is the normal probability distribution related to the normal curve", "positive_ctxs": [{"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. The area under the normal distribution curve represents probability and the total area under the curve sums to one."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. The area under the normal distribution curve represents probability and the total area under the curve sums to one."}, {"text": "Probability and the Normal Curve The normal distribution is a continuous probability distribution.  The total area under the normal curve is equal to 1. The probability that a normal random variable X equals any particular value is 0."}, {"text": "Probability and the Normal Curve The normal distribution is a continuous probability distribution. This has several implications for probability. The total area under the normal curve is equal to 1. The probability that a normal random variable X equals any particular value is 0."}, {"text": "Probability and the Normal Curve The normal distribution is a continuous probability distribution. This has several implications for probability. The total area under the normal curve is equal to 1. The probability that a normal random variable X equals any particular value is 0."}, {"text": "The normal curve is called Mesokurtic curve. If the curve of a distribution is peaked than a normal or mesokurtic curve then it is referred to as a Leptokurtic curve. If a curve is less peaked than a normal curve, it is called as a Platykurtic curve. That's why kurtosis of normal distribution equal to three."}, {"text": "A gaussian and normal distribution is the same in statistics theory.  The normal distribution contains the curve between the x values and corresponding to the y values but the gaussian distribution made the curve with the x random variables and corresponding the PDF values."}, {"text": "A gaussian and normal distribution is the same in statistics theory.  The normal distribution contains the curve between the x values and corresponding to the y values but the gaussian distribution made the curve with the x random variables and corresponding the PDF values."}]}, {"question": "How does back propagation work", "positive_ctxs": [{"text": "The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron."}, {"text": "Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron."}, {"text": "The three different ways of feature extraction are horizontal direction, vertical direction and diagonal direction. Recognition rate percentage for vertical, horizontal and diagonal based feature extraction using feed forward back propagation neural network as classification phase are 92.69, 93.68, 97.80 respectively."}, {"text": "Stochastic Gradient Descent (SGD) addresses both of these issues by following the negative gradient of the objective after seeing only a single or a few training examples. The use of SGD In the neural network setting is motivated by the high cost of running back propagation over the full training set."}, {"text": "Gradient Backward propagation"}, {"text": "How to Prevent OverfittingCross-validation. Cross-validation is a powerful preventative measure against overfitting.  Train with more data. It won't work every time, but training with more data can help algorithms detect the signal better.  Remove features.  Early stopping.  Regularization.  Ensembling."}, {"text": "Forward propagation is how neural networks make predictions. Input data is \u201cforward propagated\u201d through the network layer by layer to the final layer which outputs a prediction."}]}, {"question": "What is a good kappa statistic", "positive_ctxs": [{"text": "Cohen's kappa.  Cohen suggested the Kappa result be interpreted as follows: values \u2264 0 as indicating no agreement and 0.01\u20130.20 as none to slight, 0.21\u20130.40 as fair, 0.41\u2013 0.60 as moderate, 0.61\u20130.80 as substantial, and 0.81\u20131.00 as almost perfect agreement."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "From Wikipedia, the free encyclopedia. Cohen's kappa coefficient (\u03ba) is a statistic that is used to measure inter-rater reliability (and also Intra-rater reliability) for qualitative (categorical) items."}, {"text": "n essence, the kappa statistic is a measure of how closely the instances classified by the machine learning classifier matched the data labeled as ground truth, controlling for the accuracy of a random classifier as measured by the expected accuracy."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "\" The value(s) assigned to a population parameter based on the value of a sample statistic is called an estimate. The sample statistic used to estimate a population param-eter is called an estimator.\""}, {"text": "The weighted kappa is calculated using a predefined table of weights which measure the degree of disagreement between the two raters, the higher the disagreement the higher the weight."}]}, {"question": "What is gradient descent rule", "positive_ctxs": [{"text": "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm. #"}, {"text": "1 Answer. 1. 8. Without math: The delta rule uses gradient descent to minimize the error from a perceptron network's weights. Gradient descent is a general algorithm that gradually changes a vector of parameters in order to minimize an objective function."}, {"text": "The delta rule is a straight-forward application of gradient descent (i.e. hill climbing), and is easy to do because in a neural network with a single hidden layer, the neurons have direct access to the error signal."}, {"text": "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning."}]}, {"question": "Can logistic regression be used for multi class classification", "positive_ctxs": [{"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "This binary classifier for multiclass can be used with one-vs-all or all-vs-all reduction method. Here you can go with logistic regression, decision tree algorithms. You can go with algorithms like Naive Bayes, Neural Networks and SVM to solve multi class problem."}, {"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss."}, {"text": "Pros: It is easy and fast to predict class of test data set. It also perform well in multi class prediction. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data."}, {"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'."}, {"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'."}]}, {"question": "What are neural machine translation system", "positive_ctxs": [{"text": "Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "According to research conducted at Cornell University, researchers state that \u201cthe traditional phrase-based translation system which consists of many small sub-components that are tuned separately, neural machine translation attempts to build and train a single, large neural network that reads a sentence and outputs a"}, {"text": "We present a freely available open-source toolkit for training recurrent neural network based language models. It can be easily used to improve existing speech recognition and machine translation systems."}, {"text": "Attention is proposed as a method to both align and translate. Alignment is the problem in machine translation that identifies which parts of the input sequence are relevant to each word in the output, whereas translation is the process of using the relevant information to select the appropriate output."}, {"text": "1 Natural Language Processing. Computational linguistics (CL), natural language processing (NLP) and machine translation (MT) are domains whose perspective on natural language is different from that of linguistic fields such as semantics, pragmatics and syntax."}, {"text": "Image processing is often viewed as arbitrarily manipulating an image to achieve an aesthetic standard or to support a preferred reality. However, image processing is more accurately defined as a means of translation between the human visual system and digital imaging devices."}, {"text": "Image processing is often viewed as arbitrarily manipulating an image to achieve an aesthetic standard or to support a preferred reality. However, image processing is more accurately defined as a means of translation between the human visual system and digital imaging devices."}, {"text": "A neural network is either a system software or hardware that works similar to the tasks performed by neurons of human brain. Neural networks include various technologies like deep learning, and machine learning as a part of Artificial Intelligence (AI)."}]}, {"question": "How does a recurrent neural network work", "positive_ctxs": [{"text": "A recurrent neural network, however, is able to remember those characters because of its internal memory. It produces output, copies that output and loops it back into the network. Simply put: recurrent neural networks add the immediate past to the present."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A recurrent neural network (RNN) is a type of artificial neural network commonly used in speech recognition and natural language processing (NLP). RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario."}, {"text": "A recurrent neural network (RNN) is a type of artificial neural network commonly used in speech recognition and natural language processing (NLP). RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario."}, {"text": "A recurrent neural network (RNN) is a type of artificial neural network commonly used in speech recognition and natural language processing (NLP). RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario."}, {"text": "A recurrent neural network (RNN) is a type of neural network commonly used in speech recognition. RNNs are designed to recognize the sequential characteristics in data and use patterns to predict the next likely scenario."}, {"text": "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence.  Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs."}, {"text": "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior.  Both classes of networks exhibit temporal dynamic behavior."}, {"text": "We present a freely available open-source toolkit for training recurrent neural network based language models. It can be easily used to improve existing speech recognition and machine translation systems."}]}, {"question": "Why do we use quota sampling", "positive_ctxs": [{"text": "The main reason why researchers choose quota samples is that it allows the researchers to sample a subgroup that is of great interest to the study. If a study aims to investigate a trait or a characteristic of a certain subgroup, this type of sampling is the ideal technique."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between quota and stratified sampling can be explained in a way that in quota sampling researchers use non-random sampling methods to gather data from one stratum until the required quota fixed by the researcher is fulfilled."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "The difference between quota sampling and stratified sampling is: although both \"group\" participants by an important characteristic, stratified sampling relies on random selection within each group, while quota sampling relies on convenience sampling within each group."}, {"text": "In contrast, quota sampling in qualitative research is a specific technique for selecting a sample that has been defined using a purposive sampling strategy to define the categories of data sources that are eligible for a study."}, {"text": "With cluster sampling, in contrast, the sample includes elements only from sampled clusters. Multistage sampling. With multistage sampling, we select a sample by using combinations of different sampling methods. For example, in Stage 1, we might use cluster sampling to choose clusters from a population."}, {"text": "In non-probability sampling, the sample is selected based on non-random criteria, and not every member of the population has a chance of being included. Common non-probability sampling methods include convenience sampling, voluntary response sampling, purposive sampling, snowball sampling, and quota sampling."}, {"text": "One common method of probability sampling is random sampling, which assumes that each member of a population has an equal chance of being selected.  In a quota sample, a researcher deliberately sets the proportions of levels of members chosen within the sample."}]}, {"question": "Why is predictive analytics important", "positive_ctxs": [{"text": "Predictive analytics are used to determine customer responses or purchases, as well as promote cross-sell opportunities. Predictive models help businesses attract, retain and grow their most profitable customers. Improving operations. Many companies use predictive models to forecast inventory and manage resources."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Predictive analytics uses historical data to predict future events. Typically, historical data is used to build a mathematical model that captures important trends. That predictive model is then used on current data to predict what will happen next, or to suggest actions to take for optimal outcomes."}, {"text": "Predictive analytics is the process of using data analytics to make predictions based on data. This process uses data along with analysis, statistics, and machine learning techniques to create a predictive model for forecasting future events."}, {"text": "According to SAS, predictive analytics is \u201cthe use of data, statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data.  In short, predictive intelligence drives marketing decisions.\u201d"}, {"text": "Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. Confusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives."}, {"text": "Predictive analytics uses predictors or known features to create predictive models that will be used in obtaining an output. A predictive model is able to learn how different points of data connect with each other. Two of the most widely used predictive modeling techniques are regression and neural networks."}, {"text": "Cross-validation is a standard tool in analytics and is an important feature for helping you develop and fine-tune data mining models.  Cross-validation has the following applications: Validating the robustness of a particular mining model. Evaluating multiple models from a single statement."}, {"text": "Predictive analytics requires a data-driven culture: 5 steps to startDefine the business result you want to achieve.  Collect relevant data from all available sources.  Improve the quality of data using data cleaning techniques.  Choose predictive analytics solutions or build your own models to test the data.More items\u2022"}]}, {"question": "When should you use classification vs regression", "positive_ctxs": [{"text": "The most significant difference between regression vs classification is that while regression helps predict a continuous quantity, classification predicts discrete class labels. There are also some overlaps between the two types of machine learning algorithms."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The most significant difference between regression vs classification is that while regression helps predict a continuous quantity, classification predicts discrete class labels. There are also some overlaps between the two types of machine learning algorithms."}, {"text": "2 Answers. If you have two classes (i.e. binary classification), you should use a binary crossentropy loss. If you have more than two you should use a categorical crossentropy loss."}, {"text": "Logistic regression is a pretty flexible method. It can readily use as independent variables categorical variables. Most software that use Logistic regression should let you use categorical variables.  A single column in your model can handle as many categories as needed for a single categorical variable."}, {"text": "3 Answers. Since your response is ordinal then you should use ordinal regression. At a very high level, the main difference ordinal regression and linear regression is that with linear regression the dependent variable is continuous and ordinal the dependent variable is ordinal."}, {"text": "Weighted regression The idea is to give small weights to observations associated with higher variances to shrink their squared residuals. Weighted regression minimizes the sum of the weighted squared residuals. When you use the correct weights, heteroscedasticity is replaced by homoscedasticity."}, {"text": "Unlike R-squared, you can use the standard error of the regression to assess the precision of the predictions. Approximately 95% of the observations should fall within plus/minus 2*standard error of the regression from the regression line, which is also a quick approximation of a 95% prediction interval."}, {"text": "For multi class classification using SVM; It is NOT (one vs one) and NOT (one vs REST). Instead learn a two-class classifier where the feature vector is (x, y) where x is data and y is the correct label associated with the data."}]}, {"question": "Is Chi square a two tailed test", "positive_ctxs": [{"text": "Even though it evaluates the upper tail area, the chi-square test is regarded as a two-tailed test (non-directional), since it is basically just asking if the frequencies differ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "All Answers (6) Chi square test requires 2 categorical variables. T test requires 1 categorical and 1 continuous variables. You can't use them interchangeably."}, {"text": "Depending on the alternative hypothesis operator, greater than operator will be a right tailed test, less than operator is a left tailed test, and not equal operator is a two tailed test."}, {"text": "If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28. For a two tailed test, use \u03b1/2 = 0.05 and the critical region is below z = -1.645 and above z = 1.645."}, {"text": "Content validity: Is the test fully representative of what it aims to measure?  Criterion validity: Do the results correspond to a different test of the same thing?"}, {"text": "Before you can figure out if you have a left tailed test or right tailed test, you have to make sure you have a single tail to begin with. A tail in hypothesis testing refers to the tail at either end of a distribution curve. Area under a normal distribution curve. Two tails (both left and right) are shaded."}, {"text": "Chi Square distributions are positively skewed, with the degree of skew decreasing with increasing degrees of freedom. As the degrees of freedom increases, the Chi Square distribution approaches a normal distribution. Figure 1 shows density functions for three Chi Square distributions."}, {"text": "There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship."}]}, {"question": "When you reject the null hypothesis in a two tailed test at the 0.05 level of significance the probability you are making a Type I error is", "positive_ctxs": [{"text": "When the null hypothesis is true and you reject it, you make a type I error. The probability of making a type I error is \u03b1, which is the level of significance you set for your hypothesis test. An \u03b1 of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The probability of making a type I error is \u03b1, which is the level of significance you set for your hypothesis test. An \u03b1 of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis.  The probability of rejecting the null hypothesis when it is false is equal to 1\u2013\u03b2."}, {"text": "The probability of making a type I error is represented by your alpha level (\u03b1), which is the p-value below which you reject the null hypothesis. A p-value of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis."}, {"text": "in a test involving multiple comparisons, the probability of making at least one Type I error over an entire research study. The experiment-wise error rate differs from the testwise error rate, which is the probability of making a Type I error when performing a specific test or comparison."}, {"text": "Set the significance level, , the probability of making a Type I error to be small \u2014 0.01, 0.05, or 0.10. Compare the P-value to . If the P-value is less than (or equal to) , reject the null hypothesis in favor of the alternative hypothesis. If the P-value is greater than , do not reject the null hypothesis."}, {"text": "Rejecting the null hypothesis when it is in fact true is called a Type I error.  When a hypothesis test results in a p-value that is less than the significance level, the result of the hypothesis test is called statistically significant. Common mistake: Confusing statistical significance and practical significance."}, {"text": "When you reject the null hypothesis with a t-test, you are saying that the means are statistically different. The difference is meaningful. Chi Square:  When you reject the null hypothesis with a Chi-Square, you are saying that there is a relationship between the two variables."}, {"text": "In terms of the courtroom example, a type I error corresponds to convicting an innocent defendant. Type II error. The second kind of error is the failure to reject a false null hypothesis as the result of a test procedure."}]}, {"question": "What are examples of ordinal variables", "positive_ctxs": [{"text": "Examples of ordinal variables include: socio economic status (\u201clow income\u201d,\u201dmiddle income\u201d,\u201dhigh income\u201d), education level (\u201chigh school\u201d,\u201dBS\u201d,\u201dMS\u201d,\u201dPhD\u201d), income level (\u201cless than 50K\u201d, \u201c50K-100K\u201d, \u201cover 100K\u201d), satisfaction rating (\u201cextremely dislike\u201d, \u201cdislike\u201d, \u201cneutral\u201d, \u201clike\u201d, \u201cextremely like\u201d)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, a rank correlation is any of several statistics that measure an ordinal association\u2014the relationship between rankings of different ordinal variables or different rankings of the same variable, where a \"ranking\" is the assignment of the ordering labels \"first\", \"second\", \"third\", etc. to different"}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable."}, {"text": "Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable."}, {"text": "Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable."}, {"text": "The Spearman rank-order correlation coefficient (Spearman's correlation, for short) is a nonparametric measure of the strength and direction of association that exists between two variables measured on at least an ordinal scale."}, {"text": "The key assumption in ordinal regression is that the effects of any explanatory variables are consistent or proportional across the different thresholds, hence this is usually termed the assumption of proportional odds (SPSS calls this the assumption of parallel lines but it's the same thing)."}]}, {"question": "What is artificial neural network explain with example", "positive_ctxs": [{"text": "An artificial neural network (ANN) is the piece of a computing system designed to simulate the way the human brain analyzes and processes information.  ANNs have self-learning capabilities that enable them to produce better results as more data becomes available."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation."}, {"text": "Backpropagation, short for \"backward propagation of errors,\" is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights."}, {"text": "Deep neural networks. A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed."}, {"text": "A feedforward neural network is an artificial neural network wherein connections between the units do not form a cycle.  The simplest kind of neural network is a single-layer perceptron network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}]}, {"question": "What is considered a good Brier score", "positive_ctxs": [{"text": "Because it is a cost function, a lower Brier score indicates more accurate predictions while a higher Brier score indicates less accurate predictions. In its most common formulation, the best and worst possible Brier scores are 0 and 1 respectively."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An IQ (Intelligence Quotient) score from a standardized test of intelligences is a good example of an interval scale score.  IQ scores are created so that a score of 100 represents the average IQ of the population and the standard deviation (or average variability) of scores is 15."}, {"text": "Generally, a value of r greater than 0.7 is considered a strong correlation. Anything between 0.5 and 0.7 is a moderate correlation, and anything less than 0.4 is considered a weak or no correlation."}, {"text": "A score between 0 and 30 is a good range to be in, however, there is still room for progress. If your NPS is higher than 30 that would indicate that your company is doing great and has far more happy customers than unhappy ones."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The IOU is a number between 0 and 1, with larger being better. Ideally, the predicted box and the ground-truth have an IOU of 100% but in practice anything over 50% is usually considered to be a correct prediction. For the above example the IOU is 74.9% and you can see the boxes are a good match."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "Definition. A score that is derived from an individual's raw score within a distribution of scores. The standard score describes the difference of the raw score from a sample mean, expressed in standard deviations. Standard scores preserve the absolute differences between scores."}]}, {"question": "What are the types of dependent variables", "positive_ctxs": [{"text": "A dependent variable is also called:An experimental variable.An explained variable.A measured variable.An outcome variable.An output variable.A responding variable.A regressand (in regression analysis.)A response variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Regression analysis is a powerful statistical method that allows you to examine the relationship between two or more variables of interest. While there are many types of regression analysis, at their core they all examine the influence of one or more independent variables on a dependent variable."}, {"text": "An independent variable is defined within the context of a dependent variable. In the context of a model the independent variables are input whereas the dependent variables are the targets (Input vs Output). An exogenous variable is a variable whose state is independent of the state of other variables in a system."}, {"text": "Variables are the factors in a experiment that change or potentially change. There are two types of variables independent and dependent, these variables can also be viewed as the cause and effect of an experiment."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "In statistics, main effect is the effect of one of just one of the independent variables on the dependent variable. There will always be the same number of main effects as independent variables. An interaction effect occurs if there is an interaction between the independent variables that affect the dependent variable."}, {"text": "Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable."}, {"text": "Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable."}]}, {"question": "Is neural network hard to learn", "positive_ctxs": [{"text": "Training deep learning neural networks is very challenging. The best general algorithm known for solving this problem is stochastic gradient descent, where model weights are updated each iteration using the backpropagation of error algorithm. Optimization in general is an extremely difficult task."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d."}, {"text": "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d."}, {"text": "The purpose of a neural network is to learn to recognize patterns in your data. Once the neural network has been trained on samples of your data, it can make predictions by detecting similar patterns in future data. Software that learns is truly \"Artificial Intelligence\"."}, {"text": "Learning how to use machine learning isn't any harder than learning any other set of libraries for a programmer. The key is to focus on USING it, not designing the algorithm.  If you're a programmer and it's incredibly hard to learn ML, you're probably trying to learn the wrong things about it."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "An artificial neural network is an attempt to simulate the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a humanlike manner. ANNs are created by programming regular computers to behave as though they are interconnected brain cells."}, {"text": "An artificial neural network is an attempt to simulate the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a humanlike manner. ANNs are created by programming regular computers to behave as though they are interconnected brain cells."}]}, {"question": "What is a Synset in WordNet", "positive_ctxs": [{"text": "Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. Synset instances are the groupings of synonymous words that express the same concept. Some of the words have only one Synset and some have several."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Synset : a set of synonyms that share a common meaning. Each synset contains one or more lemmas, which represent a specific sense of a specific word."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "The WordNet is a part of Python's Natural Language Toolkit. It is a large word database of English Nouns, Adjectives, Adverbs and Verbs. These are grouped into some set of cognitive synonyms, which are called synsets.  In the wordnet, there are some groups of words, whose meaning are same."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "A clinical trial is a randomized controlled trial only when participants are randomly allocated to the group receiving the treatment and a control group. What participants are allocated among groups receiving different treatments the clinical trial is simply called a randomized trial."}]}, {"question": "How can you improve the accuracy of an object detection", "positive_ctxs": [{"text": "6 Freebies to Help You Increase the Performance of Your Object Detection ModelsVisually Coherent Image Mix-up for Object Detection (+3.55% mAP Boost)Classification Head Label Smoothening (+2.16% mAP Boost)Data Pre-processing (Mixed Results)Training Scheduler Revamping (+1.44% mAP Boost)More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Given an image or a video stream, an object detection model can identify which of a known set of objects might be present and provide information about their positions within the image."}, {"text": "One method of processing images is via face detection. Face detection is a branch of image processing that uses machine learning to detect faces in images. A Haar Cascade is an object detection method used to locate an object of interest in images."}, {"text": "Regularization is a set of techniques that can prevent overfitting in neural networks and thus improve the accuracy of a Deep Learning model when facing completely new data from the problem domain."}, {"text": "Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.  The ground-truth bounding boxes (i.e., the hand labeled bounding boxes from the testing set that specify where in the image our object is). The predicted bounding boxes from our model."}, {"text": "A class is a blueprint which you use to create objects. An object is an instance of a class - it's a concrete 'thing' that you made using a specific class. So, 'object' and 'instance' are the same thing, but the word 'instance' indicates the relationship of an object to its class."}, {"text": "Computing accuracy for clustering can be done by reordering the rows (or columns) of the confusion matrix so that the sum of the diagonal values is maximal. The linear assignment problem can be solved in O(n3) instead of O(n!). Coclust library provides an implementation of the accuracy for clustering results."}, {"text": "In edge detection, we find the boundaries or edges of objects in an image, by determining where the brightness of the image changes dramatically. Edge detection can be used to extract the structure of objects in an image."}]}, {"question": "How do you calculate odds ratio", "positive_ctxs": [{"text": "In a 2-by-2 table with cells a, b, c, and d (see figure), the odds ratio is odds of the event in the exposure group (a/b) divided by the odds of the event in the control or non-exposure group (c/d). Thus the odds ratio is (a/b) / (c/d) which simplifies to ad/bc."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "The odds ratio is the measure of association for a case-control study. It tells us how much higher the odds of exposure is among cases of a disease compared with controls. The odds ratio compares the odds of exposure to the factor of interest among cases to the odds of exposure to the factor among controls."}, {"text": "The odds ratio tells us how much higher the odds of exposure are among case-patients than among controls. An odds ratio of \u2022 1.0 (or close to 1.0) indicates that the odds of exposure among case-patients are the same as, or similar to, the odds of exposure among controls. The exposure is not associated with the disease."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "To conclude, the important thing to remember about the odds ratio is that an odds ratio greater than 1 is a positive association (i.e., higher number for the predictor means group 1 in the outcome), and an odds ratio less than 1 is negative association (i.e., higher number for the predictor means group 0 in the outcome"}, {"text": "To conclude, the important thing to remember about the odds ratio is that an odds ratio greater than 1 is a positive association (i.e., higher number for the predictor means group 1 in the outcome), and an odds ratio less than 1 is negative association (i.e., higher number for the predictor means group 0 in the outcome"}, {"text": "An odds ratio is a measure of association between the presence or absence of two properties.  The value of the odds ratio tells you how much more likely someone under 25 might be to make a claim, for example, and the associated confidence interval indicates the degree of uncertainty associated with that ratio."}]}, {"question": "In the paper Sequence to Sequence Learning with Neural Networks why does reversing the source sentence allow better performance on longer sentences", "positive_ctxs": [{"text": "By reversing the words in the source sentence, the average distance between corresponding words in the source and target language is unchanged. However, the first few words in the source language are now very close to the first few words in the target language, so the problem's minimal time lag is greatly reduced."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Recurrent Neural Networks(RNN) are a type of Neural Network where the output from the previous step is fed as input to the current step. RNN's are mainly used for, Sequence Classification \u2014 Sentiment Classification & Video Classification."}, {"text": "The reason why Convolutional Neural Networks (CNNs) do so much better than classic neural networks on images and videos is that the convolutional layers take advantage of inherent properties of images. Simple feedforward neural networks don't see any order in their inputs."}, {"text": "Underfitting in Neural Networks Underfitting happens when the network is not able to generate accurate predictions on the training set\u2014not to mention the validation set."}, {"text": "While Neural Networks use neurons to transmit data in the form of input values and output values through connections, Deep Learning is associated with the transformation and extraction of feature which attempts to establish a relationship between stimuli and associated neural responses present in the brain."}, {"text": "While Neural Networks use neurons to transmit data in the form of input values and output values through connections, Deep Learning is associated with the transformation and extraction of feature which attempts to establish a relationship between stimuli and associated neural responses present in the brain."}, {"text": "\u00b710 min read. In this article, I will present to you the most sophisticated optimization algorithms in Deep Learning that allow neural networks to learn faster and achieve better performance. These algorithms are Stochastic Gradient Descent with Momentum, AdaGrad, RMSProp, and Adam Optimizer."}, {"text": "- Population Based Training - It is open-source. The library connected with DeepMind's paper ( [1711.09846] Population Based Training of Neural Networks ) should be enough to start with something."}]}, {"question": "What is an intuitive explanation of the Akaike information criterion", "positive_ctxs": [{"text": "The Akaike information criterion (AIC) is an estimator of out-of-sample prediction error and thereby relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Akaike information criterion (AIC) is a mathematical method for evaluating how well a model fits the data it was generated from. In statistics, AIC is used to compare different possible models and determine which one is the best fit for the data."}, {"text": "The beta value is used in measuring how effectively the predictor variable influences the criterion variable, it is measured in terms of standard deviation. R, is the measure of association between the observed value and the predicted value of the criterion variable."}, {"text": "The most intuitive way to increase the frequency resolution of an FFT is to increase the size while keeping the sampling frequency constant. Doing this will increase the number of frequency bins that are created, decreasing the frequency difference between each."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Tests of Correlation: The validity of a test is measured by the strength of association, or correlation, between the results obtained by the test and by the criterion measure."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}]}, {"question": "What is the origin of the receiver operating characteristic ROC terminology logistic roc history statistics", "positive_ctxs": [{"text": "Origin of the Term The term \u201cReceiver Operating Characteristic\u201d has its roots in World War II. ROC curves were originally developed by the British as part of the \u201cChain Home\u201d radar system. ROC analysis was used to analyze radar data to differentiate between enemy aircraft and signal noise (e.g. flocks of geese)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The receiver operating characteristic (ROC) curve is a two dimensional graph in which the false positive rate is plotted on the X axis and the true positive rate is plotted on the Y axis. The ROC curves are useful to visualize and compare the performance of classifier methods (see Figure 1)."}, {"text": "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate."}, {"text": "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate."}, {"text": "No no need to standardize. Because by definition the correlation coefficient is independent of change of origin and scale. As such standardization will not alter the value of correlation."}, {"text": "It can be seen that the function of the loss of quality is a U-shaped curve, which is determined by the following simple quadratic function: L(x)= Quality loss function. x = Value of the quality characteristic (observed). N = Nominal value of the quality characteristic (Target value \u2013 target)."}, {"text": "The test statistic used in ANOVA is Student's t. One characteristic of the F distribution is that F cannot be negative. One characteristic of the F distribution is that the computed F can only range between -1 and +1."}, {"text": "As the area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test, the areas under ROC curves are used to compare the usefulness of tests. The term ROC stands for Receiver Operating Characteristic."}]}, {"question": "What should I look for in an analytics tool", "positive_ctxs": [{"text": "10 things to consider before choosing enterprise analytics platformAnalytic approach and data accuracy.Features and Tracking Types.Connectivity and Integration.Professional Services and Support.Data Storage Options: SaaS vs. Self-Hosting.Legal compliance.Supplier and Software Reliability.Ongoing and future costs.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Cross-validation is a standard tool in analytics and is an important feature for helping you develop and fine-tune data mining models.  Cross-validation has the following applications: Validating the robustness of a particular mining model. Evaluating multiple models from a single statement."}, {"text": "Top 10 Data Analytics toolsR Programming. R is the leading analytics tool in the industry and widely used for statistics and data modeling.  Tableau Public:  SAS:  Apache Spark.  Excel.  RapidMiner:KNIME.  QlikView.More items\u2022"}, {"text": "A partial correlation is basically the correlation between two variables when a third variable is held constant.  If we look at the relationship between exercise and weight loss, we see a negative correlation, which sounds bad but isn't. It means that the more I exercise, the more weight I lose."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "While many people use the terms interchangeably, data science and big data analytics are unique fields, with the major difference being the scope.  Data science produces broader insights that concentrate on which questions should be asked, while big data analytics emphasizes discovering answers to questions being asked."}, {"text": "While many people use the terms interchangeably, data science and big data analytics are unique fields, with the major difference being the scope.  Data science produces broader insights that concentrate on which questions should be asked, while big data analytics emphasizes discovering answers to questions being asked."}]}, {"question": "Do residual plots determine if a function is a good fit", "positive_ctxs": [{"text": "Mentor: Well, if the line is a good fit for the data then the residual plot will be random. However, if the line is a bad fit for the data then the plot of the residuals will have a pattern."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Mentor: Well, if the line is a good fit for the data then the residual plot will be random. However, if the line is a bad fit for the data then the plot of the residuals will have a pattern."}, {"text": "Mentor: Well, if the line is a good fit for the data then the residual plot will be random. However, if the line is a bad fit for the data then the plot of the residuals will have a pattern."}, {"text": "Answer. The low value of loss function determines whether a model is a good fit for the datasets."}, {"text": "The residual plot shows a fairly random pattern - the first residual is positive, the next two are negative, the fourth is positive, and the last residual is negative. This random pattern indicates that a linear model provides a decent fit to the data."}, {"text": "During training stage the residual network alters the weights until the output is equivalent to the identity function.  In turn the identity function helps in building a deeper network. The residual function then maps the identity, weights and biases to fit the actual value."}, {"text": "A learning curve plots the score over varying numbers of training samples, while a validation curve plots the score over a varying hyper parameter. The learning curve is a tool for finding out if an estimator would benefit from more data, or if the model is too simple (biased)."}, {"text": "Even if a model-fitting procedure has been used, R2 may still be negative, for example when linear regression is conducted without including an intercept, or when a non-linear function is used to fit the data."}]}, {"question": "What is meant by swarm intelligence", "positive_ctxs": [{"text": "Swarm intelligence is the discipline that deals with natural and artificial systems composed of many individuals that coordinate using decentralized control and self-organization."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Examples in natural systems of swarm intelligence include bird flocking, ant foraging, and fish schooling. Inspired by swarm's such behavior, a class of algorithms is proposed for tackling optimization problems, usually under the title of swarm intelligence algorithms (SIAs) [203]."}, {"text": "Rather, the swarm of humans uses software to input their opinions in real time, thus making micro-changes to the rest of the swarm and the inputs of other members. Studies show that swarm intelligence consistently outperforms individuals and crowds working without the algorithms."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "There is a direct relationship between the coefficients produced by logit and the odds ratios produced by logistic. First, let's define what is meant by a logit: A logit is defined as the log base e (log) of the odds. : [1] logit(p) = log(odds) = log(p/q) The range is negative infinity to positive infinity."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "What is Sigma in Gaussian blur", "positive_ctxs": [{"text": "The role of sigma in the Gaussian filter is to control the variation around its mean value. So as the Sigma becomes larger the more variance allowed around mean and as the Sigma becomes smaller the less variance allowed around mean.  it simply means that we apply a kernel on every pixel in the image."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A Gaussian blur effect is typically generated by convolving an image with an FIR kernel of Gaussian values.  In the first pass, a one-dimensional kernel is used to blur the image in only the horizontal or vertical direction. In the second pass, the same one-dimensional kernel is used to blur in the remaining direction."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise.  The Gaussian filter alone will blur edges and reduce contrast. The Median filter is a non-linear filter that is most commonly used as a simple way to reduce noise in an image."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "1. The Canny edge detector is a linear filter because it uses the Gaussian filter to blur the image and then uses the linear filter to compute the gradient. Solution False. Though it does those things, it also has non-linear operations: thresholding, hysteresis, non-maximum suppression."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}]}, {"question": "What is the strength of using a hashing function", "positive_ctxs": [{"text": "For digital signature applications, the security strength of a hash function is normally its collision resistance strength. When appropriate processing is applied to the data before it is hashed, the security strength may be more than the collision resistance strength (see Section 5.2. 3)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Rabin-Karp algorithm makes use of hash functions and the rolling hash technique. A hash function is essentially a function that maps one thing to a value. In particular, hashing can map data of arbitrary size to a value of fixed size."}, {"text": "Hashing is the practice of using an algorithm to map data of any size to a fixed length. This is called a hash value (or sometimes hash code or hash sums or even a hash digest if you're feeling fancy). Whereas encryption is a two-way function, hashing is a one-way function.  Every hash value is unique."}, {"text": "\u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance."}, {"text": "\u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance."}, {"text": "\u201cCovariance\u201d indicates the direction of the linear relationship between variables. \u201cCorrelation\u201d on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."}]}, {"question": "What is systematic sampling used for", "positive_ctxs": [{"text": "Use systematic sampling when there's low risk of data manipulation. Systematic sampling is the preferred method over simple random sampling when a study maintains a low risk of data manipulation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "Systematic sampling is easier to do than random sampling. In systematic sampling, the list of elements is \"counted off\". That is, every kth element is taken.  Stratified sampling also divides the population into groups called strata."}, {"text": "In simple random sampling, each data point has an equal probability of being chosen. Meanwhile, systematic sampling chooses a data point per each predetermined interval. While systematic sampling is easier to execute than simple random sampling, it can produce skewed results if the data set exhibits patterns."}, {"text": "In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).  In cluster sampling, the sampling unit is the whole cluster; Instead of sampling individuals from within each group, a researcher will study whole clusters."}, {"text": "Despite the sample population being selected in advance, systematic sampling is still thought of as being random if the periodic interval is determined beforehand and the starting point is random."}, {"text": "In systematic sampling, the list of elements is \"counted off\". That is, every kth element is taken.  Stratified sampling also divides the population into groups called strata. However, this time it is by some characteristic, not geographically."}, {"text": "Under simple random sampling, a sample of items is chosen randomly from a population, and each item has an equal probability of being chosen. Meanwhile, systematic sampling involves selecting items from an ordered population using a skip or sampling interval."}]}, {"question": "What is the need of non uniform quantization", "positive_ctxs": [{"text": "Uniform quantization may lead to either slope overload distortion or Granular noise.  Thus we go for Non Uniform quantization because step size varies based on the message signal and it will be tracked with minimal amount of error."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The type of quantization in which the quantization levels are uniformly spaced is termed as a Uniform Quantization. The type of quantization in which the quantization levels are unequal and mostly the relation between them is logarithmic, is termed as a Non-uniform Quantization."}, {"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}, {"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}, {"text": "The quantizing of an analog signal is done by discretizing the signal with a number of quantization levels. Quantization is representing the sampled values of the amplitude by a finite set of levels, which means converting a continuous-amplitude sample into a discrete-time signal."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Any LTI filter with output and input can be represented by a difference equation in the form: If at least one of the is not null, the filter is recursive. If the are all zero, it is a non recursive filter usually called FIR (Finite Input Response) filter.  This happens both to recursive and non recursive filters."}, {"text": "The sampling rate determines the spatial resolution of the digitized image, while the quantization level determines the number of grey levels in the digitized image.  The transition between continuous values of the image function and its digital equivalent is called quantization."}]}, {"question": "How does Sobel edge detection work", "positive_ctxs": [{"text": "The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image.  The result shows how abruptly or smoothly the image changes at each pixel, and therefore how likely it is that that pixel represents an edge."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Sobel operator, sometimes called the Sobel\u2013Feldman operator or Sobel filter, is used in image processing and computer vision, particularly within edge detection algorithms where it creates an image emphasising edges."}, {"text": "The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986. Canny also produced a computational theory of edge detection explaining why the technique works."}, {"text": "The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image.  The result of applying it to a pixel on an edge is a vector that points across the edge from darker to brighter values."}, {"text": "Image processing algorithms generally constitute contrast enhancement, noise reduction, edge sharpening, edge detection, segmentation etc. These techniques make the manual diagnosis process of disease detection automatic or semiautomatic."}, {"text": "The Laplacian of an image highlights regions of rapid intensity change and is therefore often used for edge detection (see zero crossing edge detectors).  The operator normally takes a single graylevel image as input and produces another graylevel image as output."}, {"text": "The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image. It finds the direction of the largest increase from light to dark and the rate of change in that direction."}, {"text": "The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image. It finds the direction of the largest increase from light to dark and the rate of change in that direction."}]}, {"question": "How do you calculate precision in machine learning", "positive_ctxs": [{"text": "Precision for Binary Classification In an imbalanced classification problem with two classes, precision is calculated as the number of true positives divided by the total number of true positives and false positives. The result is a value between 0.0 for no precision and 1.0 for full or perfect precision."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data."}, {"text": "How to Get Started with AIPick a topic you are interested in.Find a quick solution.Improve your simple solution.Share your solution.Repeat steps 1-4 for different problems.Complete a Kaggle competition.Use machine learning professionally."}, {"text": "You do not need to learn linear algebra before you get started in machine learning, but at some time you may wish to dive deeper.  It will give you the tools to help you with the other areas of mathematics required to understand and build better intuitions for machine learning algorithms."}, {"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "Machine learning algorithms can minimize forecasting error and do the forecast much faster and with the usage of more data. What's more, machine learning algorithms can analyze many alternative models at the same time, when in traditional econometrics you can analyze just one model at a time."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}]}, {"question": "How do you create a regression model", "positive_ctxs": [{"text": "Run regression analysisOn the Data tab, in the Analysis group, click the Data Analysis button.Select Regression and click OK.In the Regression dialog box, configure the following settings: Select the Input Y Range, which is your dependent variable.  Click OK and observe the regression analysis output created by Excel."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How Stepwise Regression WorksStart the test with all available predictor variables (the \u201cBackward: method), deleting one variable at a time as the regression model progresses.  Start the test with no predictor variables (the \u201cForward\u201d method), adding one at a time as the regression model progresses."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How TensorFlow works. TensorFlow allows developers to create dataflow graphs\u2014structures that describe how data moves through a graph, or a series of processing nodes. Each node in the graph represents a mathematical operation, and each connection or edge between nodes is a multidimensional data array, or tensor."}, {"text": "How to train a Machine Learning model in 5 minutesModel Naming \u2014 Give Your Model a Name: Let's start with giving your model a name, describe your model and attach tags to your model.  Data Type Selection \u2014 Choose data type(Images/Text/CSV): It's time to tell us about the type of data you want to train your model.More items"}]}, {"question": "What happens when you standardize a variable", "positive_ctxs": [{"text": "In statistics, standardization is the process of putting different variables on the same scale. This process allows you to compare scores between different types of variables. Typically, to standardize variables, you calculate the mean and standard deviation for a variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A factorial distribution happens when a set of variables are independent events. In other words, the variables don't interact at all; Given two events x and y, the probability of x doesn't change when you factor in y."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "Because when you are constructing a linear regression model you are assuming that your dependent variable \"Y\" is normally distributed. But when you have a binary dependent variable, this assumption is heavily violated. Thus, it doesn't makes sense to use linear regression when your dependent variable is binary."}, {"text": "For a dichotomous categorical variable and a continuous variable you can calculate a Pearson correlation if the categorical variable has a 0/1-coding for the categories.  But when you have more than two categories for the categorical variable the Pearson correlation is not appropriate anymore."}, {"text": "Let's Start with NLP and NLG Setting aside NLU for the moment, we can draw a really simple distinction: Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language."}, {"text": "Let's Start with NLP and NLG Setting aside NLU for the moment, we can draw a really simple distinction: Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language."}]}, {"question": "Which is the best machine learning algorithm", "positive_ctxs": [{"text": "Without Further Ado, The Top 10 Machine Learning Algorithms for Beginners:Linear Regression. In machine learning, we have a set of input variables (x) that are used to determine an output variable (y).  Logistic Regression.  CART.  Na\u00efve Bayes.  KNN."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Hypothesis Space (H): Hypothesis space is the set of all the possible legal hypothesis. This is the set from which the machine learning algorithm would determine the best possible (only one) which would best describe the target function or the outputs."}, {"text": "Deep learning itself does feature engineering whereas machine learning requires manual feature engineering. 2) Which of the following is a representation learning algorithm? Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning."}, {"text": "Logistic regression is a powerful machine learning algorithm that utilizes a sigmoid function and works best on binary classification problems, although it can be used on multi-class classification problems through the \u201cone vs. all\u201d method."}, {"text": "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging."}]}, {"question": "How do you find the Z test rejection region", "positive_ctxs": [{"text": "When we calculate Z, we will get a value. If this value falls into the middle part, then we cannot reject the null. If it falls outside, in the shaded region, then we reject the null hypothesis. That is why the shaded part is called: rejection region, as you can see below."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "\u201cThe decision of whether to use a one\u2010 or a two\u2010tailed test is important because a test statistic that falls in the region of rejection in a one\u2010tailed test may not do so in a two\u2010tailed test, even though both tests use the same probability level.\u201d"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "The rejection region is the interval, measured in the sampling distribution of the statistic under study, that leads to rejection of the null hypothesis H 0 in a hypothesis test."}, {"text": "Lets do this step by step:Step 1: find the mean.Step 2: fin the standard deviation of the mean (using the population SD)Step 3: find the Z score.Step 4: compare to the critical Z score. From the stated hypothesis, we know that we are dealing with a 1-tailed hypothesis test.  Step 4 : compare to the critical Z score."}, {"text": "No, the same values are reported. A researcher computes a one-sample z test in two studies. Both studies used the same alpha level, placed the rejection region in both tails, and measured the same sample mean."}, {"text": "To reject the null, the tail used for the rejection region should cover the extreme values of the alternative hypothesis - the area in red. The z or t score is negative and less than the score set for the rejection condition."}, {"text": "If your test statistic is positive, first find the probability that Z is greater than your test statistic (look up your test statistic on the Z-table, find its corresponding probability, and subtract it from one). Then double this result to get the p-value."}]}, {"question": "Is critical value the same as Z score", "positive_ctxs": [{"text": "The critical value is a factor used to compute the margin of error, as shown in the equations below. When the sampling distribution of the statistic is normal or nearly normal, the critical value can be expressed as a t score or as a z-score."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Lets do this step by step:Step 1: find the mean.Step 2: fin the standard deviation of the mean (using the population SD)Step 3: find the Z score.Step 4: compare to the critical Z score. From the stated hypothesis, we know that we are dealing with a 1-tailed hypothesis test.  Step 4 : compare to the critical Z score."}, {"text": "The decision rule is: Reject H0 if Z < 1.645. The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960. The complete table of critical values of Z for upper, lower and two-tailed tests can be found in the table of Z values to the right in \"Other Resources.\""}, {"text": "The decision rule is: Reject H0 if Z < 1.645. The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960. The complete table of critical values of Z for upper, lower and two-tailed tests can be found in the table of Z values to the right in \"Other Resources.\""}, {"text": "To find the critical value, follow these steps.Compute alpha (\u03b1): \u03b1 = 1 - (confidence level / 100)Find the critical probability (p*): p* = 1 - \u03b1/2.To express the critical value as a z-score, find the z-score having a cumulative probability equal to the critical probability (p*).More items"}, {"text": "Normal Distribution For a one-tailed test, the critical value is 1.645. So the critical region is Z<\u22121.645 for a left-tailed test and Z>1.645 for a right-tailed test. For a two-tailed test, the critical value is 1.96."}, {"text": "If the absolute value of the t-value is greater than the critical value, you reject the null hypothesis. If the absolute value of the t-value is less than the critical value, you fail to reject the null hypothesis."}]}, {"question": "What is the difference between artificial intelligence machine learning statistics and data mining", "positive_ctxs": [{"text": "Machine learning uses neural networks and automated algorithms to predict outcomes. Accuracy of data mining depends on how data is collected. Data Mining produces accurate results which are used by machine learning making machine learning produce better results."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or"}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}, {"text": "Text mining (also referred to as text analytics) is an artificial intelligence (AI) technology that uses natural language processing (NLP) to transform the free (unstructured) text in documents and databases into normalized, structured data suitable for analysis or to drive machine learning (ML) algorithms."}, {"text": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Multimodal machine learning is a vibrant multi-disciplinary research field which addresses some of the original goals of artificial intelligence by integrating and modeling multiple communicative modalities, including linguistic, acoustic and visual messages."}, {"text": "The answer is a big NO. Data science gets solutions and results to specific business problems using AI as a tool. If data science is to insights, machine learning is to predictions and artificial intelligence is to actions."}]}, {"question": "Is K means clustering supervised or unsupervised", "positive_ctxs": [{"text": "There are a ton of 'smart' algorithms that assist data scientists do the wizardry.  k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows\u2022"}, {"text": "In the absence of a class label, clustering analysis is also called unsupervised learning, as opposed to supervised learning that includes classification and regression. Accordingly, approaches to clustering analysis are typically quite different from supervised learning."}, {"text": "k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}, {"text": "KNN represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points, while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters."}, {"text": "KNN represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points, while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters."}, {"text": "K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups).  The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided."}]}, {"question": "What is the formula for dependent probability", "positive_ctxs": [{"text": "If they are dependent, then P(A and B) = P(A)*P(B|A) which is the probability of A times the probability of \"B happening if A has occurred,\" which is different than the \"Probability of B if A has not occurred.\""}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The least squares criterion is a formula used to measure the accuracy of a straight line in depicting the data that was used to generate it. That is, the formula determines the line of best fit. This mathematical formula is used to predict the behavior of the dependent variables."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The least squares criterion is a formula used to measure the accuracy of a straight line in depicting the data that was used to generate it.  This mathematical formula is used to predict the behavior of the dependent variables. The approach is also called the least squares regression line."}, {"text": "The formula for a simple linear regression is:y is the predicted value of the dependent variable (y) for any given value of the independent variable (x).B0 is the intercept, the predicted value of y when the x is 0.B1 is the regression coefficient \u2013 how much we expect y to change as x increases.More items\u2022"}, {"text": "The formula for calculating a z-score is is z = (x-\u03bc)/\u03c3, where x is the raw score, \u03bc is the population mean, and \u03c3 is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation."}, {"text": "For the coin flip example, N = 2 and \u03c0 = 0.5. The formula for the binomial distribution is shown below: where P(x) is the probability of x successes out of N trials, N is the number of trials, and \u03c0 is the probability of success on a given trial.Number of HeadsProbability21/42 more rows"}, {"text": "The formula for calculating a z-score is is z = (x-\u03bc)/\u03c3, where x is the raw score, \u03bc is the population mean, and \u03c3 is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation. Figure 2."}]}, {"question": "Where are machine learning models stored", "positive_ctxs": [{"text": "When dealing with Machine Learning models, it is usually recommended that you store them somewhere. At the private sector, you oftentimes train them and store them before production, while in research and for future model tuning it is a good idea to store them locally."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables."}, {"text": "In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory."}, {"text": "In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory."}, {"text": "Difference between rule-based AI and machine learning Machine learning systems are probabilistic and rule-based AI models are deterministic.  Machine learning systems require more data as compared to rule-based models. Rule-based AI models can operate with simple basic information and data."}, {"text": "Optimizing Neural Networks \u2014 Where to Start?Start with learning rate;Then try number of hidden units, mini-batch size and momentum term;Lastly, tune number of layers and learning rate decay."}, {"text": "It's more of an approach than a process. Predictive analytics and machine learning go hand-in-hand, as predictive models typically include a machine learning algorithm.  These models are then made up of algorithms. The algorithms perform the data mining and statistical analysis, determining trends and patterns in data."}, {"text": "The traditional method of training AI models involves setting up servers where models are trained on data, often through the use of a cloud-based computing platform.  Federated learning brings machine learning models to the data source, rather than bringing the data to the model."}]}, {"question": "How do you perform a causal analysis", "positive_ctxs": [{"text": "One of the simplest causal analysis methods involves asking yourself \u201cwhy\u201d five times. You start by identifying the problem. \u201cMy house is always disorganized.\u201d Then, you ask yourself why that is the case. You create a chain of inquiry that offers insight about the core of the problem."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Logistic regression models are a great tool for analysing binary and categorical data, allowing you to perform a contextual analysis to understand the relationships between the variables, test for differences, estimate effects, make predictions, and plan for future scenarios."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "The three main methods to perform linear regression analysis in Excel are: Regression tool included with Analysis ToolPak. Scatter chart with a trendline."}]}, {"question": "What is network representation learning", "positive_ctxs": [{"text": "Network representation learning has been recently proposed as a new learning paradigm to embed network vertices into a low-dimensional vector space, by preserving network topology structure, vertex content, and other side information."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep learning itself does feature engineering whereas machine learning requires manual feature engineering. 2) Which of the following is a representation learning algorithm? Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning."}, {"text": "Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Explanation: If a bayesian network is a representation of the joint distribution, then it can solve any query, by summing all the relevant joint entries."}, {"text": "Abstract. Network representation learning aims to embed the vertexes in a network into low-dimensional dense representations, in which similar vertices in the network should have \u201cclose\u201d representations (usually measured by cosine similarity or Euclidean distance of their representations)."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d."}]}, {"question": "How does Gaussian smoothing work", "positive_ctxs": [{"text": "The effect of Gaussian smoothing is to blur an image, in a similar fashion to the mean filter. The degree of smoothing is determined by the standard deviation of the Gaussian. (Larger standard deviation Gaussians, of course, require larger convolution kernels in order to be accurately represented.)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Gaussian smoothing filters are commonly used to reduce noise.  Gaussian filters are generally isotropic, that is, they have the same standard deviation along both dimensions. An image can be filtered by an isotropic Gaussian filter by specifying a scalar value for sigma ."}, {"text": "The Laplacian is a 2-D isotropic measure of the 2nd spatial derivative of an image.  The Laplacian is often applied to an image that has first been smoothed with something approximating a Gaussian smoothing filter in order to reduce its sensitivity to noise, and hence the two variants will be described together here."}, {"text": "It is called Laplace smoothing because the smoothing proceeds from a logic of slightly correcting the observed proportions (in the case of categorical variables) in the direction of a uniform distribution among the categories (i.e., injecting a bit of equi-probability among them)."}, {"text": "Data smoothing uses an algorithm to remove noise from a data set, allowing important patterns to stand out. It can be used to predict trends, such as those found in securities prices. Different data smoothing models include the random method, random walk, and the moving average."}, {"text": "Deep Learning does this by utilizing neural networks with many hidden layers, big data, and powerful computational resources.  In unsupervised learning, algorithms such as k-Means, hierarchical clustering, and Gaussian mixture models attempt to learn meaningful structures in the data."}, {"text": "Additive smoothing plays an important role in Naive Bayes classification, as long as not all events were observed at least ones. In this case of having at least one event with no observation, the probability for this event is absolut zero.  To prevent this problem, addative smoothing is used."}, {"text": "How to Prevent OverfittingCross-validation. Cross-validation is a powerful preventative measure against overfitting.  Train with more data. It won't work every time, but training with more data can help algorithms detect the signal better.  Remove features.  Early stopping.  Regularization.  Ensembling."}]}, {"question": "What is the need of quantization", "positive_ctxs": [{"text": "Quantization, in mathematics and digital signal processing, is the process of mapping input values from a large set (often a continuous set) to output values in a (countable) smaller set, often with a finite number of elements. Rounding and truncation are typical examples of quantization processes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The type of quantization in which the quantization levels are uniformly spaced is termed as a Uniform Quantization. The type of quantization in which the quantization levels are unequal and mostly the relation between them is logarithmic, is termed as a Non-uniform Quantization."}, {"text": "The quantizing of an analog signal is done by discretizing the signal with a number of quantization levels. Quantization is representing the sampled values of the amplitude by a finite set of levels, which means converting a continuous-amplitude sample into a discrete-time signal."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The sampling rate determines the spatial resolution of the digitized image, while the quantization level determines the number of grey levels in the digitized image.  The transition between continuous values of the image function and its digital equivalent is called quantization."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data.  Models and algorithms based on the principle of competitive learning include vector quantization and self-organizing maps (Kohonen maps)."}, {"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}]}, {"question": "What are the types of predictive models", "positive_ctxs": [{"text": "Types of predictive modelsForecast models. A forecast model is one of the most common predictive analytics models.  Classification models.  Outliers Models.  Time series model.  Clustering Model.  The need for massive training datasets.  Properly categorising data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Linear regressions are among the simplest types of predictive models.  Other more complex predictive models include decision trees, k-means clustering and Bayesian inference, to name just a few potential methods. The most complex area of predictive modeling is the neural network."}, {"text": "Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point.  Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood)."}, {"text": "Predictive modeling, a tool used in predictive analytics, refers to the process of using mathematical and computational methods to develop predictive models that examine current and historical datasets for underlying patterns and calculate the probability of an outcome."}, {"text": "Predictive analytics uses predictors or known features to create predictive models that will be used in obtaining an output. A predictive model is able to learn how different points of data connect with each other. Two of the most widely used predictive modeling techniques are regression and neural networks."}, {"text": "Path analysis is a special case of SEM.  Most of the models that you will see in the literature are SEM rather than path analyses. The main difference between the two types of models is that path analysis assumes that all variables are measured without error. SEM uses latent variables to account for measurement error."}, {"text": "For example, polynomial regression consists of performing multiple regression with variables. in order to find the polynomial coefficients (parameters). These types of regression are known as parametric regression since they are based on models that require the estimation of a finite number of parameters."}, {"text": "Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model."}]}, {"question": "Is P value the same as Type I error", "positive_ctxs": [{"text": "This might sound confusing but here it goes: The p-value is the probability of observing data as extreme as (or more extreme than) your actual observed data, assuming that the Null hypothesis is true. A Type 1 Error is a false positive -- i.e. you falsely reject the (true) null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Type I error is equivalent to a False positive. Type II error is equivalent to a False negative. Type I error refers to non-acceptance of hypothesis which ought to be accepted. Type II error is the acceptance of hypothesis which ought to be rejected."}, {"text": "The comparison - wise error rate is the probability of a Type I error set by the experimentor for evaluating each comparison. The experiment - wise error rate is the probability of making at least one Type I error when performing the whole set of comparisons."}, {"text": "in a test involving multiple comparisons, the probability of making at least one Type I error over an entire research study. The experiment-wise error rate differs from the testwise error rate, which is the probability of making a Type I error when performing a specific test or comparison."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive."}, {"text": "In terms of the courtroom example, a type I error corresponds to convicting an innocent defendant. Type II error. The second kind of error is the failure to reject a false null hypothesis as the result of a test procedure."}, {"text": "Type I and type II errors are instrumental for the understanding of hypothesis testing in a clinical research scenario.  A type II error can be thought of as the opposite of a type I error and is when a researcher fails to reject the null hypothesis that is actually false in reality."}, {"text": "Every time you conduct a t-test there is a chance that you will make a Type I error.  An ANOVA controls for these errors so that the Type I error remains at 5% and you can be more confident that any statistically significant result you find is not just running lots of tests."}]}, {"question": "What is KNN imputation method", "positive_ctxs": [{"text": "Imputation is a term that denotes a procedure that replaces the missing values in a data set by some plausible values. Our anal- ysis indicates that missing data imputation based on the k-nearest neighbour algorithm can outperform the internal methods used by C4. 5 and CN2 to treat missing data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "KNN algorithm is one of the simplest classification algorithm. Even with such simplicity, it can give highly competitive results. KNN algorithm can also be used for regression problems."}, {"text": "They are often confused with each other. The 'K' in K-Means Clustering has nothing to do with the 'K' in KNN algorithm. k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}, {"text": "They are often confused with each other. The 'K' in K-Means Clustering has nothing to do with the 'K' in KNN algorithm. k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "In statistics, imputation is the process of replacing missing data with substituted values. When substituting for a data point, it is known as \"unit imputation\"; when substituting for a component of a data point, it is known as \"item imputation\"."}]}, {"question": "What is logistic regression log odds", "positive_ctxs": [{"text": "In the logistic model, the log-odds (the logarithm of the odds) for the value labeled \"1\" is a linear combination of one or more independent variables (\"predictors\"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Mixed effects logistic regression is used to model binary outcome variables, in which the log odds of the outcomes are modeled as a linear combination of the predictor variables when data are clustered or there are both fixed and random effects."}, {"text": "To see what the bias term represents, simply set all to 0. The resulting log odds is the bias term. In other words, the bias term is the \"default\" log odds for the case that all predictors equal 0 (or equal to reference value for categorical predictors). For example, if = 2.5, then the log odds of the outcome is 2.5."}, {"text": "The problem is that probability and odds have different properties that give odds some advantages in statistics.  For example, in logistic regression the odds ratio represents the constant effect of a predictor X, on the likelihood that one outcome will occur."}, {"text": "Loss function for Logistic Regression The loss function for linear regression is squared loss. The loss function for logistic regression is Log Loss, which is defined as follows: Log Loss = \u2211 ( x , y ) \u2208 D \u2212 y log \u2061 ( y \u2032 ) \u2212 ( 1 \u2212 y ) log \u2061 where: ( x , y ) \u2208 D."}, {"text": "Multinomial logistic regression deals with situations where the outcome can have three or more possible types (e.g., \"disease A\" vs. \"disease B\" vs. \"disease C\") that are not ordered.  Binary logistic regression is used to predict the odds of being a case based on the values of the independent variables (predictors)."}, {"text": "The coefficient of a continuous predictor is the estimated change in the natural log of the odds for the reference event for each unit increase in the predictor."}, {"text": "Univariate logistic analysis: When there is one dependent variable, and one independent variable; both are categorical; generally produce Unadjusted model (crude odds ratio) by taking just one independent variable at a time..  Multivariate regression : It's a regression approach of more than one dependent variable."}]}, {"question": "What are different machine learning models", "positive_ctxs": [{"text": "Amazon ML supports three types of ML models: binary classification, multiclass classification, and regression. The type of model you should choose depends on the type of target that you want to predict."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model."}, {"text": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables."}, {"text": "Difference between rule-based AI and machine learning Machine learning systems are probabilistic and rule-based AI models are deterministic.  Machine learning systems require more data as compared to rule-based models. Rule-based AI models can operate with simple basic information and data."}, {"text": "It's more of an approach than a process. Predictive analytics and machine learning go hand-in-hand, as predictive models typically include a machine learning algorithm.  These models are then made up of algorithms. The algorithms perform the data mining and statistical analysis, determining trends and patterns in data."}, {"text": "The traditional method of training AI models involves setting up servers where models are trained on data, often through the use of a cloud-based computing platform.  Federated learning brings machine learning models to the data source, rather than bringing the data to the model."}, {"text": "In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis."}, {"text": "In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis."}]}, {"question": "Does decoherence solve the measurement problem", "positive_ctxs": [{"text": "Yes, decoherence does solve the measurement problem of quantum mechanics. Decoherence explains why, after a measurement, you would get the same result if you immediately made the same measurement again.  So the claim being made is that decoherence explains why the wavefunction appears to collapse."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "1. Why is the XOR problem exceptionally interesting to neural network researchers?  Explanation: Linearly separable problems of interest of neural network researchers because they are the only class of problem that Perceptron can solve successfully."}, {"text": "Does not affect R2 or adjusted R2 (since these estimate the POPULATION variances which are not conditional on X)"}, {"text": "Backtracking is a technique based on algorithm to solve problem. It uses recursive calling to find the solution by building a solution step by step increasing values with time. It removes the solutions that doesn't give rise to the solution of the problem based on the constraints given to solve the problem."}, {"text": "Machine learning field allows you to code in a way so that the application or system can learn to solve the problem on it's own. Learning is a iterative process."}, {"text": "Introduction. Linear regression and logistic regression are two types of regression analysis techniques that are used to solve the regression problem using machine learning. They are the most prominent techniques of regression."}, {"text": "The reality is that stepwise regression is less effective the larger the number of potential explanatory variables. Stepwise regression does not solve the Big-Data problem of too many explanatory variables. Big Data exacerbates the failings of stepwise regression."}]}, {"question": "What is quota sampling advantages and disadvantages", "positive_ctxs": [{"text": "Quota Sampling also has its pros and cons. As this process sets criteria to choose samples, disadvantages are mainly due to its non-random nature. Some of the disadvantages are as follows: Since quota sampling is a non-random sampling method, it is impossible to find the sampling error."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between quota and stratified sampling can be explained in a way that in quota sampling researchers use non-random sampling methods to gather data from one stratum until the required quota fixed by the researcher is fulfilled."}, {"text": "The difference between quota sampling and stratified sampling is: although both \"group\" participants by an important characteristic, stratified sampling relies on random selection within each group, while quota sampling relies on convenience sampling within each group."}, {"text": "Disadvantages include its \"black box\" nature, greater computational burden, proneness to overfitting, and the empirical nature of model development. An overview of the features of neural networks and logistic regression is presented, and the advantages and disadvantages of using this modeling technique are discussed."}, {"text": "Let's discuss some advantages and disadvantages of Linear Regression. Logistic regression is easier to implement, interpret, and very efficient to train. If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting."}, {"text": "Major advantages include its simplicity and lack of bias. Among the disadvantages are difficulty gaining access to a list of a larger population, time, costs, and that bias can still occur under certain circumstances."}, {"text": "In non-probability sampling, the sample is selected based on non-random criteria, and not every member of the population has a chance of being included. Common non-probability sampling methods include convenience sampling, voluntary response sampling, purposive sampling, snowball sampling, and quota sampling."}, {"text": "In contrast, quota sampling in qualitative research is a specific technique for selecting a sample that has been defined using a purposive sampling strategy to define the categories of data sources that are eligible for a study."}]}, {"question": "What is feature detection in image processing", "positive_ctxs": [{"text": "Feature detection is a low-level image processing operation. That is, it is usually performed as the first operation on an image, and examines every pixel to see if there is a feature present at that pixel."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Edge detection is an image processing technique for finding the boundaries of objects within images. It works by detecting discontinuities in brightness. Edge detection is used for image segmentation and data extraction in areas such as image processing, computer vision, and machine vision."}, {"text": "One method of processing images is via face detection. Face detection is a branch of image processing that uses machine learning to detect faces in images. A Haar Cascade is an object detection method used to locate an object of interest in images."}, {"text": "Face-detection algorithms focus on the detection of frontal human faces. It is analogous to image detection in which the image of a person is matched bit by bit. Image matches with the image stores in database. Any facial feature changes in the database will invalidate the matching process."}, {"text": "The Sobel operator, sometimes called the Sobel\u2013Feldman operator or Sobel filter, is used in image processing and computer vision, particularly within edge detection algorithms where it creates an image emphasising edges."}, {"text": "Image processing techniques use filters to enhance an image. Their main applications are to transform the contrast, brightness, resolution and noise level of an image. Contouring, image sharpening, blurring, embossing and edge detection are typical image processing functions (see Table 4.1)."}, {"text": "The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image)."}, {"text": "The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image)."}]}, {"question": "What are true positives and false positives", "positive_ctxs": [{"text": "A true positive is an outcome where the model correctly predicts the positive class. Similarly, a true negative is an outcome where the model correctly predicts the negative class. A false positive is an outcome where the model incorrectly predicts the positive class."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "These include: true positives, false positives (type 1 error), true negatives, and false negatives (type 2 error)."}, {"text": "The metric our intuition tells us we should maximize is known in statistics as recall, or the ability of a model to find all the relevant cases within a dataset. The precise definition of recall is the number of true positives divided by the number of true positives plus the number of false negatives."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "AUC and accuracy are fairly different things.  For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set. AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else."}, {"text": "AUC and accuracy are fairly different things.  For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set. AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else."}]}, {"question": "What is the difference between variance and standard deviation", "positive_ctxs": [{"text": "Key Takeaways. Standard deviation looks at how spread out a group of numbers is from the mean, by looking at the square root of the variance. The variance measures the average degree to which each point differs from the mean\u2014the average of all data points."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "The difference between the two norms is that the standard deviation is calculating the square of the difference whereas the mean absolute deviation is only looking at the absolute difference. Hence large outliers will create a higher dispersion when using the standard deviation instead of the other method."}, {"text": "Describe the scores in such a sample. If the standard deviation is 0 then the variance is 0 and the mean of the squared deviation scores must be 0.  Thus, when the standard deviation equals 0, all the scores are identical and equal to the mean."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data."}, {"text": "The range is the difference between the high and low values. Since it uses only the extreme values, it is greatly affected by extreme values. The variance is the average squared deviation from the mean. It usefulness is limited because the units are squared and not the same as the original data."}, {"text": "Quartile deviation is the difference between \u201cfirst and third quartiles\u201d in any distribution. Standard deviation measures the \u201cdispersion of the data set\u201d that is relative to its mean."}]}, {"question": "What is best feature extraction algorithm for Twitter sentiment analysis", "positive_ctxs": [{"text": "One of the best is using recurrent neural networks for automatic feature extraction. You can use wors2vec as raw inputs to the network. A step further is to use LSTM nodes in RNNs for modeling long term dependencies."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Sentiment analysis is the automated process of analyzing text data and sorting it into sentiments positive, negative, or neutral. Using sentiment analysis tools to analyze opinions in Twitter data can help companies understand how people are talking about their brand."}, {"text": "Let's get right into the steps to use Twitter data for sentiment analysis of events:Get Twitter API Credentials:  Setup the API Credentials in Python:  Getting Tweet Data via Streaming API:  Get Sentiment Information:  Plot Sentiment Information:  Set this up on AWS or Google Cloud Platform:"}, {"text": "Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection. Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature extraction identifies those product aspects which are being commented by customers, sentiment prediction identifies the text containing sentiment or opinion by deciding sentiment polarity as positive, negative or neutral and finally summarization module aggregates the results obtained from previous two steps."}]}, {"question": "What is the purpose of the input () function", "positive_ctxs": [{"text": "The input() method reads a line from the input (usually from the user), converts the line into a string by removing the trailing newline, and returns it."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "The non-linear functions do the mappings between the inputs and response variables. Their main purpose is to convert an input signal of a node in an ANN(Artificial Neural Network) to an output signal. That output signal is now used as an input in the next layer in the stack."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "Abstract. A memory-based learning system is an extended memory management system that decomposes the input space either statically or dynamically into subregions for the purpose of storing and retrieving functional information."}]}, {"question": "How does random forest split", "positive_ctxs": [{"text": "The random forest combines hundreds or thousands of decision trees, trains each one on a slightly different set of the observations, splitting nodes in each tree considering a limited number of the features. The final predictions of the random forest are made by averaging the predictions of each individual tree."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How to reduce False Positive and False Negative in binary classificationfirstly random forest overfits if the training data and testing data are not drawn from same distribution.check the data for linearity,multicollinearity ,outliers,etc.More items"}, {"text": "Random forest improves on bagging because it decorrelates the trees with the introduction of splitting on a random subset of features. This means that at each split of the tree, the model considers only a small subset of features rather than all of the features of the model."}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "Random Forest is less computationally expensive and does not require a GPU to finish training. A random forest can give you a different interpretation of a decision tree but with better performance. Neural Networks will require much more data than an everyday person might have on hand to actually be effective."}, {"text": "Random forest will reduce variance part of error rather than bias part, so on a given training data set decision tree may be more accurate than a random forest. But on an unexpected validation data set, Random forest always wins in terms of accuracy."}, {"text": "The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree."}]}, {"question": "What is ground truth in AI", "positive_ctxs": [{"text": "Ground truth is a term used in statistics and machine learning that means checking the results of machine learning for accuracy against the real world. The term is borrowed from meteorology, where \"ground truth\" refers to information obtained on site."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Recall is the true positive rate, also referred to as sensitivity, measures the probability of ground truth objects being correctly detected."}, {"text": "Image annotation is the process of manually defining regions in an image and creating text-based descriptions of those regions.  You can use the following image annotation tools to quickly and accurately build the ground truth for your computer vision models."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "It is calculated in the same way - by running the network forward over inputs xi and comparing the network outputs \u02c6yi with the ground truth values yi using a loss function e.g. J=1N\u2211Ni=1L(\u02c6yi,yi) where L is the individual loss function based somehow on the difference between predicted value and target."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "An XOR (exclusive OR gate) is a digital logic gate that gives a true output only when both its inputs differ from each other. The truth table for an XOR gate is shown below: Truth Table for XOR. The goal of the neural network is to classify the input patterns according to the above truth table."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What does shift invariant mean", "positive_ctxs": [{"text": "Shift-invariance: this means that if we shift the input in time (or shift the entries in a vector) then the output is shifted by the same amount. Mathematically, we can say that if f(x(t)) = y(t), shift invariance means that f(x(t + \u2327)) = y(t + \u2327)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In spite of being linear, the Fourier transform is not shift invariant. In other words, a shift in the time domain does not correspond to a shift in the frequency domain."}, {"text": "In spite of being linear, the Fourier transform is not shift invariant. In other words, a shift in the time domain does not correspond to a shift in the frequency domain."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right. In a negatively skewed distribution, the mean is usually less than the median because the few low scores tend to shift the mean to the left."}, {"text": "In signal processing, a nonlinear (or non-linear) filter is a filter whose output is not a linear function of its input.  Like linear filters, nonlinear filters may be shift invariant or not. Non-linear filters have many applications, especially in the removal of certain types of noise that are not additive."}, {"text": "In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right.  In a positively skewed distribution, the mode is always less than the mean and median."}]}, {"question": "What is an example of modus Ponens", "positive_ctxs": [{"text": "An example of an argument that fits the form modus ponens: If today is Tuesday, then John will go to work.  An argument can be valid but nonetheless unsound if one or more premises are false; if an argument is valid and all the premises are true, then the argument is sound."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The definition of an ensemble is two or more people or things that function together as a whole. An example of an ensemble is a string quartet. An example of an ensemble is a group of actors in a play.  A small group of musicians playing or singing together."}, {"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}, {"text": "An example of statistics is a report of numbers saying how many followers of each religion there are in a particular country. An example of statistics is a math class offered in high schools and colleges. The definition of a statistic is a number, or a person who is an unnamed piece of data to be studied."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Output is defined as the act of producing something, the amount of something that is produced or the process in which something is delivered. An example of output is the electricity produced by a power plant. An example of output is producing 1,000 cases of a product."}, {"text": "Output is defined as the act of producing something, the amount of something that is produced or the process in which something is delivered. An example of output is the electricity produced by a power plant. An example of output is producing 1,000 cases of a product."}, {"text": "It is known as a top-down approach. Backward-chaining is based on modus ponens inference rule. In backward chaining, the goal is broken into sub-goal or sub-goals to prove the facts true. It is called a goal-driven approach, as a list of goals decides which rules are selected and used."}]}, {"question": "What does a regression equation tell you", "positive_ctxs": [{"text": "A regression equation is used in stats to find out what relationship, if any, exists between sets of data. For example, if you measure a child's height every year you might find that they grow about 3 inches a year. That trend (growing three inches a year) can be modeled with a regression equation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "As you experiment with your algorithm to try and improve your model, your loss function will tell you if you're getting(or reaching) anywhere. At its core, a loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value)."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Linear regression attempts to model the relationship between two variables by fitting a linear equation (= a straight line) to the observed data.  If you have a hunch that the data follows a straight line trend, linear regression can give you quick and reasonably accurate results."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "Even when multicollinearity is great, the least-squares regression equation can be highly predictive. So, if you are only interested in prediction, multicollinearity is not a problem."}]}, {"question": "What is GLM in logistic regression", "positive_ctxs": [{"text": "In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Thus logit regression is simply the GLM when describing it in terms of its link function, and logistic regression describes the GLM in terms of its activation function."}, {"text": ". Thus logit regression is simply the GLM when describing it in terms of its link function, and logistic regression describes the GLM in terms of its activation function."}, {"text": "The General Linear Model (GLM) is a useful framework for comparing how several variables affect different continuous variables. In it's simplest form, GLM is described as: Data = Model + Error (Rutherford, 2001, p.3) GLM is the foundation for several statistical tests, including ANOVA, ANCOVA and regression analysis."}, {"text": "Introduction to Poisson Regression Poisson regression is also a type of GLM model where the random component is specified by the Poisson distribution of the response variable which is a count. When all explanatory variables are discrete, log-linear model is equivalent to poisson regression model."}, {"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "The logit is a transformation. Logistic regression is a regression model. The logit transformation transforms a line to a logistic curve. Logistic regression fits a logistic curve to set of data where the dependent variable can only take the values 0 and 1."}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}]}, {"question": "What is kernel size", "positive_ctxs": [{"text": "The kernel size here refers to the widthxheight of the filter mask. The max pooling layer, for example, returns the pixel with maximum value from a set of pixels within a mask (kernel). That kernel is swept across the input, subsampling it."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The receptive field size of a unit can be increased in a number of ways. One option is to stack more layers to make the network deeper, which increases the receptive field size linearly by theory, as each extra layer increases the receptive field size by the kernel size."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "Kernel vs Filter The dimensions of the kernel matrix is how the convolution gets it's name. For example, in 2D convolutions, the kernel matrix is a 2D matrix. A filter however is a concatenation of multiple kernels, each kernel assigned to a particular channel of the input."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "What is a Gaussian process latent variable model", "positive_ctxs": [{"text": "The Gaussian Process Latent Variable Model (GPLVM) is a dimensionality reduction method that uses a Gaussian process to learn a low-dimensional representation of (potentially) high-dimensional data.  Instead, we set a Gaussian prior for X and learn the mean and variance of the approximate (gaussian) posterior q(X|y)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "On a technical note, estimation of a latent variable is done by analyzing the variance and covariance of the indicators. The measurement model of a latent variable with effect indicators is the set of relationships (modeled as equations) in which the latent variable is set as the predictor of the indicators."}, {"text": "A latent variable is a variable that is inferred using models from observed data.  Approaches to inferring latent variables from data include: using a single observed variable, multi-item scales, predictive models, dimension reduction techniques such as factor analysis, structural equation models, and mixture models."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "Gaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal distribution. For example, if a random process is modelled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly."}, {"text": "A manifest variable is a variable or factor that can be directly measured or observed. It is the opposite of a latent variable, which is a factor that cannot be directly observed, and which needs a manifest variable assigned to it as an indicator to test whether it is present."}, {"text": "The standard solution that psychologists take to measuring latent variables is to use a series of questions that are all designed to measure the latent variable. This is known as a multi-item scale, where an \u201citem\u201d is a question, and a \u201cscale\u201d is the resulting estimate of the latent variable."}]}, {"question": "Is sample variance and standard deviation the same", "positive_ctxs": [{"text": "The variance is the average of the squared differences from the mean. Standard deviation is the square root of the variance so that the standard deviation would be about 3.03.  Because of this squaring, the variance is no longer in the same unit of measurement as the original data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data."}, {"text": "Mean, variance, and standard deviation The mean of the sampling distribution of the sample mean will always be the same as the mean of the original non-normal distribution. In other words, the sample mean is equal to the population mean. where \u03c3 is population standard deviation and n is sample size."}, {"text": "The population mean of the distribution of sample means is the same as the population mean of the distribution being sampled from.  Thus as the sample size increases, the standard deviation of the means decreases; and as the sample size decreases, the standard deviation of the sample means increases."}, {"text": "Describe the scores in such a sample. If the standard deviation is 0 then the variance is 0 and the mean of the squared deviation scores must be 0.  Thus, when the standard deviation equals 0, all the scores are identical and equal to the mean."}, {"text": "On this site, we use the normal distribution when the population standard deviation is known and the sample size is large. We might use either distribution when standard deviation is unknown and the sample size is very large."}, {"text": "Mean, variance, and standard deviation The mean of the sampling distribution of the sample mean will always be the same as the mean of the original non-normal distribution. In other words, the sample mean is equal to the population mean."}, {"text": "The central limit theorem states that the sampling distribution of the mean approaches a normal distribution, as the sample size increases.  Therefore, as a sample size increases, the sample mean and standard deviation will be closer in value to the population mean \u03bc and standard deviation \u03c3 ."}]}, {"question": "Why is negative binomial called negative", "positive_ctxs": [{"text": "The term \"negative binomial\" is likely due to the fact that a certain binomial coefficient that appears in the formula for the probability mass function of the distribution can be written more simply with negative numbers."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution.  Suppose we flip a coin repeatedly and count the number of heads (successes)."}, {"text": "The mean of the negative binomial distribution with parameters r and p is rq / p, where q = 1 \u2013 p. The variance is rq / p2. The simplest motivation for the negative binomial is the case of successive random trials, each having a constant probability P of success."}, {"text": "Specificity (True negative rate) Specificity (SP) is calculated as the number of correct negative predictions divided by the total number of negatives. It is also called true negative rate (TNR). The best specificity is 1.0, whereas the worst is 0.0."}, {"text": "Why the Lognormal Distribution is used to Model Stock Prices Since the lognormal distribution is bound by zero on the lower side, it is therefore perfect for modeling asset prices which cannot take negative values. The normal distribution cannot be used for the same purpose because it has a negative side."}, {"text": "Remember that the Poisson distribution assumes that the mean and variance are the same.  The negative binomial distribution has one parameter more than the Poisson regression that adjusts the variance independently from the mean. In fact, the Poisson distribution is a special case of the negative binomial distribution."}, {"text": "follows a negative binomial distribution with parameters r and p. The geometric distribution is a special case of discrete compound Poisson distribution."}]}, {"question": "What is TF function", "positive_ctxs": [{"text": "You can use tf. function to make graphs out of your programs. It is a transformation tool that creates Python-independent dataflow graphs out of your Python code. This will help you create performant and portable models, and it is required to use SavedModel .  function works under the hood so you can use it effectively."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is the difference between cost function and gradient descent", "positive_ctxs": [{"text": "A cost function is something you want to minimize. For example, your cost function might be the sum of squared errors over your training set. Gradient descent is a method for finding the minimum of a function of multiple variables. So you can use gradient descent to minimize your cost function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Delta learning does this using the difference between a target activation and an actual obtained activation. Using a linear activation function, network connections are adjusted. Another way to explain the Delta rule is that it uses an error function to perform gradient descent learning."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning."}, {"text": "The gradient is a vector which gives us the direction in which loss function has the steepest ascent. The direction of steepest descent is the direction exactly opposite to the gradient, and that is why we are subtracting the gradient vector from the weights vector."}, {"text": "According to a senior data scientist, one of the distinct advantages of using Stochastic Gradient Descent is that it does the calculations faster than gradient descent and batch gradient descent. However, gradient descent is the best approach if one wants a speedier result."}, {"text": "Adaptive learning rate methods are an optimization of gradient descent methods with the goal of minimizing the objective function of a network by using the gradient of the function and the parameters of the network."}]}, {"question": "What is exclusive class interval in statistics", "positive_ctxs": [{"text": "Exclusive Class Interval: When the lower limit is included, but the upper limit is excluded, then it is an exclusive class interval."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "there are mainly five types of class interval such as exclusive class interval, inclusive class interval, less than class interval, more than class interval, mid value class interval , which has been discussed."}, {"text": "In class limit, the upper extreme value of the first class interval and the lower extreme value of the next class interval will not be equal. In class boundary, the upper extreme value of the first class interval and the lower extreme value of the next class interval will be equal."}, {"text": "In exclusive form, the lower and upper limits are known as true lower limit and true upper limit of the class interval. Thus, class limits of 10 - 20 class intervals in the exclusive form are 10 and 20. In inclusive form, class limits are obtained by subtracting 0.5 from lower limitand adding 0.5 to the upper limit."}, {"text": "This is answered by examining the meaning of each term in the phrase: modal means the one that occurs most often (averages: mode), a class interval is the width of one of your groups in the frequency table or, the class interval is what you use when grouping data together, e.g., if you counted the number of pencils in"}, {"text": "All the classes may have the same class size or they may have different classes sizes depending on how you group your data. The class interval is always a whole number."}, {"text": "In case of mean and median, it is not necessary. However, the accuracy of the mean would be higher if the class intervals are short. Similarly the median would be more accurate if the 'median class', class interval in which median falls, is of short length."}, {"text": "An example of a mutually exclusive event is when a coin is a tossed and there are two events that can occur, either it will be a head or a tail. Hence, both the events here are mutually exclusive.Difference between Mutually exclusive and independent eventsMutually exclusive eventsIndependent events4 more rows"}]}, {"question": "What is Markov random field in image processing", "positive_ctxs": [{"text": "In other words, a random field is said to be a Markov random field if it satisfies Markov properties.  In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph.  The underlying graph of a Markov random field may be finite or infinite."}, {"text": "We discuss some wonders in the field of image processing with machine learning advancements. Image processing can be defined as the technical analysis of an image by using complex algorithms. Here, image is used as the input, where the useful information returns as the output."}, {"text": "Some techniques which are used in digital image processing include:Anisotropic diffusion.Hidden Markov models.Image editing.Image restoration.Independent component analysis.Linear filtering.Neural networks.Partial differential equations.More items"}, {"text": "A Bayesian network is a directed graphical model. (A Markov random field is a undirected graphical model.) A graphical model captures the conditional independence, which can be different from the Markovian property."}, {"text": "A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic , whereas Markov networks are undirected and may be cyclic.  The underlying graph of a Markov random field may be finite or infinite."}, {"text": "A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic , whereas Markov networks are undirected and may be cyclic.  The underlying graph of a Markov random field may be finite or infinite."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}]}, {"question": "Is quota a sampling representative", "positive_ctxs": [{"text": "Quota sampling achieves a representative age distribution, but it isn't a random sample, because the sampling frame is unknown. Therefore, the sample may not be representative of the population."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between quota and stratified sampling can be explained in a way that in quota sampling researchers use non-random sampling methods to gather data from one stratum until the required quota fixed by the researcher is fulfilled."}, {"text": "The difference between quota sampling and stratified sampling is: although both \"group\" participants by an important characteristic, stratified sampling relies on random selection within each group, while quota sampling relies on convenience sampling within each group."}, {"text": "Content validity: Is the test fully representative of what it aims to measure?  Criterion validity: Do the results correspond to a different test of the same thing?"}, {"text": "In contrast, quota sampling in qualitative research is a specific technique for selecting a sample that has been defined using a purposive sampling strategy to define the categories of data sources that are eligible for a study."}, {"text": "Quota sampling is different from stratified sampling, because in a stratified sample individuals within each stratum are selected at random. Quota sampling achieves a representative age distribution, but it isn't a random sample, because the sampling frame is unknown."}, {"text": "Probability sampling allows researchers to create a sample that is accurately representative of the real-life population of interest."}, {"text": "Acceptance sampling is a statistical measure used in quality control. It allows a company to determine the quality of a batch of products by selecting a specified number for testing.  Acceptance sampling solves these problems by testing a representative sample of the product for defects."}]}, {"question": "How does a Boolean model help in information retrieval", "positive_ctxs": [{"text": "The BIR is based on Boolean logic and classical set theory in that both the documents to be searched and the user's query are conceived as sets of terms (a bag-of-words model).  Retrieval is based on whether or not the documents contain the query terms."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity."}, {"text": "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity."}, {"text": "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity."}, {"text": "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus."}, {"text": "Definition. Average precision is a measure that combines recall and precision for ranked retrieval results. For one information need, the average precision is the mean of the precision scores after each relevant document is retrieved."}, {"text": "Boolean searching allows the user to combine or limit words and phrases in an online search in order to retrieve relevant results. Using the Boolean terms: AND, OR, NOT, the searcher is able to define relationships among concepts. Use OR to broaden search results."}, {"text": "How to deploy an Object Detection Model with TensorFlow servingCreate a production ready model for TF-Serving.  Create TF-serving environment using Docker.  Creating a client to request the model server running in the Docker container for inference on a test image."}]}, {"question": "What is the difference between FFT and PSD", "positive_ctxs": [{"text": "FFTs are great at analyzing vibration when there are a finite number of dominant frequency components; but power spectral densities (PSD) are used to characterize random vibration signals."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The most intuitive way to increase the frequency resolution of an FFT is to increase the size while keeping the sampling frequency constant. Doing this will increase the number of frequency bins that are created, decreasing the frequency difference between each."}, {"text": "A Power Spectral Density (PSD) is the measure of signal's power content versus frequency. A PSD is typically used to characterize broadband random signals. The amplitude of the PSD is normalized by the spectral resolution employed to digitize the signal."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "The Range is the difference between the lowest and highest values. Example: In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9. So the range is 9 \u2212 3 = 6."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}]}, {"question": "Is deep learning convex optimization", "positive_ctxs": [{"text": "In general, in deep learning NNs are quite large and so the number of (local) minima is much larger than in simple cases of NNs. TL;DR: In general they are non convex."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "6 Answers. Machine learning algorithms use optimization all the time.  Nonetheless, as mentioned in other answers, convex optimization is faster, simpler and less computationally intensive, so it is often easier to \"convexify\" a problem (make it convex optimization friendly), then use non-convex optimization."}, {"text": "Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum."}, {"text": "Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}, {"text": "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."}]}, {"question": "What is asymptotic analysis of an algorithm explain", "positive_ctxs": [{"text": "Asymptotic analysis of an algorithm refers to defining the mathematical boundation/framing of its run-time performance.  Asymptotic analysis is input bound i.e., if there's no input to the algorithm, it is concluded to work in a constant time. Other than the \"input\" all other factors are considered constant."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In mathematics and statistics, an asymptotic distribution is a probability distribution that is in a sense the \"limiting\" distribution of a sequence of distributions."}, {"text": "A p-value that is calculated using an approximation to the true distribution is called an asymptotic p-value.  A p-value calculated using the true distribution is called an exact p-value. For large sample sizes, the exact and asymptotic p-values are very similar."}, {"text": "The basic assumption of factor analysis is that for a collection of observed variables there are a set of underlying variables called factors (smaller than the observed variables), that can explain the interrelationships among those variables."}, {"text": "Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability."}, {"text": "Kmeans clustering algorithm is applied to reduced datasets which is done by principal component analysis dimension reduction method. Cluster analysis is one of the major data analysis methods widely used for many practical applications in emerging areas[12]."}, {"text": "Factor analysis is a statistical data reduction and analysis technique that strives to explain correlations among multiple outcomes as the result of one or more underlying explanations, or factors. The technique involves data reduction, as it attempts to represent a set of variables by a smaller number."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "How are the terms artificial intelligence machine learning and deep learning related", "positive_ctxs": [{"text": "AI means getting a computer to mimic human behavior in some way.  Deep learning, meanwhile, is a subset of machine learning that enables computers to solve more complex problems."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}, {"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}, {"text": "there are three general categories of learning that artificial intelligence (AI)/machine learning utilizes to actually learn. They are Supervised Learning, Unsupervised Learning and Reinforcement learning.  The machine then maps the inputs and the outputs."}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}]}, {"question": "Can decision trees be used for regression", "positive_ctxs": [{"text": "Decision tree builds regression or classification models in the form of a tree structure. The topmost decision node in a tree which corresponds to the best predictor called root node.  Decision trees can handle both categorical and numerical data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two main types of decision trees that are based on the target variable, i.e., categorical variable decision trees and continuous variable decision trees.Categorical variable decision tree.  Continuous variable decision tree.  Assessing prospective growth opportunities.More items"}, {"text": "Multicollinearity happens when one predictor variable in a multiple regression model can be linearly predicted from the others with a high degree of accuracy. This can lead to skewed or misleading results. Luckily, decision trees and boosted trees algorithms are immune to multicollinearity by nature ."}, {"text": "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the"}, {"text": "Advantages and disadvantagesAre simple to understand and interpret.  Have value even with little hard data.  Help determine worst, best and expected values for different scenarios.Use a white box model.  Can be combined with other decision techniques."}, {"text": "Decision trees can help organizations structure and automate (complex) information. Decision trees are decision models that answer a specific question based on a question structure and certain conditions."}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}]}, {"question": "What is sampling distribution of mean with replacement", "positive_ctxs": [{"text": "In sampling with replacement the mean of all sample means equals the mean of the population:  Whatever the shape of the population distribution, the distribution of sample means is approximately normal with better approximations as the sample size, n, increases."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean \u03bc, then the mean of the sampling distribution of the mean is also \u03bc. The symbol \u03bcM is used to refer to the mean of the sampling distribution of the mean."}, {"text": "Typically by the time the sample size is 30 the distribution of the sample mean is practically the same as a normal distribution.  \u00afX, the mean of the measurements in a sample of size n; the distribution of \u00afX is its sampling distribution, with mean \u03bc\u00afX=\u03bc and standard deviation \u03c3\u00afX=\u03c3\u221an."}, {"text": "The variance of the sampling distribution of the mean is computed as follows: That is, the variance of the sampling distribution of the mean is the population variance divided by N, the sample size (the number of scores used to compute a mean)."}, {"text": "With \"infinite\" numbers of successive random samples, the mean of the sampling distribution is equal to the population mean (\u00b5). As the sample sizes increase, the variability of each sampling distribution decreases so that they become increasingly more leptokurtic."}, {"text": "The distribution of sample statistics is called sampling distribution.  Next a new sample of sixteen is taken, and the mean is again computed. If this process were repeated an infinite number of times, the distribution of the now infinite number of sample means would be called the sampling distribution of the mean."}, {"text": "The distribution of sample statistics is called sampling distribution.  Next a new sample of sixteen is taken, and the mean is again computed. If this process were repeated an infinite number of times, the distribution of the now infinite number of sample means would be called the sampling distribution of the mean."}, {"text": "The sampling distribution of the sample mean is very useful because it can tell us the probability of getting any specific mean from a random sample."}]}, {"question": "How do you find the sample space", "positive_ctxs": [{"text": "The size of the sample space is the total number of possible outcomes. For example, when you roll 1 die, the sample space is 1, 2, 3, 4, 5, or 6. So the size of the sample space is 6."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A continuous sample space is based on the same principles, but it has an infinite number of items in the space.  In other words, you can't write out the space in the same way that you would write out the sample space for a die roll."}, {"text": "1 Answer. In fact for a sample space containing 2 possible outcomes \u03a9={a,b}, the event space contains 4 events, F={a,b,ab,\u2205}. In general, for a sample space containing n possible outcomes, the event space is the power set of the sample space, so contains 2n events."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "The set of all the possible outcomes is called the sample space of the experiment and is usually denoted by S. Any subset E of the sample space S is called an event."}, {"text": "The basic steps to build a stochastic model are:Create the sample space (\u03a9) \u2014 a list of all possible outcomes,Assign probabilities to sample space elements,Identify the events of interest,Calculate the probabilities for the events of interest."}, {"text": "Sample space is all the possible outcomes of an event. Sometimes the sample space is easy to determine. For example, if you roll a dice, 6 things could happen. You could roll a 1, 2, 3, 4, 5, or 6."}]}, {"question": "What are the parameters of negative binomial distribution", "positive_ctxs": [{"text": "The distribution defined by the density function in (1) is known as the negative binomial distribution ; it has two parameters, the stopping parameter k and the success probability p. In the negative binomial experiment, vary k and p with the scroll bars and note the shape of the density function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "The mean of the negative binomial distribution with parameters r and p is rq / p, where q = 1 \u2013 p. The variance is rq / p2. The simplest motivation for the negative binomial is the case of successive random trials, each having a constant probability P of success."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution.  Suppose we flip a coin repeatedly and count the number of heads (successes)."}, {"text": "follows a negative binomial distribution with parameters r and p. The geometric distribution is a special case of discrete compound Poisson distribution."}, {"text": "Remember that the Poisson distribution assumes that the mean and variance are the same.  The negative binomial distribution has one parameter more than the Poisson regression that adjusts the variance independently from the mean. In fact, the Poisson distribution is a special case of the negative binomial distribution."}, {"text": "The probability mass function of the negative binomial distribution is. where r is the number of successes, k is the number of failures, and p is the probability of success."}, {"text": "The binomial distribution is a probability distribution that summarizes the likelihood that a value will take one of two independent values under a given set of parameters or assumptions."}]}, {"question": "How is hash function calculated", "positive_ctxs": [{"text": "If r is the number of possible character codes on an computer, and if table_size is a prime such that r % table_size equal 1, then hash function h(key) = key % table_size is simply the sum of the binary representation of the characters in the key mod table_size."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A Hash Collision Attack is an attempt to find two input strings of a hash function that produce the same hash result. If two separate inputs produce the same hash output, it is called a collision."}, {"text": "The Rabin-Karp algorithm makes use of hash functions and the rolling hash technique. A hash function is essentially a function that maps one thing to a value. In particular, hashing can map data of arbitrary size to a value of fixed size."}, {"text": "A rolling hash (also known as recursive hashing or rolling checksum) is a hash function where the input is hashed in a window that moves through the input.  At best, rolling hash values are pairwise independent or strongly universal. They cannot be 3-wise independent, for example."}, {"text": "Hashing is the practice of using an algorithm to map data of any size to a fixed length. This is called a hash value (or sometimes hash code or hash sums or even a hash digest if you're feeling fancy). Whereas encryption is a two-way function, hashing is a one-way function.  Every hash value is unique."}, {"text": "Definition: A hash algorithm is a function that converts a data string into a numeric string output of fixed length. The output string is generally much smaller than the original data.  Two of the most common hash algorithms are the MD5 (Message-Digest algorithm 5) and the SHA-1 (Secure Hash Algorithm)."}, {"text": "The basic requirements for a cryptographic hash function are:the input can be of any length,the output has a fixed length,H(x) is relatively easy to compute for any given x ,H(x) is one-way,H(x) is collision-free."}, {"text": "Typically, with neural networks, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply \u201closs.\u201d"}]}, {"question": "Why Artificial intelligence is the future of growth", "positive_ctxs": [{"text": "According to Accenture's Technology Vision 2017, AI has the potential to double annual economic growth rates by 2035.  To avoid missing out on this opportunity, policy makers and business leaders must prepare for, and work toward, a future with artificial intelligence."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Artificial intelligence is impacting the future of virtually every industry and every human being. Artificial intelligence has acted as the main driver of emerging technologies like big data, robotics and IoT, and it will continue to act as a technological innovator for the foreseeable future."}, {"text": "Artificial intelligence is impacting the future of virtually every industry and every human being. Artificial intelligence has acted as the main driver of emerging technologies like big data, robotics and IoT, and it will continue to act as a technological innovator for the foreseeable future."}, {"text": "Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing (NLP), speech recognition and machine vision."}, {"text": "Artificial Intelligence (AI) is the branch of computer sciences that emphasizes the development of intelligence machines, thinking and working like humans. For example, speech recognition, problem-solving, learning and planning."}, {"text": "The two types of growth curves that are most common are logarithmic growth curves and exponential growth curves. Essentially, they are the opposite of each other. I'll start by explaining and exponential growth curve as that is the one people are typically more familiar with."}, {"text": "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception."}, {"text": "It is considered that humans intelligence is real intelligence. Human beings are the creator of machines and giving them the ability of decisions making.  This is the reason Artificial Intelligence got its name. In the coming time, there would be a large demand for AI engineers because it is a fast-growing technology."}]}, {"question": "What is Softmax layer in CNN", "positive_ctxs": [{"text": "Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem.  Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Dense CNN is a type of Deep CNN in which each layer is connected with another layer deeper than itself."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig. 2."}, {"text": "It is a Softmax activation plus a Cross-Entropy loss.  If we use this loss, we will train a CNN to output a probability over the C classes for each image. It is used for multi-class classification."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "A CNN LSTM can be defined by adding CNN layers on the front end followed by LSTM layers with a Dense layer on the output. It is helpful to think of this architecture as defining two sub-models: the CNN Model for feature extraction and the LSTM Model for interpreting the features across time steps."}, {"text": "The role of a fully connected layer in a CNN architecture The objective of a fully connected layer is to take the results of the convolution/pooling process and use them to classify the image into a label (in a simple classification example)."}]}, {"question": "What does stepwise regression do", "positive_ctxs": [{"text": "Stepwise regression is the step-by-step iterative construction of a regression model that involves the selection of independent variables to be used in a final model. It involves adding or removing potential explanatory variables in succession and testing for statistical significance after each iteration."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The reality is that stepwise regression is less effective the larger the number of potential explanatory variables. Stepwise regression does not solve the Big-Data problem of too many explanatory variables. Big Data exacerbates the failings of stepwise regression."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Stepwise regression is an appropriate analysis when you have many variables and you're interested in identifying a useful subset of the predictors. In Minitab, the standard stepwise regression procedure both adds and removes predictors one at a time."}, {"text": "Stepwise regression is an appropriate analysis when you have many variables and you're interested in identifying a useful subset of the predictors. In Minitab, the standard stepwise regression procedure both adds and removes predictors one at a time."}, {"text": "BACKWARD STEPWISE REGRESSION is a stepwise regression approach that begins with a full (saturated) model and at each step gradually eliminates variables from the regression model to find a reduced model that best explains the data. Also known as Backward Elimination regression."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Findings. A fundamental problem with stepwise regression is that some real explanatory variables that have causal effects on the dependent variable may happen to not be statistically significant, while nuisance variables may be coincidentally significant."}]}, {"question": "What is a B testing and how does it work", "positive_ctxs": [{"text": "AB testing is essentially an experiment where two or more variants of a page are shown to users at random, and statistical analysis is used to determine which variation performs better for a given conversion goal."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "This is a form of hypothesis testing and it is used to optimize a particular feature of a business. It is called A/B testing and refers to a way of comparing two versions of something to figure out which performs better."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In statistics, a two-tailed test is a method in which the critical area of a distribution is two-sided and tests whether a sample is greater than or less than a certain range of values. It is used in null-hypothesis testing and testing for statistical significance."}, {"text": "In statistics, a two-tailed test is a method in which the critical area of a distribution is two-sided and tests whether a sample is greater than or less than a certain range of values. It is used in null-hypothesis testing and testing for statistical significance."}, {"text": "API KPIs (Key Performance Indicators) Defining the key performance indicators (KPIs) for APIs being used is a critical part of understanding not just how they work but how well they can work and the impact they have on your services, users or partners."}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}]}, {"question": "What is deviation and standard deviation", "positive_ctxs": [{"text": "The standard deviation is a statistic that measures the dispersion of a dataset relative to its mean and is calculated as the square root of the variance. The standard deviation is calculated as the square root of variance by determining each data point's deviation relative to the mean."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Standard deviation is the deviation from the mean, and a standard deviation is nothing but the square root of the variance. Mean is an average of all set of data available with an investor or company. Standard deviation used for measuring the volatility of a stock.  Standard deviation is easier to picture and apply."}, {"text": "On this site, we use the normal distribution when the population standard deviation is known and the sample size is large. We might use either distribution when standard deviation is unknown and the sample size is very large."}, {"text": "Describe the scores in such a sample. If the standard deviation is 0 then the variance is 0 and the mean of the squared deviation scores must be 0.  Thus, when the standard deviation equals 0, all the scores are identical and equal to the mean."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean."}, {"text": "The coefficient of variation is a better risk measure than the standard deviation alone because the CV adjusts for the size of the project. The CV measures the standard deviation divided by the mean and therefore puts the standard deviation into context."}, {"text": "Standard deviation is never negative. Standard deviation is sensitive to outliers. A single outlier can raise the standard deviation and in turn, distort the picture of spread. For data with approximately the same mean, the greater the spread, the greater the standard deviation."}, {"text": "In a somewhat similar fashion you can estimate the standard deviation based on the box plot:the standard deviation is approximately equal to the range / 4.the standard deviation is approximately equal to 3/4 * IQR."}]}, {"question": "How do you explain regression", "positive_ctxs": [{"text": "Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  After fitting a linear regression model, you need to determine how well the model fits the data."}, {"text": "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  After fitting a linear regression model, you need to determine how well the model fits the data."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}]}, {"question": "When should you use binning", "positive_ctxs": [{"text": "Binning is a way to group a number of more or less continuous values into a smaller number of \"bins\". For example, if you have data about a group of people, you might want to arrange their ages into a smaller number of age intervals."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "2 Answers. If you have two classes (i.e. binary classification), you should use a binary crossentropy loss. If you have more than two you should use a categorical crossentropy loss."}, {"text": "You should put it after the non-linearity (eg. relu layer). If you are using dropout remember to use it before."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "When n * p and n * q are greater than 5, you can use the normal approximation to the binomial to solve a problem."}, {"text": "When to use it Use Spearman rank correlation when you have two ranked variables, and you want to see whether the two variables covary; whether, as one variable increases, the other variable tends to increase or decrease."}, {"text": "Accuracy is well defined for any number of classes, so if you use this, a single plot should suffice. Precision and recall, however, are defined only for binary problems."}]}, {"question": "What is the cost function used in logistic regression", "positive_ctxs": [{"text": "We can call a Logistic Regression a Linear Regression model but the Logistic Regression uses a more complex cost function, this cost function can be defined as the 'Sigmoid function' or also known as the 'logistic function' instead of a linear function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Mean Squared Error, commonly used for linear regression models, isn't convex for logistic regression. This is because the logistic function isn't always convex. The logarithm of the likelihood function is however always convex."}, {"text": "The logistic function is the inverse of the natural logit function and so can be used to convert the logarithm of odds into a probability. In mathematical notation the logistic function is sometimes written as expit in the same form as logit."}, {"text": "The logistic function is the inverse of the natural logit function and so can be used to convert the logarithm of odds into a probability. In mathematical notation the logistic function is sometimes written as expit in the same form as logit."}, {"text": "Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true ."}, {"text": "Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true ."}, {"text": "Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative distribution function of logistic distribution."}, {"text": "Linear regression is used for predicting the continuous dependent variable using a given set of independent features whereas Logistic Regression is used to predict the categorical. Linear regression is used to solve regression problems whereas logistic regression is used to solve classification problems."}]}, {"question": "What is AUC score in machine learning", "positive_ctxs": [{"text": "AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.  AUC is scale-invariant."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively."}, {"text": "The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively."}, {"text": "The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively."}, {"text": "The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively."}, {"text": "The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "A learning curve plots the score over varying numbers of training samples, while a validation curve plots the score over a varying hyper parameter. The learning curve is a tool for finding out if an estimator would benefit from more data, or if the model is too simple (biased)."}]}, {"question": "What is the difference between deep learning and usual machine learning", "positive_ctxs": [{"text": "To recap the differences between the two: Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned.  While both fall under the broad category of artificial intelligence, deep learning is what powers the most human-like artificial intelligence."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}, {"text": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "When one should use a linear by linear association chi square test", "positive_ctxs": [{"text": "The \u201cLinear-by-Linear Association\u201d statistic is used when the variables are ordinal, but many simply use the Pearson for those as well. Column 2 shows the Chi Square values for each alternative test. The main one of interest is the Pearson Chi-Square value of ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship."}, {"text": "Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables. For instance, when we predict rent based on square feet alone that is simple linear regression."}, {"text": "When we have a high degree linear polynomial that is used to fit a set of points in a linear regression setup, to prevent overfitting, we use regularization, and we include a lambda parameter in the cost function. This lambda is then used to update the theta parameters in the gradient descent algorithm."}, {"text": "3 Answers. Since your response is ordinal then you should use ordinal regression. At a very high level, the main difference ordinal regression and linear regression is that with linear regression the dependent variable is continuous and ordinal the dependent variable is ordinal."}, {"text": "In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.  Such models are called linear models."}, {"text": "In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression."}, {"text": "You can use a bivariate Pearson Correlation to test whether there is a statistically significant linear relationship between height and weight, and to determine the strength and direction of the association."}]}, {"question": "Can Lagrangian multiplier be negative", "positive_ctxs": [{"text": "Because the Lagrangian multiplier can be considered as a penalty term, and a negative penalty does not make sense. When is zero you are not violating any constraint, however when is infinity you have to satisfy the constraint (i.e - ) or else your objective function will be unbounded."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Yes. We Can Always Use The Normal Distribution To Approximate The Binomial Distribution."}, {"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "As you have seen, in order to perform a likelihood ratio test, one must estimate both of the models one wishes to compare. The advantage of the Wald and Lagrange multiplier (or score) tests is that they approximate the LR test, but require that only one model be estimated."}, {"text": "Discrete control systems, as considered here, refer to the control theory of discrete\u2010time Lagrangian or Hamiltonian systems.  Geometric integrators are numericalintegration methods that preserve geometric properties of continuous systems, such as conservation of the symplectic form, momentum, and energy."}, {"text": "Advantages and disadvantagesAre simple to understand and interpret.  Have value even with little hard data.  Help determine worst, best and expected values for different scenarios.Use a white box model.  Can be combined with other decision techniques."}, {"text": "Dual Booting Can Impact Disk Swap Space. In most cases there shouldn't be too much impact on your hardware from dual booting.  Both Linux and Windows use chunks of the hard disk drive to improve performance while the computer is running."}]}, {"question": "What does a correlation coefficient of negative 1 mean", "positive_ctxs": [{"text": "A negative correlation can indicate a strong relationship or a weak relationship. Many people think that a correlation of \u20131 indicates no relationship. But the opposite is true. A correlation of -1 indicates a near perfect relationship along a straight line, which is the strongest relationship possible."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The sign of a regression coefficient tells you whether there is a positive or negative correlation between each independent variable the dependent variable. A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase."}, {"text": "The correlation coefficient is a statistical measure of the strength of the relationship between the relative movements of two variables. The values range between -1.0 and 1.0.  A correlation of -1.0 shows a perfect negative correlation, while a correlation of 1.0 shows a perfect positive correlation."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Simply stated: the R2 value is simply the square of the correlation coefficient R . The correlation coefficient ( R ) of a model (say with variables x and y ) takes values between \u22121 and 1 . It describes how x and y are correlated."}, {"text": "The correlation coefficient is the specific measure that quantifies the strength of the linear relationship between two variables in a correlation analysis. The coefficient is what we symbolize with the r in a correlation report."}, {"text": "Any point directly on the y-axis has an X value of 0. Multiple Choice: In a simple Linear regression problem, r and b1. Explanation: r= correlation coefficient and b1= slope. If we have a downward sloping trend-line then that means we have a negative (or inverse) correlation coefficient."}, {"text": "A coefficient of correlation of +0.8 or -0.8 indicates a strong correlation between the independent variable and the dependent variable. An r of +0.20 or -0.20 indicates a weak correlation between the variables."}]}, {"question": "What are the advantages of mini batch gradient descent over full batch gradient descent", "positive_ctxs": [{"text": "Advantages of Mini-Batch Gradient Descent Stable Convergence: Another advantage is the more stable converge towards the global minimum since we calculate an average gradient over n samples that results in less noise."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "According to a senior data scientist, one of the distinct advantages of using Stochastic Gradient Descent is that it does the calculations faster than gradient descent and batch gradient descent. However, gradient descent is the best approach if one wants a speedier result."}, {"text": "The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated. The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset."}, {"text": "Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process."}, {"text": "Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process."}, {"text": "Batch means a group of training samples. In gradient descent algorithms, you can calculate the sum of gradients with respect to several examples and then update the parameters using this cumulative gradient. If you 'see' all training examples before one 'update', then it's called full batch learning."}, {"text": "Unlike the batch gradient descent which computes the gradient using the whole dataset, because the SGD, also known as incremental gradient descent, tries to find minimums or maximums by iteration from a single randomly picked training example, the error is typically noisier than in gradient descent."}, {"text": "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning."}]}, {"question": "What is a probability distribution explain your answer", "positive_ctxs": [{"text": "A probability distribution is a statistical function that describes all the possible values and likelihoods that a random variable can take within a given range.  These factors include the distribution's mean (average), standard deviation, skewness, and kurtosis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Today, machines are intelligent because of a science called the Artificial Intelligence.  A simple answer to explain what makes a machine intelligent is Artificial Intelligence. AI allows a machine to interact with the environment in an intelligent manner."}, {"text": "A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function."}, {"text": "A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function."}, {"text": "Hierarchical regression is a way to show if variables of your interest explain a statistically significant amount of variance in your Dependent Variable (DV) after accounting for all other variables. This is a framework for model comparison rather than a statistical method."}, {"text": "Normal Distribution is a probability distribution where probability of x is highest at centre and lowest in the ends whereas in Uniform Distribution probability of x is constant. Uniform Distribution is a probability distribution where probability of x is constant."}, {"text": "The beta distribution is a continuous probability distribution that can be used to represent proportion or probability outcomes. For example, the beta distribution might be used to find how likely it is that your preferred candidate for mayor will receive 70% of the vote."}]}, {"question": "How do you find the expected value of a random variable", "positive_ctxs": [{"text": "For a discrete random variable, the expected value, usually denoted as or , is calculated using: \u03bc = E ( X ) = \u2211 x i f ( x i )"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To find the expected value, E(X), or mean \u03bc of a discrete random variable X, simply multiply each value of the random variable by its probability and add the products. The formula is given as E(X)=\u03bc=\u2211xP(x)."}, {"text": "The probability of each value of the discrete random variable is between 0 and\u200b 1, inclusive, and the sum of all the probabilities is 1.  It is the expected value of a discrete random variable."}, {"text": "Descriptive statistics. The expected value and variance of a Poisson-distributed random variable are both equal to \u03bb. , while the index of dispersion is 1."}, {"text": "The cumulative density function gives you the probability of a random variable being on or below a certain value. The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value.  A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "For a random variable yt, the unconditional mean is simply the expected value, E ( y t ) . In contrast, the conditional mean of yt is the expected value of yt given a conditioning set of variables, \u03a9t. A conditional mean model specifies a functional form for E ( y t | \u03a9 t ) . ."}, {"text": "In probability and statistics, the quantile function, associated with a probability distribution of a random variable, specifies the value of the random variable such that the probability of the variable being less than or equal to that value equals the given probability."}]}, {"question": "How do you find the probability of posterior in Excel", "positive_ctxs": [{"text": "3:295:25Suggested clip \u00b7 42 secondsExcel Statistics 55.5: Bayes Theorem Posterior Probabilities - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Variational Bayesian methods are primarily used for two purposes: To provide an analytical approximation to the posterior probability of the unobserved variables, in order to do statistical inference over these variables."}, {"text": "Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point.  Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood)."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred."}, {"text": "A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred."}]}, {"question": "What is a good MCC score", "positive_ctxs": [{"text": "Similar to Correlation Coefficient, the range of values of MCC lie between -1 to +1. A model with a score of +1 is a perfect model and -1 is a poor model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "v) Matthews Correlation Coefficient (MCC) Similar to Correlation Coefficient, the range of values of MCC lie between -1 to +1. A model with a score of +1 is a perfect model and -1 is a poor model."}, {"text": "An IQ (Intelligence Quotient) score from a standardized test of intelligences is a good example of an interval scale score.  IQ scores are created so that a score of 100 represents the average IQ of the population and the standard deviation (or average variability) of scores is 15."}, {"text": "A score between 0 and 30 is a good range to be in, however, there is still room for progress. If your NPS is higher than 30 that would indicate that your company is doing great and has far more happy customers than unhappy ones."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "Definition. A score that is derived from an individual's raw score within a distribution of scores. The standard score describes the difference of the raw score from a sample mean, expressed in standard deviations. Standard scores preserve the absolute differences between scores."}, {"text": "A t score is one form of a standardized test statistic (the other you'll come across in elementary statistics is the z-score). The t score formula enables you to take an individual score and transform it into a standardized form>one which helps you to compare scores."}]}, {"question": "How does face recognition work", "positive_ctxs": [{"text": "Facial recognition is a way of recognizing a human face through technology. A facial recognition system uses biometrics to map facial features from a photograph or video. It compares the information with a database of known faces to find a match."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Based on Deep convolutional neural networks, DeepFace is a deep learning face recognition system. Created by Facebook, it detects and determines the identity of an individual's face through digital images, reportedly with an accuracy of 97.35%."}, {"text": "Traditional algorithms involving face recognition work by identifying facial features by extracting features, or landmarks, from the image of the face. For example, to extract facial features, an algorithm may analyse the shape and size of the eyes, the size of nose, and its relative position with the eyes."}, {"text": "4:551:11:29Suggested clip \u00b7 112 secondsRodrigo Agundez: Building a live face recognition system | Pydata YouTubeStart of suggested clipEnd of suggested clip"}, {"text": "Abstract: The Local Binary Pattern Histogram(LBPH) algorithm is a simple solution on face recognition problem, which can recognize both front face and side face.  To solve this problem, a modified LBPH algorithm based on pixel neighborhood gray median(MLBPH) is proposed."}, {"text": "Face detection is a broader term than face recognition. Face detection just means that a system is able to identify that there is a human face present in an image or video.  Face recognition can confirm identity. It is therefore used to control access to sensitive areas."}, {"text": "Face detection is a broader term than face recognition. Face detection just means that a system is able to identify that there is a human face present in an image or video.  Face recognition can confirm identity. It is therefore used to control access to sensitive areas."}, {"text": "Face recognition systems use computer algorithms to pick out specific, distinctive details about a person's face. These details, such as distance between the eyes or shape of the chin, are then converted into a mathematical representation and compared to data on other faces collected in a face recognition database."}]}, {"question": "What is the difference between OLS and linear regression", "positive_ctxs": [{"text": "Yes, although 'linear regression' refers to any approach to model the relationship between one or more variables, OLS is the method used to find the simple linear regression of a set of data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "3 Answers. Since your response is ordinal then you should use ordinal regression. At a very high level, the main difference ordinal regression and linear regression is that with linear regression the dependent variable is continuous and ordinal the dependent variable is ordinal."}, {"text": "OLS cannot be used because the regression function is not a linear function of the regression coefficients (the coefficients appear inside the nonlinear functions \u03a6 or \u039b)."}, {"text": "Yes, although 'linear regression' refers to any approach to model the relationship between one or more variables, OLS is the method used to find the simple linear regression of a set of data."}, {"text": "In ordinary least squares, the relevant assumption of the classical linear regression model is that the error term is uncorrelated with the regressors. The presence of omitted-variable bias violates this particular assumption. The violation causes the OLS estimator to be biased and inconsistent."}, {"text": "Ridge regression does not really select variables in the many predictors situation.  Both ridge regression and the LASSO can outperform OLS regression in some predictive situations \u2013 exploiting the tradeoff between variance and bias in the mean square error."}, {"text": "The essential difference between these two is that Logistic regression is used when the dependent variable is binary in nature. In contrast, Linear regression is used when the dependent variable is continuous and nature of the regression line is linear."}]}, {"question": "Is sample variance always smaller than population variance", "positive_ctxs": [{"text": "The sample variance is not always smaller than the population variance."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The variance estimated as the average squared difference from the sample mean will always be less than the variance estimated as the average squared difference from the population mean unless the sample mean equals the population mean in which case they will be the same."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data. Due to this value of denominator in the formula for variance in case of sample data is 'n-1', and it is 'n' for population data."}, {"text": "Sample variance Dividing instead by n \u2212 1 yields an unbiased estimator.  In other words, the expected value of the uncorrected sample variance does not equal the population variance \u03c32, unless multiplied by a normalization factor. The sample mean, on the other hand, is an unbiased estimator of the population mean \u03bc."}, {"text": "5 Answers. N is the population size and n is the sample size. The question asks why the population variance is the mean squared deviation from the mean rather than (N\u22121)/N=1\u2212(1/N) times it."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}]}, {"question": "How does the minimax algorithm work", "positive_ctxs": [{"text": "The Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to maximize the chances of A winning, while on the next turn player B is trying to minimize the chances of A winning (i.e., to maximize B's own chances of winning)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Mini-Max Algorithm in Artificial Intelligence. Mini-max algorithm is a recursive or backtracking algorithm which is used in decision-making and game theory. It provides an optimal move for the player assuming that opponent is also playing optimally.  This Algorithm computes the minimax decision for the current state."}, {"text": "It provides an optimal move for the player assuming that opponent is also playing optimally. Mini-Max algorithm uses recursion to search through the game-tree.  This Algorithm computes the minimax decision for the current state. In this algorithm two players play the game, one is called MAX and other is called MIN."}, {"text": "The core idea is that we cannot know exactly how well an algorithm will work in practice (the true \"risk\") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the \"empirical\" risk)."}, {"text": "Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. Intuitively, underfitting occurs when the model or the algorithm does not fit the data well enough. Specifically, underfitting occurs if the model or algorithm shows low variance but high bias."}, {"text": "Advertisements. Interpolation search is an improved variant of binary search. This search algorithm works on the probing position of the required value. For this algorithm to work properly, the data collection should be in a sorted form and equally distributed."}, {"text": "Advertisements. Interpolation search is an improved variant of binary search. This search algorithm works on the probing position of the required value. For this algorithm to work properly, the data collection should be in a sorted form and equally distributed."}, {"text": "Minimax GAN loss refers to the minimax simultaneous optimization of the discriminator and generator models. Minimax refers to an optimization strategy in two-player turn-based games for minimizing the loss or cost for the worst case of the other player."}]}, {"question": "What is the difference between class interval and class width in statistics", "positive_ctxs": [{"text": "Each class will have a \u201clower class limit\u201d and an \u201cupper class limit\u201d which are the lowest and highest numbers in each class. The \u201cclass width\u201d is the distance between the lower limits of consecutive classes. The range is the difference between the maximum and minimum data entries."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In class limit, the upper extreme value of the first class interval and the lower extreme value of the next class interval will not be equal. In class boundary, the upper extreme value of the first class interval and the lower extreme value of the next class interval will be equal."}, {"text": "The lower class boundary is found by subtracting 0.5 units from the lower class limit and the upper class boundary is found by adding 0.5 units to the upper class limit. The difference between the upper and lower boundaries of any class."}, {"text": "This is answered by examining the meaning of each term in the phrase: modal means the one that occurs most often (averages: mode), a class interval is the width of one of your groups in the frequency table or, the class interval is what you use when grouping data together, e.g., if you counted the number of pencils in"}, {"text": "The main difference is the behavior concerning inheritance: class variables are shared between a class and all its subclasses, while class instance variables only belong to one specific class."}, {"text": "In case of mean and median, it is not necessary. However, the accuracy of the mean would be higher if the class intervals are short. Similarly the median would be more accurate if the 'median class', class interval in which median falls, is of short length."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "YES. we calculate height of the class interval by dividing the frequency by that class width. That class which has the maximum height will be the modal class, containing the mode."}]}, {"question": "In what situation would you use a z test rather than a t test Central Limit Theorem", "positive_ctxs": [{"text": "The z-test is best used for greater-than-30 samples because, under the central limit theorem, as the number of samples gets larger, the samples are considered to be approximately normally distributed."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "z = (x \u2013 \u03bc) / \u03c3 For example, let's say you have a test score of 190. The test has a mean (\u03bc) of 150 and a standard deviation (\u03c3) of 25. Assuming a normal distribution, your z score would be: z = (x \u2013 \u03bc) / \u03c3"}, {"text": "z = (x \u2013 \u03bc) / \u03c3 For example, let's say you have a test score of 190. The test has a mean (\u03bc) of 150 and a standard deviation (\u03c3) of 25. Assuming a normal distribution, your z score would be: z = (x \u2013 \u03bc) / \u03c3"}, {"text": "Because our sample size is greater than 30, the Central Limit Theorem tells us that the sampling distribution will approximate a normal distribution.  Because we know the population standard deviation and the sample size is large, we'll use the normal distribution to find probability."}, {"text": "The Finite Population Correction Factor (FPC) is used when you sample without replacement from more than 5% of a finite population. It's needed because under these circumstances, the Central Limit Theorem doesn't hold and the standard error of the estimate (e.g. the mean or proportion) will be too big."}, {"text": "The Finite Population Correction Factor (FPC) is used when you sample without replacement from more than 5% of a finite population. It's needed because under these circumstances, the Central Limit Theorem doesn't hold and the standard error of the estimate (e.g. the mean or proportion) will be too big."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Face validity refers to the extent to which a test appears to measure what it is intended to measure. A test in which most people would agree that the test items appear to measure what the test is intended to measure would have strong face validity."}]}, {"question": "What is pruning in decision trees Why is it important", "positive_ctxs": [{"text": "Post-pruning (or just pruning) is the most common way of simplifying trees. Here, nodes and subtrees are replaced with leaves to improve complexity. Pruning can not only significantly reduce the size but also improve the classification accuracy of unseen objects."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood."}, {"text": "SVMs and decision trees are discriminative models because they learn explicit boundaties between classes. SVM is a maximal margin classifier, meaning that it learns a decision boundary that maximizes the distance between samples of the two classes, given a kernel."}, {"text": "Decision trees are a classic machine learning technique. The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree. By Narendra Nath Joshi, Carnegie Mellon.  The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree."}, {"text": "A decision tree is a non-linear classifier. If your dataset contains consistent samples, namely you don't have the same input features and contradictory labels, decision trees can classify the data entirely and overfit it."}, {"text": "The fundamental reason to use a random forest instead of a decision tree is to combine the predictions of many decision trees into a single model. The logic is that a single even made up of many mediocre models will still be better than one good model."}, {"text": "Decision trees can handle both categorical and numerical variables at the same time as features, there is not any problem in doing that. Edit: Every split in a decision tree is based on a feature. If the feature is categorical, the split is done with the elements belonging to a particular class."}, {"text": "A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results."}]}, {"question": "What is the formula for multiple linear regression", "positive_ctxs": [{"text": "Since the observed values for y vary about their means y, the multiple regression model includes a term for this variation. In words, the model is expressed as DATA = FIT + RESIDUAL, where the \"FIT\" term represents the expression 0 + 1x1 + 2x2 +  xp."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The formula for a simple linear regression is:y is the predicted value of the dependent variable (y) for any given value of the independent variable (x).B0 is the intercept, the predicted value of y when the x is 0.B1 is the regression coefficient \u2013 how much we expect y to change as x increases.More items\u2022"}, {"text": "Test for Significance of Regression. The test for significance of regression in the case of multiple linear regression analysis is carried out using the analysis of variance. The test is used to check if a linear statistical relationship exists between the response variable and at least one of the predictor variables."}, {"text": "ANCOVA and multiple linear regression are similar, but regression is more appropriate when the emphasis is on the dependent outcome variable, while ANCOVA is more appropriate when the emphasis is on comparing the groups from one of the independent variables."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A linear regression model extended to include more than one independent variable is called a multiple regression model. It is more accurate than to the simple regression.  The principal adventage of multiple regression model is that it gives us more of the information available to us who estimate the dependent variable."}, {"text": "Linear regression is one of the most common techniques of regression analysis. Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables."}, {"text": "Linear regression is one of the most common techniques of regression analysis. Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables."}]}, {"question": "What is the difference between autocorrelation cross correlation", "positive_ctxs": [{"text": "Difference Between Cross Correlation and Autocorrelation Cross correlation happens when two different sequences are correlated. Autocorrelation is the correlation between two of the same sequences. In other words, you correlate a signal with itself."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The cross product is a calculation used in order to define the correlation coefficient between two variables. SP is the sum of all cross products between two variables."}, {"text": "Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series.  The autocorrelation coefficients are plotted to show the autocorrelation function or ACF. The plot is also known as a correlogram."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Serial correlation is the relationship between a variable and a lagged version of itself over various time intervals. Repeating patterns often show serial correlation when the level of a variable affects its future level.  Serial correlation is also known as autocorrelation or lagged correlation."}, {"text": "Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series. There are several autocorrelation coefficients, corresponding to each panel in the lag plot."}, {"text": "Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series. There are several autocorrelation coefficients, corresponding to each panel in the lag plot."}, {"text": "Cross correlation and autocorrelation are very similar, but they involve different types of correlation: Cross correlation happens when two different sequences are correlated. Autocorrelation is the correlation between two of the same sequences. In other words, you correlate a signal with itself."}]}, {"question": "What are different tools used for text mining", "positive_ctxs": [{"text": "Top 8 Text Mining ToolsMonkeyLearn | User-friendly text mining.Aylien | Simple API for text mining.IBM Watson | Powerful AI platform.Thematic | Text mining for customer feedback.Google Cloud NLP | Custom machine learning models.Amazon Comprehend | Pre-trained text mining models.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Big data analytics and data mining are not the same. Both of them involve the use of large data sets, handling the collection of the data or reporting of the data which is mostly used by businesses. However, both big data analytics and data mining are both used for two different operations."}, {"text": "This article lists out 10 comprehensive data mining tools widely used in the big data industry.Rapid Miner.  Oracle Data Mining.  IBM SPSS Modeler.  KNIME.  Python.  Orange.  Kaggle.  Rattle.More items\u2022"}, {"text": "Text mining (also referred to as text analytics) is an artificial intelligence (AI) technology that uses natural language processing (NLP) to transform the free (unstructured) text in documents and databases into normalized, structured data suitable for analysis or to drive machine learning (ML) algorithms."}, {"text": "POS tags make it possible for automatic text processing tools to take into account which part of speech each word is. This facilitates the use of linguistic criteria in addition to statistics."}, {"text": "N-grams of texts are extensively used in text mining and natural language processing tasks. They are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios)."}, {"text": "What is the best way to store data used for Natural Language Processing?stream data from disk when you can.  inverted indexes each get their own text file (more relevant for search, maybe not what you're doing)use sparse data structures (e.g. sparse matrix) as much as possible when you need to load stuff into memory."}, {"text": "Data mining has several types, including pictorial data mining, text mining, social media mining, web mining, and audio and video mining amongst others.Read: Data Mining vs Machine Learning.Learn more: Association Rule Mining.Check out: Difference between Data Science and Data Mining.Read: Data Mining Project Ideas."}]}, {"question": "How do you rank data for the Kruskal Wallis test", "positive_ctxs": [{"text": "When working with a measurement variable, the Kruskal\u2013Wallis test starts by substituting the rank in the overall data set for each measurement value. The smallest value gets a rank of 1, the second-smallest gets a rank of 2, etc."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Kruskal Wallis H test uses ranks instead of actual data.  It is sometimes called the one-way ANOVA on ranks, as the ranks of the data values are used in the test rather than the actual data points. The test determines whether the medians of two or more groups are different."}, {"text": "Assumptions for the Kruskal Wallis Test One independent variable with two or more levels (independent groups). The test is more commonly used when you have three or more levels. For two levels, consider using the Mann Whitney U Test instead. Ordinal scale, Ratio Scale or Interval scale dependent variables."}, {"text": "There is no equivalent. A Kruskal Wallis is a non-parametric test. You say \u201cis this difference larger than I would expect by chance\u201d. You don't have a parameter, which is the size of the difference."}, {"text": "Correlation Test Between Two Variables in RR functions.Import your data into R.Visualize your data using scatter plots.Preleminary test to check the test assumptions.Pearson correlation test. Interpretation of the result. Access to the values returned by cor.test() function.Kendall rank correlation test.Spearman rank correlation coefficient."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Mean rank. The mean rank is the average of the ranks for all observations within each sample. Minitab uses the mean rank to calculate the H-value, which is the test statistic for the Kruskal-Wallis test.  If two or more observations are tied, Minitab assigns the average rank to each tied observation."}, {"text": "The Wilcoxon signed rank test is a nonparametric test that compares the median of a set of numbers against a hypothetical median. The Wilcoxon rank sum test is a nonparametric test to compare two unmatched groups. It is equivalent to the Mann-Whitney test."}]}, {"question": "How does selection bias affect results", "positive_ctxs": [{"text": "Selection bias can result when the selection of subjects into a study or their likelihood of being retained in the study leads to a result that is different from what you would have gotten if you had enrolled the entire target population."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Types of selection bias The most common type of selection bias in research or statistical analysis is a sample selection bias.  In principle, the bias can occur through selection effects in other aspects of the research process, such as which variables to use in analysis, and which tools to use to perform measurement."}, {"text": "Events are independent if the outcome of one event does not affect the outcome of another. For example, if you throw a die and a coin, the number on the die does not affect whether the result you get on the coin."}, {"text": "Another way researchers try to minimize selection bias is by conducting experimental studies, in which participants are randomly assigned to the study or control groups (i.e. randomized controlled studies or RCTs). However, selection bias can still occur in RCTs."}, {"text": "In probability, two events are independent if the incidence of one event does not affect the probability of the other event. If the incidence of one event does affect the probability of the other event, then the events are dependent. There is a red 6-sided fair die and a blue 6-sided fair die."}, {"text": "Independent EventsTwo events A and B are said to be independent if the fact that one event has occurred does not affect the probability that the other event will occur.If whether or not one event occurs does affect the probability that the other event will occur, then the two events are said to be dependent."}, {"text": "Independent EventsTwo events A and B are said to be independent if the fact that one event has occurred does not affect the probability that the other event will occur.If whether or not one event occurs does affect the probability that the other event will occur, then the two events are said to be dependent."}, {"text": "Also known as implicit social cognition, implicit bias refers to the attitudes or stereotypes that affect our understanding, actions, and decisions in an unconscious manner."}]}, {"question": "Is year a quantitative or categorical variable", "positive_ctxs": [{"text": "The year is a categorical variable. The ratio between two years is not meaningful which is why its not appropriate to classify it as a quantitative variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The year is a categorical variable. The ratio between two years is not meaningful which is why its not appropriate to classify it as a quantitative variable."}, {"text": "Also known as a parallel boxplot or comparative boxplot, a side-by-side boxplot is a visual display comparing the levels (the possible values) of one categorical variable by means of a quantitative variable."}, {"text": "An ordinal variable is a categorical variable for which the possible values are ordered. Ordinal variables can be considered \u201cin between\u201d categorical and quantitative variables. Thus it does not make sense to take a mean of the values."}, {"text": "Just like the post period dummy variable controls for factors changing over time that are common to both treatment and control groups, the year fixed effects (i.e. year dummy variables) control for factors changing each year that are common to all cities for a given year."}, {"text": "For a dichotomous categorical variable and a continuous variable you can calculate a Pearson correlation if the categorical variable has a 0/1-coding for the categories.  But when you have more than two categories for the categorical variable the Pearson correlation is not appropriate anymore."}, {"text": "A dummy variable (aka, an indicator variable) is a numeric variable that represents categorical data, such as gender, race, political affiliation, etc.  For example, suppose we are interested in political affiliation, a categorical variable that might assume three values - Republican, Democrat, or Independent."}, {"text": "The probability distribution associated with a random categorical variable is called a categorical distribution. Categorical data is the statistical data type consisting of categorical variables or of data that has been converted into that form, for example as grouped data."}]}, {"question": "What is systematic and cluster sampling", "positive_ctxs": [{"text": "Systematic sampling involves selecting fixed intervals from the larger population to create the sample. Cluster sampling divides the population into groups, then takes a random sample from each cluster."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "The main types of probability sampling methods are simple random sampling, stratified sampling, cluster sampling, multistage sampling, and systematic random sampling."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).  In cluster sampling, the sampling unit is the whole cluster; Instead of sampling individuals from within each group, a researcher will study whole clusters."}, {"text": "Cluster Sampling: Advantages and Disadvantages Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}]}, {"question": "How do you explain bootstrapping", "positive_ctxs": [{"text": "Bootstrapping is a type of resampling where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "False negatives \u2014 that is, a test that says you don't have the virus when you actually do have the virus \u2014 may occur."}, {"text": "In statistics, bootstrapping is any test or metric that relies on random sampling with replacement. Bootstrapping allows assigning measures of accuracy (defined in terms of bias, variance, confidence intervals, prediction error or some other such measure) to sample estimates. 231 views."}]}, {"question": "What is decision tree diagram", "positive_ctxs": [{"text": "A decision tree is a flowchart-like diagram that shows the various outcomes from a series of decisions. It can be used as a decision-making tool, for research analysis, or for planning strategy. A primary advantage for using a decision tree is that it is easy to follow and understand."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision tree learning is a supervised machine learning technique for inducing a decision tree from training data. A decision tree (also referred to as a classification tree or a reduction tree) is a predictive model which is a mapping from observations about an item to conclusions about its target value."}, {"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value."}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes"}, {"text": "A greedy algorithm is used to construct a Huffman tree during Huffman coding where it finds an optimal solution. In decision tree learning, greedy algorithms are commonly used, however they are not guaranteed to find the optimal solution. One popular such algorithm is the ID3 algorithm for decision tree construction."}]}, {"question": "What does AGI stand for in artificial intelligence", "positive_ctxs": [{"text": "Artificial General Intelligence"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action. Some academic sources reserve the term \"strong AI\" for machines that can experience consciousness."}, {"text": "It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action. Some academic sources reserve the term \"strong AI\" for machines that can experience consciousness."}, {"text": "Artificial intelligence is generally divided into two types \u2013 narrow (or weak) AI and general AI, also known as AGI or strong AI."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features or business intelligence in the input data. Perceptron was introduced by Frank Rosenblatt in 1957.  A Perceptron is an algorithm for supervised learning of binary classifiers."}, {"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}]}, {"question": "What is an intuitive explanation of the AdaDelta Deep Learning optimizer", "positive_ctxs": [{"text": "The gradient vector is scaled uniformly by a scalar learning rate . The learning rate remains constant throughout the learning process."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep Learning is the evolution of Machine Learning and it will definitely help in making machines better than what Machine Learning does. But one thing to note is that Deep Learning models require a very large amount of data to train the model otherwise it won't work as expected."}, {"text": "Much of the modern innovations in image recognition is reliant on Deep Learning technology, an advanced type of Machine Learning, and the modern wonder of Artificial Intelligence.  For image recognition, the kind of neural network used is called convolutional neural networks."}, {"text": "Tensorflow is the more popular of the two. Tensorflow is typically used more in Deep Learning and Neural Networks. SciKit learn is more general Machine Learning."}, {"text": "Tensorflow is the most used library used in development of Deep Learning models.  Keras, on the other end, is a high-level API that is built on top of TensorFlow. It is extremely user-friendly and comparatively easier than TensorFlow."}, {"text": "Regularization is a set of techniques that can prevent overfitting in neural networks and thus improve the accuracy of a Deep Learning model when facing completely new data from the problem domain."}, {"text": "The most intuitive way to increase the frequency resolution of an FFT is to increase the size while keeping the sampling frequency constant. Doing this will increase the number of frequency bins that are created, decreasing the frequency difference between each."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "How can I measure the correlation between continuous and categorical variables", "positive_ctxs": [{"text": "There are three big-picture methods to understand if a continuous and categorical are significantly correlated \u2014 point biserial correlation, logistic regression, and Kruskal Wallis H Test. The point biserial correlation coefficient is a special case of Pearson's correlation coefficient."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Partial correlation is a measure of the strength and direction of a linear relationship between two continuous variables whilst controlling for the effect of one or more other continuous variables (also known as 'covariates' or 'control' variables)."}, {"text": "Association between two variables means the values of one variable relate in some way to the values of the other. Association is usually measured by correlation for two continuous variables and by cross tabulation and a Chi-square test for two categorical variables."}, {"text": "Association between two variables means the values of one variable relate in some way to the values of the other. Association is usually measured by correlation for two continuous variables and by cross tabulation and a Chi-square test for two categorical variables."}, {"text": "It is possible to find the correlation between a categorical variable and a continuous variable using the analysis of covariance technique."}, {"text": "For a dichotomous categorical variable and a continuous variable you can calculate a Pearson correlation if the categorical variable has a 0/1-coding for the categories.  But when you have more than two categories for the categorical variable the Pearson correlation is not appropriate anymore."}, {"text": "To measure the relationship between numeric variable and categorical variable with > 2 levels you should use eta correlation (square root of the R2 of the multifactorial regression). If the categorical variable has 2 levels, point-biserial correlation is used (equivalent to the Pearson correlation)."}, {"text": "A partial correlation is basically the correlation between two variables when a third variable is held constant.  If we look at the relationship between exercise and weight loss, we see a negative correlation, which sounds bad but isn't. It means that the more I exercise, the more weight I lose."}]}, {"question": "What is an attention layer", "positive_ctxs": [{"text": "Attention is simply a vector, often the outputs of dense layer using softmax function.  However, attention partially fixes this problem. It allows machine translator to look over all the information the original sentence holds, then generate the proper word according to current word it works on and the context."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Convolutional Neural Networks have a different architecture than regular Neural Networks.  Every layer is made up of a set of neurons, where each layer is fully connected to all neurons in the layer before. Finally, there is a last fully-connected layer \u2014 the output layer \u2014 that represent the predictions."}, {"text": "Forward propagation is how neural networks make predictions. Input data is \u201cforward propagated\u201d through the network layer by layer to the final layer which outputs a prediction."}]}, {"question": "How do you convert logit to probability", "positive_ctxs": [{"text": "To convert a logit ( glm output) to probability, follow these 3 steps:Take glm output coefficient (logit)compute e-function on the logit using exp() \u201cde-logarithimize\u201d (you'll get odds then)convert odds to probability using this formula prob = odds / (1 + odds) ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To convert a logit ( glm output) to probability, follow these 3 steps:Take glm output coefficient (logit)compute e-function on the logit using exp() \u201cde-logarithimize\u201d (you'll get odds then)convert odds to probability using this formula prob = odds / (1 + odds) ."}, {"text": "To convert a logit ( glm output) to probability, follow these 3 steps:Take glm output coefficient (logit)compute e-function on the logit using exp() \u201cde-logarithimize\u201d (you'll get odds then)convert odds to probability using this formula prob = odds / (1 + odds) ."}, {"text": "Weighted percentages allow you to account for this. All you have to do is convert the percentage the assignment is worth into a decimal and multiply that by your grade. To convert, just divide the percentage of your final grade the assignment represents by 100."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "The logistic function is the inverse of the natural logit function and so can be used to convert the logarithm of odds into a probability. In mathematical notation the logistic function is sometimes written as expit in the same form as logit."}, {"text": "The logistic function is the inverse of the natural logit function and so can be used to convert the logarithm of odds into a probability. In mathematical notation the logistic function is sometimes written as expit in the same form as logit."}, {"text": "3.4. 1 The Logit Link Function. The logit link function is used to model the probability of 'success' as a function of covariates (e.g., logistic regression)."}]}, {"question": "How does a Type I error differ from a Type II error", "positive_ctxs": [{"text": "In statistical hypothesis testing, a type I error is the rejection of a true null hypothesis (also known as a \"false positive\" finding or conclusion; example: \"an innocent person is convicted\"), while a type II error is the non-rejection of a false null hypothesis (also known as a \"false negative\" finding or conclusion"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Type I error is equivalent to a False positive. Type II error is equivalent to a False negative. Type I error refers to non-acceptance of hypothesis which ought to be accepted. Type II error is the acceptance of hypothesis which ought to be rejected."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive."}, {"text": "in a test involving multiple comparisons, the probability of making at least one Type I error over an entire research study. The experiment-wise error rate differs from the testwise error rate, which is the probability of making a Type I error when performing a specific test or comparison."}, {"text": "Type I and type II errors are instrumental for the understanding of hypothesis testing in a clinical research scenario.  A type II error can be thought of as the opposite of a type I error and is when a researcher fails to reject the null hypothesis that is actually false in reality."}, {"text": "A Type I is a false positive where a true null hypothesis that there is nothing going on is rejected. A Type II error is a false negative, where a false null hypothesis is not rejected \u2013 something is going on \u2013 but we decide to ignore it."}, {"text": "In terms of the courtroom example, a type I error corresponds to convicting an innocent defendant. Type II error. The second kind of error is the failure to reject a false null hypothesis as the result of a test procedure."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true."}]}, {"question": "How do I calculate class boundaries in statistics", "positive_ctxs": [{"text": "Class boundaries are the numbers used to separate classes. The size of the gap between classes is the difference between the upper class limit of one class and the lower class limit of the next class. In this case, gap=21.83\u221221.82=0.01 gap = 21.83 - 21.82 = 0.01 ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "When I calculate population variance, I then divide the sum of squared deviations from the mean by the number of items in the population (in example 1 I was dividing by 12). When I calculate sample variance, I divide it by the number of items in the sample less one. In our example 2, I divide by 99 (100 less 1)."}, {"text": "Class Boundaries. Separate one class in a grouped frequency distribution from another. The boundaries have one more decimal place than the raw data and therefore do not appear in the data. There is no gap between the upper boundary of one class and the lower boundary of the next class."}, {"text": "Class limits specify the span of data values that fall within a class. Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next.  Class limits are not possible data values. Class boundaries specify the span of data values that fall within a class."}, {"text": "Class limits specify the span of data values that fall within a class. Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next."}, {"text": "The lower class boundary is found by subtracting 0.5 units from the lower class limit and the upper class boundary is found by adding 0.5 units to the upper class limit. The difference between the upper and lower boundaries of any class."}, {"text": "(Select all that apply.) Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next. Class limits specify the span of data values that fall within a class."}]}, {"question": "Can Excel be used for statistical analysis", "positive_ctxs": [{"text": "You can perform statistical analysis with the help of Excel.  If it is not there, go to Excel \u2192 File \u2192 Options \u2192 Add-in and enable the Analysis ToolPak by selecting the Excel Add-ins option in manage tab and then, click GO. This will open a small window; select the Analysis ToolPak option and enable it."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Start by learning key data analysis tools such as Microsoft Excel, Python, SQL and R. Excel is the most widely used spreadsheet program and is excellent for data analysis and visualization. Enroll in one of the free Excel courses and learn how to use this powerful software."}, {"text": "Multivariate analysis is a set of statistical techniques used for analysis of data that contain more than one variable.  Multivariate analysis refers to any statistical technique used to analyse more complex sets of data."}, {"text": "Multivariate analysis is a set of statistical techniques used for analysis of data that contain more than one variable.  Multivariate analysis refers to any statistical technique used to analyse more complex sets of data."}, {"text": "The three main methods to perform linear regression analysis in Excel are: Regression tool included with Analysis ToolPak. Scatter chart with a trendline."}, {"text": "Cluster analysis is a statistical method used to group similar objects into respective categories. It can also be referred to as segmentation analysis, taxonomy analysis, or clustering.  For example, when cluster analysis is performed as part of market research, specific groups can be identified within a population."}, {"text": "Discriminant analysis is statistical technique used to classify observations into non-overlapping groups, based on scores on one or more quantitative predictor variables. For example, a doctor could perform a discriminant analysis to identify patients at high or low risk for stroke."}, {"text": "Discriminant analysis is statistical technique used to classify observations into non-overlapping groups, based on scores on one or more quantitative predictor variables. For example, a doctor could perform a discriminant analysis to identify patients at high or low risk for stroke."}]}, {"question": "When would you use a multinomial", "positive_ctxs": [{"text": "Multinomial logistic regression is used when the dependent variable in question is nominal (equivalently categorical, meaning that it falls into any one of a set of categories that cannot be ordered in any meaningful way) and for which there are more than two categories."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "When the response categories are ordered, you could run a multinomial regression model. The disadvantage is that you are throwing away information about the ordering. An ordinal logistic regression model preserves that information, but it is slightly more involved."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "A multinomial experiment is almost identical with one main difference: a binomial experiment can have two outcomes, while a multinomial experiment can have multiple outcomes.  A binomial experiment will have a binomial distribution."}, {"text": "When n * p and n * q are greater than 5, you can use the normal approximation to the binomial to solve a problem."}, {"text": "When you reject the null hypothesis with a t-test, you are saying that the means are statistically different. The difference is meaningful. Chi Square:  When you reject the null hypothesis with a Chi-Square, you are saying that there is a relationship between the two variables."}, {"text": "LSI Graph is a free LSI keyword tool designed to help you identify dozens of related terms to use in your copy. Visit the website and enter your target keyword to generate a long list of potential LSI keywords. When you have a long list of LSI keywords, it may be tempting to use as many as possible in your content."}]}, {"question": "What do you mean by descriptive statistics", "positive_ctxs": [{"text": "Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures.  Descriptive statistics are typically distinguished from inferential statistics. With descriptive statistics you are simply describing what is or what the data shows."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "When analysing data, such as the grades earned by 100 students, it is possible to use both descriptive and inferential statistics in your analysis. Typically, in most research conducted on groups of people, you will use both descriptive and inferential statistics to analyse your results and draw conclusions."}, {"text": "Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population."}, {"text": "Inferential statistics helps to suggest explanations for a situation or phenomenon. It allows you to draw conclusions based on extrapolations, and is in that way fundamentally different from descriptive statistics that merely summarize the data that has actually been measured."}, {"text": "Inferential statistics helps to suggest explanations for a situation or phenomenon. It allows you to draw conclusions based on extrapolations, and is in that way fundamentally different from descriptive statistics that merely summarize the data that has actually been measured."}, {"text": "Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures.  Descriptive statistics are typically distinguished from inferential statistics. With descriptive statistics you are simply describing what is or what the data shows."}, {"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "Statistics Needed for Data Science For example, data analysis requires descriptive statistics and probability theory, at a minimum. These concepts will help you make better business decisions from data. Key concepts include probability distributions, statistical significance, hypothesis testing, and regression."}]}, {"question": "What is random error", "positive_ctxs": [{"text": "Random errors in experimental measurements are caused by unknown and unpredictable changes in the experiment. These changes may occur in the measuring instruments or in the environmental conditions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "In statistical classification, Bayes error rate is the lowest possible error rate for any classifier of a random outcome (into, for example, one of two categories) and is analogous to the irreducible error. A number of approaches to the estimation of the Bayes error rate exist."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}]}, {"question": "What is the operator norm of a matrix", "positive_ctxs": [{"text": "In mathematics, the operator norm is a means to measure the \"size\" of certain linear operators. Formally, it is a norm defined on the space of bounded linear operators between two given normed vector spaces."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "30.4. Introduction. A matrix norm is a number defined in terms of the entries of the matrix. The norm is a useful quantity which can give important information about a matrix."}, {"text": "The matrix norm is similar to the magnitude of a vector. It is useful whenever a system/problem can be formulated into a matrix that has some physical meaning."}, {"text": "The trace of a matrix is the sum of its (complex) eigenvalues, and it is invariant with respect to a change of basis. This characterization can be used to define the trace of a linear operator in general. The trace is only defined for a square matrix (n \u00d7 n)."}, {"text": "A matrix with rows and columns over a field is a function from the set of all ordered pairs of integers in range to .  A linear operator is a linear function from a Vector space to itself. In notations, given a vector space , a linear operator is a function which satisfies for all in the underlying Field and vectors ."}, {"text": "The integral operator is a linear operator because it preserves two operations; the addition between functions and the multiplication of a function"}, {"text": "SVD is the decomposition of a matrix A into 3 matrices \u2013 U, S, and V. S is the diagonal matrix of singular values. Think of singular values as the importance values of different features in the matrix. The rank of a matrix is a measure of the unique information stored in a matrix."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is the goal of a generative adversarial network Gan", "positive_ctxs": [{"text": "The generator is a convolutional neural network and the discriminator is a deconvolutional neural network. The goal of the generator is to artificially manufacture outputs that could easily be mistaken for real data. The goal of the discriminator is to identify which outputs it receives have been artificially created."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss)."}, {"text": "A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss)."}, {"text": "Minibatch Discrimination is a discriminative technique for generative adversarial networks where we discriminate between whole minibatches of samples rather than between individual samples. This is intended to avoid collapse of the generator."}, {"text": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks."}, {"text": "The deep convolutional generative adversarial network, or DCGAN for short, is an extension of the GAN architecture for using deep convolutional neural networks for both the generator and discriminator models and configurations for the models and training that result in the stable training of a generator model."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is K means algorithm in machine learning", "positive_ctxs": [{"text": "K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.  In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Different classifiers are then added on top of this feature extractor to classify images.Support Vector Machines. It is a supervised machine learning algorithm used for both regression and classification problems.  Decision Trees.  K Nearest Neighbor.  Artificial Neural Networks.  Convolutional Neural Networks."}, {"text": "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging."}, {"text": "In general, K-means is a heuristic algorithm that partitions a data set into K clusters by minimizing the sum of squared distance in each cluster.  In this paper, the simulation of basic k-means algorithm is done, which is implemented using Euclidian distance metric."}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "XGboost is the most widely used algorithm in machine learning, whether the problem is a classification or a regression problem. It is known for its good performance as compared to all other machine learning algorithms."}, {"text": "The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems."}, {"text": "The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.  The results of the K-means clustering algorithm are: The centroids of the K clusters, which can be used to label new data. Labels for the training data (each data point is assigned to a single cluster)"}]}, {"question": "Who came up with the law of averages", "positive_ctxs": [{"text": "Jakob Bernoulli"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The law of averages is a false belief, sometimes known as the 'gambler's fallacy,' that is derived from the law of large numbers.  The law of averages is a misconception that probability occurs with a small number of consecutive experiments so they will certainly have to 'average out' sooner rather than later."}, {"text": "The law of averages is not a mathematical principle, whereas the law of large numbers is. In probability theory, the law of large numbers is a theorem that describes the result of performing the same experiment a large number of times."}, {"text": "The law of averages is often mistaken by many people as the law of large numbers, but there is a big difference. The law of averages is a spurious belief that any deviation in expected probability will have to average out in a small sample of consecutive experiments, but this is not necessarily true."}, {"text": "The law of averages is sometimes known as \u201cGambler's Fallacy. \u201d It evokes the idea that an event is \u201cdue\u201d to happen.  The law of averages says it's due to land on black! \u201d Of course, the wheel has no memory and its probabilities do not change according to past results."}, {"text": "The law of averages is not a mathematical principle, whereas the law of large numbers is.  According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed."}, {"text": "Functions of Random Variables One law is called the \u201cweak\u201d law of large numbers, and the other is called the \u201cstrong\u201d law of large numbers. The weak law describes how a sequence of probabilities converges, and the strong law describes how a sequence of random variables behaves in the limit."}, {"text": "Cohen came up with a mechanism to calculate a value which represents the level of agreement between judges negating the agreement by chance.  You can see that balls which are agreed on by chance are removed both from agreed and total number of balls. And that is the whole intuition of Kappa value aka Kappa coefficient."}]}, {"question": "Can Latent Semantic Analysis used for document classification", "positive_ctxs": [{"text": "Latent Semantic Analysis is a technique for creating a vector representation of a document.  This in turn means you can do handy things like classifying documents to determine which of a set of known topics they most likely belong to."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Linear Discriminant Analysis or Normal Discriminant Analysis or Discriminant Function Analysis is a dimensionality reduction technique which is commonly used for the supervised classification problems. It is used for modeling differences in groups i.e. separating two or more classes."}, {"text": "Linear Discriminant Analysis or Normal Discriminant Analysis or Discriminant Function Analysis is a dimensionality reduction technique which is commonly used for the supervised classification problems. It is used for modeling differences in groups i.e. separating two or more classes."}, {"text": "Photo Credit: Pixabay. Topic modeling is a type of statistical modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic."}, {"text": "Cluster Analysis and Factor Analysis. Latent Class Analysis is similar to cluster analysis. Observed data is analyzed, connections are found, and the data is grouped into clusters.  Another difference is that LCA includes discrete latent categorical variables that have a multinomial distribution."}, {"text": "The various methods used for dimensionality reduction include: Principal Component Analysis (PCA) Linear Discriminant Analysis (LDA) Generalized Discriminant Analysis (GDA)"}, {"text": "Latent Class Analysis (LCA) is a statistical method for identifying unmeasured class membership among subjects using categorical and/or continuous observed variables. For example, you may wish to categorize people based on their drinking behaviors (observations) into different types of drinkers (latent classes)."}, {"text": "LDA stands for Latent Dirichlet Allocation, and it is a type of topic modeling algorithm. The purpose of LDA is to learn the representation of a fixed number of topics, and given this number of topics learn the topic distribution that each document in a collection of documents has."}]}, {"question": "What is likelihood in probability", "positive_ctxs": [{"text": "Probability is about a finite set of possible outcomes, given a probability. Likelihood is about an infinite set of possible probabilities, given an outcome."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable."}, {"text": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable."}, {"text": "The distinction between probability and likelihood is fundamentally important: Probability attaches to possible results; likelihood attaches to hypotheses. Explaining this distinction is the purpose of this first column. Possible results are mutually exclusive and exhaustive."}, {"text": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable."}, {"text": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable."}]}, {"question": "Can you split a multinomial logistic regression model into separate binary logistic regression models", "positive_ctxs": [{"text": "If the outcomes are mutually independent, then yes the method is valid. If the outcomes are mutually exclusive, then no, the method is not valid. It's easy to see why this is the case. If you have three binary models, then the sum of the outcomes do not necessarily sum to one."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)\u2208{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits."}, {"text": "When the response categories are ordered, you could run a multinomial regression model. The disadvantage is that you are throwing away information about the ordering. An ordinal logistic regression model preserves that information, but it is slightly more involved."}, {"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "The Softmax regression is a form of logistic regression that normalizes an input value into a vector of values that follows a probability distribution whose total sums up to 1."}, {"text": "The logit is a transformation. Logistic regression is a regression model. The logit transformation transforms a line to a logistic curve. Logistic regression fits a logistic curve to set of data where the dependent variable can only take the values 0 and 1."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}]}, {"question": "What is negatively skewed distribution", "positive_ctxs": [{"text": "In statistics, a negatively skewed (also known as left-skewed) distribution is a type of distribution in which more values are concentrated on the right side (tail) of the distribution graph while the left tail of the distribution graph is longer."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A left-skewed distribution has a long left tail.  The normal distribution is the most common distribution you'll come across. Next, you'll see a fair amount of negatively skewed distributions. For example, household income in the U.S. is negatively skewed with a very long left tail."}, {"text": "If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical."}, {"text": "If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical.  If skewness is less than \u22121 or greater than +1, the distribution is highly skewed."}, {"text": "In statistics, a negatively skewed (also known as left-skewed) distribution is a type of distribution in which more values are concentrated on the right side (tail) of the distribution graph while the left tail of the distribution graph is longer."}, {"text": "In positively skewed distributions, the mean is usually greater than the median, which is always greater than the mode. In negatively skewed distributions, the mean is usually less than the median, which is always less than the mode."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right. In a negatively skewed distribution, the mean is usually less than the median because the few low scores tend to shift the mean to the left."}]}, {"question": "What is Kappa used for", "positive_ctxs": [{"text": "Kappa is widely used on Twitch in chats to signal you are being sarcastic or ironic, are trolling, or otherwise playing around with someone. It is usually typed at the end of a string of text, but, as can often the case on Twitch, it is also often used on its own or repeatedly (to spam someone)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Accuracy is the percentage of correctly classifies instances out of all instances.  Kappa or Cohen's Kappa is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The equation used to calculate kappa is: \u039a = PR(e), where Pr(a) is the observed agreement among the raters and Pr(e) is the hypothetical probability of the raters indicating a chance agreement. The formula was entered into Microsoft Excel and it was used to calculate the Kappa coefficient."}, {"text": "The equation used to calculate kappa is: \u039a = PR(e), where Pr(a) is the observed agreement among the raters and Pr(e) is the hypothetical probability of the raters indicating a chance agreement. The formula was entered into Microsoft Excel and it was used to calculate the Kappa coefficient."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "Which type of neural networks can be used for time series data", "positive_ctxs": [{"text": "Although many types of neural network models have been developed to solve different problems, the most widely used model by far for time series forecasting has been the feedforward neural network."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bayesian networks are a type of Probabilistic Graphical Model that can be used to build models from data and/or expert opinion. They can be used for a wide range of tasks including prediction, anomaly detection, diagnostics, automated insight, reasoning, time series prediction and decision making under uncertainty."}, {"text": "Bayesian networks are a type of Probabilistic Graphical Model that can be used to build models from data and/or expert opinion. They can be used for a wide range of tasks including prediction, anomaly detection, diagnostics, automated insight, reasoning, time series prediction and decision making under uncertainty."}, {"text": "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.  LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series."}, {"text": "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.  LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series."}, {"text": "Generally, we use linear regression for time series analysis, it is used for predicting the result for time series as its trends. For example, If we have a dataset of time series with the help of linear regression we can predict the sales with the time."}, {"text": "7.2. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems. Radial basis function networks are distinguished from other neural networks due to their universal approximation and faster learning speed."}, {"text": "8 Radial Basis Function Networks. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems.  An RBF network is a type of feed forward neural network composed of three layers, namely the input layer, the hidden layer and the output layer."}]}, {"question": "How do you find how many standard deviations away from the mean", "positive_ctxs": [{"text": "You calculate the mean, say it's 10. You calculate the standard deviation: it's 12. That means that any number from 10 to 22 is within one standard deviation away from the mean. Now if your data are symmetric (say normal), any number from -2 to 10 is also within a standard deviation from the mean."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Any point (x) from a normal distribution can be converted to the standard normal distribution (z) with the formula z = (x-mean) / standard deviation. z for any particular x value shows how many standard deviations x is away from the mean for all x values."}, {"text": "The value of the z-score tells you how many standard deviations you are away from the mean. If a z-score is equal to 0, it is on the mean. A positive z-score indicates the raw score is higher than the mean average. For example, if a z-score is equal to +1, it is 1 standard deviation above the mean."}, {"text": "The value of the z-score tells you how many standard deviations you are away from the mean. If a z-score is equal to 0, it is on the mean. A positive z-score indicates the raw score is higher than the mean average. For example, if a z-score is equal to +1, it is 1 standard deviation above the mean."}, {"text": "Simply put, a z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it's a measure of how many standard deviations below or above the population mean a raw score is. A z-score can be placed on a normal distribution curve."}, {"text": "A z-score tells you how many standard deviations from the mean your result is. You can use your knowledge of normal distributions (like the 68 95 and 99.7 rule) or the z-table to determine what percentage of the population will fall below or above your result."}, {"text": "A Z-score is a score which indicates how many standard deviations an observation is from the mean of the distribution. Z-scores tend to be used mainly in the context of the normal curve, and their interpretation based on the standard normal table.  Non-normal distributions can also be transformed into sets of Z-scores."}, {"text": "To calculate the standard error, follow these steps:Record the number of measurements (n) and calculate the sample mean (\u03bc).  Calculate how much each measurement deviates from the mean (subtract the sample mean from the measurement).Square all the deviations calculated in step 2 and add these together:More items\u2022"}]}, {"question": "What is the relationship between PDF and CDF", "positive_ctxs": [{"text": "The pdf represents the relative frequency of failure times as a function of time. The cdf is a function, F(x)\\,\\!, of a random variable X\\,\\!, and is defined for a number x\\,\\!"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "PDF according to input X being discrete or continuous generates probability mass functions and CDF does the same but generates cumulative mass function. That means, PDF is derivative of CDF and CDF can be applied at any point where PDF has been applied.  The cumulative function is the integral of the density function."}, {"text": "PDF according to input X being discrete or continuous generates probability mass functions and CDF does the same but generates cumulative mass function. That means, PDF is derivative of CDF and CDF can be applied at any point where PDF has been applied.  The cumulative function is the integral of the density function."}, {"text": "The Relationship Between a CDF and a PDF In technical terms, a probability density function (pdf) is the derivative of a cumulative density function (cdf). Futhermore, the area under the curve of a pdf between negative infinity and x is equal to the value of x on the cdf."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "It is usually more straightforward to start from the CDF and then to find the PDF by taking the derivative of the CDF. Note that before differentiating the CDF, we should check that the CDF is continuous. As we will see later, the function of a continuous random variable might be a non-continuous random variable."}, {"text": "So a CDF is a function whose output is a probability. The PDF is a function whose output is a nonnegative number. The PDF itself is not a probability (unlike the CDF), but it can be used to calculate probabilities."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "Why is word2vec a log linear model", "positive_ctxs": [{"text": "Word2vec being log-linear means we calculate the gradient at the output and then directly propagate this back into the embedding parameters (the main computational burden during training). This means faster trainer over bigger datasets yielding more accurate embedding vectors."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Log-Log linear regression A regression model where the outcome and at least one predictor are log transformed is called a log-log linear model."}, {"text": "Log-Log linear regression A regression model where the outcome and at least one predictor are log transformed is called a log-log linear model."}, {"text": "Logistic regression, also called a logit model, is used to model dichotomous outcome variables. In the logit model the log odds of the outcome is modeled as a linear combination of the predictor variables."}, {"text": "Logistic regression, also called a logit model, is used to model dichotomous outcome variables. In the logit model the log odds of the outcome is modeled as a linear combination of the predictor variables."}, {"text": "word2vec itself is a simple bi-layered neural network architecture, it turns text into meaningful vectors form that deeper networks can understand. In other words the out put of simple neural word2vec model is used as input for Deep Networks."}, {"text": "word2vec itself is a simple bi-layered neural network architecture, it turns text into meaningful vectors form that deeper networks can understand. In other words the out put of simple neural word2vec model is used as input for Deep Networks."}, {"text": "Why You Should Care About the Classical OLS Assumptions In a nutshell, your linear model should produce residuals that have a mean of zero, have a constant variance, and are not correlated with themselves or other variables."}]}, {"question": "Is K means clustering considered supervised or unsupervised machine learning", "positive_ctxs": [{"text": "K-means is an unsupervised learning algorithm as it infers a clustering (or labels) for a set of provided samples that do not initially have labels. The goal of k-means is to partition the n samples from your dataset in to k clusters where each datapoint belongs to the single cluster for which it is nearest to."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space."}, {"text": "k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}, {"text": "In the absence of a class label, clustering analysis is also called unsupervised learning, as opposed to supervised learning that includes classification and regression. Accordingly, approaches to clustering analysis are typically quite different from supervised learning."}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows\u2022"}, {"text": "You can use an unsupervised learning algorithm (like clustering) to create your training data for the supervised learning algorithm but you cannot simply convert an unsupervised learning algorithm into a supervised one."}, {"text": "KNN represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points, while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters."}]}, {"question": "Which is worse brca1 or brca2", "positive_ctxs": [{"text": "Abnormal BRCA1 and BRCA2 genes are found in 5% to 10% of all breast cancer cases in the United States. A study found that women with an abnormal BRCA1 gene had a worse prognosis than women with an abnormal BRCA2 gene 5 years after diagnosis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "An allocation is Pareto efficient if there is no other allocation in which some other individual is better off and no individual is worse off. Notes: There is no connection between Pareto efficiency and equity! In particular, a Pareto efficient outcome may be very inequitable."}, {"text": "Deviance is a measure of error; lower deviance means better fit to data. The greater the deviance, the worse the model fits compared to the best case (saturated). Deviance is a quality-of-fit statistic for a model that is often used for statistical hypothesis testing."}, {"text": "The Spearman correlation is the same as the Pearson correlation, but it is used on data from an ordinal scale. Which situation would be appropriate for obtaining a phi-coefficient with a Pearson test?"}, {"text": "Note that it is possible to get a negative R-square for equations that do not contain a constant term. Because R-square is defined as the proportion of variance explained by the fit, if the fit is actually worse than just fitting a horizontal line then R-square is negative."}, {"text": "Which intuitively says that the probability of has to be \u201creally high\u201d. In other words, if your value is smaller than E[X], then the upper bound of it taking that value is 1 (basically sort of an uninteresting statement, since you already knew the upper bound was 1 or greater)."}]}, {"question": "Why is random sampling used", "positive_ctxs": [{"text": "Random sampling ensures that results obtained from your sample should approximate what would have been obtained if the entire population had been measured (Shadish et al., 2002). The simplest random sample allows all the units in the population to have an equal chance of being selected."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Systematic sampling is frequently used to select a specified number of records from a computer file. Stratified sampling is commonly used probability method that is superior to random sampling because it reduces sampling error. A stratum is a subset of the population that share at least one common characteristic."}, {"text": "Cluster sampling is best used when the clusters occur naturally in a population, when you don't have access to the entire population, and when the clusters are geographically convenient. However, cluster sampling is not as precise as simple random sampling or stratified random sampling."}, {"text": "Use. Cluster sampling is typically used in market research. It's used when a researcher can't get information about the population as a whole, but they can get information about the clusters.  Cluster sampling is often more economical or more practical than stratified sampling or simple random sampling."}, {"text": "Use. Cluster sampling is typically used in market research. It's used when a researcher can't get information about the population as a whole, but they can get information about the clusters.  Cluster sampling is often more economical or more practical than stratified sampling or simple random sampling."}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).  In cluster sampling, the sampling unit is the whole cluster; Instead of sampling individuals from within each group, a researcher will study whole clusters."}, {"text": "Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}]}, {"question": "What is a good Cohen's kappa", "positive_ctxs": [{"text": "Cohen's kappa.  Cohen suggested the Kappa result be interpreted as follows: values \u2264 0 as indicating no agreement and 0.01\u20130.20 as none to slight, 0.21\u20130.40 as fair, 0.41\u2013 0.60 as moderate, 0.61\u20130.80 as substantial, and 0.81\u20131.00 as almost perfect agreement."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "From Wikipedia, the free encyclopedia. Cohen's kappa coefficient (\u03ba) is a statistic that is used to measure inter-rater reliability (and also Intra-rater reliability) for qualitative (categorical) items."}, {"text": "Cohen's kappa measures the agreement between two raters who each classify N items into C mutually exclusive categories.\u00b9 A simple way to think this is that Cohen's Kappa is a quantitative measure of reliability for two raters that are rating the same thing, corrected for how often that the raters may agree by chance."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The standardized mean difference (SMD) measure of effect is used when studies report efficacy in terms of a continuous measurement, such as a score on a pain-intensity rating scale. The SMD is also known as Cohen's d.  The SMD is a point estimate of the effect of a treatment."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "The weighted kappa is calculated using a predefined table of weights which measure the degree of disagreement between the two raters, the higher the disagreement the higher the weight."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "How do you prove that two random variables are independent", "positive_ctxs": [{"text": "You can tell if two random variables are independent by looking at their individual probabilities. If those probabilities don't change when the events meet, then those variables are independent. Another way of saying this is that if the two variables are correlated, then they are not independent."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "You can tell if two random variables are independent by looking at their individual probabilities. If those probabilities don't change when the events meet, then those variables are independent. Another way of saying this is that if the two variables are correlated, then they are not independent."}, {"text": "You can tell if two random variables are independent by looking at their individual probabilities. If those probabilities don't change when the events meet, then those variables are independent. Another way of saying this is that if the two variables are correlated, then they are not independent."}, {"text": "You can tell if two random variables are independent by looking at their individual probabilities. If those probabilities don't change when the events meet, then those variables are independent. Another way of saying this is that if the two variables are correlated, then they are not independent."}, {"text": "With two-way ANOVA, you have one continuous dependent variable and two categorical grouping variables for the independent variables. MANOVA models several dependent variables simultaneously and you can include a variety of independent variables."}, {"text": "In linear regression the independent variables can be categorical and/or continuous. But, when you fit the model if you have more than two category in the categorical independent variable make sure you are creating dummy variables."}, {"text": "In linear regression the independent variables can be categorical and/or continuous. But, when you fit the model if you have more than two category in the categorical independent variable make sure you are creating dummy variables."}, {"text": "For independent random variables X and Y, the variance of their sum or difference is the sum of their variances: Variances are added for both the sum and difference of two independent random variables because the variation in each variable contributes to the variation in each case."}]}, {"question": "How networks do deep learning", "positive_ctxs": [{"text": "Most deep learning methods use neural network architectures, which is why deep learning models are often referred to as deep neural networks.  A CNN convolves learned features with input data, and uses 2D convolutional layers, making this architecture well suited to processing 2D data, such as images."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. In fact, it is the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three."}, {"text": "Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}, {"text": "Deep learning (sometimes known as deep structured learning) is a subset of machine learning, where machines employ artificial neural networks to process information. Inspired by biological nodes in the human body, deep learning helps computers to quickly recognize and process images and speech."}, {"text": "Three reasons that you should NOT use deep learning(1) It doesn't work so well with small data. To achieve high performance, deep networks require extremely large datasets.  (2) Deep Learning in practice is hard and expensive. Deep learning is still a very cutting edge technique.  (3) Deep networks are not easily interpreted."}, {"text": "Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised."}]}, {"question": "Are based on the idea that subjects are randomly assigned to groups", "positive_ctxs": [{"text": "Random assignment is however a process of randomly assigning subjects to experimental or control groups. This is a standard practice in true experimental research to ensure that treatment groups are similar (equivalent) to each other and to the control group, prior to treatment administration."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Another way researchers try to minimize selection bias is by conducting experimental studies, in which participants are randomly assigned to the study or control groups (i.e. randomized controlled studies or RCTs). However, selection bias can still occur in RCTs."}, {"text": "The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.  The results of the K-means clustering algorithm are: The centroids of the K clusters, which can be used to label new data. Labels for the training data (each data point is assigned to a single cluster)"}, {"text": "(non-RAN-duh-mized KLIH-nih-kul TRY-ul) A clinical trial in which the participants are not assigned by chance to different treatment groups. Participants may choose which group they want to be in, or they may be assigned to the groups by the researchers."}, {"text": "Cluster analysis is a multivariate method which aims to classify a sample of subjects (or ob- jects) on the basis of a set of measured variables into a number of different groups such that similar subjects are placed in the same group.  \u2013 Agglomerative methods, in which subjects start in their own separate cluster."}, {"text": "In clustering, a group of different data objects is classified as similar objects. One group means a cluster of data. Data sets are divided into different groups in the cluster analysis, which is based on the similarity of the data. After the classification of data into various groups, a label is assigned to the group."}, {"text": "Three of the more widely used experimental designs are the completely randomized design, the randomized block design, and the factorial design. In a completely randomized experimental design, the treatments are randomly assigned to the experimental units."}, {"text": "Naive Bayes is a Supervised Machine Learning algorithm based on the Bayes Theorem that is used to solve classification problems by following a probabilistic approach. It is based on the idea that the predictor variables in a Machine Learning model are independent of each other."}]}, {"question": "What are pros and cons for stratified random sampling", "positive_ctxs": [{"text": "Stratified random sampling involves first dividing a population into subpopulations and then applying random sampling methods to each subpopulation to form a test group. A disadvantage is when researchers can't classify every member of the population into a subgroup."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The difference between quota sampling and stratified sampling is: although both \"group\" participants by an important characteristic, stratified sampling relies on random selection within each group, while quota sampling relies on convenience sampling within each group."}, {"text": "The main types of probability sampling methods are simple random sampling, stratified sampling, cluster sampling, multistage sampling, and systematic random sampling."}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "Compared to simple random sampling, stratified sampling has two main disadvantages.Advantages and DisadvantagesA stratified sample can provide greater precision than a simple random sample of the same size.Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money.More items"}, {"text": "A sample may be selected from a population through a number of ways, one of which is the stratified random sampling method. A stratified random sampling involves dividing the entire population into homogeneous groups called strata (plural for stratum). Random samples are then selected from each stratum."}, {"text": "Cluster sampling is best used when the clusters occur naturally in a population, when you don't have access to the entire population, and when the clusters are geographically convenient. However, cluster sampling is not as precise as simple random sampling or stratified random sampling."}, {"text": "Quota sampling is different from stratified sampling, because in a stratified sample individuals within each stratum are selected at random. Quota sampling achieves a representative age distribution, but it isn't a random sample, because the sampling frame is unknown."}]}, {"question": "How is a decision tree trained", "positive_ctxs": [{"text": "Decision Trees in Machine Learning. Decision Tree models are created using 2 steps: Induction and Pruning. Induction is where we actually build the tree i.e set all of the hierarchical decision boundaries based on our data. Because of the nature of training decision trees they can be prone to major overfitting."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision tree learning is a supervised machine learning technique for inducing a decision tree from training data. A decision tree (also referred to as a classification tree or a reduction tree) is a predictive model which is a mapping from observations about an item to conclusions about its target value."}, {"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value."}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes"}, {"text": "Decision trees are a classic machine learning technique. The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree. By Narendra Nath Joshi, Carnegie Mellon.  The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree."}]}, {"question": "How many types of distribution are there", "positive_ctxs": [{"text": "There are two types of probability distribution which are used for different purposes and various types of the data generation process. Let us discuss now both the types along with its definition, formula and examples."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Regression analysis is a powerful statistical method that allows you to examine the relationship between two or more variables of interest. While there are many types of regression analysis, at their core they all examine the influence of one or more independent variables on a dependent variable."}, {"text": "There are two types of probability distribution which are used for different purposes and various types of the data generation process.Normal or Cumulative Probability Distribution.Binomial or Discrete Probability Distribution."}, {"text": "How to Find the Mean. The mean is the average of the numbers. It is easy to calculate: add up all the numbers, then divide by how many numbers there are. In other words it is the sum divided by the count."}, {"text": "How to Compare Data SetsCenter. Graphically, the center of a distribution is the point where about half of the observations are on either side.Spread. The spread of a distribution refers to the variability of the data.  Shape. The shape of a distribution is described by symmetry, skewness, number of peaks, etc.Unusual features."}, {"text": "Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model."}, {"text": "Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters."}, {"text": "How to use them while designing a CNN: Conv2D filters are used only in the initial layers of a Convolutional Neural Network. They are put there to extract the initial high level features from an image."}]}, {"question": "Why use multiple regression instead of Anova", "positive_ctxs": [{"text": "Regression is mainly used in order to make estimates or predictions for the dependent variable with the help of single or multiple independent variables, and ANOVA is used to find a common mean between variables of different groups."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}, {"text": "Multinomial logistic regression is used when you have a categorical dependent variable with two or more unordered levels (i.e. two or more discrete outcomes). It is practically identical to logistic regression, except that you have multiple possible outcomes instead of just one."}, {"text": "Whereas multiple regression predicts a single dependent variable from a set of multiple independent variables, canonical correlation simultaneously predicts multiple dependent variables from multiple independent variables."}, {"text": "Linear regression is one of the most common techniques of regression analysis. Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables."}, {"text": "Linear regression is one of the most common techniques of regression analysis. Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables."}, {"text": "For example, polynomial regression consists of performing multiple regression with variables. in order to find the polynomial coefficients (parameters). These types of regression are known as parametric regression since they are based on models that require the estimation of a finite number of parameters."}]}, {"question": "Is Z score the same as critical value", "positive_ctxs": [{"text": "A critical value of z (Z-score) is used when the sampling distribution is normal, or close to normal. Z-scores are used when the population standard deviation is known or when you have larger sample sizes.  See also: T Critical Value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Lets do this step by step:Step 1: find the mean.Step 2: fin the standard deviation of the mean (using the population SD)Step 3: find the Z score.Step 4: compare to the critical Z score. From the stated hypothesis, we know that we are dealing with a 1-tailed hypothesis test.  Step 4 : compare to the critical Z score."}, {"text": "The decision rule is: Reject H0 if Z < 1.645. The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960. The complete table of critical values of Z for upper, lower and two-tailed tests can be found in the table of Z values to the right in \"Other Resources.\""}, {"text": "The decision rule is: Reject H0 if Z < 1.645. The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960. The complete table of critical values of Z for upper, lower and two-tailed tests can be found in the table of Z values to the right in \"Other Resources.\""}, {"text": "To find the critical value, follow these steps.Compute alpha (\u03b1): \u03b1 = 1 - (confidence level / 100)Find the critical probability (p*): p* = 1 - \u03b1/2.To express the critical value as a z-score, find the z-score having a cumulative probability equal to the critical probability (p*).More items"}, {"text": "Normal Distribution For a one-tailed test, the critical value is 1.645. So the critical region is Z<\u22121.645 for a left-tailed test and Z>1.645 for a right-tailed test. For a two-tailed test, the critical value is 1.96."}, {"text": "If the absolute value of the t-value is greater than the critical value, you reject the null hypothesis. If the absolute value of the t-value is less than the critical value, you fail to reject the null hypothesis."}]}, {"question": "What causes positive skewness", "positive_ctxs": [{"text": "Another cause of skewness is start-up effects. For example, if a procedure initially has a lot of successes during a long start-up period, this could create a positive skew on the data. (On the opposite hand, a start-up period with several initial failures can negatively skew data.)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Negative values for the skewness indicate data that are skewed left and positive values for the skewness indicate data that are skewed right. By skewed left, we mean that the left tail is long relative to the right tail."}, {"text": "If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical.  If skewness is less than \u22121 or greater than +1, the distribution is highly skewed."}, {"text": "If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical."}, {"text": "In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The lognormal distribution is a distribution skewed to the right. The pdf starts at zero, increases to its mode, and decreases thereafter. The degree of skewness increases as increases, for a given . For the same , the pdf's skewness increases as increases."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is a cluster in sampling", "positive_ctxs": [{"text": "In cluster sampling, researchers divide a population into smaller groups known as clusters. They then randomly select among these clusters to form a sample. Cluster sampling is a method of probability sampling that is often used to study large populations, particularly those that are widely geographically dispersed."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "In stratified sampling, a random sample is drawn from each of the strata, whereas in cluster sampling only the selected clusters are sampled. A common motivation of cluster sampling is to reduce costs by increasing sampling efficiency."}, {"text": "Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).  In cluster sampling, the sampling unit is the whole cluster; Instead of sampling individuals from within each group, a researcher will study whole clusters."}, {"text": "Cluster Sampling: Advantages and Disadvantages Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}]}, {"question": "What is entropy of a source", "positive_ctxs": [{"text": "Definition. An entropy source is an input device or a measured characteristic of an I/O device on a computer that supplies random bits: specifically, bits that an attacker cannot know."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition: Entropy is a measure of uncertainty of a random variable. The entropy of a discrete random variable X with alphabet X is H(X) = -) p(x) log p(2) DEX When the base of the logarithm is 2, entropy is measured in bits."}, {"text": "The maximum entropy principle is defined as modeling a given set of data by finding the highest entropy to satisfy the constraints of our prior knowledge.  The maximum entropy model is a conditional probability model p(y|x) that allows us to predict class labels given a set of features for a given data point."}, {"text": "Definition: Entropy is a measure of uncertainty of a random variable. The entropy of a discrete random variable X with alphabet X is H(X) = -) p(x) log p(2) DEX When the base of the logarithm is 2, entropy is measured in bits.  (Note: you can prove this by assigning a variable pi to the probability of outcome i."}, {"text": "The entropy of a substance can be obtained by measuring the heat required to raise the temperature a given amount, using a reversible process. The standard molar entropy, So, is the entropy of 1 mole of a substance in its standard state, at 1 atm of pressure."}, {"text": "\u201cHuman error\u201d is not a source of experimental error. You must classify specific errors as random or systematic and identify the source of the error. Human error cannot be stated as experimental error."}, {"text": "Decision tree algorithms use information gain to split a node. Both gini and entropy are measures of impurity of a node. A node having multiple classes is impure whereas a node having only one class is pure. Entropy in statistics is analogous to entropy in thermodynamics where it signifies disorder."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "How do you interpret multinomial logistic regression", "positive_ctxs": [{"text": "0:3513:46Suggested clip \u00b7 70 secondsInterpreting Odds Ratio for Multinomial Logistic Regression using YouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "When the response categories are ordered, you could run a multinomial regression model. The disadvantage is that you are throwing away information about the ordering. An ordinal logistic regression model preserves that information, but it is slightly more involved."}, {"text": "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)\u2208{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits."}, {"text": "A logistic regression estimates the mean of your response given that your data is distributed Bernoulli or is a Binomial trial. Since the mean of a Binomial trial is the probability of success, you can interpret the output from a Logistic regression (after logit transformation) as a probability of success."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "It is used in multinomial logistic regression and is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes. , and the components will add up to 1, so that they can be interpreted as probabilities."}, {"text": "Multinomial logistic regression is used when you have a categorical dependent variable with two or more unordered levels (i.e. two or more discrete outcomes). It is practically identical to logistic regression, except that you have multiple possible outcomes instead of just one."}]}, {"question": "How do you analyze survey data", "positive_ctxs": [{"text": "How to Analyze Survey ResultsUnderstand the four measurement levels.  Select your research question(s).  Analyze quantitative data first.  Use cross-tabulation to better understand your target audience.  Understand the statistical significance.  Take into consideration causation versus correlation.  Compare data with that of past data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Machine learning algorithms can minimize forecasting error and do the forecast much faster and with the usage of more data. What's more, machine learning algorithms can analyze many alternative models at the same time, when in traditional econometrics you can analyze just one model at a time."}, {"text": "How big data analytics worksdata mining, which sift through data sets in search of patterns and relationships;predictive analytics, which build models to forecast customer behavior and other future developments;machine learning, which taps algorithms to analyze large data sets; and.More items"}, {"text": "In statistics, scale analysis is a set of methods to analyze survey data, in which responses to questions are combined to measure a latent variable.  Any measurement for such data is required to be reliable, valid, and homogeneous with comparable results over different studies."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "So, assuming a 15% survey response rate, we see that you should send your NPS survey to 1,700 customers. What if you're a smaller company and don't have enough customers to send the recommended number of invitations?"}, {"text": "For conducting effective research across multiple geographies, one needs to form complicated clusters that can be achieved only using the multiple-stage sampling technique. An example of Multiple stage sampling by clusters \u2013 An organization intends to survey to analyze the performance of smartphones across Germany."}]}, {"question": "How do you write a null and alternative hypothesis", "positive_ctxs": [{"text": "In a hypothesis test, we:Evaluate the null hypothesis, typically denoted with H0.  Always write the alternative hypothesis, typically denoted with Ha or H1, using less than, greater than, or not equals symbols, i.e., (\u2260, >, or <).More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How to Conduct Hypothesis TestsState the hypotheses. Every hypothesis test requires the analyst to state a null hypothesis and an alternative hypothesis.  Formulate an analysis plan. The analysis plan describes how to use sample data to accept or reject the null hypothesis.  Analyze sample data.  Interpret the results."}, {"text": "Statistical analysts test a hypothesis by measuring and examining a random sample of the population being analyzed. All analysts use a random population sample to test two different hypotheses: the null hypothesis and the alternative hypothesis."}, {"text": "In a hypothesis test, we:Evaluate the null hypothesis, typically denoted with H0.  Always write the alternative hypothesis, typically denoted with Ha or H1, using less than, greater than, or not equals symbols, i.e., (\u2260, >, or <).More items"}, {"text": "In research, there is a convention that the hypothesis is written in two forms, the null hypothesis, and the alternative hypothesis (called the experimental hypothesis when the method of investigation is an experiment)."}, {"text": "How to Formulate an Effective HypothesisState the problem that you are trying to solve. Make sure that the hypothesis clearly defines the topic and the focus of the experiment.Try to write the hypothesis as an if-then statement.  Define the variables."}, {"text": "The purpose and importance of the null hypothesis and alternative hypothesis are that they provide an approximate description of the phenomena. The purpose is to provide the researcher or an investigator with a relational statement that is directly tested in a research study."}]}, {"question": "What is K nearest Knn data mining algorithm", "positive_ctxs": [{"text": "K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970's as a non-parametric technique."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Here is step by step on how to compute K-nearest neighbors KNN algorithm:Determine parameter K = number of nearest neighbors.Calculate the distance between the query-instance and all the training samples.Sort the distance and determine nearest neighbors based on the K-th minimum distance.More items"}, {"text": "The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.  The results of the K-means clustering algorithm are: The centroids of the K clusters, which can be used to label new data. Labels for the training data (each data point is assigned to a single cluster)"}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "In general, K-means is a heuristic algorithm that partitions a data set into K clusters by minimizing the sum of squared distance in each cluster.  In this paper, the simulation of basic k-means algorithm is done, which is implemented using Euclidian distance metric."}, {"text": "K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups).  The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided."}, {"text": "Association rules mining is another key unsupervised data mining method, after clustering, that finds interesting associations (relationships, dependencies) in large sets of data items."}, {"text": "Chebyshev's inequality says that at least 1\u22121K2 of data from a sample must fall within K standard deviations from the mean, where K is any positive real number greater than one."}]}, {"question": "What do feature detectors detect", "positive_ctxs": [{"text": "The ability to detect certain types of stimuli, like movements, shape, and angles, requires specialized cells in the brain called feature detectors. Without these, it would be difficult, if not impossible, to detect a round object, like a baseball, hurdling toward you at 90 miles per hour."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Blob detectors can detect areas in an image which are too smooth to be detected by a corner detector. Consider shrinking an image and then performing corner detection. The detector will respond to points which are sharp in the shrunk image, but may be smooth in the original image."}, {"text": "The scale-invariant feature transform (SIFT) is a feature detection algorithm in computer vision to detect and describe local features in images.  SIFT keypoints of objects are first extracted from a set of reference images and stored in a database."}, {"text": "Excessive dust, spider webs, and loose sensors and detectors can all be the source of false alarms."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "Does downsampling reduce image quality", "positive_ctxs": [{"text": "Downsampling an image When data is removed the image also degrades to some extent, although not nearly as much as when you upsample. By removing this extra data ( downsampling) this results in a much smaller file size. For example, you can see below that our original image was 17.2 MB at 3000 by 2000 pixels."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Does not affect R2 or adjusted R2 (since these estimate the POPULATION variances which are not conditional on X)"}, {"text": "The initial task of image processing is to enhance the quality of digital images for further analysis.  This chapter also reviews methods that are used to quantitatively determine specific image information, such as relative composition, particle size, interparticle distance, intensity profile, etc."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise.  The Gaussian filter alone will blur edges and reduce contrast. The Median filter is a non-linear filter that is most commonly used as a simple way to reduce noise in an image."}, {"text": "Computational photography is a digital image processing technique that uses algorithms to replace optical processes, and it seeks to improve image quality by using machine vision to identify the content of an image.  \u201cFor example, we use AI to train algorithms about the features of people's faces.\u201d"}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}]}, {"question": "What is parametric statistics and nonparametric statistics", "positive_ctxs": [{"text": "Parametric statistics are based on assumptions about the distribution of population from which the sample was taken. Nonparametric statistics are not based on assumptions, that is, the data can be collected from a sample that does not follow a specific distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Classical statistics uses techniques such as Ordinary Least Squares and Maximum Likelihood \u2013 this is the conventional type of statistics that you see in most textbooks covering estimation, regression, hypothesis testing, confidence intervals, etc.  In fact Bayesian statistics is all about probability calculations!"}, {"text": "Statistics is the study of the collection, organization, analysis, and interpretation of data.  Mathematical statistics is the study of statistics from a mathematical standpoint, using probability theory as well as other branches of mathematics such as linear algebra and analysis."}, {"text": "Linear models, generalized linear models, and nonlinear models are examples of parametric regression models because we know the function that describes the relationship between the response and explanatory variables.  If the relationship is unknown and nonlinear, nonparametric regression models should be used."}, {"text": "Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population."}, {"text": "In statistics, the kth order statistic of a statistical sample is equal to its kth-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference."}, {"text": "Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures.  Descriptive statistics are typically distinguished from inferential statistics. With descriptive statistics you are simply describing what is or what the data shows."}, {"text": "Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables."}]}, {"question": "What is a sampling frame in qualitative research", "positive_ctxs": [{"text": "A sampling frame is a list or map that identifies most units within the target population.  When evaluating the effectiveness and efficiency of any sampling frame for qualitative research, it is important, as with quantitative research, to consider whether the frame is comprehensive."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In contrast, quota sampling in qualitative research is a specific technique for selecting a sample that has been defined using a purposive sampling strategy to define the categories of data sources that are eligible for a study."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "Uses. Quota sampling is useful when time is limited, a sampling frame is not available, the research budget is very tight or detailed accuracy is not important. Subsets are chosen and then either convenience or judgment sampling is used to choose people from each subset."}, {"text": "Nonprobability sampling is a common technique in qualitative research where researchers use their judgment to select a sample.  In convenience sampling, participants are selected because they are accessible and therefore relatively easy for the researcher to recruit."}, {"text": "A sampling frame is a list or other device used to define a researcher's population of interest. The sampling frame defines a set of elements from which a researcher can select a sample of the target population."}]}, {"question": "How do you find the degrees of freedom for a chi square distribution", "positive_ctxs": [{"text": "The degrees of freedom for the chi-square are calculated using the following formula: df = (r-1)(c-1) where r is the number of rows and c is the number of columns. If the observed chi-square test statistic is greater than the critical value, the null hypothesis can be rejected."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two sets of degrees of freedom; one for the numerator and one for the denominator. For example, if F follows an F distribution and the number of degrees of freedom for the numerator is four, and the number of degrees of freedom for the denominator is ten, then F ~ F 4,10."}, {"text": "Chi Square distributions are positively skewed, with the degree of skew decreasing with increasing degrees of freedom. As the degrees of freedom increases, the Chi Square distribution approaches a normal distribution."}, {"text": "Chi Square distributions are positively skewed, with the degree of skew decreasing with increasing degrees of freedom. As the degrees of freedom increases, the Chi Square distribution approaches a normal distribution. Figure 1 shows density functions for three Chi Square distributions."}, {"text": "The shape of the t distribution depends on the degrees of freedom (df) that went into the estimate of the standard deviation. With very few degrees of freedom, the t distribution is very leptokurtic. With 100 or more degrees of freedom, the t distribution is almost indistinguishable from the normal distribution."}, {"text": "The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time.  The P value is determined from the F ratio and the two values for degrees of freedom shown in the ANOVA table."}, {"text": "The degree of freedom is not a property of the distribution, it's the name of the distribution. It refers to the number of degrees of freedom of some variable that has the distribution."}, {"text": "The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution. But where the chi-squared distribution deals with the degree of freedom with one set of variables, the F-distribution deals with multiple levels of events having different degrees of freedom."}]}, {"question": "What is the difference between optimal control theory and reinforcement learning", "positive_ctxs": [{"text": "Optimal control focuses on a subset of problems, but solves these problems very well, and has a rich history. RL can be thought of as a way of generalizing or extending ideas from optimal control to non-traditional control problems."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Stochastic control or stochastic optimal control is a sub field of control theory that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Bellman equation is the basic block of solving reinforcement learning and is omnipresent in RL. It helps us to solve MDP. To solve means finding the optimal policy and value functions. The optimal value function V*(S) is one that yields maximum value."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "Definition. A study design that randomly assigns participants into an experimental group or a control group. As the study is conducted, the only expected difference between the control and experimental groups in a randomized controlled trial (RCT) is the outcome variable being studied."}, {"text": "Definition. A study design that randomly assigns participants into an experimental group or a control group. As the study is conducted, the only expected difference between the control and experimental groups in a randomized controlled trial (RCT) is the outcome variable being studied."}, {"text": "The main difference between Independant and Independent is that the Independant is a misspelling of independent and Independent is a Not dependent; free; not subject to control by others; not relying on others."}]}, {"question": "What is the pooling layer used in a convolution neural network", "positive_ctxs": [{"text": "A pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep NN is just a deep neural network, with a lot of layers. It can be CNN, or just a plain multilayer perceptron. CNN, or convolutional neural network, is a neural network using convolution layer and pooling layer."}, {"text": "A pooling or subsampling layer often immediately follows a convolution layer in CNN. Its role is to downsample the output of a convolution layer along both the spatial dimensions of height and width."}, {"text": "Convolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input. A pooling layer is a new layer added after the convolutional layer."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "A pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling."}, {"text": "A pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling."}, {"text": "A pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling."}]}, {"question": "How statistics can be skewed", "positive_ctxs": [{"text": "Data are skewed right when most of the data are on the left side of the graph and the long skinny tail extends to the right. Data are skewed left when most of the data are on the right side of the graph and the long skinny tail extends to the left."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "5 Ways to Avoid Being Fooled By Statistics.  Do A Little Bit of Math and apply Common Sense.  Always Look for the Source and check the authority of the source.  Question if the statistics are biased or statistically insignificant.  Question if the statistics are skewed purposely or Misinterpreted.More items\u2022"}, {"text": "If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical."}, {"text": "The log transformation can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics. Figure 1 shows an example of how a log transformation can make patterns more visible."}, {"text": "The log transformation can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics. Figure 1 shows an example of how a log transformation can make patterns more visible."}, {"text": "Negative values for the skewness indicate data that are skewed left and positive values for the skewness indicate data that are skewed right. By skewed left, we mean that the left tail is long relative to the right tail."}, {"text": "Skewed data often occur due to lower or upper bounds on the data. That is, data that have a lower bound are often skewed right while data that have an upper bound are often skewed left. Skewness can also result from start-up effects."}, {"text": "The chi-square distribution curve is skewed to the right, and its shape depends on the degrees of freedom df. For df > 90, the curve approximates the normal distribution. Test statistics based on the chi-square distribution are always greater than or equal to zero."}]}, {"question": "What happens when you have more unknowns than equations", "positive_ctxs": [{"text": "If there are more variables than equations, you cannot find a unique solution, because there isnt one."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "For example, if you have daily sales data and you expect that it exhibits annual seasonality, you should have more than 365 data points to train a successful model. If you have hourly data and you expect your data exhibits weekly seasonality, you should have more than 7*24 = 168 observations to train a model."}, {"text": "There are multiple uses of eigenvalues and eigenvectors: Eigenvalues and Eigenvectors have their importance in linear differential equations where you want to find a rate of change or when you want to maintain relationships between two variables."}, {"text": "In linear regression the independent variables can be categorical and/or continuous. But, when you fit the model if you have more than two category in the categorical independent variable make sure you are creating dummy variables."}, {"text": "In linear regression the independent variables can be categorical and/or continuous. But, when you fit the model if you have more than two category in the categorical independent variable make sure you are creating dummy variables."}, {"text": "2 Answers. If you have two classes (i.e. binary classification), you should use a binary crossentropy loss. If you have more than two you should use a categorical crossentropy loss."}, {"text": "Than sentence examplesHe thinks you are better than us.  He has lived more than eighty years.  Alex had been hiding more than a father.  Less than a week later she passed another milestone.  No one could have been more private than Josh.  \"That's all right,\" returned the man's voice, more pleasantly than before.More items"}, {"text": "For a dichotomous categorical variable and a continuous variable you can calculate a Pearson correlation if the categorical variable has a 0/1-coding for the categories.  But when you have more than two categories for the categorical variable the Pearson correlation is not appropriate anymore."}]}, {"question": "What is noise in signal detection theory", "positive_ctxs": [{"text": "Detection theory or signal detection theory is a means to measure the ability to differentiate between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Detection theory or signal detection theory is a means to measure the ability to differentiate between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the"}, {"text": "False Alarm Rate. A false alarm is \u201can erroneous radar target detection decision caused by noise or other interfering signals exceeding the detection threshold\u201d. In general, it is an indication of the presence of radar target when there is no valid aim."}, {"text": "The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image)."}, {"text": "The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image)."}, {"text": "The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image)."}, {"text": "The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986. Canny also produced a computational theory of edge detection explaining why the technique works."}, {"text": "Stochastic control or stochastic optimal control is a sub field of control theory that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system."}]}, {"question": "What is the difference between gradient descent and gradient boosting", "positive_ctxs": [{"text": "Gradient boosting is a technique for building an ensemble of weak models such that the predictions of the ensemble minimize a loss function.  Gradient descent \"descends\" the gradient by introducing changes to parameters, whereas gradient boosting descends the gradient by introducing new models."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning."}, {"text": "According to a senior data scientist, one of the distinct advantages of using Stochastic Gradient Descent is that it does the calculations faster than gradient descent and batch gradient descent. However, gradient descent is the best approach if one wants a speedier result."}, {"text": "In the context of gradient boosting, the training loss is the function that is optimized using gradient descent, e.g., the \u201cgradient\u201d part of gradient boosting models. Specifically, the gradient of the training loss is used to change the target variables for each successive tree."}, {"text": "Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process."}, {"text": "Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process."}, {"text": "AdaBoost is the shortcut for adaptive boosting. So what's the differences between Adaptive boosting and Gradient boosting? Both are boosting algorithms which means that they convert a set of weak learners into a single strong learner.  On the other hand, gradient boosting doesn't modify the sample distribution."}]}, {"question": "How does K nearest neighbor work", "positive_ctxs": [{"text": "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Abstract: The dimensionality curse phenomenon states that in high dimensional spaces distances between nearest and farthest points from query points become almost equal. Therefore, nearest neighbor calculations cannot discriminate candidate points."}, {"text": "Here is step by step on how to compute K-nearest neighbors KNN algorithm:Determine parameter K = number of nearest neighbors.Calculate the distance between the query-instance and all the training samples.Sort the distance and determine nearest neighbors based on the K-th minimum distance.More items"}, {"text": "K-nearest neighbors K- nearest neighbor (kNN) is a simple supervised machine learning algorithm that can be used to solve both classification and regression problems. kNN stores available inputs and classifies new inputs based on a similar measure i.e. the distance function."}, {"text": "A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical."}, {"text": "Fig. 1Determine the number of nearest neighbours (K values).Compute the distance between test sample and all the training samples.Sort the distance and determine nearest neighbours based on the K-th minimum distance.Assemble the categories of the nearest neighbours.More items\u2022"}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "How to Use K-means Cluster Algorithms in Predictive AnalysisPick k random items from the dataset and label them as cluster representatives.Associate each remaining item in the dataset with the nearest cluster representative, using a Euclidean distance calculated by a similarity function.Recalculate the new clusters' representatives.More items"}]}, {"question": "What is an example of using statistics to mislead", "positive_ctxs": [{"text": "Perhaps the most famous case ever of misleading statistics in the news is the case of Sally Clark, who was convicted of murdering her children. She was freed after it was found the statistics used in her murder trial were completely wrong."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An example of statistics is a report of numbers saying how many followers of each religion there are in a particular country. An example of statistics is a math class offered in high schools and colleges. The definition of a statistic is a number, or a person who is an unnamed piece of data to be studied."}, {"text": "Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates."}, {"text": "Univariate is a term commonly used in statistics to describe a type of data which consists of observations on only a single characteristic or attribute. A simple example of univariate data would be the salaries of workers in industry."}, {"text": "Univariate is a term commonly used in statistics to describe a type of data which consists of observations on only a single characteristic or attribute. A simple example of univariate data would be the salaries of workers in industry."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}]}, {"question": "What is segmentation in CNN", "positive_ctxs": [{"text": "Convolutional Neural Networks (CNNs) Image segmentation with CNN involves feeding segments of an image as input to a convolutional neural network, which labels the pixels. The CNN cannot process the whole image at once."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Dense CNN is a type of Deep CNN in which each layer is connected with another layer deeper than itself."}, {"text": "In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects).  Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig. 2."}, {"text": "In 1D CNN, kernel moves in 1 direction. Input and output data of 1D CNN is 2 dimensional. Mostly used on Time-Series data. In 2D CNN, kernel moves in 2 directions. Input and output data of 2D CNN is 3 dimensional."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is the formula for the one sample z test statistic", "positive_ctxs": [{"text": "Define hypotheses.  The test statistic is a z-score (z) defined by the following equation. z = (x - M ) / [ \u03c3 /sqrt(n) ] where x is the observed sample mean, M is the hypothesized population mean (from the null hypothesis), and \u03c3 is the standard deviation of the population."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28. For a two tailed test, use \u03b1/2 = 0.05 and the critical region is below z = -1.645 and above z = 1.645."}, {"text": "The formula for calculating a z-score is is z = (x-\u03bc)/\u03c3, where x is the raw score, \u03bc is the population mean, and \u03c3 is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation."}, {"text": "\u27a2 To determine the critical region for a normal distribution, we use the table for the standard normal distribution. If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28."}, {"text": "The formula for calculating a z-score is is z = (x-\u03bc)/\u03c3, where x is the raw score, \u03bc is the population mean, and \u03c3 is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation. Figure 2."}, {"text": "You can use test statistics to determine whether to reject the null hypothesis. The test statistic compares your data with what is expected under the null hypothesis. The test statistic is used to calculate the p-value. A test statistic measures the degree of agreement between a sample of data and the null hypothesis."}, {"text": "The test statistic is used to calculate the p-value. A test statistic measures the degree of agreement between a sample of data and the null hypothesis. Its observed value changes randomly from one random sample to a different sample.  This causes the test's p-value to become small enough to reject the null hypothesis."}]}, {"question": "What is a random experiment in probability", "positive_ctxs": [{"text": "Definition : A random experiment is an experiment or a process for which the outcome cannot be predicted with certainty. Definition : The sample space (denoted S) of a random experiment is the set of all possible outcomes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The sample space of a random experiment is the collection of all possible outcomes. An event associated with a random experiment is a subset of the sample space. The probability of any outcome is a number between 0 and 1. The probabilities of all the outcomes add up to 1."}, {"text": "The sample space of a random experiment is the collection of all possible outcomes. An event associated with a random experiment is a subset of the sample space. The probability of any outcome is a number between 0 and 1. The probabilities of all the outcomes add up to 1."}, {"text": "In particular, a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space."}, {"text": "In particular, a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space."}, {"text": "Definition : A random experiment is an experiment or a process for which the outcome cannot be predicted with certainty. Definition : The sample space (denoted S) of a random experiment is the set of all possible outcomes."}, {"text": "Definition : A random experiment is an experiment or a process for which the outcome cannot be predicted with certainty. Definition : The sample space (denoted S) of a random experiment is the set of all possible outcomes."}, {"text": "In probability theory, an event is an outcome or defined collection of outcomes of a random experiment. Since the collection of all possible outcomes to a random experiment is called the sample space, another definiton of event is any subset of a sample space."}]}, {"question": "What is the difference between GloVe and Word2Vec", "positive_ctxs": [{"text": "Word2Vec takes texts as training data for a neural network. The resulting embedding captures whether words appear in similar contexts. GloVe focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Word2Vec takes texts as training data for a neural network. The resulting embedding captures whether words appear in similar contexts. GloVe focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together."}, {"text": "Word2Vec takes texts as training data for a neural network. The resulting embedding captures whether words appear in similar contexts. GloVe focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together."}, {"text": "Word2Vec takes texts as training data for a neural network. The resulting embedding captures whether words appear in similar contexts. GloVe focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "The Range is the difference between the lowest and highest values. Example: In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9. So the range is 9 \u2212 3 = 6."}]}, {"question": "How do you interpret Cox regression coefficients", "positive_ctxs": [{"text": "The coefficients in a Cox regression relate to hazard; a positive coefficient indicates a worse prognosis and a negative coefficient indicates a protective effect of the variable with which it is associated."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two types of coefficients that are typically be displayed in a multiple regression table: unstandardized coefficients, and standardized coefficients. To interpret an unstandardized regression coefficient: for every metric unit change in the independent variable, the dependent variable changes by X units."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients.  On the other hand, L2 regularization (e.g. Ridge regression) doesn't result in elimination of coefficients or sparse models. This makes the Lasso far easier to interpret than the Ridge."}, {"text": "OLS cannot be used because the regression function is not a linear function of the regression coefficients (the coefficients appear inside the nonlinear functions \u03a6 or \u039b)."}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "If there are other predictor variables, all coefficients will be changed.  All the coefficients are jointly estimated, so every new variable changes all the other coefficients already in the model. This is one reason we do multiple regression, to estimate coefficient B1 net of the effect of variable Xm."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}]}, {"question": "Does boosting use bootstrap", "positive_ctxs": [{"text": "Boosting. Another general machine learning ensemble method is known as boosting. Boosting differs somewhat from bagging as it does not involve bootstrap sampling."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Does not affect R2 or adjusted R2 (since these estimate the POPULATION variances which are not conditional on X)"}, {"text": "The idea behind bootstrap is to use the data of a sample study at hand as a \u201csurrogate population\u201d, for the purpose of approximating the sampling distribution of a statistic; i.e. to resample (with replacement) from the sample data at hand and create a large number of \u201cphantom samples\u201d known as bootstrap samples."}, {"text": "The number of bootstrap samples can be indicated with B (e.g. if you resample 10 times then B = 10). A star next to a statistic, like s* or x\u0304* indicates the statistic was calculated by resampling. A bootstrap statistic is sometimes denoted with a T, where T*b would be the Bth bootstrap sample statistic T."}, {"text": "AdaBoost is the shortcut for adaptive boosting. So what's the differences between Adaptive boosting and Gradient boosting? Both are boosting algorithms which means that they convert a set of weak learners into a single strong learner.  On the other hand, gradient boosting doesn't modify the sample distribution."}, {"text": "The bootstrap is a tool, which allows us to obtain better finite sample approximation of estimators. The bootstrap is used all over the place to estimate the variance, correct bias and construct CIs etc. There are many, many different types of bootstraps."}, {"text": "Overview of stacking. Stacking mainly differ from bagging and boosting on two points.  Second, stacking learns to combine the base models using a meta-model whereas bagging and boosting combine weak learners following deterministic algorithms."}, {"text": "Comparison of bootstrap and jackknife Although there are huge theoretical differences in their mathematical insights, the main practical difference for statistics users is that the bootstrap gives different results when repeated on the same data, whereas the jackknife gives exactly the same result each time."}]}, {"question": "How do you address omitted variable bias", "positive_ctxs": [{"text": "For omitted variable bias to occur, the following two conditions must exist:The omitted variable must correlate with the dependent variable.The omitted variable must correlate with at least one independent variable that is in the regression model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "Intuitively, omitted variable bias occurs when the independent variable (the X) that we have included in our model picks up the effect of some other variable that we have omitted from the model. The reason for the bias is that we are attributing effects to X that should be attributed to the omitted variable."}, {"text": "If the correlation between education and unobserved ability is positive, omitted variables bias will occur in an upward direction. Conversely, if the correlation between an explanatory variable and an unobserved relevant variable is negative, omitted variables bias will occur in a downward direction."}, {"text": "Fixed effects models remove omitted variable bias by measuring changes within groups across time, usually by including dummy variables for the missing or unknown characteristics."}, {"text": "Omitted variable bias occurs when a regression model leaves out relevant independent variables, which are known as confounding variables. This condition forces the model to attribute the effects of omitted variables to variables that are in the model, which biases the coefficient estimates."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}]}, {"question": "Which of the following is part of the sequence learning problem", "positive_ctxs": [{"text": "There are four basic sequence learning problems: sequence prediction, sequence generation, sequence recognition, and sequential decision making. These \u201cproblems\u201d show how sequences are formulated."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "Deep learning itself does feature engineering whereas machine learning requires manual feature engineering. 2) Which of the following is a representation learning algorithm? Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning."}, {"text": "If the biggest problem with supervised learning is the expense of labeling the training data, the biggest problem with unsupervised learning (where the data is not labeled) is that it often doesn't work very well."}, {"text": "Hebbian learning is one of the oldest learning algorithms, and is based in large part on the dynamics of biological systems. A synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs."}, {"text": "Attention is proposed as a method to both align and translate. Alignment is the problem in machine translation that identifies which parts of the input sequence are relevant to each word in the output, whereas translation is the process of using the relevant information to select the appropriate output."}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}]}, {"question": "What is beta in signal detection theory", "positive_ctxs": [{"text": "Signal Detection Theory assumes that, given this situation, we make our judgment of whether the signal is present, or not, by setting up a Criterion value, \u03b2 (beta).  When a value is picked up that exceeds \u03b2, we respond that the signal is present."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Detection theory or signal detection theory is a means to measure the ability to differentiate between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the"}, {"text": "The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986. Canny also produced a computational theory of edge detection explaining why the technique works."}, {"text": "Ridge regression has an additional factor called \u03bb (lambda) which is called the penalty factor which is added while estimating beta coefficients. This penalty factor penalizes high value of beta which in turn shrinks beta coefficients thereby reducing the mean squared error and predicted error."}, {"text": "Signal detection assumes that there is \u201cnoise\u201d in any system. In this example, if we have an old car, we may hear clunks even when the car is operating effectively, or even tinnitus in our ear, or something rustling in the trunk. The signal is what you are trying to detect."}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In probability theory and statistics, the beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parameterized by two positive shape parameters, denoted by \u03b1 and \u03b2, that appear as exponents of the random variable and control the shape of the distribution."}]}, {"question": "Why do we use ROC curves", "positive_ctxs": [{"text": "ROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests.  In addition, the area under the ROC curve gives an idea about the benefit of using the test(s) in question."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "As the area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test, the areas under ROC curves are used to compare the usefulness of tests. The term ROC stands for Receiver Operating Characteristic."}, {"text": "The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 \u2013 FPR). Classifiers that give curves closer to the top-left corner indicate a better performance.  The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test."}, {"text": "ROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests.  In addition, the area under the ROC curve gives an idea about the benefit of using the test(s) in question."}, {"text": "Interpreting the ROC curve Classifiers that give curves closer to the top-left corner indicate a better performance. As a baseline, a random classifier is expected to give points lying along the diagonal (FPR = TPR). The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test."}, {"text": "The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 \u2013 FPR). Classifiers that give curves closer to the top-left corner indicate a better performance. As a baseline, a random classifier is expected to give points lying along the diagonal (FPR = TPR)."}, {"text": "The receiver operating characteristic (ROC) curve is a two dimensional graph in which the false positive rate is plotted on the X axis and the true positive rate is plotted on the Y axis. The ROC curves are useful to visualize and compare the performance of classifier methods (see Figure 1)."}]}, {"question": "What is the difference between endogenous and exogenous variables", "positive_ctxs": [{"text": "In an economic model, an exogenous variable is one whose value is determined outside the model and is imposed on the model, and an exogenous change is a change in an exogenous variable.  In contrast, an endogenous variable is a variable whose value is determined by the model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In an economic model, an exogenous variable is one whose value is determined outside the model and is imposed on the model, and an exogenous change is a change in an exogenous variable. In contrast, an endogenous variable is a variable whose value is determined by the model."}, {"text": "An endogenous variable is a variable in a statistical model that's changed or determined by its relationship with other variables within the model.  Endogenous variables are the opposite of exogenous variables, which are independent variables or outside forces."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The definition of an endogenous variable, exogenous variable and parameter are as follows: An Endogenous Variable- is a variable whose value is determined within the model itself.  An Exogenous Variable \u2013 is a variable whose value is assumed to be determined outside the model."}, {"text": "A variable xj is said to be endogenous within the causal model M if its value is determined or influenced by one or more of the independent variables X (excluding itself). A purely endogenous variable is a factor that is entirely determined by the states of other variables in the system."}, {"text": "The concept of exclusion restrictions denotes that some of the exogenous variables are not in some of the equations. Often this idea is expressed by saying the coefficient next to that exogenous variable is zero."}, {"text": "An exogenous variable is a variable that is not affected by other variables in the system. For example, take a simple causal system like farming. Variables like weather, farmer skill, pests, and availability of seed are all exogenous to crop production."}]}, {"question": "Why do we use eigenvalues and eigenvectors", "positive_ctxs": [{"text": "Eigenvalues and eigenvectors allow us to \"reduce\" a linear operation to separate, simpler, problems. For example, if a stress is applied to a \"plastic\" solid, the deformation can be dissected into \"principle directions\"- those directions in which the deformation is greatest."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA. The eigenvectors of ATA make up the columns of V , the eigenvectors of AAT make up the columns of U. Also, the singular values in S are square roots of eigenvalues from AAT or ATA."}, {"text": "Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA. The eigenvectors of ATA make up the columns of V , the eigenvectors of AAT make up the columns of U. Also, the singular values in S are square roots of eigenvalues from AAT or ATA."}, {"text": "The function scipy. linalg. eig computes eigenvalues and eigenvectors of a square matrix ."}, {"text": "0:005:03Suggested clip \u00b7 117 secondsPCA 5: finding eigenvalues and eigenvectors - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "0:001:38Suggested clip \u00b7 98 secondsFind the matrix A given the eigenvalues and eigenvectors - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}, {"text": "If two matrices are similar, they have the same eigenvalues and the same number of independent eigenvectors (but probably not the same eigenvectors)."}]}, {"question": "Is coding involved in machine learning", "positive_ctxs": [{"text": "A little bit of coding skills is enough, but it's better to have knowledge of data structures, algorithms, and OOPs concept. Some of the popular programming languages to learn machine learning in are Python, R, Java, and C++."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A: Bootstrap aggregation, or \"bagging,\" in machine learning decreases variance through building more advanced models of complex data sets. Specifically, the bagging approach creates subsets which are often overlapping to model the data in a more involved way."}, {"text": "A little bit of coding skills is enough, but it's better to have knowledge of data structures, algorithms, and OOPs concept. Some of the popular programming languages to learn machine learning in are Python, R, Java, and C++."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2."}, {"text": "From implementation point of view, Huffman coding is easier than arithmetic coding. Arithmetic algorithm yields much more compression ratio than Huffman algorithm while Huffman coding needs less execution time than the arithmetic coding."}, {"text": "Different types of deep learning models.Autoencoders. An autoencoder is an artificial neural network that is capable of learning various coding patterns.  Deep Belief Net.  Convolutional Neural Networks.  Recurrent Neural Networks.  Reinforcement Learning to Neural Networks."}, {"text": "The constraints for the maximization problems all involved inequalities, and the constraints for the minimization problems all involved inequalities. Linear programming problems for which the constraints involve both types of inequali- ties are called mixed-constraint problems."}]}, {"question": "What is Fourier transform of an image", "positive_ctxs": [{"text": "Brief Description. The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In mathematics, a Fourier series (/\u02c8f\u028arie\u026a, -i\u0259r/) is a periodic function composed of harmonically related sinusoids, combined by a weighted summation.  The discrete-time Fourier transform is an example of Fourier series. The process of deriving the weights that describe a given function is a form of Fourier analysis."}, {"text": "Fourier Methods in Signal Processing The Fourier transform and discrete-time Fourier transform are mathematical analysis tools and cannot be evaluated exactly in a computer. The Fourier transform is used to analyze problems involving continuous-time signals or mixtures of continuous- and discrete-time signals."}, {"text": "The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent."}, {"text": "The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent."}, {"text": "A fast Fourier transform (FFT) is an algorithm that computes the discrete Fourier transform (DFT) of a sequence, or its inverse (IDFT). Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa."}, {"text": "In the Fourier domain image, each point represents a particular frequency contained in the spatial domain image. The Fourier Transform is used in a wide range of applications, such as image analysis, image filtering, image reconstruction and image compression."}, {"text": "In mathematics, a Fourier transform (FT) is a mathematical transform that decomposes a function (often a function of time, or a signal) into its constituent frequencies, such as the expression of a musical chord in terms of the volumes and frequencies of its constituent notes."}]}, {"question": "What is a quantile in statistics", "positive_ctxs": [{"text": "Definition Quantile. A quantile defines a particular part of a data set, i.e. a quantile determines how many values in a distribution are above or below a certain limit. Special quantiles are the quartile (quarter), the quintile (fifth) and percentiles (hundredth)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times. Given a prediction yi^p and outcome yi, the mean regression loss for a quantile q is. For a set of predictions, the loss will be its average."}, {"text": "In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one fewer quantile than the number of groups created."}, {"text": "In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one fewer quantile than the number of groups created."}, {"text": "The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value. So the median is the value of the quantile at the probability value of 0.5. A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75."}, {"text": "The cumulative density function gives you the probability of a random variable being on or below a certain value. The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value.  A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "A quantile defines a particular part of a data set, i.e. a quantile determines how many values in a distribution are above or below a certain limit. Special quantiles are the quartile (quarter), the quintile (fifth) and percentiles (hundredth)."}]}, {"question": "What is the null hypothesis for a Wilcoxon test", "positive_ctxs": [{"text": "Whereas the null hypothesis of the two-sample t test is equal means, the null hypothesis of the Wilcoxon test is usually taken as equal medians. Another way to think of the null is that the two populations have the same distribution with the same median."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Statistical power, or the power of a hypothesis test is the probability that the test correctly rejects the null hypothesis. That is, the probability of a true positive result.  statistical power is the probability that a test will correctly reject a false null hypothesis."}, {"text": "In statistical hypothesis testing, the null distribution is the probability distribution of the test statistic when the null hypothesis is true. For example, in an F-test, the null distribution is an F-distribution. Null distribution is a tool scientists often use when conducting experiments."}, {"text": "In statistical hypothesis testing, the null distribution is the probability distribution of the test statistic when the null hypothesis is true. For example, in an F-test, the null distribution is an F-distribution. Null distribution is a tool scientists often use when conducting experiments."}, {"text": "The Wilcoxon signed rank test is a nonparametric test that compares the median of a set of numbers against a hypothetical median. The Wilcoxon rank sum test is a nonparametric test to compare two unmatched groups. It is equivalent to the Mann-Whitney test."}, {"text": "Basically, the test compares the fit of two models. The null hypothesis is that the smaller model is the \u201cbest\u201d model; It is rejected when the test statistic is large. In other words, if the null hypothesis is rejected, then the larger model is a significant improvement over the smaller one."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Rejecting the null hypothesis when it is in fact true is called a Type I error.  When a hypothesis test results in a p-value that is less than the significance level, the result of the hypothesis test is called statistically significant. Common mistake: Confusing statistical significance and practical significance."}]}, {"question": "How do you calculate probability in naive Bayes", "positive_ctxs": [{"text": "The conditional probability can be calculated using the joint probability, although it would be intractable. Bayes Theorem provides a principled way for calculating the conditional probability. The simple form of the calculation for Bayes Theorem is as follows: P(A|B) = P(B|A) * P(A) / P(B)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Naive Bayes Classifier example by hand and how to do in Scikit-Learn, You can use any kind of predictor in a naive Bayes classifier, as long as you can specify a discriminative linear models take a mixture of categorical and continuous predictors."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Platt scaling works well for SVMs(Support Vector Machine) as well as other types of classification models, including boosted models and even naive Bayes classifiers, which produce distorted probability distributions."}, {"text": "TL; DR: The naive Bayes classifier is an approximation to the Bayes classifier, in which we assume that the features are conditionally independent given the class instead of modeling their full conditional distribution given the class. A Bayes classifier is best interpreted as a decision rule."}, {"text": "There are three main methods for handling continuous variables in naive Bayes classifiers, namely, the normal method (parametric approach), the kernel method (non parametric approach) and discretization."}, {"text": "Popular ML algorithms include: linear regression, logistic regression, SVMs, nearest neighbor, decision trees, PCA, naive Bayes classifier, and k-means clustering. Classical machine learning algorithms are used for a wide range of applications."}, {"text": "Bayes theorem provides a way to calculate the probability of a hypothesis based on its prior probability, the probabilities of observing various data given the hypothesis, and the observed data itself. \u2014 Page 156, Machine Learning, 1997."}]}, {"question": "What does a distribution tell us about a set of data", "positive_ctxs": [{"text": "A data distribution is a function or a listing which shows all the possible values (or intervals) of the data. It also (and this is important) tells you how often each value occurs."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The sampling distribution of the sample mean is very useful because it can tell us the probability of getting any specific mean from a random sample."}, {"text": "The range can only tell you basic details about the spread of a set of data. By giving the difference between the lowest and highest scores of a set of data it gives a rough idea of how widely spread out the most extreme observations are, but gives no information as to where any of the other data points lie."}, {"text": "Characteristics of a Relationship. Correlations have three important characterstics. They can tell us about the direction of the relationship, the form (shape) of the relationship, and the degree (strength) of the relationship between two variables."}, {"text": "How to train a Machine Learning model in 5 minutesModel Naming \u2014 Give Your Model a Name: Let's start with giving your model a name, describe your model and attach tags to your model.  Data Type Selection \u2014 Choose data type(Images/Text/CSV): It's time to tell us about the type of data you want to train your model.More items"}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "A non parametric test (sometimes called a distribution free test) does not assume anything about the underlying distribution (for example, that the data comes from a normal distribution).  It usually means that you know the population data does not have a normal distribution."}, {"text": "The binomial distribution model allows us to compute the probability of observing a specified number of \"successes\" when the process is repeated a specific number of times (e.g., in a set of patients) and the outcome for a given patient is either a success or a failure."}]}, {"question": "What is intelligence in artificial intelligence terms", "positive_ctxs": [{"text": "When most people hear the term artificial intelligence, the first thing they usually think of is robots.  Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}, {"text": "Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subtopic of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem."}]}, {"question": "What does a one sample z test tell you", "positive_ctxs": [{"text": "The one-sample Z test is used only for tests of the sample mean. Thus, our hypothesis tests whether the average of our sample (M) suggests that our students come from a population with a know mean (m) or whether it comes from a different population."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "z = (x \u2013 \u03bc) / \u03c3 For example, let's say you have a test score of 190. The test has a mean (\u03bc) of 150 and a standard deviation (\u03c3) of 25. Assuming a normal distribution, your z score would be: z = (x \u2013 \u03bc) / \u03c3"}, {"text": "z = (x \u2013 \u03bc) / \u03c3 For example, let's say you have a test score of 190. The test has a mean (\u03bc) of 150 and a standard deviation (\u03c3) of 25. Assuming a normal distribution, your z score would be: z = (x \u2013 \u03bc) / \u03c3"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28. For a two tailed test, use \u03b1/2 = 0.05 and the critical region is below z = -1.645 and above z = 1.645."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "\u27a2 To determine the critical region for a normal distribution, we use the table for the standard normal distribution. If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28."}]}, {"question": "How do you validate a machine learning model", "positive_ctxs": [{"text": "The following methods for validation will be demonstrated:Train/test split.k-Fold Cross-Validation.Leave-one-out Cross-Validation.Leave-one-group-out Cross-Validation.Nested Cross-Validation.Time-series Cross-Validation.Wilcoxon signed-rank test.McNemar's test.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data."}, {"text": "How to Get Started with AIPick a topic you are interested in.Find a quick solution.Improve your simple solution.Share your solution.Repeat steps 1-4 for different problems.Complete a Kaggle competition.Use machine learning professionally."}, {"text": "Machine learning algorithms can minimize forecasting error and do the forecast much faster and with the usage of more data. What's more, machine learning algorithms can analyze many alternative models at the same time, when in traditional econometrics you can analyze just one model at a time."}, {"text": "Model fitting is a measure of how well a machine learning model generalizes to similar data to that on which it was trained.  During the fitting process, you run an algorithm on data for which you know the target variable, known as \u201clabeled\u201d data, and produce a machine learning model."}, {"text": "Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost."}, {"text": "In a dataset, a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}, {"text": "In a dataset a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}]}, {"question": "What is Hebbian learning in neural networks", "positive_ctxs": [{"text": "Hebb proposed a mechanism to update weights between neurons in a neural network. This method of weight updation enabled neurons to learn and was named as Hebbian Learning.  Information is stored in the connections between neurons in neural networks, in the form of weights."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. In fact, it is the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three."}, {"text": "Hebbian learning is one of the oldest learning algorithms, and is based in large part on the dynamics of biological systems. A synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs."}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}, {"text": "Neural networks are sets of algorithms intended to recognize patterns and interpret data through clustering or labeling. In other words, neural networks are algorithms. A training algorithm is the method you use to execute the neural network's learning process."}, {"text": "Neural network ensemble is a learning paradigm where many neural networks are jointly used to solve a problem.  Then it assigns random weights to those networks and employs genetic algorithm to evolve the weights so that they can characterize to some extent the fitness of the neural networks in constituting an ensemble."}, {"text": "Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}]}, {"question": "Why does the normal distribution show up so often in nature", "positive_ctxs": [{"text": "Since most natural phenomena are complex and have many factors, the same logic as above applies and distribution of measures of such phenomena tend to have most values near the mean (normal distibution has a desirable property of mean and mode being the same - i.e. the mean is the same as the most frequent value)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The distribution becomes normal when you have several different forces of varying magnitude acting together. Generally, the more forces then the more normal the distribution will become. This occurs a lot in nature which is why the normal distribution is so prevalent."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side.  The normal distribution is often called the bell curve because the graph of its probability density looks like a bell."}, {"text": "The multivariate normal distribution has two or more random variables \u2014 so the bivariate normal distribution is actually a special case of the multivariate normal distribution."}, {"text": "A non parametric test (sometimes called a distribution free test) does not assume anything about the underlying distribution (for example, that the data comes from a normal distribution).  It usually means that you know the population data does not have a normal distribution."}, {"text": "Why the Lognormal Distribution is used to Model Stock Prices Since the lognormal distribution is bound by zero on the lower side, it is therefore perfect for modeling asset prices which cannot take negative values. The normal distribution cannot be used for the same purpose because it has a negative side."}, {"text": "Data structure and algorithms help in understanding the nature of the problem at a deeper level and thereby a better understanding of the world. If you want to know more about Why Data Structures and Algorithms then you must watch this video of Mr."}, {"text": "For a perfectly normal distribution the mean, median and mode will be the same value, visually represented by the peak of the curve. The normal distribution is often called the bell curve because the graph of its probability density looks like a bell."}]}, {"question": "What are the similarities and differences between reinforcement learning and supervised learning", "positive_ctxs": [{"text": "In reinforcement learning, the output depends on the state of current input and the output of the next state depends on the out of the previous output. Whereas in supervised learning, the decision made is based only on the current input. It uses labeled data sets to make decisions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "Deep reinforcement learning is a promising combination between two artificial intelligence techniques: reinforcement learning, which uses sequential trial and error to learn the best action to take in every situation, and deep learning, which can evaluate complex inputs and select the best response."}, {"text": "What are the five steps in the backpropagation learning algorithm?Initialize weights with random values and set other parameters.Read in the input vector and the desired output.Compute the actual output via the calculations, working forward through the layers."}, {"text": "Two types of reinforcement learning are 1) Positive 2) Negative. Two widely used learning model are 1) Markov Decision Process 2) Q learning. Reinforcement Learning method works on interacting with the environment, whereas the supervised learning method works on given sample data or example."}, {"text": "These are three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning."}, {"text": "These are three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning."}, {"text": "Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks.  In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly."}]}, {"question": "What algorithms use gradient descent", "positive_ctxs": [{"text": "Common examples of algorithms with coefficients that can be optimized using gradient descent are Linear Regression and Logistic Regression."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "Common examples of algorithms with coefficients that can be optimized using gradient descent are Linear Regression and Logistic Regression."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is a simple optimization procedure that you can use with many machine learning algorithms.  Stochastic gradient descent refers to calculating the derivative from each training data instance and calculating the update immediately."}]}, {"question": "Which model is best suited for recursive data", "positive_ctxs": [{"text": "Recursive neural network models"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Recurrent Neural Networks are best suited for Text Processing."}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Decision tree learning is generally best suited to problems with the following characteristics: Instances are represented by attribute-value pairs. There is a finite list of attributes (e.g. hair colour) and each instance stores a value for that attribute (e.g. blonde)."}, {"text": "The Spearman correlation is the same as the Pearson correlation, but it is used on data from an ordinal scale. Which situation would be appropriate for obtaining a phi-coefficient with a Pearson test?"}, {"text": "Any LTI filter with output and input can be represented by a difference equation in the form: If at least one of the is not null, the filter is recursive. If the are all zero, it is a non recursive filter usually called FIR (Finite Input Response) filter.  This happens both to recursive and non recursive filters."}]}, {"question": "How many variables should be in a logistic regression model", "positive_ctxs": [{"text": "Dear researchers, in real world, a \"reasonable\" sample size for a logistic regression model is: at least 10 events (not 10 samples) per independent variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Model specification refers to the determination of which independent variables should be included in or excluded from a regression equation.  A multiple regression model is, in fact, a theoretical statement about the causal relationship between one or more independent variables and a dependent variable."}, {"text": "Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results."}, {"text": "Logistic regression is a pretty flexible method. It can readily use as independent variables categorical variables. Most software that use Logistic regression should let you use categorical variables.  A single column in your model can handle as many categories as needed for a single categorical variable."}, {"text": "How Stepwise Regression WorksStart the test with all available predictor variables (the \u201cBackward: method), deleting one variable at a time as the regression model progresses.  Start the test with no predictor variables (the \u201cForward\u201d method), adding one at a time as the regression model progresses."}, {"text": "Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used."}, {"text": "Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used."}, {"text": "A generative model on the other hand will be able to produce a new picture of a either class. Typical discriminative models include logistic regression (LR), support vector machines (SVM), conditional random fields (CRFs) (specified over an undirected graph), decision trees, neural networks, and many others."}]}, {"question": "What is the purpose of cluster analysis", "positive_ctxs": [{"text": "The purpose of cluster analysis is to place objects into groups, or clusters, suggested by the data, not defined a priori, such that objects in a given cluster tend to be similar to each other in some sense, and objects in different clusters tend to be dissimilar."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.  Like univariate analysis, bivariate analysis can be descriptive or inferential."}, {"text": "Latent classes divide the cases into their respective dimensions in relation to the variable. For example, cluster analysis groups similar cases and puts them into one group. The numbers of clusters in the cluster analysis are called the latent classes. In SEM, the number of constructs is called the latent classed."}, {"text": "Univariate analysis has the purpose to describe a single variable distribution in one sample. It is the first important step of every clinical trial."}, {"text": "The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models."}, {"text": "Robust regression is an alternative to least squares regression when data is contaminated with outliers or influential observations and it can also be used for the purpose of detecting influential observations. Please note: The purpose of this page is to show how to use various data analysis commands."}]}, {"question": "How many nodes are in the output layer", "positive_ctxs": [{"text": "3 nodes"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Hidden layer of the neural network is the intermediate layer between Input and Output layer. Activation function applies on hidden layer if it is available.  Hidden nodes or hidden neurons are the neurons that are neither in the input layer nor the output layer [3]."}, {"text": "The output layer is responsible for producing the final result. There must always be one output layer in a neural network. The output layer takes in the inputs which are passed in from the layers before it, performs the calculations via its neurons and then the output is computed."}, {"text": "Softmax Thus sigmoid is widely used for binary classification problems. While building a network for a multiclass problem, the output layer would have as many neurons as the number of classes in the target. For instance if you have three classes, there would be three neurons in the output layer."}, {"text": "In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data.  The input at the bottom layer is raw data, and the output of the final layer is the final low-dimensional feature or representation."}, {"text": "Logits are the raw scores output by the last layer of a neural network. Before activation takes place."}, {"text": "A local minimum is a suboptimal equilibrium point at which system error is non-zero and the hidden output matrix is singular [12]. The complex problem which has a large number of patterns needs as many hidden nodes as patterns in order not to cause a singular hidden output matrix."}, {"text": "How to choose the size of the convolution filter or Kernel size1x1 kernel size is only used for dimensionality reduction that aims to reduce the number of channels. It captures the interaction of input channels in just one pixel of feature map.  2x2 and 4x4 are generally not preferred because odd-sized filters symmetrically divide the previous layer pixels around the output pixel."}]}, {"question": "What does it mean to reject the null hypothesis at the .05 level", "positive_ctxs": [{"text": "Significance levels The convention in most biological research is to use a significance level of 0.05. This means that if the P value is less than 0.05, you reject the null hypothesis; if P is greater than or equal to 0.05, you don't reject the null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Compare the P-value to the \u03b1 significance level stated earlier. If it is less than \u03b1, reject the null hypothesis. If the result is greater than \u03b1, fail to reject the null hypothesis. If you reject the null hypothesis, this implies that your alternative hypothesis is correct, and that the data is significant."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Rejecting or failing to reject the null hypothesis If our statistical analysis shows that the significance level is below the cut-off value we have set (e.g., either 0.05 or 0.01), we reject the null hypothesis and accept the alternative hypothesis."}, {"text": "We reject the null hypothesis when the p-value is less than \u03b1. But 0.07 > 0.05 so we fail to reject H0.  For example if the p-value = 0.08, then we would fail to reject H0 at the significance level of \u03b1=0.05 since 0.08 > 0.05, but we would reject H0 at the significance level of \u03b1 = 0.10 since 0.08 < 0.10."}, {"text": "Remember that the decision to reject the null hypothesis (H 0) or fail to reject it can be based on the p-value and your chosen significance level (also called \u03b1). If the p-value is less than or equal to \u03b1, you reject H 0; if it is greater than \u03b1, you fail to reject H 0."}, {"text": "The probability of Type 1 error is alpha -- the criterion that we set as the level at which we will reject the null hypothesis. The p value is something else -- it tells you how UNUSUAL the data are, given the assumption that the null hypothesis is true."}, {"text": "The disadvantage of the ANOVA F-test is that if we reject the null hypothesis, we do not know which treatments can be said to be significantly different from the others, nor, if the F-test is performed at level \u03b1, can we state that the treatment pair with the greatest mean difference is significantly different at level"}]}, {"question": "How do anchor boxes in object detection really work", "positive_ctxs": [{"text": "An object detector that uses anchor boxes can process an entire image at once, making real-time object detection systems possible. Because a convolutional neural network (CNN) can process an input image in a convolutional manner, a spatial location in the input can be related to a spatial location in the output."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bounding-box regression is a popular technique to refine or predict localization boxes in recent object detection approaches. Typically, bounding-box regressors are trained to regress from either region proposals or fixed anchor boxes to nearby bounding boxes of a pre-defined target object classes."}, {"text": "Anchor boxes are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect and are typically chosen based on object sizes in your training datasets."}, {"text": "The ground-truth bounding boxes (i.e., the hand labeled bounding boxes from the testing set that specify where in the image our object is)."}, {"text": "Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.  The ground-truth bounding boxes (i.e., the hand labeled bounding boxes from the testing set that specify where in the image our object is). The predicted bounding boxes from our model."}, {"text": "Bounding boxes is one of the most popular and recognizable image annotation method used in machine learning and deep learning. Using bounding boxes annotators are asked to outline the object in a box as per the machine learning project requirements."}, {"text": "One method of processing images is via face detection. Face detection is a branch of image processing that uses machine learning to detect faces in images. A Haar Cascade is an object detection method used to locate an object of interest in images."}, {"text": "RPN Loss Function The first term is the classification loss over 2 classes (There is object or not). The second term is the regression loss of bounding boxes only when there is object (i.e. p_i* =1). Thus, RPN network is to pre-check which location contains object."}]}, {"question": "Will reinforcement learning become big", "positive_ctxs": [{"text": "Reinforcement learning will be the next big thing in data science in 2019.  The potential value in using RL in proactive analytics and AI is enormous, but it also demands a greater skillset to master."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "The next big thing after deep learning Artificial General Intelligence (AGI) that is building machines that can surpass human intelligence. The next big thing after deep learning Artificial General Intelligence (AGI) that is building machines that can surpass human intelligence."}, {"text": "Some of the practical applications of reinforcement learning are:Manufacturing. In Fanuc, a robot uses deep reinforcement learning to pick a device from one box and putting it in a container.  Inventory Management.  Delivery Management.  Power Systems.  Finance Sector."}, {"text": "Big data might be big business, but overzealous data mining can seriously destroy your brand.  As companies become experts at slicing and dicing data to reveal details as personal as mortgage defaults and heart attack risks, the threat of egregious privacy violations grows."}, {"text": "The example of reinforcement learning is your cat is an agent that is exposed to the environment. The biggest characteristic of this method is that there is no supervisor, only a real number or reward signal. Two types of reinforcement learning are 1) Positive 2) Negative."}, {"text": "Distributed deep learning is a sub-area of general distributed machine learning that has recently become very prominent because of its effectiveness in various applications."}, {"text": "Deep reinforcement learning is a promising combination between two artificial intelligence techniques: reinforcement learning, which uses sequential trial and error to learn the best action to take in every situation, and deep learning, which can evaluate complex inputs and select the best response."}]}, {"question": "What are the applications of machine learning", "positive_ctxs": [{"text": "Top 10 Machine Learning ApplicationsTraffic Alerts.Social Media.Transportation and Commuting.Products Recommendations.Virtual Personal Assistants.Self Driving Cars.Dynamic Pricing.Google Translate.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "TensorFlow 2.0 is an updated version of TensorFlow that has been designed with a focus on simple execution, ease of use, and developer's productivity. TensorFlow 2.0 makes the development of machine learning applications even easier."}, {"text": "In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data."}, {"text": "Automated machine learning (AutoML) is the process of automating the process of applying machine learning to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model."}, {"text": "Some applications of unsupervised machine learning techniques include: Clustering allows you to automatically split the dataset into groups according to similarity. Often, however, cluster analysis overestimates the similarity between groups and doesn't treat data points as individuals."}, {"text": "Some applications of unsupervised machine learning techniques include: Clustering allows you to automatically split the dataset into groups according to similarity. Often, however, cluster analysis overestimates the similarity between groups and doesn't treat data points as individuals."}, {"text": "Some applications of unsupervised machine learning techniques are: Clustering automatically split the dataset into groups base on their similarities. Anomaly detection can discover unusual data points in your dataset. It is useful for finding fraudulent transactions."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}]}, {"question": "Why do we use 0.05 level of significance", "positive_ctxs": [{"text": "The significance level, also denoted as alpha or \u03b1, is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The significance level is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference."}, {"text": "The significance level is the probability of rejecting the null hypothesis when it is true.  For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference."}, {"text": "We reject the null hypothesis when the p-value is less than \u03b1. But 0.07 > 0.05 so we fail to reject H0.  For example if the p-value = 0.08, then we would fail to reject H0 at the significance level of \u03b1=0.05 since 0.08 > 0.05, but we would reject H0 at the significance level of \u03b1 = 0.10 since 0.08 < 0.10."}, {"text": "To determine whether the correlation between variables is significant, compare the p-value to your significance level. Usually, a significance level (denoted as \u03b1 or alpha) of 0.05 works well. An \u03b1 of 0.05 indicates that the risk of concluding that a correlation exists\u2014when, actually, no correlation exists\u2014is 5%."}, {"text": "The significance level, also denoted as alpha or \u03b1, is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference."}, {"text": "Rejecting or failing to reject the null hypothesis If our statistical analysis shows that the significance level is below the cut-off value we have set (e.g., either 0.05 or 0.01), we reject the null hypothesis and accept the alternative hypothesis."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}]}, {"question": "What is interpolation time series", "positive_ctxs": [{"text": "volves the estimation of some components for some dates by interpola- tion. between values (\"benchmarks\") for earlier and later dates. This is often done by using a related series known for all relevant dates. In practice, the bulk of such interpolation uses only a single related."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Time series forecasting is an important area of machine learning that is often neglected. It is important because there are so many prediction problems that involve a time component.  Standard definitions of time series, time series analysis, and time series forecasting."}, {"text": "A stationary time series is one whose properties do not depend on the time at which the series is observed. 14. Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times."}, {"text": "Multivariate interpolation is the interpolation of functions of more than one variable. Methods include bilinear interpolation and bicubic interpolation in two dimensions, and trilinear interpolation in three dimensions. They can be applied to gridded or scattered data."}, {"text": "Time series analysis is a statistical technique that deals with time series data, or trend analysis. Time series data means that data is in a series of particular time periods or intervals.  Time series data: A set of observations on the values that a variable takes at different times."}, {"text": "Generally, we use linear regression for time series analysis, it is used for predicting the result for time series as its trends. For example, If we have a dataset of time series with the help of linear regression we can predict the sales with the time."}, {"text": "Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.  Interrupted time series analysis is the analysis of interventions on a single time series. Time series data have a natural temporal ordering."}, {"text": "A time series is a stochastic process that operates in continuous state space and discrete time set. A stochastic process is nothing but a set of random variables. It is a time dependent random phenomenon. Same is time series."}]}, {"question": "How is facial recognition being used today", "positive_ctxs": [{"text": "In many US airports, Customs and Border Protection now uses facial recognition to screen passengers on international flights. And in cities such as Baltimore, police have used facial recognition software to identify and arrest individuals at protests."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A facial recognition system uses biometrics to map facial features from a photograph or video. It compares the information with a database of known faces to find a match.  That's because facial recognition has all kinds of commercial applications. It can be used for everything from surveillance to marketing."}, {"text": "The global facial recognition market size was valued at USD 3.4 billion in 2019 and is anticipated to expand at a CAGR of 14.5% from 2020 to 2027. The technology is improving, evolving, and expanding at an explosive rate. Technologies such as biometrics are extensively used in order to enhance security."}, {"text": "Traditional algorithms involving face recognition work by identifying facial features by extracting features, or landmarks, from the image of the face. For example, to extract facial features, an algorithm may analyse the shape and size of the eyes, the size of nose, and its relative position with the eyes."}, {"text": "The global facial recognition market size is projected to grow from USD 3.2 billion in 2019 to USD 7.0 billion by 2024, at a CAGR of 16.6% from 2019 to 2024. The major factors driving the market include increased technological advancements across verticals."}, {"text": "In ideal conditions, facial recognition systems can have near-perfect accuracy. Verification algorithms used to match subjects to clear reference images (like a passport photo or mugshot) can achieve accuracy scores as high as 99.97% on standard assessments like NIST's Facial Recognition Vendor Test (FRVT)."}, {"text": "In ideal conditions, facial recognition systems can have near-perfect accuracy. Verification algorithms used to match subjects to clear reference images (like a passport photo or mugshot) can achieve accuracy scores as high as 99.97% on standard assessments like NIST's Facial Recognition Vendor Test (FRVT)."}, {"text": "Facial recognition is a category of biometric software that maps an individual's facial features mathematically and stores the data as a faceprint. The software uses deep learning algorithms to compare a live capture or digital image to the stored faceprint in order to verify an individual's identity."}]}, {"question": "What is clustering in machine learning", "positive_ctxs": [{"text": "Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}, {"text": "Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "A machine learning task is the type of prediction or inference being made, based on the problem or question that is being asked, and the available data. For example, the classification task assigns data to categories, and the clustering task groups data according to similarity."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Supervised clustering is applied on classified examples with the objective of identifying clusters that have high probability density to a single class.  Semi-supervised clustering is to enhance a clustering algorithm by using side information in clustering process."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}]}, {"question": "What's with the Z in Z test what similarity does it have to a simple Z or standard score", "positive_ctxs": [{"text": "The (BIG) Z is a similar to the small z for one very good reason: It is a standard score. The z score has the sample standard deviation as the denominator, whereas the z-test value has the standard error of the mean ( or a measure of the variability of all the means from the population) as the dominator."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "Lets do this step by step:Step 1: find the mean.Step 2: fin the standard deviation of the mean (using the population SD)Step 3: find the Z score.Step 4: compare to the critical Z score. From the stated hypothesis, we know that we are dealing with a 1-tailed hypothesis test.  Step 4 : compare to the critical Z score."}]}, {"question": "How do I change the position of an image in CSS", "positive_ctxs": [{"text": "Absolute Positioning You can use two values top and left along with the position property to move an HTML element anywhere in the HTML document. Move Left - Use a negative value for left. Move Right - Use a positive value for left. Move Up - Use a negative value for top."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Selectors are the names given to styles in internal and external style sheets. In this CSS Beginner Tutorial we will be concentrating on HTML selectors, which are simply the names of HTML tags and are used to change the style of a specific type of element."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Traditional algorithms involving face recognition work by identifying facial features by extracting features, or landmarks, from the image of the face. For example, to extract facial features, an algorithm may analyse the shape and size of the eyes, the size of nose, and its relative position with the eyes."}, {"text": "Gradient images are created from the original image (generally by convolving with a filter, one of the simplest being the Sobel filter) for this purpose. Each pixel of a gradient image measures the change in intensity of that same point in the original image, in a given direction."}, {"text": "The median is a measure of center (location) of a list of numbers.  This will be the median. If there are an even number on the list then average the n/2 and the (N + 2)/2 numbers. In general, the median is at position (n + 1)/2. If this position is a whole number then you have the median at that position in the list."}, {"text": "Support vectors are the elements of the training set that would change the position of the dividing hyperplane if removed. d+ = the shortest distance to the closest positive point d- = the shortest distance to the closest negative point The margin (gutter) of a separating hyperplane is d+ + d\u2013."}]}, {"question": "How do you use the binomial theorem", "positive_ctxs": [{"text": "The Binomial Theorem is a quick way (okay, it's a less slow way) of expanding (or multiplying out) a binomial expression that has been raised to some (generally inconveniently large) power. For instance, the expression (3x \u2013 2)10 would be very painful to multiply out by hand."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The binomial theorem is valid more generally for any elements x and y of a semiring satisfying xy = yx. The theorem is true even more generally: alternativity suffices in place of associativity. The binomial theorem can be stated by saying that the polynomial sequence {1, x, x2, x3, } is of binomial type."}, {"text": "The normal approximation to the binomial is when you use a continuous distribution (the normal distribution) to approximate a discrete distribution (the binomial distribution)."}, {"text": "In general, you should probably use the divergence theorem whenever you wish to evaluate a vector surface integral over a closed surface. The divergence theorem can also be used to evaluate triple integrals by turning them into surface integrals."}, {"text": "When n * p and n * q are greater than 5, you can use the normal approximation to the binomial to solve a problem."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}]}, {"question": "What is the purpose of statistics", "positive_ctxs": [{"text": "The Purpose of Statistics: Statistics teaches people to use a limited sample to make intelligent and accurate conclusions about a greater population. The use of tables, graphs, and charts play a vital role in presenting the data being used to draw these conclusions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The purpose and importance of the null hypothesis and alternative hypothesis are that they provide an approximate description of the phenomena. The purpose is to provide the researcher or an investigator with a relational statement that is directly tested in a research study."}, {"text": "Statistics is the study of the collection, organization, analysis, and interpretation of data.  Mathematical statistics is the study of statistics from a mathematical standpoint, using probability theory as well as other branches of mathematics such as linear algebra and analysis."}, {"text": "A point to remember is that the main purpose of acceptance sampling is to decide whether or not the lot is likely to be acceptable, not to estimate the quality of the lot. Acceptance sampling is employed when one or several of the following hold: Testing is destructive. The cost of 100% inspection is very high."}, {"text": "A simple definition of a sampling frame is the set of source materials from which the sample is selected. The definition also encompasses the purpose of sampling frames, which is to provide a means for choosing the particular members of the target population that are to be interviewed in the survey."}, {"text": "The purpose of such selection is to determine a set of variables that will provide the best fit for the model so that accurate predictions can be made. Variable selection is one of the most difficult aspects of model building."}]}, {"question": "What is an affine layer in machine learning", "positive_ctxs": [{"text": "An affine layer, or fully connected layer, is a layer of an artificial neural network in which all contained nodes connect to all nodes of the subsequent layer. Affine layers are commonly used in both convolutional neural networks and recurrent neural networks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "15 Most Used Machine Learning Tools By ExpertsKnime. Knime is again an open-source machine learning tool that is based on GUI.  Accord.net. Accord.net is a computational machine learning framework.  Scikit-Learn. Scikit-Learn is an open-source machine learning package.  TensorFlow.  Weka.  Pytorch.  RapidMiner.  Google Cloud AutoML.More items\u2022"}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Boltzmann machine is an unsupervised machine learning algorithm. It helps discover latent features present in the dataset. Dataset is composed of binary vectors. Connection between nodes are undirected."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Multi-view learning is an emerging direction in machine learning which considers learning with multiple views to improve the generalization performance. Multi-view learning is also known as data fusion or data integration from multiple feature sets."}, {"text": "And, unsupervised learning is where the machine is given training based on unlabeled data without any guidance.  Whereas reinforcement learning is when a machine or an agent interacts with its environment, performs actions, and learns by a trial-and-error method."}]}, {"question": "How do you explain standard deviation in statistics", "positive_ctxs": [{"text": "Standard deviation (represented by the symbol sigma, \u03c3 ) shows how much variation or dispersion exists from the average (mean), or expected value. More precisely, it is a measure of the average distance between the values of the data in the set and the mean."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In a somewhat similar fashion you can estimate the standard deviation based on the box plot:the standard deviation is approximately equal to the range / 4.the standard deviation is approximately equal to 3/4 * IQR."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "So standard deviation gives you more deviation than mean deviation whem there are certain data points that are too far from its mean."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "The T distribution is a continuous probability distribution of the z-score when the estimated standard deviation is used in the denominator rather than the true standard deviation.  T-tests are used in statistics to estimate significance."}, {"text": "How to Calculate a CorrelationFind the mean of all the x-values.Find the standard deviation of all the x-values (call it sx) and the standard deviation of all the y-values (call it sy).  For each of the n pairs (x, y) in the data set, take.Add up the n results from Step 3.Divide the sum by sx \u2217 sy.More items"}]}, {"question": "What is the S curve in photography", "positive_ctxs": [{"text": "An S-curve is simply a curve of some object, line or path in the image that curves back and forth horizontally as you proceed vertically, much like the letter S\u2013in fact, usually exactly like the letter S."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "S is known both as the standard error of the regression and as the standard error of the estimate. S represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Density values can be greater than 1. In the frequency histogram the y-axis was percentage, but in the density curve the y-axis is density and the area gives the percentage. When creating the density curve the values on the y-axis are calculated (scaled) so that the total area under the curve is 1."}, {"text": "The set of all the possible outcomes is called the sample space of the experiment and is usually denoted by S. Any subset E of the sample space S is called an event."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. The area under the normal distribution curve represents probability and the total area under the curve sums to one."}, {"text": "For a spontaneous reaction, the sign on Delta G must be negative. Gibbs free energy relates enthalpy, entropy and temperature. A spontaneous reaction will always occur when Delta H is negative and Delta S is positive, and a reaction will always be non-spontaneous when Delta H is positive and Delta S is negative."}]}, {"question": "Why is it called inverted index", "positive_ctxs": [{"text": "This type of index is called an inverted index, namely because it is an inversion of the forward index.  In some search engines the index includes additional information such as frequency of the terms, e.g. how often a term occurs in each document, or the position of the term in each document."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In computer science, an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content)."}, {"text": "In computer science, an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content)."}, {"text": "Chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval (IR). The second more subtle advantage of compression is faster transfer of data from disk to memory."}, {"text": "An Inverted file is an index data structure that maps content to its location within a database file, in a document or in a set of documents.  The inverted file is the most popular data structure used in document retrieval systems to support full text search."}, {"text": "Principal Component Analysis PCA's approach to data reduction is to create one or more index variables from a larger set of measured variables. It does this using a linear combination (basically a weighted average) of a set of variables. The created index variables are called components."}, {"text": "Why the Lognormal Distribution is used to Model Stock Prices Since the lognormal distribution is bound by zero on the lower side, it is therefore perfect for modeling asset prices which cannot take negative values. The normal distribution cannot be used for the same purpose because it has a negative side."}, {"text": "A stochastic process is a family of random variables {X\u03b8}, where the parameter \u03b8 is drawn from an index set \u0398. For example, let's say the index set is \u201ctime\u201d.  One example of a stochastic process that evolves over time is the number of customers (X) in a checkout line."}]}, {"question": "What is distance weighted KNN", "positive_ctxs": [{"text": "Weighted kNN is a modified version of k nearest neighbors.  The simplest method is to take the majority vote, but this can be a problem if the nearest neighbors vary widely in their distance and the closest neighbors more reliably indicate the class of the object."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Here is step by step on how to compute K-nearest neighbors KNN algorithm:Determine parameter K = number of nearest neighbors.Calculate the distance between the query-instance and all the training samples.Sort the distance and determine nearest neighbors based on the K-th minimum distance.More items"}, {"text": "The problem is we always prefer an output having highest probability or lowest distance from reference as our answer and while we are dealing with it, KNN will always give same output for a given set of input repeatedly tested. That means it is quit deterministic."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "KNN algorithm is one of the simplest classification algorithm. Even with such simplicity, it can give highly competitive results. KNN algorithm can also be used for regression problems."}, {"text": "They are often confused with each other. The 'K' in K-Means Clustering has nothing to do with the 'K' in KNN algorithm. k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}, {"text": "They are often confused with each other. The 'K' in K-Means Clustering has nothing to do with the 'K' in KNN algorithm. k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}]}, {"question": "How is gravity related to entropy", "positive_ctxs": [{"text": "Gravity tries to keep things together through attraction and thus tends to lower statistical entropy. The universal law of increasing entropy (2nd law of thermodynamics) states that the entropy of an isolated system which is not in equilibrium will tend to increase with time, approaching a maximum value at equilibrium."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The maximum entropy principle is defined as modeling a given set of data by finding the highest entropy to satisfy the constraints of our prior knowledge.  The maximum entropy model is a conditional probability model p(y|x) that allows us to predict class labels given a set of features for a given data point."}, {"text": "Decision tree algorithms use information gain to split a node. Both gini and entropy are measures of impurity of a node. A node having multiple classes is impure whereas a node having only one class is pure. Entropy in statistics is analogous to entropy in thermodynamics where it signifies disorder."}, {"text": "Binary cross-entropy is for multi-label classifications, whereas categorical cross entropy is for multi-class classification where each example belongs to a single class."}, {"text": "The center of mass is the mean position of the mass in an object. Then there's the center of gravity, which is the point where gravity appears to act. For many objects, these two points are in exactly the same place. But they're only the same when the gravitational field is uniform across an object."}, {"text": "The main difference between these two approaches is the goals (not the methods used). Therefore, Image processing is related to enhancing the image and play with features like colors. While computer vision is related to \"Image Understanding\" and it can use machine learning as well."}, {"text": "The intuition for entropy is that it is the average number of bits required to represent or transmit an event drawn from the probability distribution for the random variable. \u2026 the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution."}, {"text": "Definition: Entropy is a measure of uncertainty of a random variable. The entropy of a discrete random variable X with alphabet X is H(X) = -) p(x) log p(2) DEX When the base of the logarithm is 2, entropy is measured in bits.  (Note: you can prove this by assigning a variable pi to the probability of outcome i."}]}, {"question": "How do you find the standard error of the mean difference", "positive_ctxs": [{"text": "Calculating Standard Error of the MeanFirst, take the square of the difference between each data point and the sample mean, finding the sum of those values.Then, divide that sum by the sample size minus one, which is the variance.Finally, take the square root of the variance to get the SD."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "While the variance and the standard error of the mean are different estimates of variability, one can be derived from the other. Multiply the standard error of the mean by itself to square it. This step assumes that the standard error is a known quantity."}, {"text": "So the standard error of a mean provides a statement of probability about the difference between the mean of the population and the mean of the sample.  This is called the 95% confidence interval , and we can say that there is only a 5% chance that the range 86.96 to 89.04 mmHg excludes the mean of the population."}, {"text": "The standard error is also inversely proportional to the sample size; the larger the sample size, the smaller the standard error because the statistic will approach the actual value. The standard error is considered part of descriptive statistics. It represents the standard deviation of the mean within a dataset."}, {"text": "When a population is finite, the formula that determines the standard error of the mean \u03c3\u00afx. needs to be adjusted. If N is the size of the population and n is the size of the sample ( where n\u22650.05N) , then the standard error of the mean is \u03c3\u00afx=\u03c3\u221an\u221aN\u2212nN\u22121."}, {"text": "The difference is pretty simple: in squared error, you are penalizing large deviations more.  The mean absolute error is a common measure of forecast error in time [2]series analysis, where the terms \"mean absolute deviation\" is sometimes used in confusion with the more standard definition of mean absolute deviation."}, {"text": "The standard error is a statistical term that measures the accuracy with which a sample distribution represents a population by using standard deviation. In statistics, a sample mean deviates from the actual mean of a population\u2014this deviation is the standard error of the mean."}, {"text": "The standard error is a statistical term that measures the accuracy with which a sample distribution represents a population by using standard deviation. In statistics, a sample mean deviates from the actual mean of a population\u2014this deviation is the standard error of the mean."}]}, {"question": "How is Pointwise mutual information calculated", "positive_ctxs": [{"text": "The general formula for pointwise mutual information is given below; it is the binary logarithm of the joint probability of X = a and Y = b, divided by the product of the individual probabilities that X = a and Y = b."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Pointwise mutual information (PMI), or point mutual information, is a measure of association used in information theory and statistics. In contrast to mutual information (MI) which builds upon PMI, it refers to single events, whereas MI refers to the average of all possible events."}, {"text": "Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.  The mutual information between two random variables X and Y can be stated formally as follows: I(X ; Y) = H(X) \u2013 H(X | Y)"}, {"text": "In computational linguistics, second-order co-occurrence pointwise mutual information is a semantic similarity measure. To assess the degree of association between two given words, it uses pointwise mutual information (PMI) to sort lists of important neighbor words of the two target words from a large corpus."}, {"text": "Thus, the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the"}, {"text": "Information gain can also be used for feature selection, by evaluating the gain of each variable in the context of the target variable. In this slightly different usage, the calculation is referred to as mutual information between the two random variables."}, {"text": "The prior probability of an event will be revised as new data or information becomes available, to produce a more accurate measure of a potential outcome. That revised probability becomes the posterior probability and is calculated using Bayes' theorem."}, {"text": "Relative Risk is calculated by dividing the probability of an event occurring for group 1 (A) divided by the probability of an event occurring for group 2 (B). Relative Risk is very similar to Odds Ratio, however, RR is calculated by using percentages, whereas Odds Ratio is calculated by using the ratio of odds."}]}, {"question": "How do you plot probability distribution", "positive_ctxs": [{"text": "How to create probability distribution plots in MinitabChoose Graph > Probability Distribution Plot > View Probability.Click OK.From Distribution, choose Normal.In Mean, type 100.In Standard deviation, type 15."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "A probability density plot simply means a density plot of probability density function (Y-axis) vs data points of a variable (X-axis).  By showing probability density plots, we're only able to understand the distribution of data visually without knowing the exact probability for a certain range of values."}, {"text": "To plot the probability density function for a log normal distribution in R, we can use the following functions: dlnorm(x, meanlog = 0, sdlog = 1) to create the probability density function. curve(function, from = NULL, to = NULL) to plot the probability density function."}, {"text": "Insufficient Data can cause a normal distribution to look completely scattered.  An extreme example: if you choose three random students and plot the results on a graph, you won't get a normal distribution. You might get a uniform distribution (i.e. 62 62 63) or you might get a skewed distribution (80 92 99)."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point.  Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood)."}]}, {"question": "What is kernel of Matrix", "positive_ctxs": [{"text": "To find the kernel of a matrix A is the same as to solve the system AX = 0, and one usually does this by putting A in rref.  In both cases, the kernel is the set of solutions of the corresponding homogeneous linear equations, AX = 0 or BX = 0."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The significance of Matrix is - they represent Linear transformations like rotation/scaling. A Matrix is just a stack of numbers - but very special - you can add them and subtract them and multiply them [restrictions]. The significance of Matrix is - they represent Linear transformations like rotation/scaling."}, {"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Kernel vs Filter The dimensions of the kernel matrix is how the convolution gets it's name. For example, in 2D convolutions, the kernel matrix is a 2D matrix. A filter however is a concatenation of multiple kernels, each kernel assigned to a particular channel of the input."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In 1D CNN, kernel moves in 1 direction. Input and output data of 1D CNN is 2 dimensional. Mostly used on Time-Series data. In 2D CNN, kernel moves in 2 directions. Input and output data of 2D CNN is 3 dimensional."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is feature selection and feature extraction", "positive_ctxs": [{"text": "Feature Selection. Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection. Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Again, feature selection keeps a subset of the original features while feature extraction creates new ones. As with feature selection, some algorithms already have built-in feature extraction.  As a stand-alone task, feature extraction can be unsupervised (i.e. PCA) or supervised (i.e. LDA)."}, {"text": "The three different ways of feature extraction are horizontal direction, vertical direction and diagonal direction. Recognition rate percentage for vertical, horizontal and diagonal based feature extraction using feed forward back propagation neural network as classification phase are 92.69, 93.68, 97.80 respectively."}, {"text": "So while L2 regularization does not perform feature selection the same way as L1 does, it is more useful for feature *interpretation*: a predictive feature will get a non-zero coefficient, which is often not the case with L1."}]}, {"question": "What is difference between Fourier series and Fourier transform", "positive_ctxs": [{"text": "In short, fourier series is for periodic signals and fourier transform is for aperiodic signals. Fourier series is used to decompose signals into basis elements (complex exponentials) while fourier transforms are used to analyze signal in another domain (e.g. from time to frequency, or vice versa)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Fourier Methods in Signal Processing The Fourier transform and discrete-time Fourier transform are mathematical analysis tools and cannot be evaluated exactly in a computer. The Fourier transform is used to analyze problems involving continuous-time signals or mixtures of continuous- and discrete-time signals."}, {"text": "In mathematics, a Fourier series (/\u02c8f\u028arie\u026a, -i\u0259r/) is a periodic function composed of harmonically related sinusoids, combined by a weighted summation.  The discrete-time Fourier transform is an example of Fourier series. The process of deriving the weights that describe a given function is a form of Fourier analysis."}, {"text": "A fast Fourier transform (FFT) is an algorithm that computes the discrete Fourier transform (DFT) of a sequence, or its inverse (IDFT). Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa."}, {"text": "In short, fourier series is for periodic signals and fourier transform is for aperiodic signals. Fourier series is used to decompose signals into basis elements (complex exponentials) while fourier transforms are used to analyze signal in another domain (e.g. from time to frequency, or vice versa)."}, {"text": "In short, fourier series is for periodic signals and fourier transform is for aperiodic signals. Fourier series is used to decompose signals into basis elements (complex exponentials) while fourier transforms are used to analyze signal in another domain (e.g. from time to frequency, or vice versa)."}, {"text": "First, after looking around on the web, it seems that there is no way to compute a (discrete) Fourier transform through a neural network. You can hack it by hard-coding the thing to include the Fourier constants for the transform and then get a decent result."}, {"text": "The ``wavelet transform'' maps each f(x) to its coefficients with respect to this basis.  The mathematics is simple and the transform is fast (faster than the Fast Fourier Transform, which we briefly explain), but approximation by piecewise constants is poor."}]}, {"question": "What data augmentation techniques are available for deep learning on text", "positive_ctxs": [{"text": "Data Augmentation in NLPSynonym Replacement: Randomly choose n words from the sentence that are not stop words.  Random Insertion: Find a random synonym of a random word in the sentence that is not a stop word.  Random Swap: Randomly choose two words in the sentence and swap their positions.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks."}, {"text": "Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks."}, {"text": "Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model."}, {"text": "The performance of deep learning neural networks often improves with the amount of data available. Data augmentation is a technique to artificially create new training data from existing training data. This means, variations of the training set images that are likely to be seen by the model."}, {"text": "Text classification using word embeddings and deep learning in python \u2014 classifying tweets from twitterSplit the data into text (X) and labels (Y)Preprocess X.Create a word embedding matrix from X.Create a tensor input from X.Train a deep learning model using the tensor inputs and labels (Y)More items\u2022"}, {"text": "Sentiment Analysis is a procedure used to determine if a chunk of text is positive, negative or neutral. In text analytics, natural language processing (NLP) and machine learning (ML) techniques are combined to assign sentiment scores to the topics, categories or entities within a phrase."}, {"text": "Definition. Predictive analytics is an area of statistics that deals with extracting information from data and using it to predict trends and behavior patterns.  Predictive analytics statistical techniques include data modeling, machine learning, AI, deep learning algorithms and data mining."}]}, {"question": "What is learning in neural networks", "positive_ctxs": [{"text": "An artificial neural network's learning rule or learning process is a method, mathematical logic or algorithm which improves the network's performance and/or training time.  Depending upon the process to develop the network there are three main models of machine learning: Unsupervised learning. Supervised learning."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. In fact, it is the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three."}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}, {"text": "Neural networks are sets of algorithms intended to recognize patterns and interpret data through clustering or labeling. In other words, neural networks are algorithms. A training algorithm is the method you use to execute the neural network's learning process."}, {"text": "Neural network ensemble is a learning paradigm where many neural networks are jointly used to solve a problem.  Then it assigns random weights to those networks and employs genetic algorithm to evolve the weights so that they can characterize to some extent the fitness of the neural networks in constituting an ensemble."}, {"text": "Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}, {"text": "Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}]}, {"question": "How do you visualize data effectively", "positive_ctxs": [{"text": "For more tips, read 10 Best Practices for Effective Dashboards.Choose the right charts and graphs for the job.  Use predictable patterns for layouts.  Tell data stories quickly with clear color cues.  Incorporate contextual clues with shapes and designs.  Strategically use size to visualize values.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "To visualize a small data set containing multiple categorical (or qualitative) variables, you can create either a bar plot, a balloon plot or a mosaic plot.  These methods make it possible to analyze and visualize the association (i.e. correlation) between a large number of qualitative variables."}, {"text": "Classification requires labels. Therefore you first cluster your data and save the resulting cluster labels. Then you train a classifier using these labels as a target variable. By saving the labels you effectively seperate the steps of clustering and classification."}, {"text": "To visualize the weights, you can use a tf. image_summary() op to transform a convolutional filter (or a slice of a filter) into a summary proto, write them to a log using a tf. train. SummaryWriter , and visualize the log using TensorBoard."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}]}, {"question": "How do you find the variance of a sum of squares", "positive_ctxs": [{"text": "The variance is the average of the sum of squares (i.e., the sum of squares divided by the number of observations). The standard deviation is the square root of the variance."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The mean of the sum of squares (SS) is the variance of a set of scores, and the square root of the variance is its standard deviation. This simple calculator uses the computational formula SS = \u03a3X2 - ((\u03a3X)2 / N) - to calculate the sum of squares for a single set of scores."}, {"text": "A residual sum of squares (RSS) is a statistical technique used to measure the amount of variance in a data set that is not explained by a regression model.  The residual sum of squares measures the amount of error remaining between the regression function and the data set."}, {"text": "The least squares method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve. Least squares regression is used to predict the behavior of dependent variables."}, {"text": "The least squares method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve. Least squares regression is used to predict the behavior of dependent variables."}, {"text": "In statistics, the residual sum of squares (RSS), also known as the sum of squared residuals (SSR) or the sum of squared estimate of errors (SSE), is the sum of the squares of residuals (deviations predicted from actual empirical values of data)."}, {"text": "Definition. The explained sum of squares (ESS) is the sum of the squares of the deviations of the predicted values from the mean value of a response variable, in a standard regression model \u2014 for example, yi = a + b1x1i + b2x2i +   the value estimated by the regression line ."}, {"text": "F is the ratio of two chi-squares, each divided by its df. A chi-square divided by its df is a variance estimate, that is, a sum of squares divided by degrees of freedom. F = t2."}]}, {"question": "What is the difference between boosting and bagging", "positive_ctxs": [{"text": "Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data. Boosting is an iterative technique which adjusts the weight of an observation based on the last classification."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Overview of stacking. Stacking mainly differ from bagging and boosting on two points.  Second, stacking learns to combine the base models using a meta-model whereas bagging and boosting combine weak learners following deterministic algorithms."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "AdaBoost is the shortcut for adaptive boosting. So what's the differences between Adaptive boosting and Gradient boosting? Both are boosting algorithms which means that they convert a set of weak learners into a single strong learner.  On the other hand, gradient boosting doesn't modify the sample distribution."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "The Range is the difference between the lowest and highest values. Example: In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9. So the range is 9 \u2212 3 = 6."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}]}, {"question": "What is CalibratedClassifierCV", "positive_ctxs": [{"text": "In scikit-learn we can use the CalibratedClassifierCV class to create well calibrated predicted probabilities using k-fold cross-validation.  In CalibratedClassifierCV the training sets are used to train the model and the test sets is used to calibrate the predicted probabilities."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What is elastic net regularization in machine learning", "positive_ctxs": [{"text": "From Wikipedia, the free encyclopedia. In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "You can use the Lasso or elastic net regularization for generalized linear model regression which can be used for classification problems. Here data is the data matrix with rows as observations and columns as features. group is the labels."}, {"text": "In the context of machine learning, regularization is the process which regularizes or shrinks the coefficients towards zero. In simple words, regularization discourages learning a more complex or flexible model, to prevent overfitting. Moving on with this article on Regularization in Machine Learning."}, {"text": "Elastic net is a popular type of regularized linear regression that combines two popular penalties, specifically the L1 and L2 penalty functions.  Elastic Net is an extension of linear regression that adds regularization penalties to the loss function during training."}, {"text": "In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting."}, {"text": "Elastic net regularization adds an additional ridge regression-like penalty which improves performance when the number of predictors is larger than the sample size, allows the method to select strongly correlated variables together, and improves overall prediction accuracy."}, {"text": "In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting. Regularization applies to objective functions in ill-posed optimization problems."}, {"text": "Back-propagation is the essence of neural net training.  It is the practice of fine-tuning the weights of a neural net based on the error rate (i.e. loss) obtained in the previous epoch (i.e. iteration)."}]}, {"question": "What is Gaussian blur in image processing", "positive_ctxs": [{"text": "In image processing, a Gaussian blur (also known as Gaussian smoothing) is the result of blurring an image by a Gaussian function (named after mathematician and scientist Carl Friedrich Gauss). It is a widely used effect in graphics software, typically to reduce image noise and reduce detail."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A Gaussian blur effect is typically generated by convolving an image with an FIR kernel of Gaussian values.  In the first pass, a one-dimensional kernel is used to blur the image in only the horizontal or vertical direction. In the second pass, the same one-dimensional kernel is used to blur in the remaining direction."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise.  The Gaussian filter alone will blur edges and reduce contrast. The Median filter is a non-linear filter that is most commonly used as a simple way to reduce noise in an image."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for \"unsharp masking\" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."}, {"text": "1. The Canny edge detector is a linear filter because it uses the Gaussian filter to blur the image and then uses the linear filter to compute the gradient. Solution False. Though it does those things, it also has non-linear operations: thresholding, hysteresis, non-maximum suppression."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}]}, {"question": "How do you find K for K means", "positive_ctxs": [{"text": "There is a popular method known as elbow method which is used to determine the optimal value of K to perform the K-Means Clustering Algorithm. The basic idea behind this method is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer elements in the cluster."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows\u2022"}, {"text": "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers."}, {"text": "KNN for Classification And the inverse, use an even number for K when you have an odd number of classes. Ties can be broken consistently by expanding K by 1 and looking at the class of the next most similar instance in the training dataset."}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "Chebyshev's inequality says that at least 1\u22121K2 of data from a sample must fall within K standard deviations from the mean, where K is any positive real number greater than one."}, {"text": "The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.  The results of the K-means clustering algorithm are: The centroids of the K clusters, which can be used to label new data. Labels for the training data (each data point is assigned to a single cluster)"}, {"text": "The softmax function is a function that turns a vector of K real values into a vector of K real values that sum to 1. The input values can be positive, negative, zero, or greater than one, but the softmax transforms them into values between 0 and 1, so that they can be interpreted as probabilities."}]}, {"question": "When X and Y are statistically independent then I xy is", "positive_ctxs": [{"text": "If two random variables X and Y are independent, then their covariance Cov(X, Y) = E(XY) \u2212 E(X)E(Y) = 0, that is, they are uncorrelated."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The covariance between X and Y is defined as Cov(X,Y)=E[(X\u2212EX)(Y\u2212EY)]=E[XY]\u2212(EX)(EY).The covariance has the following properties:Cov(X,X)=Var(X);if X and Y are independent then Cov(X,Y)=0;Cov(X,Y)=Cov(Y,X);Cov(aX,Y)=aCov(X,Y);Cov(X+c,Y)=Cov(X,Y);Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z);more generally,"}, {"text": "The covariance between X and Y is defined as Cov(X,Y)=E[(X\u2212EX)(Y\u2212EY)]=E[XY]\u2212(EX)(EY).The covariance has the following properties:Cov(X,X)=Var(X);if X and Y are independent then Cov(X,Y)=0;Cov(X,Y)=Cov(Y,X);Cov(aX,Y)=aCov(X,Y);Cov(X+c,Y)=Cov(X,Y);Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z);more generally,"}, {"text": "The covariance between X and Y is defined as Cov(X,Y)=E[(X\u2212EX)(Y\u2212EY)]=E[XY]\u2212(EX)(EY).The covariance has the following properties:Cov(X,X)=Var(X);if X and Y are independent then Cov(X,Y)=0;Cov(X,Y)=Cov(Y,X);Cov(aX,Y)=aCov(X,Y);Cov(X+c,Y)=Cov(X,Y);Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z);more generally,"}, {"text": "X and Y are independent iff fX,Y (x,y) = g(x)h(y) for all x,y for some functions g and h. Proof. If X and Y are independent then you need only take g(x) = fX(x) and h(y) = fY (y)."}, {"text": "Covariance can be positive, zero, or negative.  If X and Y are independent variables, then their covariance is 0: Cov(X, Y ) = E(XY ) \u2212 \u00b5X\u00b5Y = E(X)E(Y ) \u2212 \u00b5X\u00b5Y = 0 The converse, however, is not always true."}, {"text": "Proof. If X and Y are independent then you need only take g(x) = fX(x) and h(y) = fY (y). Note When fX,Y (x,y) = g(x)h(y) for all x,y you can easily write down the marginal p.d.f.'s. h(y) for a suitable choice of C."}, {"text": "We say that X and Y are independent if P(X=x,Y=y)=P(X=x)P(Y=y), for all x,y.  Intuitively, two random variables X and Y are independent if knowing the value of one of them does not change the probabilities for the other one. In other words, if X and Y are independent, we can write P(Y=y|X=x)=P(Y=y), for all x,y."}]}, {"question": "How do you find the mean of a geometric distribution", "positive_ctxs": [{"text": "0:002:44Suggested clip \u00b7 118 secondsGeometric Distribution: Mean - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The geometric distribution represents the number of failures before you get a success in a series of Bernoulli trials. This discrete probability distribution is represented by the probability density function: f(x) = (1 \u2212 p)x \u2212 1p."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "In mathematics, the inequality of arithmetic and geometric means, or more briefly the AM\u2013GM inequality, states that the arithmetic mean of a list of non-negative real numbers is greater than or equal to the geometric mean of the same list; and further, that the two means are equal if and only if every number in the"}, {"text": "The mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean \u03bc, then the mean of the sampling distribution of the mean is also \u03bc. The symbol \u03bcM is used to refer to the mean of the sampling distribution of the mean."}]}, {"question": "What is the difference between a distribution and a sampling distribution", "positive_ctxs": [{"text": "The averaged height is just one number now. Sample distribution: Just the distribution of the data from the sample. Sampling distribution: The distribution of a statistic from several samples."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A sampling distribution is the theoretical distribution of a sample statistic that would be obtained from a large number of random samples of equal size from a population. Consequently, the sampling distribution serves as a statistical \u201cbridge\u201d between a known sample and the unknown population."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}]}, {"question": "What are the unsupervised machine learning algorithms", "positive_ctxs": [{"text": "Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To recap, we have covered some of the the most important machine learning algorithms for data science: 5 supervised learning techniques- Linear Regression, Logistic Regression, CART, Na\u00efve Bayes, KNN. 3 unsupervised learning techniques- Apriori, K-means, PCA."}, {"text": "Supervised learning algorithms are trained using labeled data. Unsupervised learning algorithms are trained using unlabeled data.  In unsupervised learning, only input data is provided to the model. The goal of supervised learning is to train the model so that it can predict the output when it is given new data."}, {"text": "Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space."}, {"text": "Boltzmann machine is an unsupervised machine learning algorithm. It helps discover latent features present in the dataset. Dataset is composed of binary vectors. Connection between nodes are undirected."}, {"text": "In unsupervised learning, an AI system is presented with unlabeled, uncategorized data and the system's algorithms act on the data without prior training. The output is dependent upon the coded algorithms. Subjecting a system to unsupervised learning is an established way of testing the capabilities of that system."}, {"text": "Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model."}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}]}, {"question": "What does it mean to condition on a variable", "positive_ctxs": [{"text": "A condition variable indicates an event and has no value. More precisely, one cannot store a value into nor retrieve a value from a condition variable. If a thread must wait for an event to occur, that thread waits on the corresponding condition variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "A false negative is a test result that indicates a person does not have a disease or condition when the person actually does have it, according to the National Institute of Health (NIH)."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "An ordinal variable is a categorical variable for which the possible values are ordered. Ordinal variables can be considered \u201cin between\u201d categorical and quantitative variables. Thus it does not make sense to take a mean of the values."}, {"text": "In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time."}, {"text": "In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time."}, {"text": "Homoskedastic (also spelled \"homoscedastic\") refers to a condition in which the variance of the residual, or error term, in a regression model is constant. That is, the error term does not vary much as the value of the predictor variable changes."}]}, {"question": "What is linear and non linear model", "positive_ctxs": [{"text": "A linear regression equation simply sums the terms. While the model must be linear in the parameters, you can raise an independent variable by an exponent to fit a curve. For instance, you can include a squared or cubed term. Nonlinear regression models are anything that doesn't follow this one form."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}, {"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Log-Log linear regression A regression model where the outcome and at least one predictor are log transformed is called a log-log linear model."}, {"text": "Log-Log linear regression A regression model where the outcome and at least one predictor are log transformed is called a log-log linear model."}, {"text": "A Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer). While a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non \u2013 linear functions."}, {"text": "Linear algebra is called linear because it is the study of straight lines. A linear function is any function that graphs to a straight line, and linear algebra is the mathematics for solving systems that are modeled with multiple linear functions.  Multiple linear equations can be expressed as vectors and matrices."}]}, {"question": "What is the output of logistic regression", "positive_ctxs": [{"text": "The output from the logistic regression analysis gives a p-value of , which is based on the Wald z-score. Rather than the Wald method, the recommended method to calculate the p-value for logistic regression is the likelihood-ratio test (LRT), which for this data gives ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A logistic regression estimates the mean of your response given that your data is distributed Bernoulli or is a Binomial trial. Since the mean of a Binomial trial is the probability of success, you can interpret the output from a Logistic regression (after logit transformation) as a probability of success."}, {"text": "It is used in multinomial logistic regression and is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes. , and the components will add up to 1, so that they can be interpreted as probabilities."}, {"text": "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)\u2208{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits."}, {"text": "The logit is a transformation. Logistic regression is a regression model. The logit transformation transforms a line to a logistic curve. Logistic regression fits a logistic curve to set of data where the dependent variable can only take the values 0 and 1."}, {"text": "Thus logit regression is simply the GLM when describing it in terms of its link function, and logistic regression describes the GLM in terms of its activation function."}, {"text": "Mean Squared Error, commonly used for linear regression models, isn't convex for logistic regression. This is because the logistic function isn't always convex. The logarithm of the likelihood function is however always convex."}, {"text": ". Thus logit regression is simply the GLM when describing it in terms of its link function, and logistic regression describes the GLM in terms of its activation function."}]}, {"question": "What are some applications of artificial intelligence", "positive_ctxs": [{"text": "List of applicationsOptical character recognition.Handwriting recognition.Speech recognition.Face recognition.Artificial creativity.Computer vision.Virtual reality.Image processing.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "\"AI is a computer system able to perform tasks that ordinarily require human intelligence Many of these artificial intelligence systems are powered by machine learning, some of them are powered by deep learning and some of them are powered by very boring things like rules.\""}, {"text": "These are some of the most popular examples of artificial intelligence that's being used today. Everyone is familiar with Apple's personal assistant, Siri. She's the friendly voice-activated computer that we interact with on a daily basis."}, {"text": "Multimodal machine learning is a vibrant multi-disciplinary research field which addresses some of the original goals of artificial intelligence by integrating and modeling multiple communicative modalities, including linguistic, acoustic and visual messages."}, {"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}, {"text": "Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing (NLP), speech recognition and machine vision."}]}, {"question": "Is SVM a non linear classifier", "positive_ctxs": [{"text": "SVM could be considered as a linear classifier, because it uses one or several hyperplanes as well as nonlinear with a kernel function (Gaussian or radial basis in BCI applications)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Though SVM is a linear classifier which learns an (n \u2013 1)-dimensional classifier for classification of data into two classes. But SVM it can be used for classifying a non-linear dataset."}, {"text": "As mentioned above SVM is a linear classifier which learns an (n \u2013 1)-dimensional classifier for classification of data into two classes. However, it can be used for classifying a non-linear dataset. This can be done by projecting the dataset into a higher dimension in which it is linearly separable!"}, {"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}, {"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}, {"text": "The SVM classifier is a frontier which best segregates the two classes (hyper-plane/ line). You can look at support vector machines and a few examples of its working here."}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them."}, {"text": "A Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer). While a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non \u2013 linear functions."}]}, {"question": "How do you avoid bias in a survey", "positive_ctxs": [{"text": "1. Be careful while framing your survey questionnaireKeep your questions short and clear. Although framing straightforward questions may sound simple enough, most surveys fail in this area.  Avoid leading questions.  Avoid or break down difficult concepts.  Use interval questions.  Keep the time period short and relevant."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "During the initial stages of survey research, researchers usually prefer using convenience sampling as it's quick and easy to deliver results. Even if many statisticians avoid implementing this technique, it is vital in situations where you intend to get insights in a shorter period or without investing too much money."}, {"text": "So, assuming a 15% survey response rate, we see that you should send your NPS survey to 1,700 customers. What if you're a smaller company and don't have enough customers to send the recommended number of invitations?"}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to Deal with MulticollinearityRedesign the study to avoid multicollinearity.  Increase sample size.  Remove one or more of the highly-correlated independent variables.  Define a new variable equal to a linear combination of the highly-correlated variables."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Sampling Frame Error: A type of nonsampling error in a survey caused by a sampling frame (i.e., a list) that is not a perfect representation of the population or universe. That is, the sample list might contain respondents who do not meet the definition of the population or universe."}]}, {"question": "Why do we use wavelet transform", "positive_ctxs": [{"text": "The ability to slide the signal is the what gives Engineers a more accurate representation of the signal and therefore a better resolution in time.  So when you use a Wavelet Transform the signal is deconstructed using the same wavelet at different scales, rather than the same sin() wave at different frequencies."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Applications. The discrete wavelet transform has a huge number of applications in science, engineering, mathematics and computer science. Most notably, it is used for signal coding, to represent a discrete signal in a more redundant form, often as a preconditioning for data compression."}, {"text": "An algorithm that uses random numbers to decide what to do next anywhere in its logic is called a Randomized Algorithm. For example, in Randomized Quick Sort, we use a random number to pick the next pivot (or we randomly shuffle the array). And in Karger's algorithm, we randomly pick an edge."}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}, {"text": "Spectral analysis is a technique that allows us to discover underlying periodicities. To perform spectral analysis, we first must transform data from time domain to frequency domain."}, {"text": "A model is a simplified representation of a system. over some time period or spatial extent intended to promote understanding of the real system. Why Build a Model? Building models helps us understand the problem. (and its surrounding system) we are investigating solutions for."}, {"text": "SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types."}]}, {"question": "How do you find x and y variables in regression", "positive_ctxs": [{"text": "In regression analysis, the dependent variable is denoted Y and the independent variable is denoted X. So, in this case, Y=total cholesterol and X=BMI. When there is a single continuous dependent variable and a single independent variable, the analysis is called a simple linear regression analysis ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables. For instance, when we predict rent based on square feet alone that is simple linear regression."}, {"text": "Simply stated: the R2 value is simply the square of the correlation coefficient R . The correlation coefficient ( R ) of a model (say with variables x and y ) takes values between \u22121 and 1 . It describes how x and y are correlated."}, {"text": "Therefore the slope represents how much the y value changes when the x value changes by 1 unit. In statistics, especially regression analysis, the x value has real life meaning and so does the y value. Interpret the slope of the regression line in the context of the study."}, {"text": "If you want to control for the effects of some variables on some dependent variable, you just include them into the model. Say, you make a regression with a dependent variable y and independent variable x. You think that z has also influence on y too and you want to control for this influence."}, {"text": "If you want to control for the effects of some variables on some dependent variable, you just include them into the model. Say, you make a regression with a dependent variable y and independent variable x. You think that z has also influence on y too and you want to control for this influence."}, {"text": "The intercept of the regression line is just the predicted value for y, when x is 0. Any line has an equation, in terms of its slope and intercept: y = slope x x + intercept."}, {"text": "A regression model will have unit changes between the x and y variables, where a single unit change in x will coincide with a constant change in y. Taking the log of one or both variables will effectively change the case from a unit change to a percent change."}]}, {"question": "What is the difference between synchronous and asynchronous learning", "positive_ctxs": [{"text": "Specifically, synchronous learning is a type of group learning where everyone learns at the same time.  On the contrary, asynchronous learning is more self-directed, and the student decides the times that he will learn. TeachThought explains that, historically, online learning was asynchronous."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "The main difference between these two approaches is the goals (not the methods used). Therefore, Image processing is related to enhancing the image and play with features like colors. While computer vision is related to \"Image Understanding\" and it can use machine learning as well."}, {"text": "The Range is the difference between the lowest and highest values. Example: In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9. So the range is 9 \u2212 3 = 6."}]}, {"question": "What is stepwise variable selection", "positive_ctxs": [{"text": "Stepwise Selection Stepwise regression is a modification of the forward selection so that after each step in which a variable was added, all candidate variables in the model are checked to see if their significance has been reduced below the specified tolerance level."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Regression Analysis > Forward selection is a type of stepwise regression which begins with an empty model and adds in variables one by one. In each forward step, you add the one variable that gives the single best improvement to your model."}, {"text": "The reality is that stepwise regression is less effective the larger the number of potential explanatory variables. Stepwise regression does not solve the Big-Data problem of too many explanatory variables. Big Data exacerbates the failings of stepwise regression."}, {"text": "Findings. A fundamental problem with stepwise regression is that some real explanatory variables that have causal effects on the dependent variable may happen to not be statistically significant, while nuisance variables may be coincidentally significant."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces."}, {"text": "In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces."}]}, {"question": "What is the mean of a hypergeometric distribution", "positive_ctxs": [{"text": "The hypergeometric distribution has the following properties: The mean of the distribution is equal to n * k / N . The variance is n * k * ( N - k ) * ( N - n ) / [ N2 * ( N - 1 ) ] ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean \u03bc, then the mean of the sampling distribution of the mean is also \u03bc. The symbol \u03bcM is used to refer to the mean of the sampling distribution of the mean."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "In probability theory and statistics, the hypergeometric distribution is a discrete probability distribution that describes the probability of successes (random draws for which the object drawn has a specified feature) in draws, without replacement, from a finite population of size that contains exactly objects with"}, {"text": "In probability theory and statistics, the hypergeometric distribution is a discrete probability distribution that describes the probability of successes (random draws for which the object drawn has a specified feature) in draws, without replacement, from a finite population of size that contains exactly objects with"}, {"text": "Typically by the time the sample size is 30 the distribution of the sample mean is practically the same as a normal distribution.  \u00afX, the mean of the measurements in a sample of size n; the distribution of \u00afX is its sampling distribution, with mean \u03bc\u00afX=\u03bc and standard deviation \u03c3\u00afX=\u03c3\u221an."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "Use the hypergeometric distribution with populations that are so small that the outcome of a trial has a large effect on the probability that the next outcome is an event or non-event. For example, in a population of 10 people, 7 people have O+ blood."}]}, {"question": "How do you report Poisson regression results", "positive_ctxs": [{"text": "1:006:34Suggested clip \u00b7 111 secondsPoisson regression interpreting SPSS results (brief demo) - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Analysis methods you might considerNegative binomial regression \u2013 Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean.  Poisson regression \u2013 Poisson regression is often used for modeling count data.More items"}, {"text": "You simply measure the number of correct decisions your classifier makes, divide by the total number of test examples, and the result is the accuracy of your classifier. It's that simple. The vast majority of research results report accuracy, and many practical projects do too."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Poisson regression \u2013 Poisson regression is often used for modeling count data. Poisson regression has a number of extensions useful for count models. Negative binomial regression \u2013 Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean."}, {"text": "Poisson regression \u2013 Poisson regression is often used for modeling count data. Poisson regression has a number of extensions useful for count models. Negative binomial regression \u2013 Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean."}, {"text": "Introduction to Poisson Regression Poisson regression is also a type of GLM model where the random component is specified by the Poisson distribution of the response variable which is a count. When all explanatory variables are discrete, log-linear model is equivalent to poisson regression model."}, {"text": "When you have a statistically significant interaction, reporting the main effects can be misleading. Therefore, you will need to report the simple main effects."}]}, {"question": "What happens to uncertainty as the number of measurements increases", "positive_ctxs": [{"text": "The average value becomes more and more precise as the number of measurements increases. Although the uncertainty of any single measurement is always \u2206, the uncertainty in the mean \u2206 avg becomes smaller (by a factor of 1/\u221a) as more measurements are made."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "So, if we want to say how widely scattered some measurements are, we use the standard deviation. If we want to indicate the uncertainty around the estimate of the mean measurement, we quote the standard error of the mean. The standard error is most useful as a means of calculating a confidence interval."}, {"text": "The receptive field size of a unit can be increased in a number of ways. One option is to stack more layers to make the network deeper, which increases the receptive field size linearly by theory, as each extra layer increases the receptive field size by the kernel size."}, {"text": "Gaussian process regression (GPR) is a nonparametric, Bayesian approach to regression that is making waves in the area of machine learning. GPR has several benefits, working well on small datasets and having the ability to provide uncertainty measurements on the predictions."}, {"text": "Gaussian process regression (GPR) is a nonparametric, Bayesian approach to regression that is making waves in the area of machine learning. GPR has several benefits, working well on small datasets and having the ability to provide uncertainty measurements on the predictions."}, {"text": "The lognormal distribution is a distribution skewed to the right. The pdf starts at zero, increases to its mode, and decreases thereafter. The degree of skewness increases as increases, for a given . For the same , the pdf's skewness increases as increases."}, {"text": "Random errors often have a Gaussian normal distribution (see Fig. 2). In such cases statistical methods may be used to analyze the data. The mean m of a number of measurements of the same quantity is the best estimate of that quantity, and the standard deviation s of the measurements shows the accuracy of the estimate."}, {"text": "Empirical probability uses the number of occurrences of an outcome within a sample set as a basis for determining the probability of that outcome. The number of times \"event X\" happens out of 100 trials will be the probability of event X happening."}]}, {"question": "Is binomial distribution discrete or continuous", "positive_ctxs": [{"text": "The binomial distribution is a common discrete distribution used in statistics, as opposed to a continuous distribution, such as the normal distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "They are continuous vs discrete distributions. A first difference is that multinomial distribution M(N,p) is discrete (it generalises binomial disrtibution) whereas Dirichlet distribution is continuous (it generalizes Beta distribution)."}, {"text": "The normal approximation to the binomial is when you use a continuous distribution (the normal distribution) to approximate a discrete distribution (the binomial distribution)."}, {"text": "The binomial distribution is a common discrete distribution used in statistics, as opposed to a continuous distribution, such as the normal distribution."}, {"text": "follows a negative binomial distribution with parameters r and p. The geometric distribution is a special case of discrete compound Poisson distribution."}, {"text": "The exponential distribution is the only continuous distribution that is memoryless (or with a constant failure rate). Geometric distribution, its discrete counterpart, is the only discrete distribution that is memoryless."}, {"text": "The chi-square test is the most commonly used to test the goodness of fit tests and is used for discrete distributions like the binomial distribution and the Poisson distribution, whereas The Kolmogorov-Smirnov and Anderson-Darling goodness of fit tests are used for continuous distributions."}, {"text": "A probability distribution may be either discrete or continuous. A discrete distribution means that X can assume one of a countable (usually finite) number of values, while a continuous distribution means that X can assume one of an infinite (uncountable) number of different values."}]}, {"question": "Why is test error higher than training error", "positive_ctxs": [{"text": "Test error is consistently higher than training error: if this is by a small margin, and both error curves are decreasing with epochs, it should be fine. However if your test set error is not decreasing, while your training error is decreasing alot, it means you are over fitting severely."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Since is more degree 4 will be more complex(overfit the data) than the degree 3 model so it will again perfectly fit the data. In such case training error will be zero but test error may not be zero."}, {"text": "Training loss is the error on the training set of data. Validation loss is the error after running the validation set of data through the trained network. Train/valid is the ratio between the two. Unexpectedly, as the epochs increase both validation and training error drop."}, {"text": "approximation of the expected error called the empirical error which is the average error on the. training set. Given a function f, a loss function V , and a training set S consisting of n data points, the empirical error of f is: IS[f] ="}, {"text": "in a test involving multiple comparisons, the probability of making at least one Type I error over an entire research study. The experiment-wise error rate differs from the testwise error rate, which is the probability of making a Type I error when performing a specific test or comparison."}, {"text": "A type II error produces a false negative, also known as an error of omission. For example, a test for a disease may report a negative result, when the patient is, in fact, infected. This is a type II error because we accept the conclusion of the test as negative, even though it is incorrect."}, {"text": "The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is an error from sensitivity to small fluctuations in the training set."}, {"text": "The clear sign of a machine learning overfitting is if its error on testing set is much greater than the error on training set.  For instance if the model accuracy for train data is 85% and the accuracy for test/validation data is 65% then its very obvious that the model has overlearned and you should check that."}]}, {"question": "What is a Subdistribution hazard ratio", "positive_ctxs": [{"text": "The subdistribution hazard function, introduced by Fine and Gray, for a given type of event is defined as the instantaneous rate of occurrence of the given type of event in subjects who have not yet experienced an event of that type."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "If the hazard ratio is less than 1, then the predictor is protective (i.e., associated with improved survival) and if the hazard ratio is greater than 1, then the predictor is associated with increased risk (or decreased survival)."}, {"text": "The hazard function is the instantaneous rate of failure at a given time. Characteristics of a hazard function are frequently associated with certain products and applications. Different hazard functions are modeled with different distribution models."}, {"text": "With a continuous variable, the hazard ratio indicates the change in the risk of death if the parameter in question rises by one unit, for example if the patient is one year older on diagnosis. For every additional year of patient age on diagnosis, the risk of death falls by 7% (hazard ratio 0.93)."}, {"text": "A ratio scale is a quantitative scale where there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales."}, {"text": "A ratio scale is a quantitative scale where there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales."}, {"text": "The difference between a ratio scale and an interval scale is that the zero point on an interval scale is some arbitrarily agreed value, whereas on a ratio scale it is a true zero."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "How is P value calculated", "positive_ctxs": [{"text": "The p-value is calculated using the sampling distribution of the test statistic under the null hypothesis, the sample data, and the type of test being done (lower-tailed test, upper-tailed test, or two-sided test).  a lower-tailed test is specified by: p-value = P(TS ts | H 0 is true) = cdf(ts)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "P > 0.05 is the probability that the null hypothesis is true. 1 minus the P value is the probability that the alternative hypothesis is true. A statistically significant test result (P \u2264 0.05) means that the test hypothesis is false or should be rejected. A P value greater than 0.05 means that no effect was observed."}, {"text": "P > 0.05 is the probability that the null hypothesis is true. 1 minus the P value is the probability that the alternative hypothesis is true. A statistically significant test result (P \u2264 0.05) means that the test hypothesis is false or should be rejected. A P value greater than 0.05 means that no effect was observed."}, {"text": "So you can see that the ch-sq is the statistical measurement, while the P value is the level of probability that the result was due to chance alone. As the chi-sq statistic becomes larger, the P value becomes smaller."}, {"text": "The effect size is the main finding of a quantitative study. While a P value can inform the reader whether an effect exists, the P value will not reveal the size of the effect."}, {"text": "P > 0.05 is the probability that the null hypothesis is true.  A statistically significant test result (P \u2264 0.05) means that the test hypothesis is false or should be rejected. A P value greater than 0.05 means that no effect was observed."}, {"text": "P > 0.05 is the probability that the null hypothesis is true.  A statistically significant test result (P \u2264 0.05) means that the test hypothesis is false or should be rejected. A P value greater than 0.05 means that no effect was observed."}, {"text": "A binomial random variable is the number of successes x in n repeated trials of a binomial experiment.Binomial DistributionThe mean of the distribution (\u03bcx) is equal to n * P .The variance (\u03c32x) is n * P * ( 1 - P ).The standard deviation (\u03c3x) is sqrt[ n * P * ( 1 - P ) ]."}]}, {"question": "How do you calculate harmonic mean", "positive_ctxs": [{"text": "To find the harmonic mean of a set of n numbers, add the reciprocals of the numbers in the set, divide the sum by n, then take the reciprocal of the result."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "The harmonic mean is a type of numerical average. It is calculated by dividing the number of observations by the reciprocal of each number in the series. Thus, the harmonic mean is the reciprocal of the arithmetic mean of the reciprocals."}, {"text": "There are different types of mean, viz. arithmetic mean, weighted mean, geometric mean (GM) and harmonic mean (HM). If mentioned without an adjective (as mean), it generally refers to the arithmetic mean."}, {"text": "The geometric mean must be used when working with percentages, which are derived from values, while the standard arithmetic mean works with the values themselves. The harmonic mean is best used for fractions such as rates or multiples."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "The general formula for calculating a harmonic mean is:Harmonic mean = n / (\u22111/x_i)Weighted Harmonic Mean = (\u2211w_i ) / (\u2211w_i/x_i)P/E (Index) = (0.4+0.6) / (0.4/50 + 0.6/4) = 6.33.P/E (Index) = 0.4\u00d750 + 0.6\u00d74 = 22.4."}, {"text": "It is technically defined as \"the nth root product of n numbers.\" The geometric mean must be used when working with percentages, which are derived from values, while the standard arithmetic mean works with the values themselves. The harmonic mean is best used for fractions such as rates or multiples."}]}, {"question": "What is entropy in layman's terms", "positive_ctxs": [{"text": "The definition is: \"Entropy is a measure of how evenly energy is distributed in a system. In a physical system, entropy provides a measure of the amount of energy that cannot be used to do work.\""}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "joint entropy is the amount of information in two (or more) random variables; conditional entropy is the amount of information in one random variable given we already know the other."}, {"text": "Definition: Entropy is a measure of uncertainty of a random variable. The entropy of a discrete random variable X with alphabet X is H(X) = -) p(x) log p(2) DEX When the base of the logarithm is 2, entropy is measured in bits."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "At equilibrium, the change in entropy is zero, i.e., \u0394S=0 (at equilibrium)."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes.  The minimum surprise is when p = 0 or p = 1, when the event is known and the entropy is zero bits."}]}, {"question": "What is a word2vec model", "positive_ctxs": [{"text": "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "word2vec itself is a simple bi-layered neural network architecture, it turns text into meaningful vectors form that deeper networks can understand. In other words the out put of simple neural word2vec model is used as input for Deep Networks."}, {"text": "word2vec itself is a simple bi-layered neural network architecture, it turns text into meaningful vectors form that deeper networks can understand. In other words the out put of simple neural word2vec model is used as input for Deep Networks."}, {"text": "Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."}, {"text": "Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."}, {"text": "The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."}, {"text": "The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "Can deep learning be used for regression", "positive_ctxs": [{"text": "You can \"use\" deep learning for regression.  You can use a fully connected neural network for regression, just don't use any activation unit in the end (i.e. take out the RELU, sigmoid) and just let the input parameter flow-out (y=x)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bayesian inference is a machine learning model not as widely used as deep learning or regression models."}, {"text": "Fine-tuning deep learning involves using weights of a previous deep learning algorithm for programming another similar deep learning process. Weights are used to connect each neuron in one layer to every neuron in the next layer in the neural network."}, {"text": "The coefficients used in simple linear regression can be found using stochastic gradient descent.  Linear regression does provide a useful exercise for learning stochastic gradient descent which is an important algorithm used for minimizing cost functions by machine learning algorithms."}, {"text": "In distributed training the workload to train a model is split up and shared among multiple mini processors, called worker nodes.  Distributed training can be used for traditional ML models, but is better suited for compute and time intensive tasks, like deep learning for training deep neural networks."}, {"text": "Dictionary learning is learning a set of atoms so that a given image can be well approximated by a sparse linear combination of these learned atoms, while deep learning methods aim at extracting deep semantic feature representations via a deep network."}, {"text": "Fine-tuning, in general, means making small adjustments to a process to achieve the desired output or performance. Fine-tuning deep learning involves using weights of a previous deep learning algorithm for programming another similar deep learning process."}, {"text": "While machine learning uses simpler concepts, deep learning works with artificial neural networks, which are designed to imitate how humans think and learn.  It can be used to solve any pattern recognition problem and without human intervention. Artificial neural networks, comprising many layers, drive deep learning."}]}, {"question": "What happens when the sample size decreases", "positive_ctxs": [{"text": "The population mean of the distribution of sample means is the same as the population mean of the distribution being sampled from.  Thus as the sample size increases, the standard deviation of the means decreases; and as the sample size decreases, the standard deviation of the sample means increases."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The consistency of the sampling distribution is dependent on the sample size not on the distribution of the population. As the sample size decreases the absolute value of the skewness and kurtosis of the sampling distribution increases. This sample size relationship is expressed in the central limit theorem."}, {"text": "Yes, using the additional survey information from part\u200b (b) dramatically reduces the sample size. The required sample size decreases dramatically from 423 to 50\u200b, with the addition of the survey information."}, {"text": "Center: The center is not affected by sample size. The mean of the sample means is always approximately the same as the population mean \u00b5 = 3,500. Spread: The spread is smaller for larger samples, so the standard deviation of the sample means decreases as sample size increases."}, {"text": "Center: The center is not affected by sample size. The mean of the sample means is always approximately the same as the population mean \u00b5 = 3,500. Spread: The spread is smaller for larger samples, so the standard deviation of the sample means decreases as sample size increases."}, {"text": "Center: The center is not affected by sample size. The mean of the sample means is always approximately the same as the population mean \u00b5 = 3,500. Spread: The spread is smaller for larger samples, so the standard deviation of the sample means decreases as sample size increases."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "The Central limit Theorem states that when sample size tends to infinity, the sample mean will be normally distributed. The Law of Large Number states that when sample size tends to infinity, the sample mean equals to population mean."}]}, {"question": "What is are the outcome of optimizing the likelihood function", "positive_ctxs": [{"text": "Intuitively, this selects the parameter values that make the observed data most probable. The specific value that maximizes the likelihood function is called the maximum likelihood estimate. Further, if the function so defined is measurable, then it is called the maximum likelihood estimator."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Maximum likelihood estimation refers to using a probability model for data and optimizing the joint likelihood function of the observed data over one or more parameters.  Bayesian estimation is a bit more general because we're not necessarily maximizing the Bayesian analogue of the likelihood (the posterior density)."}, {"text": "The main difference between probability and likelihood is that the former is normalized.  Probability refers to the occurrence of future events, while a likelihood refers to past events with known outcomes. Probability is used when describing a function of the outcome given a fixed parameter value."}, {"text": "The likelihood function is given by: L(p|x) \u221dp4(1 \u2212 p)6. The likelihood of p=0.5 is 9.77\u00d710\u22124, whereas the likelihood of p=0.1 is 5.31\u00d710\u22125."}, {"text": "The likelihood function is given by: L(p|x) \u221dp4(1 \u2212 p)6. The likelihood of p=0.5 is 9.77\u00d710\u22124, whereas the likelihood of p=0.1 is 5.31\u00d710\u22125."}, {"text": "Prior probability shows the likelihood of an outcome in a given dataset. For example, in the mortgage case, P(Y) is the default rate on a home mortgage, which is 2%. P(Y|X) is called the conditional probability, which provides the probability of an outcome given the evidence, that is, when the value of X is known."}, {"text": "We write the likelihood function as L(\\theta;x)=\\prod^n_{i=1}f(X_i;\\theta) or sometimes just L(\u03b8). Algebraically, the likelihood L(\u03b8 ; x) is just the same as the distribution f(x ; \u03b8), but its meaning is quite different because it is regarded as a function of \u03b8 rather than a function of x."}, {"text": "Amortized VI is the idea that instead of optimizing a set of free parameters, we can introduce a parameterized function that maps from observation space to the parameters of the approximate posterior distribution."}]}, {"question": "What is dissimilarity measure", "positive_ctxs": [{"text": "Dissimilarity Measure Numerical measure of how different two data objects are range from 0 (objects are alike) to (objects are different) Proximity refers to a similarity or dissimilarity."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Similarity is a numerical measure of how alike two data objects are, and dissimilarity is a numerical measure of how different two data objects are.  We go into more data mining in our data science bootcamp, have a look."}, {"text": "The Dissimilarity matrix is a matrix that expresses the similarity pair to pair between two sets. It's square and symmetric. The diagonal members are defined as zero, meaning that zero is the measure of dissimilarity between an element and itself."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}]}, {"question": "Is r squared the same as correlation", "positive_ctxs": [{"text": "The correlation, denoted by r, measures the amount of linear association between two variables.  The R-squared value, denoted by R 2, is the square of the correlation. It measures the proportion of variation in the dependent variable that can be attributed to the independent variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The correlation coefficient is the specific measure that quantifies the strength of the linear relationship between two variables in a correlation analysis. The coefficient is what we symbolize with the r in a correlation report."}, {"text": "A coefficient of correlation of +0.8 or -0.8 indicates a strong correlation between the independent variable and the dependent variable. An r of +0.20 or -0.20 indicates a weak correlation between the variables."}, {"text": "Compare r to the appropriate critical value in the table. If r is not between the positive and negative critical values, then the correlation coefficient is significant. If r is significant, then you may want to use the line for prediction. Suppose you computed r=0.801 using n=10 data points."}, {"text": "Regression analysis refers to assessing the relationship between the outcome variable and one or more variables.  For example, a correlation of r = 0.8 indicates a positive and strong association among two variables, while a correlation of r = -0.3 shows a negative and weak association."}, {"text": "The SD line goes through the point of averages, and has slope equal to SDY/SDX if the correlation coefficient r is greater than or equal to zero. The SD line has slope \u2212SDY/SDX if r is negative.  The line slopes up to the right, because r is positive (0.5 at first)."}, {"text": "The correlation coefficient is a number that summarizes the direction and degree (closeness) of linear relations between two variables. The correlation coefficient is also known as the Pearson Product-Moment Correlation Coefficient. The sample value is called r, and the population value is called r (rho)."}, {"text": "A scatterplot displays the strength, direction, and form of the relationship between two quantitative variables. A correlation coefficient measures the strength of that relationship.  The correlation r measures the strength of the linear relationship between two quantitative variables."}]}, {"question": "What does robustness check mean", "positive_ctxs": [{"text": "A common exercise in empirical studies is a \u201crobustness check\u201d, where the researcher examines how certain \u201ccore\u201d regression coefficient estimates behave when the regression specification is modified by adding or removing regressors."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "The main difference between the t-test and f-test is, that t-test is used to test the hypothesis whether the given mean is significantly different from the sample mean or not. On the other hand, an F-test is used to compare the two standard deviations of two samples and check the variability."}, {"text": "The main difference between the t-test and f-test is, that t-test is used to test the hypothesis whether the given mean is significantly different from the sample mean or not. On the other hand, an F-test is used to compare the two standard deviations of two samples and check the variability."}]}, {"question": "What are acceptable values for skewness and kurtosis", "positive_ctxs": [{"text": "Both skew and kurtosis can be analyzed through descriptive statistics. Acceptable values of skewness fall between \u2212 3 and + 3, and kurtosis is appropriate from a range of \u2212 10 to + 10 when utilizing SEM (Brown, 2006)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Both skew and kurtosis can be analyzed through descriptive statistics. Acceptable values of skewness fall between \u2212 3 and + 3, and kurtosis is appropriate from a range of \u2212 10 to + 10 when utilizing SEM (Brown, 2006)."}, {"text": "Negative values for the skewness indicate data that are skewed left and positive values for the skewness indicate data that are skewed right. By skewed left, we mean that the left tail is long relative to the right tail."}, {"text": "The consistency of the sampling distribution is dependent on the sample size not on the distribution of the population. As the sample size decreases the absolute value of the skewness and kurtosis of the sampling distribution increases. This sample size relationship is expressed in the central limit theorem."}, {"text": "What are the five steps in the backpropagation learning algorithm?Initialize weights with random values and set other parameters.Read in the input vector and the desired output.Compute the actual output via the calculations, working forward through the layers."}, {"text": "At its most basic, machine learning uses programmed algorithms that receive and analyse input data to predict output values within an acceptable range. As new data is fed to these algorithms, they learn and optimise their operations to improve performance, developing 'intelligence' over time."}, {"text": "The sample kurtosis is a useful measure of whether there is a problem with outliers in a data set. Larger kurtosis indicates a more serious outlier problem, and may lead the researcher to choose alternative statistical methods."}, {"text": "The lognormal distribution is a distribution skewed to the right. The pdf starts at zero, increases to its mode, and decreases thereafter. The degree of skewness increases as increases, for a given . For the same , the pdf's skewness increases as increases."}]}, {"question": "What is the paired data assumption", "positive_ctxs": [{"text": "In a paired sample t-test, the observations are defined as the differences between two sets of values, and each assumption refers to these differences, not the original data values. The paired sample t-test has four main assumptions:  The observations are independent of one another."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A significant result indicates that your data are significantly heteroscedastic, and thus the assumption of homoscedasticity in the regression residuals is violated. In your case the data violate the assumption of homoscedasticity, as your p value is 8.6\u22c510\u221228. The e is standard scientific notation for powers of 10."}, {"text": "As discussed above, these two tests should be used for different data structures. Two-sample t-test is used when the data of two samples are statistically independent, while the paired t-test is used when data is in the form of matched pairs."}, {"text": "When comparing two groups, you need to decide whether to use a paired test. When comparing three or more groups, the term paired is not apt and the term repeated measures is used instead. Use an unpaired test to compare groups when the individual values are not paired or matched with one another."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "The random effects assumption is that the individual-specific effects are uncorrelated with the independent variables. The fixed effect assumption is that the individual-specific effects are correlated with the independent variables."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The key assumption in ordinal regression is that the effects of any explanatory variables are consistent or proportional across the different thresholds, hence this is usually termed the assumption of proportional odds (SPSS calls this the assumption of parallel lines but it's the same thing)."}]}, {"question": "What is the difference between sampling error and margin of error", "positive_ctxs": [{"text": "Sampling error is one of two reasons for the difference between an estimate and the true, but unknown, value of the population parameter.  The sampling error for a given sample is unknown but when the sampling is random, the maximum likely size of the sampling error is called the margin of error."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The relationship between margin of error and sample size is simple: As the sample size increases, the margin of error decreases.  If you think about it, it makes sense that the more information you have, the more accurate your results are going to be (in other words, the smaller your margin of error will get)."}, {"text": "The margin of error is a statistic expressing the amount of random sampling error in the results of a survey. The larger the margin of error, the less confidence one should have that a poll result would reflect the result of a survey of the entire population."}, {"text": "An error term represents the margin of error within a statistical model; it refers to the sum of the deviations within the regression line, which provides an explanation for the difference between the theoretical value of the model and the actual observed results."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The Slovin's Formula is given as follows: n = N/(1+Ne2), where n is the sample size, N is the population size and e is the margin of error to be decided by the researcher."}, {"text": "Training loss is the error on the training set of data. Validation loss is the error after running the validation set of data through the trained network. Train/valid is the ratio between the two. Unexpectedly, as the epochs increase both validation and training error drop."}]}, {"question": "What loss is used for binary classification", "positive_ctxs": [{"text": "For a binary classification like our example, the typical loss function is the binary cross-entropy / log loss."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "For a binary classification like our example, the typical loss function is the binary cross-entropy / log loss."}, {"text": "Binary, multi-class and multi-label classification Cross-entropy is a commonly used loss function for classification tasks."}, {"text": "We use binary cross-entropy loss for classification models which output a probability p. The range of the sigmoid function is [0, 1] which makes it suitable for calculating probability."}, {"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss."}, {"text": "In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. A variant for classification is also sometimes used."}, {"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'."}, {"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'."}]}, {"question": "Why does the sample size required change with effect size", "positive_ctxs": [{"text": "A greater power requires a larger sample size. Effect size \u2013 This is the estimated difference between the groups that we observe in our sample. To detect a difference with a specified power, a smaller effect size will require a larger sample size."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The larger the sample size is the smaller the effect size that can be detected. The reverse is also true; small sample sizes can detect large effect sizes.  Thus an appropriate determination of the sample size used in a study is a crucial step in the design of a study."}, {"text": "Yes, using the additional survey information from part\u200b (b) dramatically reduces the sample size. The required sample size decreases dramatically from 423 to 50\u200b, with the addition of the survey information."}, {"text": "Effect size is a simple way of quantifying the difference between two groups that has many advantages over the use of tests of statistical significance alone. Effect size emphasises the size of the difference rather than confounding this with sample size.  A number of alternative measures of effect size are described."}, {"text": "The main aim of a sample size calculation is to determine the number of participants needed to detect a clinically relevant treatment effect. Pre-study calculation of the required sample size is warranted in the majority of quantitative studies."}, {"text": "The sample means do not vary as much as the individual values in the population.  As the sample size increases, the effect of a single extreme value becomes smaller because it is averaged with more values."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "The effect size is the main finding of a quantitative study. While a P value can inform the reader whether an effect exists, the P value will not reveal the size of the effect."}]}, {"question": "What does cross correlation tell you", "positive_ctxs": [{"text": "Cross correlation presents a technique for comparing two time series and finding objectively how they match up with each other, and in particular where the best match occurs. It can also reveal any periodicities in the data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The cross product is a calculation used in order to define the correlation coefficient between two variables. SP is the sum of all cross products between two variables."}, {"text": "The product moment correlation coefficient (pmcc) can be used to tell us how strong the correlation between two variables is. A positive value indicates a positive correlation and the higher the value, the stronger the correlation.  If there is a perfect negative correlation, then r = -1."}, {"text": "As you experiment with your algorithm to try and improve your model, your loss function will tell you if you're getting(or reaching) anywhere. At its core, a loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value)."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}]}, {"question": "What does the U symbol mean in probability", "positive_ctxs": [{"text": "The conditional probability of Event A, given Event B, is denoted by the symbol P(A|B).  The probability that Events A or B occur is the probability of the union of A and B. The probability of the union of Events A and B is denoted by P(A \u222a B) ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "The mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean \u03bc, then the mean of the sampling distribution of the mean is also \u03bc. The symbol \u03bcM is used to refer to the mean of the sampling distribution of the mean."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}]}, {"question": "What is Unigrams and Bigrams in Python", "positive_ctxs": [{"text": "A 1-gram (or unigram) is a one-word sequence.  A 2-gram (or bigram) is a two-word sequence of words, like \u201cI love\u201d, \u201clove reading\u201d, or \u201cAnalytics Vidhya\u201d. And a 3-gram (or trigram) is a three-word sequence of words like \u201cI love reading\u201d, \u201cabout data science\u201d or \u201con Analytics Vidhya\u201d."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Python is easy to learn and work with, and provides convenient ways to express how high-level abstractions can be coupled together. Nodes and tensors in TensorFlow are Python objects, and TensorFlow applications are themselves Python applications. The actual math operations, however, are not performed in Python."}, {"text": "Most machine learning roles will require the use of Python or C/C++ (though Python is often preferred). Background in the theory behind machine learning algorithms and an understanding of how they can be efficiently implemented in terms of both space and time is critical."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "To learn this course one needs to have enough knowledge in Python and its libraries such as NumPy, Matplotlib, Jupyter, and TensorFlow. Also, this course requires Python 3.5 or Python 3.6. Click here to learn."}, {"text": "Keras is a neural networks library written in Python that is high-level in nature \u2013 which makes it extremely simple and intuitive to use. It works as a wrapper to low-level libraries like TensorFlow or Theano high-level neural networks library, written in Python that works as a wrapper to TensorFlow or Theano."}, {"text": "TensorFlow is Google's open source AI framework for machine learning and high performance numerical computation. TensorFlow is a Python library that invokes C++ to construct and execute dataflow graphs. It supports many classification and regression algorithms, and more generally, deep learning and neural networks."}]}, {"question": "What is bootstrapping in reinforcement learning", "positive_ctxs": [{"text": "Bootstrapping: When you estimate something based on another estimation. In the case of Q-learning for example this is what is happening when you modify your current reward estimation rt by adding the correction term maxa\u2032Q(s\u2032,a\u2032) which is the maximum of the action value over all actions of the next state."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "The example of reinforcement learning is your cat is an agent that is exposed to the environment. The biggest characteristic of this method is that there is no supervisor, only a real number or reward signal. Two types of reinforcement learning are 1) Positive 2) Negative."}, {"text": "State\u2013action\u2013reward\u2013state\u2013action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning.  The acronym for the quintuple (st, at, rt, st+1, at+1) is SARSA."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "From Wikipedia, the free encyclopedia. Error-driven learning is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback. It is a type of reinforcement learning."}, {"text": "And, unsupervised learning is where the machine is given training based on unlabeled data without any guidance.  Whereas reinforcement learning is when a machine or an agent interacts with its environment, performs actions, and learns by a trial-and-error method."}, {"text": "Deep reinforcement learning is a promising combination between two artificial intelligence techniques: reinforcement learning, which uses sequential trial and error to learn the best action to take in every situation, and deep learning, which can evaluate complex inputs and select the best response."}]}, {"question": "What is an example of inferential statistics", "positive_ctxs": [{"text": "With inferential statistics, you take data from samples and make generalizations about a population. For example, you might stand in a mall and ask a sample of 100 people if they like shopping at Sears.  This is where you can use sample data to answer research questions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An example of statistics is a report of numbers saying how many followers of each religion there are in a particular country. An example of statistics is a math class offered in high schools and colleges. The definition of a statistic is a number, or a person who is an unnamed piece of data to be studied."}, {"text": "Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables."}, {"text": "Explain the difference between descriptive and inferential statistics. Descriptive statistics describes sets of data. Inferential statistics draws conclusions about the sets of data based on sampling.  A population is a set of units of interest to a study."}, {"text": "When analysing data, such as the grades earned by 100 students, it is possible to use both descriptive and inferential statistics in your analysis. Typically, in most research conducted on groups of people, you will use both descriptive and inferential statistics to analyse your results and draw conclusions."}, {"text": "Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population."}, {"text": "Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures.  Descriptive statistics are typically distinguished from inferential statistics. With descriptive statistics you are simply describing what is or what the data shows."}, {"text": "Inferential statistics lets you draw conclusions about populations by using small samples. Consequently, inferential statistics provide enormous benefits because typically you can't measure an entire population."}]}, {"question": "Can you use continuous variables in logistic regression", "positive_ctxs": [{"text": "In logistic regression, as with any flavour of regression, it is fine, indeed usually better, to have continuous predictors. Given a choice between a continuous variable as a predictor and categorising a continuous variable for predictors, the first is usually to be preferred."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes."}, {"text": "Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used."}, {"text": "Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Multinomial logistic regression is used to predict categorical placement in or the probability of category membership on a dependent variable based on multiple independent variables. The independent variables can be either dichotomous (i.e., binary) or continuous (i.e., interval or ratio in scale)."}, {"text": "Multinomial logistic regression is used to predict categorical placement in or the probability of category membership on a dependent variable based on multiple independent variables. The independent variables can be either dichotomous (i.e., binary) or continuous (i.e., interval or ratio in scale)."}]}, {"question": "How does R squared change if dependent and independent variables are switched in an ordinary linear regression", "positive_ctxs": [{"text": "If you have only one independent variable, R-squared(R2) remains the same. Because in single variable linear model - R2 is nothing but a square of the correlation between two variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Logistic regression is quite different than linear regression in that it does not make several of the key assumptions that linear and general linear models (as well as other ordinary least squares algorithm based models) hold so close: (1) logistic regression does not require a linear relationship between the dependent"}, {"text": "Logistic regression assumes linearity of independent variables and log odds. Although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds."}, {"text": "Ordinary linear squares (OLS) regression compares the response of a dependent variable given a change in some explanatory variables.  Multiple regressions are based on the assumption that there is a linear relationship between both the dependent and independent variables."}, {"text": "First, linear regression needs the relationship between the independent and dependent variables to be linear. It is also important to check for outliers since linear regression is sensitive to outlier effects.  Multicollinearity occurs when the independent variables are too highly correlated with each other."}, {"text": "We say that X and Y are independent if P(X=x,Y=y)=P(X=x)P(Y=y), for all x,y.  Intuitively, two random variables X and Y are independent if knowing the value of one of them does not change the probabilities for the other one. In other words, if X and Y are independent, we can write P(Y=y|X=x)=P(Y=y), for all x,y."}, {"text": "Intuitively, two random variables X and Y are independent if knowing the value of one of them does not change the probabilities for the other one. In other words, if X and Y are independent, we can write P(Y=y|X=x)=P(Y=y), for all x,y."}, {"text": "As the name suggests, GLM models are the generalization of the linear regression model.  we mean that rather than forcing a linear relationship between the dependent and independent variables, it allows the dependent variable to be related with the independent variables through a link function."}]}, {"question": "What is Perceptron in neural network", "positive_ctxs": [{"text": "In the context of neural networks, a perceptron is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks. Perceptron is a linear classifier (binary). Also, it is used in supervised learning. It helps to classify the given input data."}, {"text": "A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features or business intelligence in the input data. Perceptron was introduced by Frank Rosenblatt in 1957.  A Perceptron is an algorithm for supervised learning of binary classifiers."}, {"text": "1. Why is the XOR problem exceptionally interesting to neural network researchers?  Explanation: Linearly separable problems of interest of neural network researchers because they are the only class of problem that Perceptron can solve successfully."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Multilayer Perceptron (MLP) MLP is a deep learning method. A multilayer perceptron is a neural network connecting multiple layers in a directed graph, which means that the signal path through the nodes only goes one way. Each node, apart from the input nodes, has a nonlinear activation function."}, {"text": "A perceptron is a simple model of a biological neuron in an artificial neural network. Perceptron is also the name of an early algorithm for supervised learning of binary classifiers."}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}]}, {"question": "What are some binary classification algorithms", "positive_ctxs": [{"text": "Some of the methods commonly used for binary classification are:Decision trees.Random forests.Bayesian networks.Support vector machines.Neural networks.Logistic regression.Probit model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Popular algorithms that can be used for binary classification include:Logistic Regression.k-Nearest Neighbors.Decision Trees.Support Vector Machine.Naive Bayes."}, {"text": "For a binary classification like our example, the typical loss function is the binary cross-entropy / log loss."}, {"text": "all provides a way to leverage binary classification. -all solution consists of N separate binary classifiers\u2014one binary classifier for each possible outcome.  During training, the model runs through a sequence of binary classifiers, training each to answer a separate classification question."}, {"text": "Statistical binary classification It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. When there are only two categories the problem is known as statistical binary classification."}, {"text": "In machine learning we are trying to create solutions to some problem by using data or examples.  Genetic algorithms are stochastic search algorithms which are often used in machine learning applications."}, {"text": "Yes, it is ok to run a Pearson r correlation using two binary coded variables*. Pearson r has a special name in that situation (phi coefficient). There are some special issues when you look at correlations between binary or dichotomous variables."}, {"text": "There are two main methods for tackling a multi-label classification problem: problem transformation methods and algorithm adaptation methods. Problem transformation methods transform the multi-label problem into a set of binary classification problems, which can then be handled using single-class classifiers."}]}, {"question": "How do you prevent algorithmic bias", "positive_ctxs": [{"text": "How to prevent machine biasUse a representative dataset. Feeding your algorithm representative data is THE most important aspect when it comes to preventing bias in machine learning.  Choose the right model. Every AI algorithm is unique and there is no single model that can be used to avoid bias.  Monitor and review."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Five tips to prevent confirmation bias Encourage and carefully consider critical views on the working hypothesis. Ensure that all stakeholders examine the primary data. Do not rely on analysis and summary from a single individual. Design experiments to actually test the hypothesis."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Confirmation bias can make people less likely to engage with information which challenges their views.  Even when people do get exposed to challenging information, confirmation bias can cause them to reject it and, perversely, become even more certain that their own beliefs are correct."}]}, {"question": "Where do we use large numbers in real life", "positive_ctxs": [{"text": "Large numbers are numbers that are significantly larger than those typically used in everyday life, for instance in simple counting or in monetary transactions.  Very large numbers often occur in fields such as mathematics, cosmology, cryptography, and statistical mechanics.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An algorithm that uses random numbers to decide what to do next anywhere in its logic is called a Randomized Algorithm. For example, in Randomized Quick Sort, we use a random number to pick the next pivot (or we randomly shuffle the array). And in Karger's algorithm, we randomly pick an edge."}, {"text": "Factor analysis is a technique that is used to reduce a large number of variables into fewer numbers of factors. This technique extracts maximum common variance from all variables and puts them into a common score. As an index of all variables, we can use this score for further analysis."}, {"text": "The law of averages is not a mathematical principle, whereas the law of large numbers is. In probability theory, the law of large numbers is a theorem that describes the result of performing the same experiment a large number of times."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "The law of large numbers is a theorem from probability and statistics that suggests that the average result from repeating an experiment multiple times will better approximate the true or expected underlying result. The law of large numbers explains why casinos always make money in the long run."}, {"text": "Word Embeddings or Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics. The process of converting words into numbers are called Vectorization."}, {"text": "Disadvantages of randomised control trial study designTrials which test for efficacy may not be widely applicable. Trials which test for effectiveness are larger and more expensive.Results may not always mimic real life treatment situation (e.g. inclusion / exclusion criteria; highly controlled setting)"}]}, {"question": "What is the difference between discrete and continuous random variables", "positive_ctxs": [{"text": "\"A discrete variable is one that can take on finitely many, or countably infinitely many values\", whereas a continuous random variable is one that is not discrete, i.e. \"can take on uncountably infinitely many values\", such as a spectrum of real numbers."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A discrete distribution is a statistical distribution that shows the probabilities of discrete (countable) outcomes, such as 1, 2, 3  Overall, the concepts of discrete and continuous probability distributions and the random variables they describe are the underpinnings of probability theory and statistical analysis."}, {"text": "If a variable can take on any value between two specified values, it is called a continuous variable; otherwise, it is called a discrete variable. Some examples will clarify the difference between discrete and continuous variables.  The number of heads could be any integer value between 0 and plus infinity."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "The probability of each value of the discrete random variable is between 0 and\u200b 1, inclusive, and the sum of all the probabilities is 1.  It is the expected value of a discrete random variable."}, {"text": "A random variable can be either discrete (having specific values) or continuous (any value in a continuous range). The use of random variables is most common in probability and statistics, where they are used to quantify outcomes of random occurrences."}]}, {"question": "How do you reduce high bias in a neural net classifier", "positive_ctxs": [{"text": "Methods to Avoid Underfitting in Neural Networks\u2014Adding Parameters, Reducing Regularization ParameterAdding neuron layers or input parameters.  Adding more training samples, or improving their quality.  Dropout.  Decreasing regularization parameter."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "a. If your data is labeled, but you only have a limited amount, you should use a classifier with high bias (for example, Naive Bayes). I'm guessing this is because a higher-bias classifier will have lower variance, which is good because of the small amount of data."}, {"text": "Response bias can be defined as the difference between the true values of variables in a study's net sample group and the values of variables obtained in the results of the same study.  Nonresponse bias occurs when some respondents included in the sample do not respond."}, {"text": "Response bias can be defined as the difference between the true values of variables in a study's net sample group and the values of variables obtained in the results of the same study.  Nonresponse bias occurs when some respondents included in the sample do not respond."}, {"text": "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use."}, {"text": "Back-propagation is the essence of neural net training.  It is the practice of fine-tuning the weights of a neural net based on the error rate (i.e. loss) obtained in the previous epoch (i.e. iteration)."}, {"text": "They are defined as follows: Bias: Bias describes how well a model matches the training set. A model with high bias won't match the data set closely, while a model with low bias will match the data set very closely.  Typically models with high bias have low variance, and models with high variance have low bias."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}]}, {"question": "What is Maximin and Minimax principle", "positive_ctxs": [{"text": "zero-sum game: A zero-sum game is one in which the sum of the individual payoffs for each outcome is zero. Minimax strategy: minimizing one's own maximum loss. Maximin strategy: maximize one's own minimum gain."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Minimax is a kind of backtracking algorithm that is used in decision making and game theory to find the optimal move for a player, assuming that your opponent also plays optimally.  In Minimax the two players are called maximizer and minimizer."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Heisenberg's uncertainty principle is a key principle in quantum mechanics. Very roughly, it states that if we know everything about where a particle is located (the uncertainty of position is small), we know nothing about its momentum (the uncertainty of momentum is large), and vice versa."}, {"text": "Minimax GAN loss refers to the minimax simultaneous optimization of the discriminator and generator models. Minimax refers to an optimization strategy in two-player turn-based games for minimizing the loss or cost for the worst case of the other player."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Minimax is a kind of backtracking algorithm that is used in decision making and game theory to find the optimal move for a player, assuming that your opponent also plays optimally. It is widely used in two player turn-based games such as Tic-Tac-Toe, Backgammon, Mancala, Chess, etc."}]}, {"question": "What is loss in neural network", "positive_ctxs": [{"text": "The Loss Function is one of the important components of Neural Networks. Loss is nothing but a prediction error of Neural Net. And the method to calculate the loss is called Loss Function. In simple words, the Loss is used to calculate the gradients. And gradients are used to update the weights of the Neural Net."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Back-propagation is just a way of propagating the total loss back into the neural network to know how much of the loss every node is responsible for, and subsequently updating the weights in such a way that minimizes the loss by giving the nodes with higher error rates lower weights and vice versa."}, {"text": "Back-propagation is just a way of propagating the total loss back into the neural network to know how much of the loss every node is responsible for, and subsequently updating the weights in such a way that minimizes the loss by giving the nodes with higher error rates lower weights and vice versa."}, {"text": "Back-propagation is just a way of propagating the total loss back into the neural network to know how much of the loss every node is responsible for, and subsequently updating the weights in such a way that minimizes the loss by giving the nodes with higher error rates lower weights and vice versa."}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "It is calculated in the same way - by running the network forward over inputs xi and comparing the network outputs \u02c6yi with the ground truth values yi using a loss function e.g. J=1N\u2211Ni=1L(\u02c6yi,yi) where L is the individual loss function based somehow on the difference between predicted value and target."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.  Often the weights of a neural network are contained within the hidden layers of the network."}]}, {"question": "What is data binning in statistics", "positive_ctxs": [{"text": "Statistical data binning is a way to group numbers of more or less continuous values into a smaller number of \"bins\". For example, if you have data about a group of people, you might want to arrange their ages into a smaller number of age intervals (for example, grouping every five years together)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The optimal binning is the optimal discretization of a variable into bins given a discrete or continuous numeric target."}, {"text": "Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Descriptive statistics summarize the characteristics of a data set. Inferential statistics allow you to test a hypothesis or assess whether your data is generalizable to the broader population."}]}, {"question": "What is a statistically valid sample size", "positive_ctxs": [{"text": "Statistically Valid Sample Size Criteria Probability or percentage: The percentage of people you expect to respond to your survey or campaign. Confidence: How confident you need to be that your data is accurate. Expressed as a percentage, the typical value is 95% or 0.95."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "If the study is based on a very large sample size, relationships found to be statistically significant may not have much practical significance. Almost any null hypothesis can be rejected if the sample size is large enough."}, {"text": "The rank-sum test is a non-parametric hypothesis test that can be used to determine if there is a statistically significant association between categorical survey responses provided for two different survey questions. The use of this test is appropriate even when survey sample size is small."}, {"text": "The larger the sample size is the smaller the effect size that can be detected. The reverse is also true; small sample sizes can detect large effect sizes.  Thus an appropriate determination of the sample size used in a study is a crucial step in the design of a study."}, {"text": "The main aim of a sample size calculation is to determine the number of participants needed to detect a clinically relevant treatment effect. Pre-study calculation of the required sample size is warranted in the majority of quantitative studies."}, {"text": "A P value is also affected by sample size and the magnitude of effect. Generally the larger the sample size, the more likely a study will find a significant relationship if one exists. As the sample size increases the impact of random error is reduced."}, {"text": "Typically by the time the sample size is 30 the distribution of the sample mean is practically the same as a normal distribution.  \u00afX, the mean of the measurements in a sample of size n; the distribution of \u00afX is its sampling distribution, with mean \u03bc\u00afX=\u03bc and standard deviation \u03c3\u00afX=\u03c3\u221an."}]}, {"question": "What is a zeroth moment in statistics", "positive_ctxs": [{"text": "If the function is a probability distribution, then the zeroth moment is the total probability (i.e. one), the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The mean value of x is thus the first moment of its distribution, while the fact that the probability distribution is normalized means that the zeroth moment is always 1.  The variance of x is thus the second central moment of the probability distribution when xo is the mean value or first moment."}, {"text": "For example, the first moment is the expected value E[X]. The second central moment is the variance of X. Similar to mean and variance, other moments give useful information about random variables. The moment generating function (MGF) of a random variable X is a function MX(s) defined as MX(s)=E[esX]."}, {"text": "The \u201cmoments\u201d of a random variable (or of its distribution) are expected values of powers or related functions of the random variable. The rth moment of X is E(Xr). In particular, the first moment is the mean, \u00b5X = E(X). The mean is a measure of the \u201ccenter\u201d or \u201clocation\u201d of a distribution."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The term \u201cmultivariate statistics\u201d is appropriately used to include all statistics where there are more than two variables simultaneously analyzed. You are already familiar with bivariate statistics such as the Pearson product moment correlation coefficient and the independent groups t-test."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "The first four are: 1) The mean, which indicates the central tendency of a distribution. 2) The second moment is the variance, which indicates the width or deviation. 3) The third moment is the skewness, which indicates any asymmetric 'leaning' to either left or right."}]}, {"question": "What is Pit mutation testing", "positive_ctxs": [{"text": "PIT is a state of the art mutation testing system, providing gold standard test coverage for Java and the jvm. It's fast, scalable and integrates with modern test and build tooling."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived."}, {"text": "Mathematically test efficiency is calculated as a percentage of the number of alpha testing (in-house or on-site) defects divided by sum of a number of alpha testing and a number of beta testing (off-site) defects."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}]}, {"question": "What is the effect of choosing a small \u03b1 in gradient descent", "positive_ctxs": [{"text": "Pick a value for the learning rate \u03b1. The learning rate determines how big the step would be on each iteration. If \u03b1 is very small, it would take long time to converge and become computationally expensive. If \u03b1 is large, it may fail to converge and overshoot the minimum."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning."}, {"text": "According to a senior data scientist, one of the distinct advantages of using Stochastic Gradient Descent is that it does the calculations faster than gradient descent and batch gradient descent. However, gradient descent is the best approach if one wants a speedier result."}, {"text": "The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated. The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset."}, {"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. One cycle through the entire training dataset is called a training epoch."}, {"text": "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."}, {"text": "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."}]}, {"question": "What type of data are required if using Pearson's correlation coefficient", "positive_ctxs": [{"text": "Correlation is a technique for investigating the relationship between two quantitative, continuous variables, for example, age and blood pressure. Pearson's correlation coefficient (r) is a measure of the strength of the association between the two variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are three big-picture methods to understand if a continuous and categorical are significantly correlated \u2014 point biserial correlation, logistic regression, and Kruskal Wallis H Test. The point biserial correlation coefficient is a special case of Pearson's correlation coefficient."}, {"text": "The Pearson's correlation coefficient is calculated as the covariance of the two variables divided by the product of the standard deviation of each data sample. It is the normalization of the covariance between the two variables to give an interpretable score."}, {"text": "The Pearson's correlation coefficient is calculated as the covariance of the two variables divided by the product of the standard deviation of each data sample. It is the normalization of the covariance between the two variables to give an interpretable score."}, {"text": "Because the coefficient of determination is the result of squaring the correlation coefficient, the coefficient of determination cannot be negative. (Even if the correlation is negative, squaring it will result in a positive number.)"}, {"text": "Pearson's correlation coefficient is the test statistics that measures the statistical relationship, or association, between two continuous variables.  It gives information about the magnitude of the association, or correlation, as well as the direction of the relationship."}, {"text": "Hello every one, We know that Pearson linear correlation coefficient gives the strength of linear relationship, while Spearman rank correlation coefficient gives the strength of monotonic relationship between two variables."}, {"text": "According to the (Research Methods for Business Students) book, to assess the relationship between two ordinal variables is by using Spearman's rank correlation coefficient (Spearman's rho) or Kendall's rank-order correlation coefficient (Kendall's tau)."}]}, {"question": "How does Markov model work", "positive_ctxs": [{"text": "How a Markov Model Works | Fantastic!  \u201cA Markov model is a stochastic model used to model randomly changing systems where it is assumed that future states depend only on the current state not on the events that occurred before it (that is, it assumes the Markov property)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Hidden Markov model"}, {"text": "The Hidden Markov Model (HMM) is a relatively simple way to model sequential data. A hidden Markov model implies that the Markov Model underlying the data is hidden or unknown to you. More specifically, you only know observational data and not information about the states."}, {"text": "In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph.  The underlying graph of a Markov random field may be finite or infinite."}, {"text": "Markov model is a state machine with the state changes being probabilities. In a hidden Markov model, you don't know the probabilities, but you know the outcomes."}, {"text": "Markovian is an adjective that may describe: In probability theory and statistics, subjects named for Andrey Markov: A Markov chain or Markov process, a stochastic model describing a sequence of possible events. The Markov property, the memoryless property of a stochastic process."}, {"text": "Markov chains are used in a broad variety of academic fields, ranging from biology to economics. When predicting the value of an asset, Markov chains can be used to model the randomness. The price is set by a random factor which can be determined by a Markov chain."}, {"text": "Markov chains are used in a broad variety of academic fields, ranging from biology to economics. When predicting the value of an asset, Markov chains can be used to model the randomness. The price is set by a random factor which can be determined by a Markov chain."}]}, {"question": "What s so special about rectified linear units ReLU activation function", "positive_ctxs": [{"text": "The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.  The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The rectifier is, as of 2017, the most popular activation function for deep neural networks. A unit employing the rectifier is also called a rectified linear unit (ReLU)."}, {"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.  The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better."}, {"text": "ReLU has become the darling activation function of the neural network world. Short for Rectified Linear Unit, it is a piecewise linear function that is defined to be 0 for all negative values of x and equal to a \u00d7 x otherwise, where a is a learnable parameter."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "ReLU is an activation function, that nullifies negative neurons, and in its simplicity, it also aids computation speed. However, unlike ELU, it doesn't have a normalizing effect, so BatchNorm helps even better."}, {"text": "ReLU provides just enough non-linearity so that it is nearly as simple as a linear activation, but this non-linearity opens the door for extremely complex representations. Because unlike in the linear case, the more you stack non-linear ReLUs, the more it becomes non-linear."}]}, {"question": "Why are techniques like cluster sampling and systematic sampling just as externally valid as simple random sampling", "positive_ctxs": [{"text": "They all contain elements of random selection. They all measure every member of the population of interest.  They all contain elements of random selection."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "The main types of probability sampling methods are simple random sampling, stratified sampling, cluster sampling, multistage sampling, and systematic random sampling."}, {"text": "In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).  In cluster sampling, the sampling unit is the whole cluster; Instead of sampling individuals from within each group, a researcher will study whole clusters."}, {"text": "Cluster sampling is best used when the clusters occur naturally in a population, when you don't have access to the entire population, and when the clusters are geographically convenient. However, cluster sampling is not as precise as simple random sampling or stratified random sampling."}, {"text": "Cluster Sampling: Advantages and Disadvantages Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}]}, {"question": "What is machine learning and its applications", "positive_ctxs": [{"text": "Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data."}, {"text": "And, unsupervised learning is where the machine is given training based on unlabeled data without any guidance.  Whereas reinforcement learning is when a machine or an agent interacts with its environment, performs actions, and learns by a trial-and-error method."}, {"text": "TensorFlow 2.0 is an updated version of TensorFlow that has been designed with a focus on simple execution, ease of use, and developer's productivity. TensorFlow 2.0 makes the development of machine learning applications even easier."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "Is skewed distribution a normal distribution", "positive_ctxs": [{"text": "In a normal distribution, the mean and the median are the same number while the mean and median in a skewed distribution become different numbers: A left-skewed, negative distribution will have the mean to the left of the median. A right-skewed distribution will have the mean to the right of the median."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A left-skewed distribution has a long left tail.  The normal distribution is the most common distribution you'll come across. Next, you'll see a fair amount of negatively skewed distributions. For example, household income in the U.S. is negatively skewed with a very long left tail."}, {"text": "Insufficient Data can cause a normal distribution to look completely scattered.  An extreme example: if you choose three random students and plot the results on a graph, you won't get a normal distribution. You might get a uniform distribution (i.e. 62 62 63) or you might get a skewed distribution (80 92 99)."}, {"text": "In a normal distribution, the mean and the median are the same number while the mean and median in a skewed distribution become different numbers: A left-skewed, negative distribution will have the mean to the left of the median. A right-skewed distribution will have the mean to the right of the median."}, {"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}, {"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}, {"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}, {"text": "The first method involves the conditional distribution of a random variable X2 given X1. Therefore, a bivariate normal distribution can be simulated by drawing a random variable from the marginal normal distribution and then drawing a second random variable from the conditional normal distribution."}]}, {"question": "What are the goals techniques and progress of AI", "positive_ctxs": [{"text": "The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "AI has the potential to accelerate the process of achieving the global education goals through reducing barriers to access learning, automating management processes, and optimizing methods in order to improve learning outcomes."}, {"text": "Computational Learning Theory (CoLT) is a field of AI research studying the design of machine learning algorithms to determine what sorts of problems are \u201clearnable.\u201d The ultimate goals are to understand the theoretical underpinnings of deep learning programs, what makes them work or not, while improving accuracy and"}, {"text": "Explainable AI (XAI) refers to methods and techniques in the application of artificial intelligence technology (AI) such that the results of the solution can be understood by humans."}, {"text": "S-Curves are used to visualize the progress of a project over time. They plot either cumulative work, based on person-hours, or costs over time. The name is derived from the fact that the data usually takes on an S-shape, with slower progress at the beginning and end of a project."}, {"text": "String theory has not failed, and there has been progress since 1999. It's just that it's a pretty abstract field of research, so it's hard to describe the recent progress in an accessible and understandable way."}, {"text": "AI techniques work to gather information about the environment, interpret the information, model the information and use the same to make effective decisions which are acted upon."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "Can Lstm be used for classification", "positive_ctxs": [{"text": "In general, an LSTM can be used for classification or regression; it is essentially just a standard neural network that takes as input, in addition to input from that time step, a hidden state from the previous time step. So, just as a NN can be used for classification or regression, so can an LSTM."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Popular algorithms that can be used for binary classification include:Logistic Regression.k-Nearest Neighbors.Decision Trees.Support Vector Machine.Naive Bayes."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Yes. We Can Always Use The Normal Distribution To Approximate The Binomial Distribution."}, {"text": "\u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems.  Support Vectors are simply the co-ordinates of individual observation."}, {"text": "Binary, multi-class and multi-label classification Cross-entropy is a commonly used loss function for classification tasks."}, {"text": "The Gaussian Processes Classifier is a classification machine learning algorithm. Gaussian Processes are a generalization of the Gaussian probability distribution and can be used as the basis for sophisticated non-parametric machine learning algorithms for classification and regression."}, {"text": "Advantages and disadvantagesAre simple to understand and interpret.  Have value even with little hard data.  Help determine worst, best and expected values for different scenarios.Use a white box model.  Can be combined with other decision techniques."}]}, {"question": "What is Bayesian model averaging", "positive_ctxs": [{"text": "Bayesian model averaging (BMA) is an extension of the usual Bayesian inference methods in which one does not only describe parameter uncertainty through the prior distribution but also model uncertainty obtaining posterior distributions for model parameters and for the model themselves using Bayes' theorem, allowing"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "3:537:13Suggested clip \u00b7 71 secondsStatistics With R - 4.4.3C - Bayesian model averaging - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}, {"text": "Bayesian Model Averaging (BMA) is an application of Bayesian inference to the problems of model selection, combined estimation and prediction that produces a straightforward model choice criteria and less risky predictions."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Bayesian inference is a machine learning model not as widely used as deep learning or regression models."}, {"text": "\"A Bayesian network is a probabilistic graphical model which represents a set of variables and their conditional dependencies using a directed acyclic graph.\"  It is also called a Bayes network, belief network, decision network, or Bayesian model."}, {"text": "\"A Bayesian network is a probabilistic graphical model which represents a set of variables and their conditional dependencies using a directed acyclic graph.\"  It is also called a Bayes network, belief network, decision network, or Bayesian model."}, {"text": "\"A Bayesian network is a probabilistic graphical model which represents a set of variables and their conditional dependencies using a directed acyclic graph.\"  It is also called a Bayes network, belief network, decision network, or Bayesian model."}]}, {"question": "What is the appropriate statistical test for comparing the data collected from two groups when the independent variable is categorical nominal and the dependent variable is continuous interval or ratio", "positive_ctxs": [{"text": "A one-way analysis of variance (ANOVA) is used when you have a categorical independent variable (with two or more categories) and a normally distributed interval dependent variable and you wish to test for differences in the means of the dependent variable broken down by the levels of the independent variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Discriminant analysis is a technique that is used by the researcher to analyze the research data when the criterion or the dependent variable is categorical and the predictor or the independent variable is interval in nature."}, {"text": "ANCOVA and multiple linear regression are similar, but regression is more appropriate when the emphasis is on the dependent outcome variable, while ANCOVA is more appropriate when the emphasis is on comparing the groups from one of the independent variables."}, {"text": "Similar to the t-test/correlation equivalence, the relationship between two dichotomous variables is the same as the difference between two groups when the dependent variable is dichotmous. The appropriate test to compare group differences with a dichotmous outcome is the chi-square statistic."}, {"text": "For a dichotomous categorical variable and a continuous variable you can calculate a Pearson correlation if the categorical variable has a 0/1-coding for the categories.  But when you have more than two categories for the categorical variable the Pearson correlation is not appropriate anymore."}, {"text": "The t-test is commonly used in statistical analysis. It is an appropriate method for comparing two groups of continuous data which are both normally distributed. The most commonly used forms of the t- test are the test of hypothesis, the single-sample, paired t-test, and the two-sample, unpaired t-test."}, {"text": "The Mann-Whitney U test is used to compare differences between two independent groups when the dependent variable is either ordinal or continuous, but not normally distributed.  The Mann-Whitney U test is often considered the nonparametric alternative to the independent t-test although this is not always the case."}, {"text": "The essential difference between these two is that Logistic regression is used when the dependent variable is binary in nature. In contrast, Linear regression is used when the dependent variable is continuous and nature of the regression line is linear."}]}, {"question": "What is a logarithm and why is it useful", "positive_ctxs": [{"text": "Logarithms are defined as the solutions to exponential equations and so are practically useful in any situation where one needs to solve such equations (such as finding how long it will take for a population to double or for a bank balance to reach a given value with compound interest)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The logarithm is to exponentiation as division is to multiplication: The logarithm is the inverse of the exponent: it undoes exponentiation. When studying logarithms, always remember the following fundamental equivalence: if and only if . Whenever one of these is true, so is the other."}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "In mathematics, the binary logarithm (log2 n) is the power to which the number 2 must be raised to obtain the value n. That is, for any real number x, For example, the binary logarithm of 1 is 0, the binary logarithm of 2 is 1, the binary logarithm of 4 is 2, and the binary logarithm of 32 is 5."}, {"text": "The year is a categorical variable. The ratio between two years is not meaningful which is why its not appropriate to classify it as a quantitative variable."}, {"text": "This is why it is important to distinguish between the statistical significance of a result and the practical significance of that result.  Null hypothesis testing is a formal approach to deciding whether a statistical relationship in a sample reflects a real relationship in the population or is just due to chance."}, {"text": "If our model is too simple and has very few parameters then it may have high bias and low variance.  This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can't be more complex and less complex at the same time."}, {"text": "The natural logarithm function is negative for values less than one and positive for values greater than one. So yes, it is possible that you end up with a negative value for log-likelihood (for discrete variables it will always be so)."}]}, {"question": "What is Type I error in statistics", "positive_ctxs": [{"text": "A type 1 error is also known as a false positive and occurs when a researcher incorrectly rejects a true null hypothesis.  The probability of making a type I error is represented by your alpha level (\u03b1), which is the p-value below which you reject the null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Type I error is equivalent to a False positive. Type II error is equivalent to a False negative. Type I error refers to non-acceptance of hypothesis which ought to be accepted. Type II error is the acceptance of hypothesis which ought to be rejected."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive."}, {"text": "The comparison - wise error rate is the probability of a Type I error set by the experimentor for evaluating each comparison. The experiment - wise error rate is the probability of making at least one Type I error when performing the whole set of comparisons."}, {"text": "in a test involving multiple comparisons, the probability of making at least one Type I error over an entire research study. The experiment-wise error rate differs from the testwise error rate, which is the probability of making a Type I error when performing a specific test or comparison."}, {"text": "A Type I is a false positive where a true null hypothesis that there is nothing going on is rejected. A Type II error is a false negative, where a false null hypothesis is not rejected \u2013 something is going on \u2013 but we decide to ignore it."}, {"text": "Every time you conduct a t-test there is a chance that you will make a Type I error.  An ANOVA controls for these errors so that the Type I error remains at 5% and you can be more confident that any statistically significant result you find is not just running lots of tests."}, {"text": "Type I and type II errors are instrumental for the understanding of hypothesis testing in a clinical research scenario.  A type II error can be thought of as the opposite of a type I error and is when a researcher fails to reject the null hypothesis that is actually false in reality."}]}, {"question": "How do you calculate permutations", "positive_ctxs": [{"text": "To calculate permutations, we use the equation nPr, where n is the total number of choices and r is the amount of items being selected. To solve this equation, use the equation nPr = n! / (n - r)!."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "The first step would be to get comfortable with the concepts of permutations and combinations.  Step 1- learn permutations and combinations from 11th class NCERT.Step 2 - practice as many questions as you can on this topic .  Step 3 - once you have done that, read probability from 11th NCERT.More items"}, {"text": "The difference between combinations and permutations is ordering. With permutations we care about the order of the elements, whereas with combinations we don't. For example, say your locker \u201ccombo\u201d is 5432. If you enter 4325 into your locker it won't open because it is a different ordering (aka permutation)."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "If the order doesn't matter then we have a combination, if the order do matter then we have a permutation. One could say that a permutation is an ordered combination. The number of permutations of n objects taken r at a time is determined by the following formula: P(n,r)=n!"}, {"text": "If the order doesn't matter then we have a combination, if the order do matter then we have a permutation.  One could say that a permutation is an ordered combination. The number of permutations of n objects taken r at a time is determined by the following formula: P(n,r)=n!"}]}, {"question": "What does centering a variable mean", "positive_ctxs": [{"text": "Centering predictor variables is one of those simple but extremely useful practices that is easily overlooked. It's almost too simple. Centering simply means subtracting a constant from every value of a variable.  The effect is that the slope between that predictor and the response variable doesn't change at all."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Some researchers say that it is a good idea to mean center variables prior to computing a product term (to serve as a moderator term) because doing so will help reduce multicollinearity in a regression model. Other researchers say that mean centering has no effect on multicollinearity."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time."}, {"text": "In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "An ordinal variable is a categorical variable for which the possible values are ordered. Ordinal variables can be considered \u201cin between\u201d categorical and quantitative variables. Thus it does not make sense to take a mean of the values."}]}, {"question": "Why is it called an inverted index", "positive_ctxs": [{"text": "This type of index is called an inverted index, namely because it is an inversion of the forward index.  In some search engines the index includes additional information such as frequency of the terms, e.g. how often a term occurs in each document, or the position of the term in each document."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In computer science, an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content)."}, {"text": "In computer science, an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content)."}, {"text": "An Inverted file is an index data structure that maps content to its location within a database file, in a document or in a set of documents.  The inverted file is the most popular data structure used in document retrieval systems to support full text search."}, {"text": "Chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval (IR). The second more subtle advantage of compression is faster transfer of data from disk to memory."}, {"text": "Principal Component Analysis PCA's approach to data reduction is to create one or more index variables from a larger set of measured variables. It does this using a linear combination (basically a weighted average) of a set of variables. The created index variables are called components."}, {"text": "A stochastic process is a family of random variables {X\u03b8}, where the parameter \u03b8 is drawn from an index set \u0398. For example, let's say the index set is \u201ctime\u201d.  One example of a stochastic process that evolves over time is the number of customers (X) in a checkout line."}, {"text": "Thus, the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the"}]}, {"question": "What is the difference between Bayes rule and conditional probability", "positive_ctxs": [{"text": "The nominator is the joint probability and the denominator is the probability of the given outcome.  This is the conditional probability: P(A\u2223B)=P(A\u2229B)P(B) This is the Bayes' rule: P(A\u2223B)=P(B|A)\u2217P(A)P(B)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities .  The above statement is the general representation of the Bayes rule."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities ."}, {"text": "The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities ."}, {"text": "The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities ."}, {"text": "The formula for conditional probability is derived from the probability multiplication rule, P(A and B) = P(A)*P(B|A). You may also see this rule as P(A\u222aB). The Union symbol (\u222a) means \u201cand\u201d, as in event A happening and event B happening."}]}, {"question": "Why is statistics important for data science", "positive_ctxs": [{"text": "Statistical data analysis. Finding structure in data and making predictions are the most important steps in Data Science. Here, in particular, statistical methods are essential since they are able to handle many different analytical tasks.  Questions arising in data driven problems can often be translated to hypotheses."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Data science is the field of study that combines domain expertise, programming skills, and knowledge of mathematics and statistics to extract meaningful insights from data."}, {"text": "Another most important role of training data for machine learning is classifying the data sets into various categorized which is very much important for supervised machine learning.  It helps them to recognize and classify the similar objects in future, thus training data is very important for such classification."}, {"text": "The answer is a big NO. Data science gets solutions and results to specific business problems using AI as a tool. If data science is to insights, machine learning is to predictions and artificial intelligence is to actions."}, {"text": "Data visualization is an important skill in applied statistics and machine learning. Statistics does indeed focus on quantitative descriptions and estimations of data. Data visualization provides an important suite of tools for gaining a qualitative understanding."}, {"text": "Descriptive statistics describe what is going on in a population or data set. Inferential statistics, by contrast, allow scientists to take findings from a sample group and generalize them to a larger population. The two types of statistics have some important differences."}, {"text": "Because data science is a broad term for multiple disciplines, machine learning fits within data science. Machine learning uses various techniques, such as regression and supervised clustering. On the other hand, the data' in data science may or may not evolve from a machine or a mechanical process."}, {"text": "Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data."}]}, {"question": "What is the relationship between mean and variance", "positive_ctxs": [{"text": "The mean is the average of a group of numbers, and the variance measures the average degree to which each number is different from the mean."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Simple Linear RegressionLinearity: The relationship between X and the mean of Y is linear.Homoscedasticity: The variance of residual is the same for any value of X.Independence: Observations are independent of each other.Normality: For any fixed value of X, Y is normally distributed."}, {"text": "There are four assumptions associated with a linear regression model: Linearity: The relationship between X and the mean of Y is linear. Homoscedasticity: The variance of residual is the same for any value of X. Independence: Observations are independent of each other."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Unlike range and quartiles, the variance combines all the values in a data set to produce a measure of spread.  It is calculated as the average squared deviation of each number from the mean of a data set. For example, for the numbers 1, 2, and 3 the mean is 2 and the variance is 0.667."}, {"text": "5 Answers. N is the population size and n is the sample size. The question asks why the population variance is the mean squared deviation from the mean rather than (N\u22121)/N=1\u2212(1/N) times it."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}]}, {"question": "What is predictive power of a model", "positive_ctxs": [{"text": "The predictive power of a scientific theory refers to its ability to generate testable predictions.  Theories with strong predictive power are highly valued, because the predictions can often encourage the falsification of the theory."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Lift is a measure of the effectiveness of a predictive model calculated as the ratio between the results obtained with and without the predictive model. Cumulative gains and lift charts are visual aids for measuring model performance."}, {"text": "Machine Learning This phenomenon states that with a fixed number of training samples, the average (expected) predictive power of a classifier or regressor first increases as number of dimensions or features used is increased but beyond a certain dimensionality it starts deteriorating instead of improving steadily."}, {"text": "The power of a test is the probability of rejecting the null hypothesis when it is false; in other words, it is the probability of avoiding a type II error. The power may also be thought of as the likelihood that a particular study will detect a deviation from the null hypothesis given that one exists."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "PF expresses the ratio of true power used in a circuit to the apparent power delivered to the circuit. A 96% power factor demonstrates more efficiency than a 75% power factor. PF below 95% is considered inefficient in many regions."}, {"text": "Use imputation for the missing values. When the response is missing, we can use a predictive model to predict the missing response, then create a new fully-observed dataset containing the predictions instead of the missing values, and finally re-estimate the predictive model in this expanded dataset."}, {"text": "Statistical power, or the power of a hypothesis test is the probability that the test correctly rejects the null hypothesis. That is, the probability of a true positive result.  statistical power is the probability that a test will correctly reject a false null hypothesis."}]}, {"question": "What is label in machine learning", "positive_ctxs": [{"text": "Label: Labels are the final output. You can also consider the output classes to be the labels. When data scientists speak of labeled data, they mean groups of samples that have been tagged to one or more labels."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "\u201cThe distinction between white label and private label are subtle,\u201d he writes. \u201cThat's why these terms are so easily confused. Private label is a brand sold exclusively in one retailer, for example, Equate (WalMart). White label is a generic product, which is sold to multiple retailers like generic ibuprofen (Advil).\u201d"}, {"text": "\u201cThe distinction between white label and private label are subtle,\u201d he writes. \u201cThat's why these terms are so easily confused. Private label is a brand sold exclusively in one retailer, for example, Equate (WalMart). White label is a generic product, which is sold to multiple retailers like generic ibuprofen (Advil).\u201d"}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "How do you know if events are independent", "positive_ctxs": [{"text": "Events A and B are independent if the equation P(A\u2229B) = P(A) \u00b7 P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Events are considered disjoint if they never occur at the same time; these are also known as mutually exclusive events. Events are considered independent if they are unrelated. Two events that do not occur at the same time. These are also known as mutually exclusive events."}, {"text": "Events A and B are independent if the equation P(A\u2229B) = P(A) \u00b7 P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together."}, {"text": "Events A and B are independent if the equation P(A\u2229B) = P(A) \u00b7 P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together."}, {"text": "Events A and B are independent if the equation P(A\u2229B) = P(A) \u00b7 P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together."}, {"text": "Events A and B are independent if the equation P(A\u2229B) = P(A) \u00b7 P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Dependent events influence the probability of other events \u2013 or their probability of occurring is affected by other events. Independent events do not affect one another and do not increase or decrease the probability of another event happening."}]}, {"question": "What are the advantages of correlation", "positive_ctxs": [{"text": "It allows researchers to determine the strength and direction of a relationship so that later studies can narrow the findings down and, if possible, determine causation experimentally. Correlation research only uncovers a relationship; it cannot provide a conclusive reason for why there's a relationship."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The correlation is the covariance divided by the product of the standard deviations. Therefore the correlation is the gradient of the regression line multiplied by the ratio of the standard deviations. If these standard deviations are equal the correlation is equal to the gradient."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "A coefficient of correlation of +0.8 or -0.8 indicates a strong correlation between the independent variable and the dependent variable. An r of +0.20 or -0.20 indicates a weak correlation between the variables."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "\u201cThe advantages of bootstrapping are that it is a straightforward way to derive the estimates of standard errors and confidence intervals, and it is convenient since it avoids the cost of repeating the experiment to get other groups of sampled data."}, {"text": "Disadvantages include its \"black box\" nature, greater computational burden, proneness to overfitting, and the empirical nature of model development. An overview of the features of neural networks and logistic regression is presented, and the advantages and disadvantages of using this modeling technique are discussed."}, {"text": "Cross correlation and autocorrelation are very similar, but they involve different types of correlation: Cross correlation happens when two different sequences are correlated. Autocorrelation is the correlation between two of the same sequences. In other words, you correlate a signal with itself."}]}, {"question": "Which is better react VUE or angular", "positive_ctxs": [{"text": "Vue provides higher customizability and hence is easier to learn than Angular or React. Further, Vue has an overlap with Angular and React with respect to their functionality like the use of components. Hence, the transition to Vue from either of the two is an easy option."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep learning itself does feature engineering whereas machine learning requires manual feature engineering. 2) Which of the following is a representation learning algorithm? Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning."}, {"text": "Generalization is a term used to describe a model's ability to react to new data. Generalization is the ability of your model, after being trained to digest new data and make accurate predictions."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "The Spearman correlation is the same as the Pearson correlation, but it is used on data from an ordinal scale. Which situation would be appropriate for obtaining a phi-coefficient with a Pearson test?"}, {"text": "Thus, Linear regression is better for simpler modelling while neural net is better for complex or multiple-level/category modelling. Neural networks generally outperform linear regression as they have more degrees of freedom. In linear regression variables are treated as a linear combination."}, {"text": "Which intuitively says that the probability of has to be \u201creally high\u201d. In other words, if your value is smaller than E[X], then the upper bound of it taking that value is 1 (basically sort of an uninteresting statement, since you already knew the upper bound was 1 or greater)."}]}, {"question": "How does feature detection work", "positive_ctxs": [{"text": "Feature detection is a process by which the nervous system sorts or filters complex natural stimuli in order to extract behaviorally relevant cues that have a high probability of being associated with important objects or organisms in their environment, as opposed to irrelevant background or noise."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  In supervised feature learning, features are learned using labeled input data."}, {"text": "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  In supervised feature learning, features are learned using labeled input data."}, {"text": "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  In unsupervised feature learning, features are learned with unlabeled input data."}, {"text": "In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  Feature learning can be either supervised or unsupervised."}, {"text": "The scale-invariant feature transform (SIFT) is a feature detection algorithm in computer vision to detect and describe local features in images.  SIFT keypoints of objects are first extracted from a set of reference images and stored in a database."}, {"text": "So while L2 regularization does not perform feature selection the same way as L1 does, it is more useful for feature *interpretation*: a predictive feature will get a non-zero coefficient, which is often not the case with L1."}, {"text": "Face-detection algorithms focus on the detection of frontal human faces. It is analogous to image detection in which the image of a person is matched bit by bit. Image matches with the image stores in database. Any facial feature changes in the database will invalidate the matching process."}]}, {"question": "How do you find the threshold value of an image in OpenCV", "positive_ctxs": [{"text": "If f (x, y) > T then f (x, y) = 0 else f (x, y) = 255 where f (x, y) = Coordinate Pixel Value T = Threshold Value. In OpenCV with Python, the function cv2. threshold is used for thresholding."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The idea is to separate the image into two parts; the background and foreground.Select initial threshold value, typically the mean 8-bit value of the original image.Divide the original image into two portions;  Find the average mean values of the two new images.Calculate the new threshold by averaging the two means.More items"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Definition: An image processing method that creates a bitonal (aka binary) image based on setting a threshold value on the pixel intensity of the original image.  The thresholding process is sometimes described as separating an image into foreground values (black) and background values (white)."}, {"text": "Definition: An image processing method that creates a bitonal (aka binary) image based on setting a threshold value on the pixel intensity of the original image.  The thresholding process is sometimes described as separating an image into foreground values (black) and background values (white)."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "In edge detection, we find the boundaries or edges of objects in an image, by determining where the brightness of the image changes dramatically. Edge detection can be used to extract the structure of objects in an image."}, {"text": "Thresholding is a technique in OpenCV, which is the assignment of pixel values in relation to the threshold value provided. In thresholding, each pixel value is compared with the threshold value. If the pixel value is smaller than the threshold, it is set to 0, otherwise, it is set to a maximum value (generally 255)."}]}, {"question": "Can you use one way Anova for two groups", "positive_ctxs": [{"text": "Typically, a one-way ANOVA is used when you have three or more categorical, independent groups, but it can be used for just two groups (but an independent-samples t-test is more commonly used for two groups)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In your case, with three groups, you'd run ANOVA. If you need to compare the 5-point scales one at a time, then non-parametric statistics are more appropriate. To compare two groups use the Mann-Whitney U test. To compare three or more groups use the Kruskal\u2013Wallis H test."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The 2-sample t-test takes your sample data from two groups and boils it down to the t-value. The process is very similar to the 1-sample t-test, and you can still use the analogy of the signal-to-noise ratio. Unlike the paired t-test, the 2-sample t-test requires independent groups for each sample."}, {"text": "The simplest way to compare two distributions is via the Z-test. The error in the mean is calculated by dividing the dispersion by the square root of the number of data points.  This is one way you can use to determine, in fact, the likelihood that your sample means it a good indicator of the true population mean."}, {"text": "When comparing two groups, you need to decide whether to use a paired test. When comparing three or more groups, the term paired is not apt and the term repeated measures is used instead. Use an unpaired test to compare groups when the individual values are not paired or matched with one another."}, {"text": "2 Answers. If you have two classes (i.e. binary classification), you should use a binary crossentropy loss. If you have more than two you should use a categorical crossentropy loss."}, {"text": "There are two groups of metrics that may be useful for imbalanced classification because they focus on one class; they are sensitivity-specificity and precision-recall."}]}, {"question": "What does it mean when a researcher says Correlation is not causation", "positive_ctxs": [{"text": "\"Correlation is not causation\" means that just because two things correlate does not necessarily mean that one causes the other.  Correlations between two things can be caused by a third factor that affects both of them."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Five Common Types of Sampling ErrorsPopulation Specification Error\u2014This error occurs when the researcher does not understand who they should survey.  Sample Frame Error\u2014A frame error occurs when the wrong sub-population is used to select a sample.More items"}, {"text": "Five Common Types of Sampling ErrorsPopulation Specification Error\u2014This error occurs when the researcher does not understand who they should survey.  Sample Frame Error\u2014A frame error occurs when the wrong sub-population is used to select a sample.More items"}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Five Common Types of Sampling Errors. Population Specification Error\u2014This error occurs when the researcher does not understand who they should survey. For example, imagine a survey about breakfast cereal consumption.  Sample Frame Error\u2014A frame error occurs when the wrong sub-population is used to select a sample."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}]}, {"question": "What is an example of extrapolation", "positive_ctxs": [{"text": "Extrapolate is defined as speculate, estimate or arrive at a conclusion based on known facts or observations. An example of extrapolate is deciding it will take twenty minutes to get home because it took you twenty minutes to get there."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A sampling unit is a selection of a population that is used as an extrapolation of the population. For example, a household is used as a sampling unit, under the assumption that the polling results from this unit represents the opinions of a larger group. Related Courses. Guide to Audit Sampling."}, {"text": "The definition of an ensemble is two or more people or things that function together as a whole. An example of an ensemble is a string quartet. An example of an ensemble is a group of actors in a play.  A small group of musicians playing or singing together."}, {"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}, {"text": "An example of statistics is a report of numbers saying how many followers of each religion there are in a particular country. An example of statistics is a math class offered in high schools and colleges. The definition of a statistic is a number, or a person who is an unnamed piece of data to be studied."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The main argument against using linear regression for time series data is that we're usually interested in predicting the future, which would be extrapolation (prediction outside the range of the data) for linear regression. Extrapolating linear regression is seldom reliable."}, {"text": "Output is defined as the act of producing something, the amount of something that is produced or the process in which something is delivered. An example of output is the electricity produced by a power plant. An example of output is producing 1,000 cases of a product."}]}, {"question": "How does correspondence analysis work", "positive_ctxs": [{"text": "Correspondence analysis reveals the relative relationships between and within two groups of variables, based on data given in a contingency table. For brand perceptions, these two groups are brands and the attributes that apply to these brands."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "The term that does not apply to cluster analysis is factorization. Cluster analysis is a way of grouping data, based on obvious similarities. It is also called as classification analysis or numerical taxonomy.  Hierarchical cluster analysis tends to build a hierarchy within clusters."}, {"text": "How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression."}, {"text": "How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression."}, {"text": "How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression."}, {"text": "How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression."}]}, {"question": "What is the definition of squashing function in machine learning", "positive_ctxs": [{"text": "An activation function is a function used in artificial neural networks which outputs a small value for small inputs, and a larger value if its inputs exceed a threshold. If the inputs are large enough, the activation function \"fires\", otherwise it does nothing."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "2 Answers. By definition the probability density function is the derivative of the distribution function. But distribution function is an increasing function on R thus its derivative is always positive. Assume that probability density of X is -ve in the interval (a, b)."}, {"text": "A squashing function is essentially defined as a function that squashes the input to one of the ends of a small interval. In Neural Networks, these can be used at nodes in a hidden layer to squash the input. This introduces non-linearity to the NN and allows the NN to be effective."}, {"text": "Gram matrix is simply the matrix of the inner product of each vector and its corresponding vectors in same. It found use in the current machine learning is due to deep learning loss where while style transferring the loss function is computed using the gram matrix."}, {"text": "A (non-mathematical) definition I like by Miller (2017)3 is: Interpretability is the degree to which a human can understand the cause of a decision.  The higher the interpretability of a machine learning model, the easier it is for someone to comprehend why certain decisions or predictions have been made."}, {"text": "A (non-mathematical) definition I like by Miller (2017)3 is: Interpretability is the degree to which a human can understand the cause of a decision.  The higher the interpretability of a machine learning model, the easier it is for someone to comprehend why certain decisions or predictions have been made."}, {"text": "A support vector machine is a machine learning model that is able to generalise between two different classes if the set of labelled data is provided in the training set to the algorithm. The main function of the SVM is to check for that hyperplane that is able to distinguish between the two classes."}, {"text": "A support vector machine is a machine learning model that is able to generalise between two different classes if the set of labelled data is provided in the training set to the algorithm. The main function of the SVM is to check for that hyperplane that is able to distinguish between the two classes."}]}, {"question": "What is non linear decision boundary", "positive_ctxs": [{"text": "The Non-Linear Decision Boundary SVM works well when the data points are linearly separable. If the decision boundary is non-liner then SVM may struggle to classify. Observe the below examples, the classes are not linearly separable. SVM has no direct theory to set the non-liner decision boundary models."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Naive Bayes is a linear classifier Naive Bayes leads to a linear decision boundary in many common cases.  The red decision line indicates the decision boundary where P(y=1|x)=P(y=2|x)."}, {"text": "The most basic way to use a SVC is with a linear kernel, which means the decision boundary is a straight line (or hyperplane in higher dimensions)."}, {"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}, {"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}, {"text": "It is linear if there exists a function H(x) = \u03b20 + \u03b2T x such that h(x) = I(H(x) > 0). H(x) is also called a linear discriminant function. The decision boundary is therefore defined as the set {x \u2208 Rd : H(x)=0}, which corresponds to a (d \u2212 1)-dimensional hyperplane within the d-dimensional input space X."}, {"text": "Logistic regression is known and used as a linear classifier. It is used to come up with a hyperplane in feature space to separate observations that belong to a class from all the other observations that do not belong to that class. The decision boundary is thus linear.13\u200f/03\u200f/2019"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is probabilistic linear discriminant analysis used for", "positive_ctxs": [{"text": "While PPCA is used to model a probability density of data, PLDA can be used to make probabilistic inferences about the class of data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and other fields, to find a linear combination of features that characterizes or separates two or more classes"}, {"text": "Linear discriminant function analysis (i.e., discriminant analysis) performs a multivariate test of differences between groups.  In addition, discriminant analysis is used to determine the minimum number of dimensions needed to describe these differences."}, {"text": "The main difference between these two techniques is that regression analysis deals with a continuous dependent variable, while discriminant analysis must have a discrete dependent variable. The methodology used to complete a discriminant analysis is similar to regression analysis."}, {"text": "Linear discriminant analysis is primarily used here to reduce the number of features to a more manageable number before classification. Each of the new dimensions is a linear combination of pixel values, which form a template."}, {"text": "Linear discriminant analysis (LDA) is used here to reduce the number of features to a more manageable number before the process of classification. Each of the new dimensions generated is a linear combination of pixel values, which form a template."}, {"text": "Canonical discriminant analysis is a dimension-reduction technique related to principal component analysis and canonical correlation.  This maximal multiple correlation is called the first canonical correlation. The coefficients of the linear combination are the canonical coefficients or canonical weights."}, {"text": "Canonical discriminant analysis is a dimension-reduction technique related to principal component analysis and canonical correlation.  This maximal multiple correlation is called the first canonical correlation. The coefficients of the linear combination are the canonical coefficients or canonical weights."}]}, {"question": "What is Backpropagation and how does it work", "positive_ctxs": [{"text": "The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "The general algorithm is The Backpropagation algorithm is suitable for the feed forward neural network on fixed sized input-output pairs. The Backpropagation Through Time is the application of Backpropagation training algorithm which is applied to the sequence data like the time series."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "API KPIs (Key Performance Indicators) Defining the key performance indicators (KPIs) for APIs being used is a critical part of understanding not just how they work but how well they can work and the impact they have on your services, users or partners."}]}, {"question": "Can logistic regression be used to predict categorical outcome", "positive_ctxs": [{"text": "Logistic regression models are a great tool for analysing binary and categorical data, allowing you to perform a contextual analysis to understand the relationships between the variables, test for differences, estimate effects, make predictions, and plan for future scenarios."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multinomial logistic regression is used to predict categorical placement in or the probability of category membership on a dependent variable based on multiple independent variables. The independent variables can be either dichotomous (i.e., binary) or continuous (i.e., interval or ratio in scale)."}, {"text": "Multinomial logistic regression is used to predict categorical placement in or the probability of category membership on a dependent variable based on multiple independent variables. The independent variables can be either dichotomous (i.e., binary) or continuous (i.e., interval or ratio in scale)."}, {"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "Multinomial logistic regression deals with situations where the outcome can have three or more possible types (e.g., \"disease A\" vs. \"disease B\" vs. \"disease C\") that are not ordered.  Binary logistic regression is used to predict the odds of being a case based on the values of the independent variables (predictors)."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}, {"text": "Linear regression is used for predicting the continuous dependent variable using a given set of independent features whereas Logistic Regression is used to predict the categorical. Linear regression is used to solve regression problems whereas logistic regression is used to solve classification problems."}]}, {"question": "When would you use an exponential distribution", "positive_ctxs": [{"text": "The exponential distribution is often used to model the longevity of an electrical or mechanical device. In Example, the lifetime of a certain computer part has the exponential distribution with a mean of ten years (X\u223cExp(0.1))."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "It is very much like the exponential distribution, with \u03bb corresponding to 1/p, except that the geometric distribution is discrete while the exponential distribution is continuous."}, {"text": "These are generally used when direct sampling from the probability distribution would be difficult. Some of the use cases of MCMC methods are to approximate a target probability distribution or to compute an integral."}, {"text": "If you use natural log values for your dependent variable (Y) and keep your independent variables (X) in their original scale, the econometric specification is called a log-linear model. These models are typically used when you think the variables may have an exponential growth relationship."}, {"text": "Stochastic Gradient Descent: you would randomly select one of those training samples at each iteration to update your coefficients. Online Gradient Descent: you would use the \"most recent\" sample at each iteration. There is no stochasticity as you deterministically select your sample."}, {"text": "The exponential distribution is a continuous probability distribution used to model the time we need to wait before a given event occurs. It is the continuous counterpart of the geometric distribution, which is instead discrete. Sometimes it is also called negative exponential distribution."}, {"text": "You can use reinforcement learning for classification problems but it won't be giving you any added benefit and instead slow down your convergence rate. Detailed answer: yes but it's an overkill.  So, if you possess labels, it would be a LOT more faster and easier to use regular supervised learning."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}]}, {"question": "What are the properties of a T curve", "positive_ctxs": [{"text": "Important Properties Property #1: The total area under a t distribution curve is 1.0: that is 100%. Property #2: A t-curve is symmetric around 0. Property #3: While a t-curve extends infinitely in either direction, it approaches, but never touches the horizontal axis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An almost essential property is that the estimator should be consistent: T is a consistent estimator of \u03b8 if T converges to \u03b8 in probability as n \u2192 \u221e. Consistency implies that, as the sample size increases, any bias in T tends to 0 and the variance of T also tends to 0."}, {"text": "The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution."}, {"text": "The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution."}, {"text": "The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution."}, {"text": "In physics, a partition function describes the statistical properties of a system in thermodynamic equilibrium. Partition functions are functions of the thermodynamic state variables, such as the temperature and volume."}, {"text": "Properties of a normal distributionThe mean, mode and median are all equal.The curve is symmetric at the center (i.e. around the mean, \u03bc).Exactly half of the values are to the left of center and exactly half the values are to the right.The total area under the curve is 1."}, {"text": "Properties of a normal distribution The mean, mode and median are all equal. The curve is symmetric at the center (i.e. around the mean, \u03bc). Exactly half of the values are to the left of center and exactly half the values are to the right. The total area under the curve is 1."}]}, {"question": "What is the difference between a linear operator and a linear transformation", "positive_ctxs": [{"text": "Linear transformation is a function between two linear spaces over the same field of scalars, which is additive and homogeneous. Linear operator is a linear transformation for which the domain and the codomain spaces are the same and, moreover, in both of them the same basis is considered."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The integral operator is a linear operator because it preserves two operations; the addition between functions and the multiplication of a function"}, {"text": "A matrix with rows and columns over a field is a function from the set of all ordered pairs of integers in range to .  A linear operator is a linear function from a Vector space to itself. In notations, given a vector space , a linear operator is a function which satisfies for all in the underlying Field and vectors ."}, {"text": "a transformation in which measurements on a linear scale are converted into probabilities between 0 and 1. It is given by the formula y = ex/(1 + ex), where x is the scale value and e is the Eulerian number."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Given a linear operator it can have associated eigenvectors / functions.  For example, given the differential operator the exponential function is an eigenfunction of it. This is why we can solve linear homogeneous differential equations by solving a characteristic equation."}, {"text": "Now, every textbook on linear algebra gives the following definition of a linear operator: an operator T: V\u2014> W between two vector spaces V and W over the same field ! F is said to be linear if it satisfies the conditions of additivity, viz. T(u + v)=T(u)+T(v)"}, {"text": "Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x)."}]}, {"question": "Whatt are best image processing ideas", "positive_ctxs": [{"text": "Best Image Processing Projects CollectionLicense plate recognition.Face Emotion recognition.Face recognition.Cancer detection.Object detection.Pedestrian detection.Lane detection for ADAS.Blind assistance systems.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Image processing techniques use filters to enhance an image. Their main applications are to transform the contrast, brightness, resolution and noise level of an image. Contouring, image sharpening, blurring, embossing and edge detection are typical image processing functions (see Table 4.1)."}, {"text": "Answer: Autoecncoders work best for image data."}, {"text": "We discuss some wonders in the field of image processing with machine learning advancements. Image processing can be defined as the technical analysis of an image by using complex algorithms. Here, image is used as the input, where the useful information returns as the output."}, {"text": "There are two types of methods used for image processing namely, analogue and digital image processing.  Image analysts use various fundamentals of interpretation while using these visual techniques."}, {"text": "Digital image processing is the use of computer algorithms to perform image processing on digital images . Image Acquisition Image Restoration Morphological Processing Segmentation Representation & Description Image Enhancement Object Recognition Problem Domain Colour Image Processing Image Compression."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}]}, {"question": "How do you make an interactive decision tree", "positive_ctxs": [{"text": "The Most Simple Ways to Build an Interactive Decision TreeLog in to your Zingtree account, go to My Trees and select Create New Tree.  After naming your decision tree, choosing your ideal display style and providing a description, just click the Create Tree button to move on to the next step.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Not all machine learning algorithms make the iid assumption (for example, decision tree based approaches do not).  So, common learning algorithms can be used to learn time series data."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree learning is a supervised machine learning technique for inducing a decision tree from training data. A decision tree (also referred to as a classification tree or a reduction tree) is a predictive model which is a mapping from observations about an item to conclusions about its target value."}, {"text": "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value."}]}, {"question": "What do you mean by knowledge in artificial intelligence", "positive_ctxs": [{"text": "Knowledge is the information about a domain that can be used to solve problems in that domain.  As part of designing a program to solve problems, we must define how the knowledge will be represented. A representation scheme is the form of the knowledge that is used in an agent."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A semantic network is a graphic notation for representing knowledge in patterns of interconnected nodes. Semantic networks became popular in artificial intelligence and natural language processing only because it represents knowledge or supports reasoning."}, {"text": "In the real world, knowledge plays a vital role in intelligence as well as creating artificial intelligence. It demonstrates the intelligent behavior in AI agents or systems. It is possible for an agent or system to act accurately on some input only when it has the knowledge or experience about the input."}, {"text": "A knowledge-based system (KBS) is a form of artificial intelligence (AI) that aims to capture the knowledge of human experts to support decision-making. Examples of knowledge-based systems include expert systems, which are so called because of their reliance on human expertise."}, {"text": "Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used to solve complex problems.  Virtually all knowledge representation languages have a reasoning or inference engine as part of the system."}, {"text": "Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used to solve complex problems.  Virtually all knowledge representation languages have a reasoning or inference engine as part of the system."}, {"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "\"AI is a computer system able to perform tasks that ordinarily require human intelligence Many of these artificial intelligence systems are powered by machine learning, some of them are powered by deep learning and some of them are powered by very boring things like rules.\""}]}, {"question": "Why is the sum of two random variables a convolution", "positive_ctxs": [{"text": "In probability theory, convolution is a mathematical operation that allows to derive the distribution of a sum of two random variables from the distributions of the two summands.  In the case of continuous random variables, it is obtained by integrating the product of their probability density functions (pdfs)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "For independent random variables X and Y, the variance of their sum or difference is the sum of their variances: Variances are added for both the sum and difference of two independent random variables because the variation in each variable contributes to the variation in each case."}, {"text": "The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."}, {"text": "The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."}, {"text": "The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."}, {"text": "The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map."}, {"text": "A random variable is a variable whose value is a numerical outcome of a random phenomenon. A discrete random variable X has a countable number of possible values. Example: Let X represent the sum of two dice.  A continuous random variable X takes all values in a given interval of numbers."}, {"text": "A canonical variate is a new variable (variate) formed by making a linear combination of two or more variates (variables) from a data set. A linear combination of variables is the same as a weighted sum of variables."}]}, {"question": "Which of the parameters are considered to be hyper parameters", "positive_ctxs": [{"text": "Here eta (learning rate) and n_iter (number of iterations) are the hyperparameters that would have to be adjusted in order to obtain the best values for the model parameters a and b."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The parameters of a neural network are typically the weights of the connections. In this case, these parameters are learned during the training stage. So, the algorithm itself (and the input data) tunes these parameters. The hyper parameters are typically the learning rate, the batch size or the number of epochs."}, {"text": "The parameters of a neural network are typically the weights of the connections.  So, the algorithm itself (and the input data) tunes these parameters. The hyper parameters are typically the learning rate, the batch size or the number of epochs."}, {"text": "Grid-searching is the process of scanning the data to configure optimal parameters for a given model. Depending on the type of model utilized, certain parameters are necessary.  Grid-searching can be applied across machine learning to calculate the best parameters to use for any given model."}, {"text": "Grid-searching is the process of scanning the data to configure optimal parameters for a given model.  Grid-searching can be applied across machine learning to calculate the best parameters to use for any given model."}, {"text": "Parameter selection: When SVMs are used, there are a number of parameters selected to have the best performance including: (1) parameters included in the kernel functions, (2) the trade-off parameter C, and (3) the \u03b5-insensitivity parameter."}, {"text": "In summary, model parameters are estimated from data automatically and model hyperparameters are set manually and are used in processes to help estimate model parameters. Model hyperparameters are often referred to as parameters because they are the parts of the machine learning that must be set manually and tuned."}, {"text": "In summary, model parameters are estimated from data automatically and model hyperparameters are set manually and are used in processes to help estimate model parameters. Model hyperparameters are often referred to as parameters because they are the parts of the machine learning that must be set manually and tuned."}]}, {"question": "How many types of agents are there in artificial intelligence", "positive_ctxs": [{"text": "four"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multi-agent reinforcement learning is the study of numerous artificial intelligence agents cohabitating in an environment, often collaborating toward some end goal. When focusing on collaboration, it derives inspiration from other social structures in the animal kingdom. It also draws heavily on game theory."}, {"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats\u2014artificial systems that exhibit complex behaviour in an agent environment.  The term is also sometimes used in ethology or animal behavior."}, {"text": "there are three general categories of learning that artificial intelligence (AI)/machine learning utilizes to actually learn. They are Supervised Learning, Unsupervised Learning and Reinforcement learning.  The machine then maps the inputs and the outputs."}, {"text": "There are four types of artificial intelligence: reactive machines, limited memory, theory of mind and self-awareness."}, {"text": "There are four types of artificial intelligence: reactive machines, limited memory, theory of mind and self-awareness."}]}, {"question": "How do you develop a deep learning model", "positive_ctxs": [{"text": "How To Develop a Machine Learning Model From ScratchDefine adequately our problem (objective, desired outputs\u2026).Gather data.Choose a measure of success.Set an evaluation protocol and the different protocols available.Prepare the data (dealing with missing values, with categorial values\u2026).Spilit correctly the data.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}, {"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "Bayesian deep learning is a field at the intersection between deep learning and Bayesian probability theory.  Bayesian deep learning models typically form uncertainty estimates by either placing distributions over model weights, or by learning a direct mapping to probabilistic outputs."}, {"text": "Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}, {"text": "Bayesian inference is a machine learning model not as widely used as deep learning or regression models."}, {"text": "The larger the RAM the higher the amount of data it can handle hence faster processing. With larger RAM you can use your machine to perform other tasks as the model trains. Although a minimum of 8GB RAM can do the job, 16GB RAM and above is recommended for most deep learning tasks."}, {"text": "A supervised learning algorithm takes a known set of input data and known responses to the data (output) and trains a model to generate reasonable predictions for the response to new data.  Supervised learning uses classification and regression techniques to develop predictive models."}]}, {"question": "What is the difference between logit and probit regression", "positive_ctxs": [{"text": "The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(\u2217). Both functions will take any number and rescale it to fall between 0 and 1."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(\u2217). Both functions will take any number and rescale it to fall between 0 and 1."}, {"text": "The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(\u2217). Both functions will take any number and rescale it to fall between 0 and 1."}, {"text": "The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(\u2217). Both functions will take any number and rescale it to fall between 0 and 1."}, {"text": "The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(\u2217). Both functions will take any number and rescale it to fall between 0 and 1."}, {"text": "The logit is a transformation. Logistic regression is a regression model. The logit transformation transforms a line to a logistic curve. Logistic regression fits a logistic curve to set of data where the dependent variable can only take the values 0 and 1."}, {"text": "If p is a probability, then p/(1 \u2212 p) is the corresponding odds; the logit of the probability is the logarithm of the odds, i.e.  For each choice of base, the logit function takes values between negative and positive infinity."}]}, {"question": "How small of an alpha value can you choose and still have sufficient evidence to reject the null hypothesis", "positive_ctxs": [{"text": "Significance level and p-value \u03b1 is the maximum probability of rejecting the null hypothesis when the null hypothesis is true. If \u03b1 = 1 we always reject the null, if \u03b1 = 0 we never reject the null hypothesis.  If we choose to compare the p-value to \u03b1 = 0.01, we are insisting on a stronger evidence!"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Rejecting or failing to reject the null hypothesis If our statistical analysis shows that the significance level is below the cut-off value we have set (e.g., either 0.05 or 0.01), we reject the null hypothesis and accept the alternative hypothesis."}, {"text": "How to Conduct Hypothesis TestsState the hypotheses. Every hypothesis test requires the analyst to state a null hypothesis and an alternative hypothesis.  Formulate an analysis plan. The analysis plan describes how to use sample data to accept or reject the null hypothesis.  Analyze sample data.  Interpret the results."}, {"text": "Since p < 0.05 is enough to reject the null hypothesis (no association), p = 0.002 reinforce that rejection only. If the significance value that is p-value associated with chi-square statistics is 0.002, there is very strong evidence of rejecting the null hypothesis of no fit. It means good fit."}, {"text": "The probability of Type 1 error is alpha -- the criterion that we set as the level at which we will reject the null hypothesis. The p value is something else -- it tells you how UNUSUAL the data are, given the assumption that the null hypothesis is true."}, {"text": "The probability of making a type I error is represented by your alpha level (\u03b1), which is the p-value below which you reject the null hypothesis. A p-value of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis."}, {"text": "If the absolute value of the t-value is greater than the critical value, you reject the null hypothesis. If the absolute value of the t-value is less than the critical value, you fail to reject the null hypothesis."}]}, {"question": "What is the goal of cluster analysis", "positive_ctxs": [{"text": "The goal of cluster analysis is to obtain groupings or clusters of similar samples. This is accomplished by using a distance measure derived from the multivariate gene expression data that characterizes the ``distance'' of the patients' expression patterns with each other."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Latent classes divide the cases into their respective dimensions in relation to the variable. For example, cluster analysis groups similar cases and puts them into one group. The numbers of clusters in the cluster analysis are called the latent classes. In SEM, the number of constructs is called the latent classed."}, {"text": "In multivariate statistics, exploratory factor analysis (EFA) is a statistical method used to uncover the underlying structure of a relatively large set of variables. EFA is a technique within factor analysis whose overarching goal is to identify the underlying relationships between measured variables."}, {"text": "Factor analysis is an exploratory statistical technique to investigate dimensions and the factor structure underlying a set of variables (items) while cluster analysis is an exploratory statistical technique to group observations (people, things, events) into clusters or groups so that the degree of association is"}, {"text": "The term that does not apply to cluster analysis is factorization. Cluster analysis is a way of grouping data, based on obvious similarities. It is also called as classification analysis or numerical taxonomy.  Hierarchical cluster analysis tends to build a hierarchy within clusters."}, {"text": "Average Linkage is a type of hierarchical clustering in which the distance between one cluster and another cluster is considered to be equal to the average distance from any member of one cluster to any member of the other cluster."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "How do you normalize data in statistics", "positive_ctxs": [{"text": "Some of the more common ways to normalize data include:Transforming data using a z-score or t-score.  Rescaling data to have values between 0 and 1.  Standardizing residuals: Ratios used in regression analysis can force residuals into the shape of a normal distribution.Normalizing Moments using the formula \u03bc/\u03c3.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population."}, {"text": "Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (\u201cinferences\u201d) from that data. With inferential statistics, you take data from samples and make generalizations about a population."}, {"text": "Descriptive statistics summarize the characteristics of a data set. Inferential statistics allow you to test a hypothesis or assess whether your data is generalizable to the broader population."}, {"text": "Descriptive statistics summarize the characteristics of a data set. Inferential statistics allow you to test a hypothesis or assess whether your data is generalizable to the broader population."}, {"text": "When we do further analysis, like multivariate linear regression, for example, the attributed income will intrinsically influence the result more due to its larger value. But this doesn't necessarily mean it is more important as a predictor. So we normalize the data to bring all the variables to the same range."}]}, {"question": "How do you find the derivative using the chain rule", "positive_ctxs": [{"text": "0:5218:40Suggested clip \u00b7 82 secondsChain Rule For Finding Derivatives - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Why is the derivative of the LSTM cell state w.r.t. to the previous cell state equal to the forget gate?  The chain rule would extend for longer and we'd have more derivative terms in there."}, {"text": "The squared hinge loss is differentiable because the term from the chain rule forces the limits to converge to the same number from both sides."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron."}, {"text": "Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron."}]}, {"question": "How do you calculate confidence in association rule", "positive_ctxs": [{"text": "The confidence of an association rule is the support of (X U Y) divided by the support of X. Therefore, the confidence of the association rule is in this case the support of (2,5,3) divided by the support of (2,5)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The confidence of an association rule is a percentage value that shows how frequently the rule head occurs among all the groups containing the rule body.  Thus, the confidence of a rule is the percentage equivalent of m/n, where the values are: m. The number of groups containing the joined rule head and rule body."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Applications of association rule mining are stock analysis, web log mining, medical diagnosis, customer market analysis bioinformatics etc. In past, many algorithms were developed by researchers for Boolean and Fuzzy association rule mining such as Apriori, FP-tree, Fuzzy FP-tree etc."}, {"text": "Some popular examples of unsupervised learning algorithms are:k-means for clustering problems.Apriori algorithm for association rule learning problems."}, {"text": "Some popular examples of unsupervised learning algorithms are: k-means for clustering problems. Apriori algorithm for association rule learning problems."}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}]}, {"question": "What is beta in multiple linear regression", "positive_ctxs": [{"text": "A beta weight is a standardized regression coefficient (the slope of a line in a regression equation).  A beta weight will equal the correlation coefficient when there is a single predictor variable. \u03b2 can be larger than +1 or smaller than -1 if there are multiple predictor variables and multicollinearity is present."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Ridge regression has an additional factor called \u03bb (lambda) which is called the penalty factor which is added while estimating beta coefficients. This penalty factor penalizes high value of beta which in turn shrinks beta coefficients thereby reducing the mean squared error and predicted error."}, {"text": "Linear regression is one of the most common techniques of regression analysis. Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables."}, {"text": "Linear regression is one of the most common techniques of regression analysis. Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables."}, {"text": "Multiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. Multiple regression is an extension of linear (OLS) regression that uses just one explanatory variable."}, {"text": "A linear regression model extended to include more than one independent variable is called a multiple regression model. It is more accurate than to the simple regression.  The principal adventage of multiple regression model is that it gives us more of the information available to us who estimate the dependent variable."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "ANCOVA and multiple linear regression are similar, but regression is more appropriate when the emphasis is on the dependent outcome variable, while ANCOVA is more appropriate when the emphasis is on comparing the groups from one of the independent variables."}]}, {"question": "What is parametric and non parametric test in SPSS", "positive_ctxs": [{"text": "Parametric tests are those that make assumptions about the parameters of the population distribution from which the sample is drawn. This is often the assumption that the population data are normally distributed. Non-parametric tests are \u201cdistribution-free\u201d and, as such, can be used for non-Normal variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "LDA is a parametric model, and the parameter is number of topics."}, {"text": "ANOVA is available for both parametric (score data) and non-parametric (ranking/ordering) data."}, {"text": "The most common functional form is parametric linear model, as a type of parametric regression, is frequently used to describe the relationship between a dependent variable and explanatory variables. Parametric linear models require the estimation of a finite number of parameters, \u03b2."}, {"text": "A non parametric test (sometimes called a distribution free test) does not assume anything about the underlying distribution (for example, that the data comes from a normal distribution).  It usually means that you know the population data does not have a normal distribution."}, {"text": "These pages demonstrate how to use Moran's I or a Mantel test to check for spatial autocorrelation in your data. Moran's I is a parametric test while Mantel's test is semi-parametric. Both will also indicate if your spatial autocorrelation is positive or negative and provide a p-value for the level of autocorrelation."}, {"text": "Every parametric test has the assumption that the sample means are following a normal distribution. This is the case if the sample itself is normal distributed or if approximately if the sample size is big enough."}, {"text": "In the literal meaning of the terms, a parametric statistical test is one that makes assumptions about the parameters (defining properties) of the population distribution(s) from which one's data are drawn, while a non-parametric test is one that makes no such assumptions."}]}, {"question": "What is ACF and PACF in Arima", "positive_ctxs": [{"text": "The ACF stands for Autocorrelation function, and the PACF for Partial Autocorrelation function. Looking at these two plots together can help us form an idea of what models to fit. Autocorrelation computes and plots the autocorrelations of a time series."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "How to find accuracy of ARIMA model?Problem description: Prediction on CPU utilization.  Step 1: From Elasticsearch I collected 1000 observations and exported on Python.Step 2: Plotted the data and checked whether data is stationary or not.Step 3: Used log to convert the data into stationary form.Step 4: Done DF test, ACF and PACF.More items\u2022"}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "What are the advantages of linear regression", "positive_ctxs": [{"text": "The biggest advantage of linear regression models is linearity: It makes the estimation procedure simple and, most importantly, these linear equations have an easy to understand interpretation on a modular level (i.e. the weights)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Quantile regression is an extension of linear regression used when the conditions of linear regression are not met."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Some of the popular types of regression algorithms are linear regression, regression trees, lasso regression and multivariate regression."}, {"text": "Disadvantages include its \"black box\" nature, greater computational burden, proneness to overfitting, and the empirical nature of model development. An overview of the features of neural networks and logistic regression is presented, and the advantages and disadvantages of using this modeling technique are discussed."}, {"text": "In simple linear regression a single independent variable is used to predict the value of a dependent variable. In multiple linear regression two or more independent variables are used to predict the value of a dependent variable. The difference between the two is the number of independent variables."}, {"text": "In simple linear regression a single independent variable is used to predict the value of a dependent variable. In multiple linear regression two or more independent variables are used to predict the value of a dependent variable. The difference between the two is the number of independent variables."}, {"text": "In statistics, Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference."}]}, {"question": "What is regret in reinforcement learning", "positive_ctxs": [{"text": "Mathematically speaking, the regret is expressed as the difference between the payoff (reward or return) of a possible action and the payoff of the action that has been actually taken. If we denote the payoff function as u the formula becomes: regret = u(possible action) - u(action taken)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "The example of reinforcement learning is your cat is an agent that is exposed to the environment. The biggest characteristic of this method is that there is no supervisor, only a real number or reward signal. Two types of reinforcement learning are 1) Positive 2) Negative."}, {"text": "State\u2013action\u2013reward\u2013state\u2013action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning.  The acronym for the quintuple (st, at, rt, st+1, at+1) is SARSA."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "From Wikipedia, the free encyclopedia. Error-driven learning is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback. It is a type of reinforcement learning."}, {"text": "And, unsupervised learning is where the machine is given training based on unlabeled data without any guidance.  Whereas reinforcement learning is when a machine or an agent interacts with its environment, performs actions, and learns by a trial-and-error method."}, {"text": "Deep reinforcement learning is a promising combination between two artificial intelligence techniques: reinforcement learning, which uses sequential trial and error to learn the best action to take in every situation, and deep learning, which can evaluate complex inputs and select the best response."}]}, {"question": "Do we require feature extraction in deep learning", "positive_ctxs": [{"text": "The biggest advantage of Deep Learning is that we do not need to manually extract features from the image. The network learns to extract features while training. You just feed the image to the network (pixel values)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep Learning tries to find out the optimal set of features on your own and generate the output based on those features. So, in a nutshell, we can say that deep learning does not require feature selection. It will automatically find out the optimal set of features."}, {"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Again, feature selection keeps a subset of the original features while feature extraction creates new ones. As with feature selection, some algorithms already have built-in feature extraction.  As a stand-alone task, feature extraction can be unsupervised (i.e. PCA) or supervised (i.e. LDA)."}, {"text": "Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection. Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Dictionary learning is learning a set of atoms so that a given image can be well approximated by a sparse linear combination of these learned atoms, while deep learning methods aim at extracting deep semantic feature representations via a deep network."}]}, {"question": "What is ReLU function in neural network", "positive_ctxs": [{"text": "The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.  The rectified linear activation is the default activation when developing multilayer Perceptron and convolutional neural networks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "ReLU has become the darling activation function of the neural network world. Short for Rectified Linear Unit, it is a piecewise linear function that is defined to be 0 for all negative values of x and equal to a \u00d7 x otherwise, where a is a learnable parameter."}, {"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}]}, {"question": "How do you write a hypothesis and null hypothesis", "positive_ctxs": [{"text": "To write a null hypothesis, first start by asking a question. Rephrase that question in a form that assumes no relationship between the variables. In other words, assume a treatment has no effect. Write your hypothesis in a way that reflects this."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How to Formulate an Effective HypothesisState the problem that you are trying to solve. Make sure that the hypothesis clearly defines the topic and the focus of the experiment.Try to write the hypothesis as an if-then statement.  Define the variables."}, {"text": "How to Conduct Hypothesis TestsState the hypotheses. Every hypothesis test requires the analyst to state a null hypothesis and an alternative hypothesis.  Formulate an analysis plan. The analysis plan describes how to use sample data to accept or reject the null hypothesis.  Analyze sample data.  Interpret the results."}, {"text": "Null and alternate hypothesis are different and you can't interchange them. Alternate hypothesis is just the opposite of null which means there is a statistical difference in Mean / median of both the data sets."}, {"text": "When you reject the null hypothesis with a t-test, you are saying that the means are statistically different. The difference is meaningful. Chi Square:  When you reject the null hypothesis with a Chi-Square, you are saying that there is a relationship between the two variables."}, {"text": "In research, there is a convention that the hypothesis is written in two forms, the null hypothesis, and the alternative hypothesis (called the experimental hypothesis when the method of investigation is an experiment)."}, {"text": "Statistical analysts test a hypothesis by measuring and examining a random sample of the population being analyzed. All analysts use a random population sample to test two different hypotheses: the null hypothesis and the alternative hypothesis."}]}, {"question": "Which of the following is used by Sobel edge detection", "positive_ctxs": [{"text": "The sobel operator is very similar to Prewitt operator. It is also a derivate mask and is used for edge detection. Like Prewitt operator sobel operator is also used to detect two kinds of edges in an image: Vertical direction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Sobel operator, sometimes called the Sobel\u2013Feldman operator or Sobel filter, is used in image processing and computer vision, particularly within edge detection algorithms where it creates an image emphasising edges."}, {"text": "The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image.  The result of applying it to a pixel on an edge is a vector that points across the edge from darker to brighter values."}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image. It finds the direction of the largest increase from light to dark and the rate of change in that direction."}, {"text": "The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image. It finds the direction of the largest increase from light to dark and the rate of change in that direction."}, {"text": "The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986. Canny also produced a computational theory of edge detection explaining why the technique works."}, {"text": "In edge detection, we find the boundaries or edges of objects in an image, by determining where the brightness of the image changes dramatically. Edge detection can be used to extract the structure of objects in an image."}]}, {"question": "Which algorithm is used for object detection", "positive_ctxs": [{"text": "1| Fast R-CNN Written in Python and C++ (Caffe), Fast Region-Based Convolutional Network method or Fast R-CNN is a training algorithm for object detection. This algorithm mainly fixes the disadvantages of R-CNN and SPPnet, while improving on their speed and accuracy."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "One method of processing images is via face detection. Face detection is a branch of image processing that uses machine learning to detect faces in images. A Haar Cascade is an object detection method used to locate an object of interest in images."}, {"text": "The mean average precision (mAP) or sometimes simply just referred to as AP is a popular metric used to measure the performance of models doing document/information retrival and object detection tasks."}, {"text": "Image annotation for deep learning is mainly done for object detection with more precision. 3D Cuboid Annotation, Semantic Segmentation, and polygon annotation are used to annotate the images using the right tool to make the objects well-defined in the image for neural network analysis in deep learning."}, {"text": "Image annotation for deep learning is mainly done for object detection with more precision. 3D Cuboid Annotation, Semantic Segmentation, and polygon annotation are used to annotate the images using the right tool to make the objects well-defined in the image for neural network analysis in deep learning."}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "Bounding-box regression is a popular technique to refine or predict localization boxes in recent object detection approaches. Typically, bounding-box regressors are trained to regress from either region proposals or fixed anchor boxes to nearby bounding boxes of a pre-defined target object classes."}, {"text": "Outlier detection is extensively used in a wide variety of applications such as military surveillance for enemy activities to prevent attacks, intrusion detection in cyber security, fraud detection for credit cards, insurance or health care and fault detection in safety critical systems and in various kind of images."}]}, {"question": "What are the two types of errors in hypothesis testing", "positive_ctxs": [{"text": "Statisticians define two types of errors in hypothesis testing. Creatively, they call these errors Type I and Type II errors. Both types of error relate to incorrect conclusions about the null hypothesis. The table summarizes the four possible outcomes for a hypothesis test."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Type I and type II errors are instrumental for the understanding of hypothesis testing in a clinical research scenario.  A type II error can be thought of as the opposite of a type I error and is when a researcher fails to reject the null hypothesis that is actually false in reality."}, {"text": "Type II Error and Power Calculations. Recall that in hypothesis testing you can make two types of errors \u2022 Type I Error \u2013 rejecting the null when it is true. \u2022 Type II Error \u2013 failing to reject the null when it is false.  = \u239b \u239e \u2212  \u2212 \u2212 = =  = \u239b \u239e \u2212"}, {"text": "The false discovery rate (FDR) is a method of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons.  Thus, FDR-controlling procedures have greater power, at the cost of increased numbers of Type I errors."}, {"text": "The power of Hypothesis test is the probability of rejecting null hypothesis . As stated above we may commit Type I and Type II errors while testing a hypothesis.  Accordingly 1 \u2013 b value is the measure of how well the test is working or what is technically described as the power of the test."}, {"text": "Confusion matrix not only gives you insight into the errors being made by your classifier but also types of errors that are being made. This breakdown helps you to overcomes the limitation of using classification accuracy alone. Every column of the confusion matrix represents the instances of that predicted class."}, {"text": "The common assumptions in nonparametric tests are randomness and independence. The chi\u2010square test is one of the nonparametric tests for testing three types of statistical tests: the goodness of fit, independence, and homogeneity."}, {"text": "The common assumptions in nonparametric tests are randomness and independence. The chi\u2010square test is one of the nonparametric tests for testing three types of statistical tests: the goodness of fit, independence, and homogeneity."}]}, {"question": "What does the T score tell you", "positive_ctxs": [{"text": "The t-value measures the size of the difference relative to the variation in your sample data. Put another way, T is simply the calculated difference represented in units of standard error. The greater the magnitude of T, the greater the evidence against the null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "How do face recognition algorithms detect human faces", "positive_ctxs": [{"text": "Face detection algorithms typically start by searching for human eyes -- one of the easiest features to detect. The algorithm might then attempt to detect eyebrows, the mouth, nose, nostrils and the iris.  The training improves the algorithms' ability to determine whether there are faces in an image and where they are."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Face recognition systems use computer algorithms to pick out specific, distinctive details about a person's face. These details, such as distance between the eyes or shape of the chin, are then converted into a mathematical representation and compared to data on other faces collected in a face recognition database."}, {"text": "One method of processing images is via face detection. Face detection is a branch of image processing that uses machine learning to detect faces in images. A Haar Cascade is an object detection method used to locate an object of interest in images."}, {"text": "Face detection is a broader term than face recognition. Face detection just means that a system is able to identify that there is a human face present in an image or video.  Face recognition can confirm identity. It is therefore used to control access to sensitive areas."}, {"text": "Face detection is a broader term than face recognition. Face detection just means that a system is able to identify that there is a human face present in an image or video.  Face recognition can confirm identity. It is therefore used to control access to sensitive areas."}, {"text": "In neural networks, Convolutional neural network (ConvNets or CNNs) is one of the main categories to do images recognition, images classifications. Objects detections, recognition faces etc., are some of the areas where CNNs are widely used."}, {"text": "Based on Deep convolutional neural networks, DeepFace is a deep learning face recognition system. Created by Facebook, it detects and determines the identity of an individual's face through digital images, reportedly with an accuracy of 97.35%."}, {"text": "AI-based face recognition and biometric system helping to keep track the human beings and provide a safe zone to live. Security cameras and other surveillance equipment are widely used to keep the cities and habitat safe. Automated assembly lines in automotive sectors are making cars with higher production."}]}, {"question": "How do you differentiate between univariate bivariate and multivariate analysis in real world scenarios", "positive_ctxs": [{"text": "Univariate statistics summarize only one variable at a time.  Multivariate statistics compare more than two variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "- Quora. Non-significant variables on univariate analysis became significant on multivariate analysis?  Yes, it is possible that when you add more predictors (X2, X3 and so forth) in a multiple regression, X1 can become a statistically significant predictor."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.  Like univariate analysis, bivariate analysis can be descriptive or inferential."}, {"text": "Univariate and multivariate represent two approaches to statistical analysis. Univariate involves the analysis of a single variable while multivariate analysis examines two or more variables. Most multivariate analysis involves a dependent variable and multiple independent variables."}, {"text": "Univariate and multivariate represent two approaches to statistical analysis. Univariate involves the analysis of a single variable while multivariate analysis examines two or more variables. Most multivariate analysis involves a dependent variable and multiple independent variables."}, {"text": "Another common example of univariate analysis is the mean of a population distribution. Tables, charts, polygons, and histograms are all popular methods for displaying univariate analysis of a specific variable (e.g. mean, median, mode, standard variation, range, etc)."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Linear regression is a linear method to model the relationship between your independent variables and your dependent variables. Advantages include how simple it is and ease with implementation and disadvantages include how is' lack of practicality and how most problems in our real world aren't \u201clinear\u201d."}]}, {"question": "What type of algorithm is random forest", "positive_ctxs": [{"text": "Random forest is a supervised learning algorithm. The \"forest\" it builds, is an ensemble of decision trees, usually trained with the \u201cbagging\u201d method. The general idea of the bagging method is that a combination of learning models increases the overall result."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Again, random forest is very effective on a wide range of problems, but like bagging, performance of the standard algorithm is not great on imbalanced classification problems."}, {"text": "The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree."}, {"text": "The main limitation of random forest is that a large number of trees can make the algorithm too slow and ineffective for real-time predictions. In general, these algorithms are fast to train, but quite slow to create predictions once they are trained."}, {"text": "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging."}, {"text": "Bootstrap aggregating (bagging) In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy."}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}, {"text": "The fundamental reason to use a random forest instead of a decision tree is to combine the predictions of many decision trees into a single model. The logic is that a single even made up of many mediocre models will still be better than one good model."}]}, {"question": "How do you find the critical value of Z in a hypothesis test", "positive_ctxs": [{"text": "The level of significance which is selected in Step 1 (e.g., \u03b1 =0.05) dictates the critical value. For example, in an upper tailed Z test, if \u03b1 =0.05 then the critical value is Z=1.645."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Lets do this step by step:Step 1: find the mean.Step 2: fin the standard deviation of the mean (using the population SD)Step 3: find the Z score.Step 4: compare to the critical Z score. From the stated hypothesis, we know that we are dealing with a 1-tailed hypothesis test.  Step 4 : compare to the critical Z score."}, {"text": "It is a criterion under which a hypothesis tester decides whether a given hypothesis must be accepted or rejected. The general rule of thumb is that if the value of test statics is greater than the critical value then the null hypothesis is rejected in the favor of the alternate hypothesis."}, {"text": "In statistics, a two-tailed test is a method in which the critical area of a distribution is two-sided and tests whether a sample is greater or less than a range of values.  If the sample being tested falls into either of the critical areas, the alternative hypothesis is accepted instead of the null hypothesis."}, {"text": "3.2 How to test for differences between samplesDecide on a hypothesis to test, often called the \u201cnull hypothesis\u201d (H0 ). In our case, the hypothesis is that there is no difference between sets of samples.  Decide on a statistic to test the truth of the null hypothesis.Calculate the statistic.Compare it to a reference value to establish significance, the P-value."}, {"text": "Normal Distribution For a one-tailed test, the critical value is 1.645. So the critical region is Z<\u22121.645 for a left-tailed test and Z>1.645 for a right-tailed test. For a two-tailed test, the critical value is 1.96."}, {"text": "To find the critical value, follow these steps.Compute alpha (\u03b1): \u03b1 = 1 - (confidence level / 100)Find the critical probability (p*): p* = 1 - \u03b1/2.To express the critical value as a z-score, find the z-score having a cumulative probability equal to the critical probability (p*).More items"}]}, {"question": "What is the Turing test for artificial intelligence", "positive_ctxs": [{"text": "A Turing Test is a method of inquiry in artificial intelligence (AI) for determining whether or not a computer is capable of thinking like a human being. The test is named after Alan Turing, the founder of the Turing Test and an English computer scientist, cryptanalyst, mathematician and theoretical biologist."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "No. A universal Turing machine is a Turing machine that takes as its input a string of the form where is the representation of the transition table of Turing machine and is a string over the input alphabet of ."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "The universality property of Turing machines states that there exists a Turing machine, which can simulate the behaviour of any other Turing machine.  It says that a Turing machine can be adapted to different tasks by programming; from the viewpoint of computability it is not necessary to build special-purpose machines."}, {"text": "Java, Python, Lisp, Prolog, and C++ are major AI programming language used for artificial intelligence capable of satisfying different needs in the development and designing of different software.  It answers the question, 'what is the language used for artificial intelligence?"}, {"text": "In computer science, a universal Turing machine (UTM) is a Turing machine that simulates an arbitrary Turing machine on arbitrary input.  In terms of computational complexity, a multi-tape universal Turing machine need only be slower by logarithmic factor compared to the machines it simulates."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}]}, {"question": "What is logically equivalent to P \u2192 Q", "positive_ctxs": [{"text": "A compound proposition that is always True is called a tautology. Two propositions p and q are logically equivalent if their truth tables are the same. Namely, p and q are logically equivalent if p \u2194 q is a tautology. If p and q are logically equivalent, we write p \u2261 q."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "P \u2227 Q means P and Q. P \u2228 Q means P or Q. An argument is valid if the following conditional holds: If all the premises are true, the conclusion must be true.  So, when you attempt to write a valid argument, you should try to write out what the logical structure of the argument is by symbolizing it."}, {"text": "If at the limit n \u2192 \u221e the estimator tend to be always right (or at least arbitrarily close to the target), it is said to be consistent. This notion is equivalent to convergence in probability defined below."}, {"text": "The learning algorithm is called consistent with respect to F and P if the risk R(fn) converges in probability to the risk R(fF) of the best classifier in F, that is for all \u03b5 > 0, P(R(fn) \u2212 R(fF) > \u03b5) \u2192 0 as n \u2192 \u221e. 2."}, {"text": "Contrapositive: The contrapositive of a conditional statement of the form \"If p then q\" is \"If ~q then ~p\". Symbolically, the contrapositive of p q is ~q ~p. A conditional statement is logically equivalent to its contrapositive."}, {"text": "The converse of the conditional statement is \u201cIf Q then P.\u201d The contrapositive of the conditional statement is \u201cIf not Q then not P.\u201d The inverse of the conditional statement is \u201cIf not P then not Q.\u201d"}, {"text": "Type I error is equivalent to a False positive. Type II error is equivalent to a False negative. Type I error refers to non-acceptance of hypothesis which ought to be accepted. Type II error is the acceptance of hypothesis which ought to be rejected."}, {"text": "A binomial random variable is the number of successes x in n repeated trials of a binomial experiment.Binomial DistributionThe mean of the distribution (\u03bcx) is equal to n * P .The variance (\u03c32x) is n * P * ( 1 - P ).The standard deviation (\u03c3x) is sqrt[ n * P * ( 1 - P ) ]."}]}, {"question": "Do we ever use maximum likelihood estimation", "positive_ctxs": [{"text": "We can use MLE in order to get more robust parameter estimates. Thus, MLE can be defined as a method for estimating population parameters (such as the mean and variance for Normal, rate (lambda) for Poisson, etc.) from sample data such that the probability (likelihood) of obtaining the observed data is maximized."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Intuitive explanation of maximum likelihood estimation Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed."}, {"text": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable."}, {"text": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable."}, {"text": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable."}, {"text": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable."}, {"text": "Maximum likelihood estimation is a method that will find the values of \u03bc and \u03c3 that result in the curve that best fits the data.  The goal of maximum likelihood is to find the parameter values that give the distribution that maximise the probability of observing the data."}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}]}, {"question": "How do you find the correlation between two categorical variables", "positive_ctxs": [{"text": "To measure the relationship between numeric variable and categorical variable with > 2 levels you should use eta correlation (square root of the R2 of the multifactorial regression). If the categorical variable has 2 levels, point-biserial correlation is used (equivalent to the Pearson correlation)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "For a dichotomous categorical variable and a continuous variable you can calculate a Pearson correlation if the categorical variable has a 0/1-coding for the categories.  But when you have more than two categories for the categorical variable the Pearson correlation is not appropriate anymore."}, {"text": "It is possible to find the correlation between a categorical variable and a continuous variable using the analysis of covariance technique."}, {"text": "Association between two variables means the values of one variable relate in some way to the values of the other. Association is usually measured by correlation for two continuous variables and by cross tabulation and a Chi-square test for two categorical variables."}, {"text": "Association between two variables means the values of one variable relate in some way to the values of the other. Association is usually measured by correlation for two continuous variables and by cross tabulation and a Chi-square test for two categorical variables."}, {"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}, {"text": "How to Read a Correlation Matrix-1 indicates a perfectly negative linear correlation between two variables.0 indicates no linear correlation between two variables.1 indicates a perfectly positive linear correlation between two variables."}, {"text": "To measure the relationship between numeric variable and categorical variable with > 2 levels you should use eta correlation (square root of the R2 of the multifactorial regression). If the categorical variable has 2 levels, point-biserial correlation is used (equivalent to the Pearson correlation)."}]}, {"question": "What is the significance of residual networks", "positive_ctxs": [{"text": "To conclude, it can be said that residual networks have become quite popular for image recognition and classification tasks because of their ability to solve vanishing and exploding gradients when adding more layers to an already deep neural network. A ResNet with thousand layers has not much practical use as of now."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture."}, {"text": "One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture."}, {"text": "One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture."}, {"text": "One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture."}, {"text": "In statistics, a studentized residual is the quotient resulting from the division of a residual by an estimate of its standard deviation. It is a form of a Student's t-statistic, with the estimate of error varying between points. This is an important technique in the detection of outliers."}, {"text": "So, to find the residual I would subtract the predicted value from the measured value so for x-value 1 the residual would be 2 - 2.6 = -0.6. Mentor: That is right! The residual of the independent variable x=1 is -0.6."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}]}, {"question": "Should independent variables be normally distributed for ordered logit model", "positive_ctxs": [{"text": "First, logistic regression does not require a linear relationship between the dependent and independent variables. Second, the error terms (residuals) do not need to be normally distributed.  This means that the independent variables should not be too highly correlated with each other."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Standard interpretation of the ordered logit coefficient is that for a one unit increase in the predictor, the response variable level is expected to change by its respective regression coefficient in the ordered log-odds scale while the other variables in the model are held constant."}, {"text": "There are NO assumptions in any linear model about the distribution of the independent variables. Yes, you only get meaningful parameter estimates from nominal (unordered categories) or numerical (continuous or discrete) independent variables.  They do not need to be normally distributed or continuous."}, {"text": "There are NO assumptions in any linear model about the distribution of the independent variables. Yes, you only get meaningful parameter estimates from nominal (unordered categories) or numerical (continuous or discrete) independent variables.  They do not need to be normally distributed or continuous."}, {"text": "There are NO assumptions in any linear model about the distribution of the independent variables. Yes, you only get meaningful parameter estimates from nominal (unordered categories) or numerical (continuous or discrete) independent variables.  They do not need to be normally distributed or continuous."}, {"text": "A multivariate normal distribution is a vector in multiple normally distributed variables, such that any linear combination of the variables is also normally distributed."}, {"text": "normal approximation: The process of using the normal curve to estimate the shape of the distribution of a data set. central limit theorem: The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed."}, {"text": "Key Terms. normal approximation: The process of using the normal curve to estimate the shape of the distribution of a data set. central limit theorem: The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed."}]}, {"question": "Who proved the mean value theorem", "positive_ctxs": [{"text": "Augustin Louis Cauchy"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Law of large numbers, in statistics, the theorem that, as the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean. The law of large numbers was first proved by the Swiss mathematician Jakob Bernoulli in 1713."}, {"text": "The central limit theorem states that the sampling distribution of the mean approaches a normal distribution, as the sample size increases.  Therefore, as a sample size increases, the sample mean and standard deviation will be closer in value to the population mean \u03bc and standard deviation \u03c3 ."}, {"text": "The law of large numbers, in probability and statistics, states that as a sample size grows, its mean gets closer to the average of the whole population. In the 16th century, mathematician Gerolama Cardano recognized the Law of Large Numbers but never proved it."}, {"text": "The law of large numbers, in probability and statistics, states that as a sample size grows, its mean gets closer to the average of the whole population. In the 16th century, mathematician Gerolama Cardano recognized the Law of Large Numbers but never proved it."}, {"text": "The law of large numbers states that the sample mean of independent and identically distributed observations converges to a certain value. The central limit theorem describes the distribution of the difference between the sample mean and that value."}, {"text": "Mean DeviationFind the mean of all values.Find the distance of each value from that mean (subtract the mean from each value, ignore minus signs)Then find the mean of those distances."}, {"text": "To find the mean absolute deviation of the data, start by finding the mean of the data set. Find the sum of the data values, and divide the sum by the number of data values. Find the absolute value of the difference between each data value and the mean: |data value \u2013 mean|."}]}, {"question": "What is the problem with bias", "positive_ctxs": [{"text": "Bias can damage research, if the researcher chooses to allow his bias to distort the measurements and observations or their interpretation. When faculty are biased about individual students in their courses, they may grade some students more or less favorably than others, which is not fair to any of the students."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "If the biggest problem with supervised learning is the expense of labeling the training data, the biggest problem with unsupervised learning (where the data is not labeled) is that it often doesn't work very well."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated."}, {"text": "In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "How do you convert sample variance to population variance", "positive_ctxs": [{"text": "When I calculate population variance, I then divide the sum of squared deviations from the mean by the number of items in the population (in example 1 I was dividing by 12). When I calculate sample variance, I divide it by the number of items in the sample less one."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data. Due to this value of denominator in the formula for variance in case of sample data is 'n-1', and it is 'n' for population data."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}, {"text": "The variance of the sampling distribution of the mean is computed as follows: That is, the variance of the sampling distribution of the mean is the population variance divided by N, the sample size (the number of scores used to compute a mean)."}, {"text": "Sample variance Dividing instead by n \u2212 1 yields an unbiased estimator.  In other words, the expected value of the uncorrected sample variance does not equal the population variance \u03c32, unless multiplied by a normalization factor. The sample mean, on the other hand, is an unbiased estimator of the population mean \u03bc."}]}, {"question": "Which algorithm is right for machine learning", "positive_ctxs": [{"text": "An easy guide to choose the right Machine Learning algorithmSize of the training data. It is usually recommended to gather a good amount of data to get reliable predictions.  Accuracy and/or Interpretability of the output.  Speed or Training time.  Linearity.  Number of features."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Deep learning itself does feature engineering whereas machine learning requires manual feature engineering. 2) Which of the following is a representation learning algorithm? Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning."}, {"text": "Top reasons to use feature selection are: It enables the machine learning algorithm to train faster. It reduces the complexity of a model and makes it easier to interpret. It improves the accuracy of a model if the right subset is chosen."}, {"text": "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging."}, {"text": "k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}]}, {"question": "What is the purpose of causal analysis", "positive_ctxs": [{"text": "The purpose of causal analysis is trying to find the root cause of a problem instead of finding the symptoms. This technique helps to uncover the facts that lead to a certain situation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.  Like univariate analysis, bivariate analysis can be descriptive or inferential."}, {"text": "Univariate analysis has the purpose to describe a single variable distribution in one sample. It is the first important step of every clinical trial."}, {"text": "The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models."}, {"text": "Robust regression is an alternative to least squares regression when data is contaminated with outliers or influential observations and it can also be used for the purpose of detecting influential observations. Please note: The purpose of this page is to show how to use various data analysis commands."}, {"text": "The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models. Most often, factors are rotated after extraction."}]}, {"question": "What is a squared in Anderson Darling normality test", "positive_ctxs": [{"text": "A-squared is the test statistic for the Anderson-Darling Normality test. It is a measure of how closely a dataset follows the normal distribution.  So if you get an A-squared that is fairly large, then you will get a small p-value and thus reject the null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above . 05 indicates normality."}, {"text": "The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above . 05 indicates normality."}, {"text": "The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above . 05 indicates normality."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "A normality test is used to determine whether sample data has been drawn from a normally distributed population (within some tolerance). A number of statistical tests, such as the Student's t-test and the one-way and two-way ANOVA require a normally distributed sample population."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}]}, {"question": "Which devices support TensorFlow Lite for inference", "positive_ctxs": [{"text": "TensorFlow Lite inferenceAndroid Platform.iOS Platform.Linux Platform."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The workflow for using TensorFlow Lite involves the following steps:Pick a model. Bring your own TensorFlow model, find a model online, or pick a model from our Pre-trained models to drop in or retrain.Convert the model.  Deploy to your device.  Optimize your model."}, {"text": "We have compiled a list of best practices and strategies that you can use to improve your TensorFlow Lite model performance.Choose the best model for the task.  Profile your model.  Profile and optimize operators in the graph.  Optimize your model.  Tweak the number of threads.  Eliminate redundant copies.More items"}, {"text": "How to deploy an Object Detection Model with TensorFlow servingCreate a production ready model for TF-Serving.  Create TF-serving environment using Docker.  Creating a client to request the model server running in the Docker container for inference on a test image."}, {"text": "TensorFlow is an open source machine learning framework for carrying out high-performance numerical computations. It provides excellent architecture support which allows easy deployment of computations across a variety of platforms ranging from desktops to clusters of servers, mobiles, and edge devices."}, {"text": "Forward chaining starts from known facts and applies inference rule to extract more data unit it reaches to the goal. Backward chaining starts from the goal and works backward through inference rules to find the required facts that support the goal.  Backward chaining reasoning applies a depth-first search strategy."}, {"text": "TensorFlow is a free and open-source software library for machine learning. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. Tensorflow is a symbolic math library based on dataflow and differentiable programming."}, {"text": "keras is tightly integrated into the TensorFlow ecosystem, and also includes support for: tf. data, enabling you to build high performance input pipelines. If you prefer, you can train your models using data in NumPy format, or use tf."}]}, {"question": "How do neurons work", "positive_ctxs": [{"text": "Basic principle A neuron (also known as nerve cell) is an electrically excitable cell that takes up, processes and transmits information through electrical and chemical signals. It is one of the basic elements of the nervous system. In order that a human being can react to his environment, neurons transport stimuli."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "Dropout causes your network to keep only some portion of neurons/weights on each iteration. Sometimes those neurons do not fit the current minibatch well, and this may cause large fluctuations."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}]}, {"question": "What does having biased groups mean", "positive_ctxs": [{"text": "In-group favoritism, sometimes known as in-group\u2013out-group bias, in-group bias, intergroup bias, or in-group preference, is a pattern of favoring members of one's in-group over out-group members. This can be expressed in evaluation of others, in allocation of resources, and in many other ways."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "The other assumption of one-way anova is that the variation within the groups is equal (homoscedasticity). While Kruskal-Wallis does not assume that the data are normal, it does assume that the different groups have the same distribution, and groups with different standard deviations have different distributions."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "This is because of the logistic distribution having heavier tails (than the normal distribution): Any outliers would not carry as much weight under the assumptions of the logistic (blue) distribution.  In a logistic regression does a very small P value for a predictor mean a good predictor or a bad predictor?"}, {"text": "A clinical trial is a randomized controlled trial only when participants are randomly allocated to the group receiving the treatment and a control group. What participants are allocated among groups receiving different treatments the clinical trial is simply called a randomized trial."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}]}, {"question": "How is the sample variance computed differently from the population variance", "positive_ctxs": [{"text": "The sample variance will always be a smaller value than the population variance. The sample variance will always be a larger value than the population variance."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The variance of the sampling distribution of the mean is computed as follows: That is, the variance of the sampling distribution of the mean is the population variance divided by N, the sample size (the number of scores used to compute a mean)."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data. Due to this value of denominator in the formula for variance in case of sample data is 'n-1', and it is 'n' for population data."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data."}, {"text": "Sampling Distribution of Sample Variance This is the variance of the population. The variance of this sampling distribution can be computed by finding the expected value of the square of the sample variance and subtracting the square of 2.92."}, {"text": "5 Answers. N is the population size and n is the sample size. The question asks why the population variance is the mean squared deviation from the mean rather than (N\u22121)/N=1\u2212(1/N) times it."}, {"text": "The variance estimated as the average squared difference from the sample mean will always be less than the variance estimated as the average squared difference from the population mean unless the sample mean equals the population mean in which case they will be the same."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}]}, {"question": "What is the difference between a loss function and a metric", "positive_ctxs": [{"text": "The loss function is used to optimize your model. This is the function that will get minimized by the optimizer. A metric is used to judge the performance of your model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A metric is a function that is used to judge the performance of your model. Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that you may use any loss function as a metric."}, {"text": "It is calculated in the same way - by running the network forward over inputs xi and comparing the network outputs \u02c6yi with the ground truth values yi using a loss function e.g. J=1N\u2211Ni=1L(\u02c6yi,yi) where L is the individual loss function based somehow on the difference between predicted value and target."}, {"text": "A loss function is used to optimize a machine learning algorithm. The loss is calculated on training and validation and its interpretation is based on how well the model is doing in these two sets.  An accuracy metric is used to measure the algorithm's performance in an interpretable way."}, {"text": "Cross Entropy is definitely a good loss function for Classification Problems, because it minimizes the distance between two probability distributions - predicted and actual.  So cross entropy make sure we are minimizing the difference between the two probability. This is the reason."}, {"text": "Cross Entropy is definitely a good loss function for Classification Problems, because it minimizes the distance between two probability distributions - predicted and actual.  So cross entropy make sure we are minimizing the difference between the two probability. This is the reason."}, {"text": "Typically, with neural networks, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply \u201closs.\u201d"}, {"text": "Logarithmic Loss, or simply Log Loss, is a classification loss function often used as an evaluation metric in Kaggle competitions.  Log Loss quantifies the accuracy of a classifier by penalising false classifications."}]}, {"question": "Why is sift scale invariant", "positive_ctxs": [{"text": "The scale-invariant feature transform (SIFT) is an algorithm used to detect and describe local features in digital images.  The descriptors are supposed to be invariant against various transformations which might make images look different although they represent the same object(s)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The subject of this chapter is image key points which we define as a distinctive point in an input image which is invariant to rotation, scale and distortion."}, {"text": "Given two random variables X and Y, the correlation is scale and location invariant in the sense that cor(X,Y)=cor(XT,YT), if XT=a+bX, and YT=c+dY, and b and d have the same sign (either both positive or both negative)."}, {"text": "The difference between a ratio scale and an interval scale is that the zero point on an interval scale is some arbitrarily agreed value, whereas on a ratio scale it is a true zero."}, {"text": "The interval scale of measurement is a type of measurement scale that is characterized by equal intervals between scale units. A perfect example of an interval scale is the Fahrenheit scale to measure temperature.  For example, suppose it is 60 degrees Fahrenheit on Monday and 70 degrees on Tuesday."}, {"text": "A ratio scale is a quantitative scale where there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales."}, {"text": "A ratio scale is a quantitative scale where there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales."}, {"text": "The ratio scale of measurement is the most informative scale.  However, zero on the Kelvin scale is absolute zero. This makes the Kelvin scale a ratio scale. For example, if one temperature is twice as high as another as measured on the Kelvin scale, then it has twice the kinetic energy of the other temperature."}]}, {"question": "Why do we use the discriminant", "positive_ctxs": [{"text": "Discriminant analysis is statistical technique used to classify observations into non-overlapping groups, based on scores on one or more quantitative predictor variables. For example, a doctor could perform a discriminant analysis to identify patients at high or low risk for stroke."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "An algorithm that uses random numbers to decide what to do next anywhere in its logic is called a Randomized Algorithm. For example, in Randomized Quick Sort, we use a random number to pick the next pivot (or we randomly shuffle the array). And in Karger's algorithm, we randomly pick an edge."}, {"text": "Number of discriminant functions. There is one discriminant function for 2- group discriminant analysis, but for higher order DA, the number of functions is the lesser of (g - 1), where g is the number of groups, or p,the number of discriminating (independent) variables."}, {"text": "We can use the median with the interquartile range, or we can use the mean with the standard deviation."}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}, {"text": "Why is an alpha level of . 05 commonly used? Seeing as the alpha level is the probability of making a Type I error, it seems to make sense that we make this area as tiny as possible.  The smaller the alpha level, the smaller the area where you would reject the null hypothesis."}, {"text": "Linear discriminant function analysis (i.e., discriminant analysis) performs a multivariate test of differences between groups.  In addition, discriminant analysis is used to determine the minimum number of dimensions needed to describe these differences."}]}, {"question": "How do you check machine learning accuracy", "positive_ctxs": [{"text": "The three main metrics used to evaluate a classification model are accuracy, precision, and recall. Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The clear sign of a machine learning overfitting is if its error on testing set is much greater than the error on training set.  For instance if the model accuracy for train data is 85% and the accuracy for test/validation data is 65% then its very obvious that the model has overlearned and you should check that."}, {"text": "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to Get Started with AIPick a topic you are interested in.Find a quick solution.Improve your simple solution.Share your solution.Repeat steps 1-4 for different problems.Complete a Kaggle competition.Use machine learning professionally."}, {"text": "You do not need to learn linear algebra before you get started in machine learning, but at some time you may wish to dive deeper.  It will give you the tools to help you with the other areas of mathematics required to understand and build better intuitions for machine learning algorithms."}, {"text": "How many parity check bits must be included with the data word to achieve single-bit error correction and double error correction when data words are as follows: 16 bits."}, {"text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."}]}, {"question": "How do you find the sample variance", "positive_ctxs": [{"text": "To calculate the variance follow these steps:Work out the Mean (the simple average of the numbers)Then for each number: subtract the Mean and square the result (the squared difference).Then work out the average of those squared differences. (Why Square?)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data. Due to this value of denominator in the formula for variance in case of sample data is 'n-1', and it is 'n' for population data."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "Which model is used for prediction", "positive_ctxs": [{"text": "One of the most widely used predictive analytics models, the forecast model deals in metric value prediction, estimating numeric value for new data based on learnings from historical data. This model can be applied wherever historical numerical data is available."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "Q17. Which of the following is true about \u201cRidge\u201d or \u201cLasso\u201d regression methods in case of feature selection? \u201cRidge regression\u201d will use all predictors in final model whereas \u201cLasso regression\u201d can be used for feature selection because coefficient values can be zero."}, {"text": "The Spearman correlation is the same as the Pearson correlation, but it is used on data from an ordinal scale. Which situation would be appropriate for obtaining a phi-coefficient with a Pearson test?"}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. ("}, {"text": "Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning."}]}, {"question": "How do you calculate survival analysis", "positive_ctxs": [{"text": "The Kaplan-Meier estimate is the simplest way of computing the survival over time in spite of all these difficulties associated with subjects or situations. For each time interval, survival probability is calculated as the number of subjects surviving divided by the number of patients at risk."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "More generally, survival analysis involves the modelling of time to event data; in this context, death or failure is considered an \"event\" in the survival analysis literature \u2013 traditionally only a single event occurs for each subject, after which the organism or mechanism is dead or broken."}, {"text": "CONCLUSION. There are three primary goals of survival analysis, to estimate and interpret survival and / or hazard functions from the survival data; to compare survival and / or hazard functions, and to assess the relationship of explanatory variables to survival time."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "In statistics, the one in ten rule is a rule of thumb for how many predictor parameters can be estimated from data when doing regression analysis (in particular proportional hazards models in survival analysis and logistic regression) while keeping the risk of overfitting low."}, {"text": "The survival function is S(t) = Pr(T >t)=1 \u2212 F(t). \u2013 The survival function gives the probability that a subject will survive past time t."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "How do type I and type II errors relate to each other for testing the hypothesis", "positive_ctxs": [{"text": "A type I error (false-positive) occurs if an investigator rejects a null hypothesis that is actually true in the population; a type II error (false-negative) occurs if the investigator fails to reject a null hypothesis that is actually false in the population."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Type I and type II errors are instrumental for the understanding of hypothesis testing in a clinical research scenario.  A type II error can be thought of as the opposite of a type I error and is when a researcher fails to reject the null hypothesis that is actually false in reality."}, {"text": "A type I error (false-positive) occurs if an investigator rejects a null hypothesis that is actually true in the population; a type II error (false-negative) occurs if the investigator fails to reject a null hypothesis that is actually false in the population."}, {"text": "A type I error (false-positive) occurs if an investigator rejects a null hypothesis that is actually true in the population; a type II error (false-negative) occurs if the investigator fails to reject a null hypothesis that is actually false in the population."}, {"text": "The false discovery rate (FDR) is a method of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons.  Thus, FDR-controlling procedures have greater power, at the cost of increased numbers of Type I errors."}, {"text": "The consequences of making a type I error mean that changes or interventions are made which are unnecessary, and thus waste time, resources, etc. Type II errors typically lead to the preservation of the status quo (i.e. interventions remain the same) when change is needed."}, {"text": "Type II Error and Power Calculations. Recall that in hypothesis testing you can make two types of errors \u2022 Type I Error \u2013 rejecting the null when it is true. \u2022 Type II Error \u2013 failing to reject the null when it is false.  = \u239b \u239e \u2212  \u2212 \u2212 = =  = \u239b \u239e \u2212"}, {"text": "In terms of the courtroom example, a type I error corresponds to convicting an innocent defendant. Type II error. The second kind of error is the failure to reject a false null hypothesis as the result of a test procedure."}]}, {"question": "What is the z value for 96 confidence interval", "positive_ctxs": [{"text": "Confidence Levelz0.951.960.962.050.982.330.992.586 more rows"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}, {"text": "In the large-sample case, a 95% confidence interval estimate for the population mean is given by x\u0304 \u00b1 1.96\u03c3/ \u221an. When the population standard deviation, \u03c3, is unknown, the sample standard deviation is used to estimate \u03c3 in the confidence interval formula."}, {"text": "A 95% confidence interval for \u03b2i has two equivalent definitions: The interval is the set of values for which a hypothesis test to the level of 5% cannot be rejected. The interval has a probability of 95% to contain the true value of \u03b2i ."}, {"text": "So, if your significance level is 0.05, the corresponding confidence level is 95%. If the P value is less than your significance (alpha) level, the hypothesis test is statistically significant. If the confidence interval does not contain the null hypothesis value, the results are statistically significant."}, {"text": "1 AnswerTake as central point of your confidence interval the sum of central points of every confidence interval (45+70+35=150 minutes).Take as radius of your interval the square root of the sum of the squares of the radius of every confidence interval \u221a52+102+52=12.25."}, {"text": "The 95% confidence interval (CI) is a range of values calculated from our data, that most likely, includes the true value of what we're estimating about the population."}, {"text": "The selection of a confidence level for an interval determines the probability that the confidence interval produced will contain the true parameter value. Common choices for the confidence level C are 0.90, 0.95, and 0.99. These levels correspond to percentages of the area of the normal density curve."}]}, {"question": "What is the difference between SVM and random forest", "positive_ctxs": [{"text": "For a classification problem Random Forest gives you probability of belonging to class. SVM gives you distance to the boundary, you still need to convert it to probability somehow if you need probability.  SVM gives you \"support vectors\", that is points in each class closest to the boundary between classes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Try to see the difference between an estimator and an estimate. An estimator is a random variable and an estimate is a number (that is the computed value of the estimator).  Similarly, the sample median would be a natural point estimator for the population median."}, {"text": "The regularization parameter (lambda) serves as a degree of importance that is given to miss-classifications. SVM pose a quadratic optimization problem that looks for maximizing the margin between both classes and minimizing the amount of miss-classifications.  For non-linear-kernel SVM the idea is the similar."}, {"text": "A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results."}]}, {"question": "What does a weak R squared value mean", "positive_ctxs": [{"text": "- if R-squared value 0.3 < r < 0.5 this value is generally considered a weak or low effect size, - if R-squared value 0.5 < r < 0.7 this value is generally considered a Moderate effect size, - if R-squared value r > 0.7 this value is generally considered strong effect size, Ref: Source: Moore, D. S., Notz, W."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the \u201cerrors\u201d) and squaring them. The squaring is necessary to remove any negative signs."}, {"text": "One way of finding a point estimate \u02c6x=g(y) is to find a function g(Y) that minimizes the mean squared error (MSE). Here, we show that g(y)=E[X|Y=y] has the lowest MSE among all possible estimators. That is why it is called the minimum mean squared error (MMSE) estimate. h(a)=E[(X\u2212a)2]=EX2\u22122aEX+a2."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "How to Calculate VarianceFind the mean of the data set. Add all data values and divide by the sample size n.Find the squared difference from the mean for each data value. Subtract the mean from each data value and square the result.Find the sum of all the squared differences.  Calculate the variance."}]}, {"question": "What are discriminant functions", "positive_ctxs": [{"text": "Discriminant function analysis (DFA) is a statistical procedure that classifies unknown individuals and the probability of their classification into a certain group (such as sex or ancestry group). Discriminant function analysis makes the assumption that the sample is normally distributed for the trait."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Number of discriminant functions. There is one discriminant function for 2- group discriminant analysis, but for higher order DA, the number of functions is the lesser of (g - 1), where g is the number of groups, or p,the number of discriminating (independent) variables."}, {"text": "Linear discriminant function analysis (i.e., discriminant analysis) performs a multivariate test of differences between groups.  In addition, discriminant analysis is used to determine the minimum number of dimensions needed to describe these differences."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and other fields, to find a linear combination of features that characterizes or separates two or more classes"}, {"text": "There are two possible objectives in a discriminant analysis: finding a predictive equation for classifying new individuals or interpreting the predictive equation to better understand the relationships that may exist among the variables. In many ways, discriminant analysis parallels multiple regression analysis."}, {"text": "Probability mass functions (pmf) are used to describe discrete probability distributions. While probability density functions (pdf) are used to describe continuous probability distributions."}, {"text": "The Bayes theorem is a basis for discriminant analysis."}]}, {"question": "What is localization in image processing", "positive_ctxs": [{"text": "The task of object localization is to predict the object in an image as well as its boundaries.  Simply, object localization aims to locate the main (or most visible) object in an image while object detection tries to find out all the objects and their boundaries."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}, {"text": "The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent."}, {"text": "The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent."}, {"text": "We discuss some wonders in the field of image processing with machine learning advancements. Image processing can be defined as the technical analysis of an image by using complex algorithms. Here, image is used as the input, where the useful information returns as the output."}, {"text": "Image processing is often viewed as arbitrarily manipulating an image to achieve an aesthetic standard or to support a preferred reality. However, image processing is more accurately defined as a means of translation between the human visual system and digital imaging devices."}, {"text": "Image processing is often viewed as arbitrarily manipulating an image to achieve an aesthetic standard or to support a preferred reality. However, image processing is more accurately defined as a means of translation between the human visual system and digital imaging devices."}]}, {"question": "What algorithm is used in Facebook", "positive_ctxs": [{"text": "It is believed that Facebook's new algorithm is based on the Vickrey-Clarke-Groves algorithm, which \u201coperates as a closed auction.\u201d Facebook's algorithm for ranking content on your News Feed is based on four factors: The Inventory of all posts available to display. Signals that tell Facebook what each post is."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Facebook Trending is a feature of the social network designed to show each user a list of topics that are spiking in popularity in updates, posts, and comments. Facebook Trending appears as a short list of keywords and phrases in a small module at the top right of the user's News Feed."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Implementing Deep Learning Methods and Feature Engineering for Text Data: FastText. Overall, FastText is a framework for learning word representations and also performing robust, fast and accurate text classification. The framework is open-sourced by Facebook on GitHub."}, {"text": "Implementing Deep Learning Methods and Feature Engineering for Text Data: FastText. Overall, FastText is a framework for learning word representations and also performing robust, fast and accurate text classification. The framework is open-sourced by Facebook on GitHub."}]}, {"question": "What is normalization in machine learning", "positive_ctxs": [{"text": "Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling.  Others sustain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks."}, {"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}]}, {"question": "How can Lstm improve performance", "positive_ctxs": [{"text": "Network StructureGated Recurrent Unit. GRU (Cho14) alternative memory cell design to LSTM.  Layer normalization. Adding layer normalization (Ba16) to all linear mappings of the recurrent network speeds up learning and often improves final performance.  Feed-forward layers first.  Stacked recurrent networks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "From the network operations perspective, streaming telemetry can improve efficiency in many use cases, including: Detecting problems by setting up network monitors and alerts based on pre-configured thresholds or network performance baselines. Troubleshooting connectivity and performance issues."}, {"text": "Gradient boosting is a greedy algorithm and can overfit a training dataset quickly. It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting."}, {"text": "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself."}, {"text": "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself."}, {"text": "Ensemble learning helps improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model."}, {"text": "Definition. Data Partitioning is the technique of distributing data across multiple tables, disks, or sites in order to improve query processing performance or increase database manageability."}, {"text": "Definition. Data Partitioning is the technique of distributing data across multiple tables, disks, or sites in order to improve query processing performance or increase database manageability."}]}, {"question": "How are machine learning data science and statistics related", "positive_ctxs": [{"text": "Machine learning models are designed to make the most accurate predictions possible.  A statistical model is a model for the data that is used either to infer something about the relationships within the data or to create a model that is able to predict future values. Often, these two go hand-in-hand."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data."}, {"text": "Statistical machine learning merges statistics with the computational sciences---computer science, systems science and optimization.  Moreover, by its interdisciplinary nature, statistical machine learning helps to forge new links among these fields."}, {"text": "Because data science is a broad term for multiple disciplines, machine learning fits within data science. Machine learning uses various techniques, such as regression and supervised clustering. On the other hand, the data' in data science may or may not evolve from a machine or a mechanical process."}, {"text": "The answer is a big NO. Data science gets solutions and results to specific business problems using AI as a tool. If data science is to insights, machine learning is to predictions and artificial intelligence is to actions."}, {"text": "Big data analysis caters to a large amount of data set which is also known as data mining, but data science makes use of the machine learning algorithms to design and develop statistical models to generate knowledge from the pile of big data."}, {"text": "Big data analysis caters to a large amount of data set which is also known as data mining, but data science makes use of the machine learning algorithms to design and develop statistical models to generate knowledge from the pile of big data."}, {"text": "Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns. Two major goals in the study of biological systems are inference and prediction.  Many methods from statistics and machine learning (ML) may, in principle, be used for both prediction and inference."}]}, {"question": "Is ReLU function differentiable", "positive_ctxs": [{"text": "ReLU is differentiable at all the point except 0. the left derivative at z = 0 is 0 and the right derivative is 1.  Hidden units that are not differentiable are usually non-differentiable at only a small number of points."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "The indicator function 1[0,\u221e) is right differentiable at every real a, but discontinuous at zero (note that this indicator function is not left differentiable at zero)."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Leaky ReLU & Parametric ReLU (PReLU) Leaky ReLU has two benefits: It fixes the \u201cdying ReLU\u201d problem, as it doesn't have zero-slope parts. It speeds up training. There is evidence that having the \u201cmean activation\u201d be close to 0 makes training faster."}, {"text": "Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2."}, {"text": "Relative Frequency Of A Class Is The Percentage Of The Data That Falls In That Class, While Cumulative Frequency Of A Class Is The Sum Of The Frequencies Of That Class And All Previous Classes."}, {"text": "Hinge loss is not differentiable and cannot be used with methods which are differentiable like stochastic gradient descent(SGD). In this case Cross entropy(log loss) can be used."}]}, {"question": "How do you prove probability convergence", "positive_ctxs": [{"text": "In this case, convergence in distribution implies convergence in probability. We can state the following theorem: Theorem If Xn d\u2192 c, where c is a constant, then Xn p\u2192 c. Since Xn d\u2192 c, we conclude that for any \u03f5>0, we have limn\u2192\u221eFXn(c\u2212\u03f5)=0,limn\u2192\u221eFXn(c+\u03f52)=1."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Convergence in probability implies convergence in distribution. In the opposite direction, convergence in distribution implies convergence in probability when the limiting random variable X is a constant. Convergence in probability does not imply almost sure convergence."}, {"text": "Convergence in probability implies convergence in distribution. In the opposite direction, convergence in distribution implies convergence in probability when the limiting random variable X is a constant. Convergence in probability does not imply almost sure convergence."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Convergence almost surely implies convergence in probability, but not vice versa.  That is, convergence to 0 in probability says that the 1's will get rarer and rarer as one looks ahead in the sequence. In contrast, almost surely is equivalent to the statement that, with probability 1, there exists such that for all ."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}]}, {"question": "What is vector autoregressive model in time series", "positive_ctxs": [{"text": "Vector autoregression (VAR) is a statistical model used to capture the relationship between multiple quantities as they change over time.  This equation includes the variable's lagged (past) values, the lagged values of the other variables in the model, and an error term."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An autoregressive model is when a value from a time series is regressed on previous values from that same time series.  The order of an autoregression is the number of immediately preceding values in the series that are used to predict the value at the present time."}, {"text": "An autoregressive integrated moving average, or ARIMA, is a statistical analysis model that uses time series data to either better understand the data set or to predict future trends."}, {"text": "An autoregressive integrated moving average, or ARIMA, is a statistical analysis model that uses time series data to either better understand the data set or to predict future trends."}, {"text": "An autoregressive model is when a value from a time series is regressed on previous values from that same time series.  In this regression model, the response variable in the previous time period has become the predictor and the errors have our usual assumptions about errors in a simple linear regression model."}, {"text": "An autoregressive (AR) model predicts future behavior based on past behavior. It's used for forecasting when there is some correlation between values in a time series and the values that precede and succeed them.  Where simple linear regression and AR models differ is that Y is dependent on X and previous values for Y."}, {"text": "Time series forecasting is an important area of machine learning that is often neglected. It is important because there are so many prediction problems that involve a time component.  Standard definitions of time series, time series analysis, and time series forecasting."}, {"text": "A stationary time series is one whose properties do not depend on the time at which the series is observed. 14. Thus, time series with trends, or with seasonality, are not stationary \u2014 the trend and seasonality will affect the value of the time series at different times."}]}, {"question": "How do you find the relative frequency distribution", "positive_ctxs": [{"text": "To find the relative frequency, divide the frequency by the total number of data values. To find the cumulative relative frequency, add all of the previous relative frequencies to the relative frequency for the current row."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To find the relative frequency, divide the frequency by the total number of data values. To find the cumulative relative frequency, add all of the previous relative frequencies to the relative frequency for the current row."}, {"text": "The only difference between a relative frequency distribution graph and a frequency distribution graph is that the vertical axis uses proportional or relative frequency rather than simple frequency. Cumulative relative frequency (also called an ogive) is the accumulation of the previous relative frequencies."}, {"text": "A marginal distribution is a frequency or relative frequency distribution of either the row or column variable in a contingency table.  A conditional distribution lists the relative frequency of each category of the response variable, given a specific value of the explanatory variable in a contingency table."}, {"text": "A marginal distribution is a frequency or relative frequency distribution of either the row or column variable in a contingency table.  A conditional distribution lists the relative frequency of each category of the response variable, given a specific value of the explanatory variable in a contingency table."}, {"text": "As the area of a bar represents the frequency of its interval, the height of the bar represents the density. If you label the scare it is either frequency per unit or, if you divide by the total frequency, relative frequency per unit."}, {"text": "How to calculate the absolute error and relative errorTo find out the absolute error, subtract the approximated value from the real one: |1.41421356237 - 1.41| = 0.00421356237.Divide this value by the real value to obtain the relative error: |0.00421356237 / 1.41421356237| = 0.298%"}, {"text": "A relative frequency distribution lists the data values along with the percent of all observations belonging to each group. These relative frequencies are calculated by dividing the frequencies for each group by the total number of observations."}]}, {"question": "What is the Fourier transform of a periodic signal", "positive_ctxs": [{"text": "Specifical- ly, for periodic signals we can define the Fourier transform as an impulse train with the impulses occurring at integer multiples of the fundamental frequency and with amplitudes equal to 27r times the Fourier series coefficients."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Fourier Series is a specialized tool that allows for any periodic signal (subject to certain conditions) to be decomposed into an infinite sum of everlasting sinusoids.  Practically, this allows the user of the Fourier Series to understand a periodic signal as the sum of various frequency components."}, {"text": "In mathematics, a Fourier series (/\u02c8f\u028arie\u026a, -i\u0259r/) is a periodic function composed of harmonically related sinusoids, combined by a weighted summation.  The discrete-time Fourier transform is an example of Fourier series. The process of deriving the weights that describe a given function is a form of Fourier analysis."}, {"text": "A fast Fourier transform (FFT) is an algorithm that computes the discrete Fourier transform (DFT) of a sequence, or its inverse (IDFT). Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa."}, {"text": "In mathematics, a Fourier transform (FT) is a mathematical transform that decomposes a function (often a function of time, or a signal) into its constituent frequencies, such as the expression of a musical chord in terms of the volumes and frequencies of its constituent notes."}, {"text": "In mathematics, a Fourier transform (FT) is a mathematical transform that decomposes a function (often a function of time, or a signal) into its constituent frequencies, such as the expression of a musical chord in terms of the volumes and frequencies of its constituent notes."}, {"text": "Fourier Methods in Signal Processing The Fourier transform and discrete-time Fourier transform are mathematical analysis tools and cannot be evaluated exactly in a computer. The Fourier transform is used to analyze problems involving continuous-time signals or mixtures of continuous- and discrete-time signals."}, {"text": "In short, fourier series is for periodic signals and fourier transform is for aperiodic signals. Fourier series is used to decompose signals into basis elements (complex exponentials) while fourier transforms are used to analyze signal in another domain (e.g. from time to frequency, or vice versa)."}]}, {"question": "What does R Squared mean in stats", "positive_ctxs": [{"text": "R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.  So, if the R2 of a model is 0.50, then approximately half of the observed variation can be explained by the model's inputs."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Performance Metrics for Regression Mean Absolute Error (MAE) Mean Squared Error (MSE) Root Mean Squared Error (RMSE) R-Squared."}, {"text": "The Mean Squared Error (MSE) is a measure of how close a fitted line is to data points.  The MSE has the units squared of whatever is plotted on the vertical axis. Another quantity that we calculate is the Root Mean Squared Error (RMSE). It is just the square root of the mean square error."}, {"text": "Coefficient of correlation is \u201cR\u201d value which is given in the summary table in the Regression output. R square is also called coefficient of determination. Multiply R times R to get the R square value. In other words Coefficient of Determination is the square of Coefficeint of Correlation."}]}, {"question": "Which machine learning algorithms use both labeled and unlabeled data for training", "positive_ctxs": [{"text": "Unsupervised: All data is unlabeled and the algorithms learn to inherent structure from the input data. Semi-supervised: Some data is labeled but most of it is unlabeled and a mixture of supervised and unsupervised techniques can be used."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data)."}, {"text": "Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data)."}, {"text": "Semi-supervised clustering is a bridge between Supervised Learning and Cluster Analysis. it's about learning with both labeled and unlabeled data: sometimes we have some prior knowledge about clusters, e.g. we could have some label information."}, {"text": "Typically, unlabeled data consists of samples of natural or human-created artifacts that you can obtain relatively easily from the world.  Semi-supervised learning attempts to combine unlabeled and labeled data (or, more generally, sets of unlabeled data where only some data points have labels) into integrated models."}, {"text": "In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own."}, {"text": "In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own."}, {"text": "In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own."}]}, {"question": "Is there a non parametric equivalent of a 2 way Anova", "positive_ctxs": [{"text": "Friedman Test Therefore, we have a non-parametric equivalent of the two way ANOVA that can be used for data sets which do not fulfill the assumptions of the parametric method. The method, which is sometimes known as Friedman's two way analysis of variance, is purely a hypothesis test."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Here it is in plain language. An OR of 1.2 means there is a 20% increase in the odds of an outcome with a given exposure. An OR of 2 means there is a 100% increase in the odds of an outcome with a given exposure. Or this could be stated that there is a doubling of the odds of the outcome."}, {"text": "A non parametric test (sometimes called a distribution free test) does not assume anything about the underlying distribution (for example, that the data comes from a normal distribution).  It usually means that you know the population data does not have a normal distribution."}, {"text": "The most common functional form is parametric linear model, as a type of parametric regression, is frequently used to describe the relationship between a dependent variable and explanatory variables. Parametric linear models require the estimation of a finite number of parameters, \u03b2."}, {"text": "A frequency count is a measure of the number of times that an event occurs. Thus, a relative frequency of 0.50 is equivalent to a percentage of 50%."}, {"text": "Beyond the agent and the environment, there are four main elements of a reinforcement learning system: a policy, a reward, a value function, and, optionally, a model of the environment. A policy defines the way the agent behaves in a given time."}, {"text": "Paired means that both samples consist of the same test subjects. A paired t-test is equivalent to a one-sample t-test. Unpaired means that both samples consist of distinct test subjects. An unpaired t-test is equivalent to a two-sample t-test."}, {"text": "LDA is a parametric model, and the parameter is number of topics."}]}, {"question": "Which of the following are examples of active learning", "positive_ctxs": [{"text": "Group projects, discussions, and writing are examples of active learning, because they involve doing something."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "Other examples of active learning techniques include role-playing, case studies, group projects, think-pair-share, peer teaching, debates, Just-in-Time Teaching, and short demonstrations followed by class discussion. There are two easy ways to promote active learning through the discussion."}, {"text": "In active learning teachers are facilitators rather than one way providers of information.  Other examples of active learning techniques include role-playing, case studies, group projects, think-pair-share, peer teaching, debates, Just-in-Time Teaching, and short demonstrations followed by class discussion."}, {"text": "Deep learning itself does feature engineering whereas machine learning requires manual feature engineering. 2) Which of the following is a representation learning algorithm? Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning."}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}]}, {"question": "What are local features of an image", "positive_ctxs": [{"text": "Local features refer to a pattern or distinct structure found in an image, such as a point, edge, or small image patch. They are usually associated with an image patch that differs from its immediate surroundings by texture, color, or intensity."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Bag-of-Words (BoW) framework is well-known in image classification. In the framework, there are two essential steps: 1) coding, which encodes local features by a visual vocabulary, and 2) pooling, which pools over the response of all features into image representation."}, {"text": "Convolutional Neural Networks (CNNs) is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough."}, {"text": "Convolutional Neural Networks (CNNs) is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough."}, {"text": "A feature vector is just a vector that contains information describing an object's important characteristics. In image processing, features can take many forms. A simple feature representation of an image is the raw intensity value of each pixel. However, more complicated feature representations are also possible."}, {"text": "The method of analyzing an image that has undergone binarization processing is called \"blob analysis\". A blob refers to a lump. Blob analysis is image processing's most basic method for analyzing the shape features of an object, such as the presence, number, area, position, length, and direction of lumps."}, {"text": "The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data."}, {"text": "Feature Extraction using Convolution Neural Networks (CNN) and Deep Learning.  It is a process which involves the following tasks of pre-processing the image (normalization), image segmentation, extraction of key features and identification of the class."}]}, {"question": "What is a random variable in probability theory", "positive_ctxs": [{"text": "A random variable is a numerical description of the outcome of a statistical experiment.  For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x). This function provides the probability for each value of the random variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined."}, {"text": "In probability theory and statistics, a categorical distribution (also called a generalized Bernoulli distribution, multinoulli distribution) is a discrete probability distribution that describes the possible results of a random variable that can take on one of K possible categories, with the probability of each"}, {"text": "In probability theory and statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d. or iid or IID."}, {"text": "In probability theory and statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d. or iid or IID."}, {"text": "In probability theory and statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d. or iid or IID."}, {"text": "In probability theory and statistics, the Bernoulli distribution, named after Swiss mathematician Jacob Bernoulli, is the discrete probability distribution of a random variable which takes the value 1 with probability and the value 0 with probability , and is sometimes denoted as ."}, {"text": "Specifically, we can compute the probability that a discrete random variable equals a specific value (probability mass function) and the probability that a random variable is less than or equal to a specific value (cumulative distribution function)."}]}, {"question": "What is perplexity in NLP", "positive_ctxs": [{"text": "In natural language processing, perplexity is a way of evaluating language models.  Using the definition of perplexity for a probability model, one might find, for example, that the average sentence xi in the test sample could be coded in 190 bits (i.e., the test sentences had an average log-probability of -190)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In natural language processing, perplexity is a way of evaluating language models. A language model is a probability distribution over entire sentences or texts."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Deep Learning is a part of Machine Learning which is applied to larger data-sets and based on ANN (Artificial Neural Networks). The main technology used in NLP (Natural Language Processing) which mainly focuses on teaching natural/human language to computers.  NLP is a part of AI which overlaps with ML & DL."}, {"text": "Developers can make use of NLP to perform tasks like speech recognition, sentiment analysis, translation, auto-correct of grammar while typing, and automated answer generation. NLP is a challenging field since it deals with human language, which is extremely diverse and can be spoken in a lot of ways."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "What do you mean by activation function", "positive_ctxs": [{"text": "An activation function is a function used in artificial neural networks which outputs a small value for small inputs, and a larger value if its inputs exceed a threshold.  Activation functions are useful because they add non-linearities into neural networks, allowing the neural networks to learn powerful operations."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "Yes a perceptron (one fully connected unit) can be used for regression. It will just be a linear regressor. If you use no activation function you get a regressor and if you put a sigmoid activation you get a classifier.  That's why the loss function for classification is called \"logistic regression\"."}, {"text": "relu . The difference is that relu is an activation function whereas LeakyReLU is a Layer defined under keras. layers .  For activation functions you need to wrap around or use inside layers such Activation but LeakyReLU gives you a shortcut to that function with an alpha value."}, {"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "sigmoid activation function"}, {"text": "An activation function is defined by and defines the output of a neuron in terms of its input (aka induced local field) . There are three types of activation functions. Threshhold function an example of which is. This function is also termed the Heaviside function. Piecewise Linear."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is NP random random", "positive_ctxs": [{"text": "random. random. Array of random floats of shape size (unless size=None , in which case a single float is returned)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "Stochastic vs. In general, stochastic is a synonym for random. For example, a stochastic variable is a random variable. A stochastic process is a random process. Typically, random is used to refer to a lack of dependence between observations in a sequence."}, {"text": "In particular, a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space."}, {"text": "In particular, a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space."}, {"text": "A continuous random variable is a random variable whose statistical distribution is continuous. Formally: A continuous random variable is a function X X X on the outcomes of some probabilistic experiment which takes values in a continuous set V V V."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "\u2022 A random process is a time-varying function that assigns the outcome of a random experiment to each time instant: X(t). \u2022 For a fixed (sample path): a random process is a time varying function, e.g., a signal."}]}, {"question": "What are four main assumptions for parametric statistics", "positive_ctxs": [{"text": "Normality: Data have a normal distribution (or at least is symmetric) Homogeneity of variances: Data from multiple groups have the same variance. Linearity: Data have a linear relationship. Independence: Data are independent."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are three main methods for handling continuous variables in naive Bayes classifiers, namely, the normal method (parametric approach), the kernel method (non parametric approach) and discretization."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "There are four assumptions associated with a linear regression model: Linearity: The relationship between X and the mean of Y is linear. Homoscedasticity: The variance of residual is the same for any value of X. Independence: Observations are independent of each other."}, {"text": "In the literal meaning of the terms, a parametric statistical test is one that makes assumptions about the parameters (defining properties) of the population distribution(s) from which one's data are drawn, while a non-parametric test is one that makes no such assumptions."}, {"text": "In the literal meaning of the terms, a parametric statistical test is one that makes assumptions about the parameters (defining properties) of the population distribution(s) from which one's data are drawn, while a non-parametric test is one that makes no such assumptions."}, {"text": "In the literal meaning of the terms, a parametric statistical test is one that makes assumptions about the parameters (defining properties) of the population distribution(s) from which one's data are drawn, while a non-parametric test is one that makes no such assumptions."}, {"text": "There are four main types of probability sample.Simple random sampling. In a simple random sample, every member of the population has an equal chance of being selected.  Systematic sampling.  Stratified sampling.  Cluster sampling."}]}, {"question": "Why is box plot non parametric", "positive_ctxs": [{"text": "Outliers may be plotted as individual points. Box plots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution (though Tukey's boxplot assumes symmetry for the whiskers and normality for their length)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Graphs that are appropriate for bivariate analysis depend on the type of variable. For two continuous variables, a scatterplot is a common graph. When one variable is categorical and the other continuous, a box plot is common and when both are categorical a mosaic plot is common."}, {"text": "A box plot (also known as box and whisker plot) is a type of chart often used in explanatory data analysis to visually show the distribution of numerical data and skewness through displaying the data quartiles (or percentiles) and averages."}, {"text": "A box plot (also known as box and whisker plot) is a type of chart often used in explanatory data analysis to visually show the distribution of numerical data and skewness through displaying the data quartiles (or percentiles) and averages."}, {"text": "The very first is a Box Plot. A box plot is a graphical display for describing the distribution of data. Box plots use the median and the lower and upper quartiles. An outlier can easily be detected via Box plot where any point above or below the whiskers represent an outlier."}, {"text": "A non parametric test (sometimes called a distribution free test) does not assume anything about the underlying distribution (for example, that the data comes from a normal distribution).  It usually means that you know the population data does not have a normal distribution."}, {"text": "LDA is a parametric model, and the parameter is number of topics."}, {"text": "Any LTI filter with output and input can be represented by a difference equation in the form: If at least one of the is not null, the filter is recursive. If the are all zero, it is a non recursive filter usually called FIR (Finite Input Response) filter.  This happens both to recursive and non recursive filters."}]}, {"question": "Whats an intuitive explanation of the chain rule of probability", "positive_ctxs": [{"text": "The definition of conditional probability can be rewritten as: P(E \u2229F) = P(E|F)P(F) which we call the Chain Rule. Intuitively it states that the probability of observing events E and F is the. probability of observing F, multiplied by the probability of observing E, given that you have observed F."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The confidence of an association rule is a percentage value that shows how frequently the rule head occurs among all the groups containing the rule body.  Thus, the confidence of a rule is the percentage equivalent of m/n, where the values are: m. The number of groups containing the joined rule head and rule body."}, {"text": "Markov Chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its stationary distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution."}, {"text": "Why is the derivative of the LSTM cell state w.r.t. to the previous cell state equal to the forget gate?  The chain rule would extend for longer and we'd have more derivative terms in there."}, {"text": "The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities ."}, {"text": "The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities ."}, {"text": "The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities ."}, {"text": "An error term represents the margin of error within a statistical model; it refers to the sum of the deviations within the regression line, which provides an explanation for the difference between the theoretical value of the model and the actual observed results."}]}, {"question": "What does hazard rate mean", "positive_ctxs": [{"text": "The hazard rate refers to the rate of death for an item of a given age (x). It is part of a larger equation called the hazard function, which analyzes the likelihood that an item will survive to a certain point in time based on its survival to an earlier time (t)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The hazard function is the instantaneous rate of failure at a given time. Characteristics of a hazard function are frequently associated with certain products and applications. Different hazard functions are modeled with different distribution models."}, {"text": "The hazard rate refers to the rate of death for an item of a given age (x). It is part of a larger equation called the hazard function, which analyzes the likelihood that an item will survive to a certain point in time based on its survival to an earlier time (t)."}, {"text": "The hazard rate refers to the rate of death for an item of a given age (x). It is part of a larger equation called the hazard function, which analyzes the likelihood that an item will survive to a certain point in time based on its survival to an earlier time (t)."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "It is a rate per unit of time similar in meaning to reading a car speedometer at a particular instant and seeing 45 mph.  The failure rate (or hazard rate) is denoted by h(t) and is calculated from h(t) = \\frac{f(t)}{1 - F(t)} = \\frac{f(t)}{R(t)} = \\mbox{the instantaneous (conditional) failure rate.}"}, {"text": "Conditions for Poisson Distribution: Events occur independently. In other words, if an event occurs, it does not affect the probability of another event occurring in the same time period. The rate of occurrence is constant; that is, the rate does not change based on time."}]}, {"question": "What is the difference between a normal distribution and the standard normal distribution", "positive_ctxs": [{"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution. Areas of the normal distribution are often represented by tables of the standard normal distribution.  For example, a Z of -2.5 represents a value 2.5 standard deviations below the mean."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean."}, {"text": "Normal distributions are symmetric around their mean. The mean, median, and mode of a normal distribution are equal. The area under the normal curve is equal to 1.0.  Approximately 95% of the area of a normal distribution is within two standard deviations of the mean."}, {"text": "The parameters of the distribution are m and s2, where m is the mean (expectation) of the distribution and s2 is the variance. We write X ~ N(m, s2) to mean that the random variable X has a normal distribution with parameters m and s2. If Z ~ N(0, 1), then Z is said to follow a standard normal distribution."}]}, {"question": "How many possibilities may be there when statistical hypothesis is tested", "positive_ctxs": [{"text": "It is important to realize that this conclusion may or may not be correct. Our acceptance or rejection of an hypothesis, and the reality of the truth or falsity of the hypothesis, creates four possibilities, shown below."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A statistical hypothesis is an assumption about a population parameter. This assumption may or may not be true. Hypothesis testing refers to the formal procedures used by statisticians to accept or reject statistical hypotheses."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive."}, {"text": "A null hypothesis is a type of hypothesis used in statistics that proposes that there is no difference between certain characteristics of a population (or data-generating process). For example, a gambler may be interested in whether a game of chance is fair."}, {"text": "A t-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related in certain features. The t-test is one of many tests used for the purpose of hypothesis testing in statistics. Calculating a t-test requires three key data values."}, {"text": "A t-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related in certain features. The t-test is one of many tests used for the purpose of hypothesis testing in statistics. Calculating a t-test requires three key data values."}, {"text": "In research, there is a convention that the hypothesis is written in two forms, the null hypothesis, and the alternative hypothesis (called the experimental hypothesis when the method of investigation is an experiment)."}]}, {"question": "How do you know if a distribution is bimodal", "positive_ctxs": [{"text": "A mixture of two normal distributions with equal standard deviations is bimodal only if their means differ by at least twice the common standard deviation. Estimates of the parameters is simplified if the variances can be assumed to be equal (the homoscedastic case)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "You can start with a bimodal distribution of data and turn it into a standard normal distribution if you want."}, {"text": "Bimodal Distribution: Two Peaks. The bimodal distribution has two peaks.  However, if you think about it, the peaks in any distribution are the most common number(s). The two peaks in a bimodal distribution also represent two local maximums; these are points where the data points stop increasing and start decreasing."}, {"text": "A unimodal distribution only has one peak in the distribution, a bimodal distribution has two peaks, and a multimodal distribution has three or more peaks. Another way to describe the shape of histograms is by describing whether the data is skewed or symmetric."}, {"text": "The Shape of a Histogram A histogram is unimodal if there is one hump, bimodal if there are two humps and multimodal if there are many humps. A nonsymmetric histogram is called skewed if it is not symmetric. If the upper tail is longer than the lower tail then it is positively skewed."}, {"text": "Markov model is a state machine with the state changes being probabilities. In a hidden Markov model, you don't know the probabilities, but you know the outcomes."}, {"text": "The posterior distribution is a way to summarize what we know about uncertain quantities in Bayesian analysis. It is a combination of the prior distribution and the likelihood function, which tells you what information is contained in your observed data (the \u201cnew evidence\u201d)."}, {"text": "In statistics, a Multimodal distribution is a probability distribution with two different modes, may also be referred to as a bimodal distribution. These appear as distinct peaks (local maxima) in the probability density function, as shown in Figures 1 and 2."}]}, {"question": "Can decision trees be used for classification tasks", "positive_ctxs": [{"text": "Decision tree is a type of supervised learning algorithm that can be used in both regression and classification problems. It works for both categorical and continuous input and output variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the"}, {"text": "There are two main types of decision trees that are based on the target variable, i.e., categorical variable decision trees and continuous variable decision trees.Categorical variable decision tree.  Continuous variable decision tree.  Assessing prospective growth opportunities.More items"}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}, {"text": "Decision trees are mainly used to perform classification tasks. Samples are submitted to a test in each node of the tree and guided through the tree based on the result. Decision trees can also be used to perform clustering, with a few adjustments."}, {"text": "Decision trees generate understandable rules. Decision trees perform classification without requiring much computation. Decision trees are capable of handling both continuous and categorical variables. Decision trees provide a clear indication of which fields are most important for prediction or classification."}, {"text": "We demonstrated that convolutional neural networks are primarily utilized for text classification tasks while recurrent neural networks are commonly used for natural language generation or machine translation."}]}, {"question": "What is the hash value of a string", "positive_ctxs": [{"text": "A Hash Value (also called as Hashes or Checksum) is a string value (of specific length), which is the result of calculation of a Hashing Algorithm. Hash Values have different uses."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition: A hash algorithm is a function that converts a data string into a numeric string output of fixed length. The output string is generally much smaller than the original data.  Two of the most common hash algorithms are the MD5 (Message-Digest algorithm 5) and the SHA-1 (Secure Hash Algorithm)."}, {"text": "Hashing is the practice of using an algorithm to map data of any size to a fixed length. This is called a hash value (or sometimes hash code or hash sums or even a hash digest if you're feeling fancy). Whereas encryption is a two-way function, hashing is a one-way function.  Every hash value is unique."}, {"text": "No. A universal Turing machine is a Turing machine that takes as its input a string of the form where is the representation of the transition table of Turing machine and is a string over the input alphabet of ."}, {"text": "An interpolated string is a string literal that might contain interpolation expressions. When an interpolated string is resolved to a result string, items with interpolation expressions are replaced by the string representations of the expression results."}, {"text": "The essential benefit achieved by using a rolling hash such as the Rabin fingerprint is that it is possible to compute the hash value of the next substring from the previous one by doing only a constant number of operations, independent of the substrings' lengths."}, {"text": "The Rabin-Karp algorithm makes use of hash functions and the rolling hash technique. A hash function is essentially a function that maps one thing to a value. In particular, hashing can map data of arbitrary size to a value of fixed size."}, {"text": "Rabin (1987) that uses hashing to find an exact match of a pattern string in a text. It uses a rolling hash to quickly filter out positions of the text that cannot match the pattern, and then checks for a match at the remaining positions."}]}, {"question": "How can you use a trend line to determine the type of linear association for a scatter plot", "positive_ctxs": [{"text": "The straight line is a trend line, designed to come as close as possible to all the data points. The trend line has a positive slope, which shows a positive relationship between X and Y. The points in the graph are tightly clustered about the trend line due to the strength of the relationship between X and Y."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A scatter plot is a special type of graph designed to show the relationship between two variables. With regression analysis, you can use a scatter plot to visually inspect the data to see whether X and Y are linearly related."}, {"text": "You can use a scatter plot to analyze trends in your data and to help you to determine whether or not there is a relationship between two variables.  If the points on the scatter plot seem to form a line that slants down from left to right, there is a negative relationship or negative correlation between the variables."}, {"text": "The correlation coefficient is a measure of the degree of linear association between two continuous variables, i.e. when plotted together, how close to a straight line is the scatter of points.  Both x and y must be continuous random variables (and Normally distributed if the hypothesis test is to be valid)."}, {"text": "Statistical researchers often use a linear relationship to predict the (average) numerical value of Y for a given value of X using a straight line (called the regression line). If you know the slope and the y-intercept of that regression line, then you can plug in a value for X and predict the average value for Y."}, {"text": "Scatter plots' primary uses are to observe and show relationships between two numeric variables. The dots in a scatter plot not only report the values of individual data points, but also patterns when the data are taken as a whole.  A scatter plot can also be useful for identifying other patterns in data."}, {"text": "How to read a stock chartIdentify the trend line. This is that blue line you see every time you hear about a stock\u2014it's either going up or down right?  Look for lines of support and resistance.  Know when dividends and stock splits occur.  Understand historic trading volumes."}, {"text": "Mentor: Well, if the line is a good fit for the data then the residual plot will be random. However, if the line is a bad fit for the data then the plot of the residuals will have a pattern."}]}, {"question": "What is the receptive field of a convolution layer", "positive_ctxs": [{"text": "The receptive field is defined by the filter size of a layer within a convolution neural network. The receptive field is also an indication of the extent of the scope of input data a neuron or unit within a layer can be exposed to (see image below)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "The receptive field size of a unit can be increased in a number of ways. One option is to stack more layers to make the network deeper, which increases the receptive field size linearly by theory, as each extra layer increases the receptive field size by the kernel size."}, {"text": "Receptive fields are defined portion of space or spatial construct containing units that provide input to a set of units within a corresponding layer. The receptive field is defined by the filter size of a layer within a convolution neural network."}, {"text": "A pooling or subsampling layer often immediately follows a convolution layer in CNN. Its role is to downsample the output of a convolution layer along both the spatial dimensions of height and width."}, {"text": "The receptive field in Convolutional Neural Networks (CNN) is the region of the input space that affects a particular unit of the network.  The numbers inside the pixels on the left image represent how many times this pixel was part of a convolution step (each sliding step of the filter)."}, {"text": "In the visual system, visual receptive fields are volumes in visual space.  The receptive field is often identified as the region of the retina where the action of light alters the firing of the neuron."}, {"text": "The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume."}]}, {"question": "What is the role of hidden layer", "positive_ctxs": [{"text": "Hidden layers, simply put, are layers of mathematical functions each designed to produce an output specific to an intended result.  Hidden layers allow for the function of a neural network to be broken down into specific transformations of the data. Each hidden layer function is specialized to produce a defined output."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "The Hidden layer of the neural network is the intermediate layer between Input and Output layer. Activation function applies on hidden layer if it is available.  Hidden nodes or hidden neurons are the neurons that are neither in the input layer nor the output layer [3]."}, {"text": "A pooling or subsampling layer often immediately follows a convolution layer in CNN. Its role is to downsample the output of a convolution layer along both the spatial dimensions of height and width."}, {"text": "One hidden layer is sufficient for the large majority of problems. Usually, each hidden layer contains the same number of neurons. The larger the number of hidden layers in a neural network, the longer it will take for the neural network to produce the output and the more complex problems the neural network can solve."}, {"text": "The role of a fully connected layer in a CNN architecture The objective of a fully connected layer is to take the results of the convolution/pooling process and use them to classify the image into a label (in a simple classification example)."}, {"text": "MLP With Batch Normalization A new BatchNormalization layer can be added to the model after the hidden layer before the output layer. Specifically, after the activation function of the prior hidden layer."}, {"text": "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer."}]}, {"question": "How do you do anomaly detection", "positive_ctxs": [{"text": "Static Rules Approach. The most simple, and maybe the best approach to start with, is using static rules. The Idea is to identify a list of known anomalies and then write rules to detect those anomalies. Rules identification is done by a domain expert, by using pattern mining techniques, or a by combination of both."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "This two-step approach actually combines two different anomaly detection techniques: univariate and multivariate. Univariate anomaly detection looks for anomalies in each individual metric, while multivariate anomaly detection learns a single model for all the metrics in the system."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "In data mining, anomaly detection is referred to the identification of items or events that do not conform to an expected pattern or to other items present in a dataset.  Machine learning algorithms have the ability to learn from data and make predictions based on that data."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "The fundamental counting principle states that if there are p ways to do one thing, and q ways to do another thing, then there are p\u00d7q ways to do both things. possible outcomes of the experiment. The counting principle can be extended to situations where you have more than 2 choices."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "How do you find the joint probability distribution", "positive_ctxs": [{"text": "To calculate probabilities involving two random variables X and Y such as P(X > 0 and Y \u2264 0), we need the joint distribution of X and Y . The way we represent the joint distribution depends on whether the random variables are discrete or continuous. p(x,y) = P(X = x and Y = y),x \u2208 RX ,y \u2208 RY ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To find the joint CDF for x>0 and y>0, we need to integrate the joint PDF: FXY(x,y)=\u222by\u2212\u221e\u222bx\u2212\u221efXY(u,v)dudv=\u222by0\u222bx0fXY(u,v)dudv=\u222bmin(y,1)0\u222bmin(x,1)0(u+32v2)dudv."}, {"text": "their joint probability distribution at (x,y), the functions given by: g(x) = \u03a3y f (x,y) and h(y) = \u03a3x f (x,y) are the marginal distributions of X and Y , respectively. If you're great with equations, that's probably all you need to know. It tells you how to find a marginal distribution."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}, {"text": "A Generative Model \u200clearns the joint probability distribution p(x,y). It predicts the conditional probability with the help of Bayes Theorem. A Discriminative model \u200clearns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems."}]}, {"question": "What are the features in machine learning", "positive_ctxs": [{"text": "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself."}, {"text": "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself."}, {"text": "Boltzmann machine is an unsupervised machine learning algorithm. It helps discover latent features present in the dataset. Dataset is composed of binary vectors. Connection between nodes are undirected."}, {"text": "What are the five steps in the backpropagation learning algorithm?Initialize weights with random values and set other parameters.Read in the input vector and the desired output.Compute the actual output via the calculations, working forward through the layers."}, {"text": "The matrix of features is a term used in machine learning to describe the list of columns that contain independent variables to be processed, including all lines in the dataset. These lines in the dataset are called lines of observation."}, {"text": "Feature scaling is essential for machine learning algorithms that calculate distances between data.  Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance."}, {"text": "\u201cDeep learning is a branch of machine learning that uses neural networks with many layers. A deep neural network analyzes data with learned representations similarly to the way a person would look at a problem,\u201d Brock says. \u201cIn traditional machine learning, the algorithm is given a set of relevant features to analyze."}]}, {"question": "What is multiclass classification in machine learning", "positive_ctxs": [{"text": "In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification)."}, {"text": "In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification)."}, {"text": "Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class. For multiclass problem you will need to reduce it into multiple binary classification problems. Random Forest works well with a mixture of numerical and categorical features."}, {"text": "The Gaussian Processes Classifier is a classification machine learning algorithm. Gaussian Processes are a generalization of the Gaussian probability distribution and can be used as the basis for sophisticated non-parametric machine learning algorithms for classification and regression."}, {"text": "XGboost is the most widely used algorithm in machine learning, whether the problem is a classification or a regression problem. It is known for its good performance as compared to all other machine learning algorithms."}, {"text": "Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks.  In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly."}, {"text": "Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks.  In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly."}]}, {"question": "How do you find the median of a filter", "positive_ctxs": [{"text": "The median is calculated by first sorting all the pixel values from the window into numerical order, and then replacing the pixel being considered with the middle (median) pixel value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A blurring filter where you move over the image with a box filter (all the same values in the window) is an example of a linear filter. A non-linear filter is one that cannot be done with convolution or Fourier multiplication. A sliding median filter is a simple example of a non-linear filter."}, {"text": "Median filtering is generally less sensitive to outliers than mean filtering. If you don't believe that the Gaussian assumption of the data will hold very accurately, then a median filter may be the better choice. However, if the Gaussian assumption holds pretty well, then the median filter may be less efficient."}, {"text": "Median filtering A median filter is a nonlinear filter in which each output sample is computed as the median value of the input samples under the window \u2013 that is, the result is the middle value after the input values have been sorted. Ordinarily, an odd number of taps is used."}, {"text": "Median filtering A median filter is a nonlinear filter in which each output sample is computed as the median value of the input samples under the window \u2013 that is, the result is the middle value after the input values have been sorted. Ordinarily, an odd number of taps is used."}, {"text": "The median is a simple measure of central tendency. To find the median, we arrange the observations in order from smallest to largest value. If there is an odd number of observations, the median is the middle value. If there is an even number of observations, the median is the average of the two middle values."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "Is the distribution uniform unimodal or bimodal", "positive_ctxs": [{"text": "Distributions with one clear peak are called unimodal, and distributions with two clear peaks are called bimodal. When a symmetric distribution has a single peak at the center, it is referred to as bell-shaped."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A unimodal distribution only has one peak in the distribution, a bimodal distribution has two peaks, and a multimodal distribution has three or more peaks. Another way to describe the shape of histograms is by describing whether the data is skewed or symmetric."}, {"text": "In statistics, a unimodal probability distribution or unimodal distribution is a probability distribution which has a single peak. The term \"mode\" in this context refers to any peak of the distribution, not just to the strict definition of mode which is usual in statistics."}, {"text": "In statistics, a unimodal probability distribution or unimodal distribution is a probability distribution which has a single peak. The term \"mode\" in this context refers to any peak of the distribution, not just to the strict definition of mode which is usual in statistics."}, {"text": "Bimodal Distribution: Two Peaks. The bimodal distribution has two peaks.  However, if you think about it, the peaks in any distribution are the most common number(s). The two peaks in a bimodal distribution also represent two local maximums; these are points where the data points stop increasing and start decreasing."}, {"text": "The Shape of a Histogram A histogram is unimodal if there is one hump, bimodal if there are two humps and multimodal if there are many humps. A nonsymmetric histogram is called skewed if it is not symmetric. If the upper tail is longer than the lower tail then it is positively skewed."}, {"text": "In statistics, uniform distribution is a probability distribution where all outcomes are equally likely. Discrete uniform distributions have a finite number of outcomes. A continuous uniform distribution is a statistical distribution with an infinite number of equally likely measurable values."}, {"text": "In statistics, a type of probability distribution in which all outcomes are equally likely.  A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same."}]}, {"question": "How do you evaluate word2vec", "positive_ctxs": [{"text": "To assess which word2vec model is best, simply calculate the distance for each pair, do it 200 times, sum up the total distance, and the smallest total distance will be your best model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "To assess which word2vec model is best, simply calculate the distance for each pair, do it 200 times, sum up the total distance, and the smallest total distance will be your best model."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "In general, you should probably use the divergence theorem whenever you wish to evaluate a vector surface integral over a closed surface. The divergence theorem can also be used to evaluate triple integrals by turning them into surface integrals."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "word2vec itself is a simple bi-layered neural network architecture, it turns text into meaningful vectors form that deeper networks can understand. In other words the out put of simple neural word2vec model is used as input for Deep Networks."}, {"text": "word2vec itself is a simple bi-layered neural network architecture, it turns text into meaningful vectors form that deeper networks can understand. In other words the out put of simple neural word2vec model is used as input for Deep Networks."}]}, {"question": "What is 3sls regression", "positive_ctxs": [{"text": "The term three-stage least squares (3SLS) refers to a method of estimation that combines system equation, sometimes known as seemingly unrelated regression (SUR), with two-stage least squares estimation.  It is assumed that each equation of the system is at least just-identified."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What are the advantages of descriptive statistics", "positive_ctxs": [{"text": "Descriptive statistics help us to simplify large amounts of data in a sensible way. Each descriptive statistic reduces lots of data into a simpler summary. For instance, consider a simple number used to summarize how well a batter is performing in baseball, the batting average."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures.  Descriptive statistics are typically distinguished from inferential statistics. With descriptive statistics you are simply describing what is or what the data shows."}, {"text": "Explain the difference between descriptive and inferential statistics. Descriptive statistics describes sets of data. Inferential statistics draws conclusions about the sets of data based on sampling.  A population is a set of units of interest to a study."}, {"text": "Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population."}, {"text": "When analysing data, such as the grades earned by 100 students, it is possible to use both descriptive and inferential statistics in your analysis. Typically, in most research conducted on groups of people, you will use both descriptive and inferential statistics to analyse your results and draw conclusions."}, {"text": "All descriptive statistics are either measures of central tendency or measures of variability, also known as measures of dispersion.  Range, quartiles, absolute deviation and variance are all examples of measures of variability. Consider the following data set: 5, 19, 24, 62, 91, 100."}, {"text": "It is one of the more common descriptive statistics functions used to calculate uncertainty.How to CalculateSubtract each value from the mean.Square each value in step 1.Add all of the values from step 2.Count the number of values and Subtract it by 1.Divide step 3 by step 4.Calculate the Square Root of step 5."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is a binomial distribution in statistics", "positive_ctxs": [{"text": "The binomial distribution is a probability distribution that summarizes the likelihood that a value will take one of two independent values under a given set of parameters or assumptions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "The normal approximation to the binomial is when you use a continuous distribution (the normal distribution) to approximate a discrete distribution (the binomial distribution)."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution.  Suppose we flip a coin repeatedly and count the number of heads (successes)."}, {"text": "The binomial distribution is a common discrete distribution used in statistics, as opposed to a continuous distribution, such as the normal distribution."}, {"text": "follows a negative binomial distribution with parameters r and p. The geometric distribution is a special case of discrete compound Poisson distribution."}, {"text": "A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix \u201cbi\u201d means two, or twice)."}, {"text": "The negative binomial distribution is a probability distribution that is used with discrete random variables. This type of distribution concerns the number of trials that must occur in order to have a predetermined number of successes."}]}, {"question": "Why does gradient boosting generally outperform random forests", "positive_ctxs": [{"text": "In random forest different features are used for each tree while in bagging different subsets of the training data are used. Gradient boosting generates an ensemble of trees too but does so in a different way, motivated by different ideas."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Like random forests, gradient boosting is a set of decision trees. The two main differences are:  Combining results: random forests combine results at the end of the process (by averaging or \"majority rules\") while gradient boosting combines results along the way."}, {"text": "Like random forests, gradient boosting is a set of decision trees. The two main differences are:  Combining results: random forests combine results at the end of the process (by averaging or \"majority rules\") while gradient boosting combines results along the way."}, {"text": "Like random forests, gradient boosting is a set of decision trees. The two main differences are:  Combining results: random forests combine results at the end of the process (by averaging or \"majority rules\") while gradient boosting combines results along the way."}, {"text": "tl;dr: Bagging and random forests are \u201cbagging\u201d algorithms that aim to reduce the complexity of models that overfit the training data. In contrast, boosting is an approach to increase the complexity of models that suffer from high bias, that is, models that underfit the training data."}, {"text": "Why gradient clipping accelerates training: A theoretical justification for adaptivity.  These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption."}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}]}, {"question": "What is the difference between Manova and Anova", "positive_ctxs": [{"text": "The obvious difference between ANOVA and a \"Multivariate Analysis of Variance\" (MANOVA) is the \u201cM\u201d, which stands for multivariate. In basic terms, A MANOVA is an ANOVA with two or more continuous response variables. Like ANOVA, MANOVA has both a one-way flavor and a two-way flavor."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "The Range is the difference between the lowest and highest values. Example: In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9. So the range is 9 \u2212 3 = 6."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "The chief difference between MEMM and CRF is that MEMM is locally renormalized and suffers from the label bias problem, while CRFs are globally renormalized."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}]}, {"question": "What is feature column", "positive_ctxs": [{"text": "Overview. Feature columns are used to specify how Tensors received from the input function should be combined and transformed before entering the model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Tensorflow feature columnsTensorflow feature columns.  If the tensor is a matrix, you can provide a shape expressing the dimensions.Partitioning a numerical column into a set of indicator categoricals can be done using bucketized_column :More items"}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Random Forest uses bootstrap sampling and feature sampling, i.e row sampling and column sampling. Therefore Random Forest is not affected by multicollinearity that much since it is picking different set of features for different models and of course every model sees a different set of data points."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Collaborative filtering is an unsupervised learning which we make predictions from ratings supplied by people. Each rows represents the ratings of movies from a person and each column indicates the ratings of a movie. In Collaborative Filtering, we do not know the feature set before hands."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Think of feature columns as the intermediaries between raw data and Estimators. Feature columns are very rich, enabling you to transform a diverse range of raw data into formats that Estimators can use, allowing easy experimentation. In simple words feature column are bridge between raw data and estimator or model."}]}, {"question": "How is Akaike information criterion calculated", "positive_ctxs": [{"text": "Log-likelihood is a measure of model fit. The higher the number, the better the fit. This is usually obtained from statistical output.AICc = -2(log-likelihood) + 2K + (2K(K+1)/(n-K-1))n = sample size,K= number of model parameters,Log-likelihood is a measure of model fit."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Akaike information criterion (AIC) is a mathematical method for evaluating how well a model fits the data it was generated from. In statistics, AIC is used to compare different possible models and determine which one is the best fit for the data."}, {"text": "The beta value is used in measuring how effectively the predictor variable influences the criterion variable, it is measured in terms of standard deviation. R, is the measure of association between the observed value and the predicted value of the criterion variable."}, {"text": "Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.  The mutual information between two random variables X and Y can be stated formally as follows: I(X ; Y) = H(X) \u2013 H(X | Y)"}, {"text": "Discriminant analysis is a technique that is used by the researcher to analyze the research data when the criterion or the dependent variable is categorical and the predictor or the independent variable is interval in nature."}, {"text": "The prior probability of an event will be revised as new data or information becomes available, to produce a more accurate measure of a potential outcome. That revised probability becomes the posterior probability and is calculated using Bayes' theorem."}, {"text": "Relative Risk is calculated by dividing the probability of an event occurring for group 1 (A) divided by the probability of an event occurring for group 2 (B). Relative Risk is very similar to Odds Ratio, however, RR is calculated by using percentages, whereas Odds Ratio is calculated by using the ratio of odds."}, {"text": "The population standard deviation is a parameter, which is a fixed value calculated from every individual in the population. A sample standard deviation is a statistic. This means that it is calculated from only some of the individuals in a population."}]}, {"question": "What is the purpose of the kernel trick", "positive_ctxs": [{"text": "In essence, what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions. With that saying, the application of the kernel trick is not limited to the SVM algorithm."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "This is when the kernel trick comes in. It allows us to operate in the original feature space without computing the coordinates of the data in a higher dimensional space.  In essence, what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions."}, {"text": "It allows us to operate in the original feature space without computing the coordinates of the data in a higher dimensional space.  In essence, what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions."}, {"text": "Kernel vs Filter The dimensions of the kernel matrix is how the convolution gets it's name. For example, in 2D convolutions, the kernel matrix is a 2D matrix. A filter however is a concatenation of multiple kernels, each kernel assigned to a particular channel of the input."}, {"text": "In Convolutional neural network, the kernel is nothing but a filter that is used to extract the features from the images. The kernel is a matrix that moves over the input data, performs the dot product with the sub-region of input data, and gets the output as the matrix of dot products."}, {"text": "A Gaussian blur effect is typically generated by convolving an image with an FIR kernel of Gaussian values.  In the first pass, a one-dimensional kernel is used to blur the image in only the horizontal or vertical direction. In the second pass, the same one-dimensional kernel is used to blur in the remaining direction."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "When to use hierarchical clustering vs K means", "positive_ctxs": [{"text": "A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows\u2022"}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical."}, {"text": "Centroid-based clustering organizes the data into non-hierarchical clusters, in contrast to hierarchical clustering defined below. k-means is the most widely-used centroid-based clustering algorithm. Centroid-based algorithms are efficient but sensitive to initial conditions and outliers."}, {"text": "The Agglomerative Hierarchical Clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity."}, {"text": "The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.  The results of the K-means clustering algorithm are: The centroids of the K clusters, which can be used to label new data. Labels for the training data (each data point is assigned to a single cluster)"}, {"text": "The hclust function in R uses the complete linkage method for hierarchical clustering by default. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components."}]}, {"question": "What is CNN batch normalization layer", "positive_ctxs": [{"text": "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling.  Others sustain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks."}, {"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling."}, {"text": "To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation."}, {"text": "Dense CNN is a type of Deep CNN in which each layer is connected with another layer deeper than itself."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig. 2."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}]}, {"question": "Why DCT is useful in compression", "positive_ctxs": [{"text": "The DCT can be used to convert the signal (spatial information) into numeric data (\"frequency\" or \"spectral\" information) so that the image's information exists in a quantitative form that can be manipulated for compression. The signal for a graphical image can be thought of as a three-dimensional signal."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "> DCT is preferred over DFT in image compression algorithms like JPEG > because DCT is a real transform which results in a single real number per > data point. In contrast, a DFT results in a complex number (real and > imaginary parts) which requires double the memory for storage."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval (IR). The second more subtle advantage of compression is faster transfer of data from disk to memory."}, {"text": "From implementation point of view, Huffman coding is easier than arithmetic coding. Arithmetic algorithm yields much more compression ratio than Huffman algorithm while Huffman coding needs less execution time than the arithmetic coding."}, {"text": "Why is the derivative of the LSTM cell state w.r.t. to the previous cell state equal to the forget gate?  The chain rule would extend for longer and we'd have more derivative terms in there."}, {"text": "Why gradient clipping accelerates training: A theoretical justification for adaptivity.  These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption."}, {"text": "The minimum description length (MDL) principle is a powerful method of inductive inference, the basis of statistical modeling, pattern recognition, and machine learning. It holds that the best explanation, given a limited set of observed data, is the one that permits the greatest compression of the data."}]}, {"question": "What is the real world in the Matrix", "positive_ctxs": [{"text": "The Real World is a term by the redpills to refer to reality, the true physical world and life outside the Matrix."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The movie and its events are happening about a hundred years after this war. The entire Human race is used for power supply. Their bodies are asleep and their minds are plugged into the Matrix. The Matrix is a virtual world that has been pulled over their minds to hide them from the truth \u2013 they are slaves now."}, {"text": "Computer vision is the process of understanding digital images and videos using computers. It seeks to automate tasks that human vision can achieve. This involves methods of acquiring, processing, analyzing, and understanding digital images, and extraction of data from the real world to produce information."}, {"text": "The world is fast evolving, with Artificial intelligence (AI) at the forefront in changing the world and the way we live.  This means that with AI, many of our everyday activities can now be carried out effectively by programmed machine technology."}, {"text": "Reinforcement learning (RL) is a significant area of machine learning, with the potential to solve a lot of real world problems in various fields, like game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics."}]}, {"question": "How is naive Bayes classifier implemented", "positive_ctxs": [{"text": "Naive Bayes Tutorial (in 5 easy steps)Step 1: Separate By Class.Step 2: Summarize Dataset.Step 3: Summarize Data By Class.Step 4: Gaussian Probability Density Function.Step 5: Class Probabilities."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "TL; DR: The naive Bayes classifier is an approximation to the Bayes classifier, in which we assume that the features are conditionally independent given the class instead of modeling their full conditional distribution given the class. A Bayes classifier is best interpreted as a decision rule."}, {"text": "The computational efficiency of Naive Bayes lies in the fact that the runtime complexity of Naive Bayes classifier is O(nK), where n is the number of features and K is the number of label classes."}, {"text": "Naive Bayes is a linear classifier Naive Bayes leads to a linear decision boundary in many common cases.  The red decision line indicates the decision boundary where P(y=1|x)=P(y=2|x)."}, {"text": "In statistical classification, Bayes error rate is the lowest possible error rate for any classifier of a random outcome (into, for example, one of two categories) and is analogous to the irreducible error. A number of approaches to the estimation of the Bayes error rate exist."}, {"text": "There are three main methods for handling continuous variables in naive Bayes classifiers, namely, the normal method (parametric approach), the kernel method (non parametric approach) and discretization."}, {"text": "Naive Bayes Classifier example by hand and how to do in Scikit-Learn, You can use any kind of predictor in a naive Bayes classifier, as long as you can specify a discriminative linear models take a mixture of categorical and continuous predictors."}, {"text": "Platt scaling works well for SVMs(Support Vector Machine) as well as other types of classification models, including boosted models and even naive Bayes classifiers, which produce distorted probability distributions."}]}, {"question": "What is triplet loss function", "positive_ctxs": [{"text": "Triplet loss is a loss function for machine learning algorithms where a baseline (anchor) input is compared to a positive (truthy) input and a negative (falsy) input.  This can be avoided by posing the problem as a similarity learning problem instead of a classification problem."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition of the loss The goal of the triplet loss is to make sure that: Two examples with the same label have their embeddings close together in the embedding space. Two examples with different labels have their embeddings far away."}, {"text": "Typically, with neural networks, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply \u201closs.\u201d"}, {"text": "Contrastive Loss: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for Pairwise Ranking Loss, but I've never seen using it in a setup with triplets. Triplet Loss: Often used as loss name when triplet training pairs are employed."}, {"text": "In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs). For an intended output t = \u00b11 and a classifier score y, the hinge loss of the prediction y is defined as."}, {"text": "In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. A variant for classification is also sometimes used."}, {"text": "The loss function used by the perceptron algorithm is called 0-1 loss. 0-1 loss simply means that for each mistaken prediction you incur a penalty of 1 and for each correct prediction incur no penalty. The problem with this loss function is given a linear classifier its hard to move towards a local optimum."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}]}, {"question": "What does the Fourier transform represent", "positive_ctxs": [{"text": "The Fourier transform of a function of time is a complex-valued function of frequency, whose magnitude (absolute value) represents the amount of that frequency present in the original function, and whose argument is the phase offset of the basic sinusoid in that frequency."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In spite of being linear, the Fourier transform is not shift invariant. In other words, a shift in the time domain does not correspond to a shift in the frequency domain."}, {"text": "In spite of being linear, the Fourier transform is not shift invariant. In other words, a shift in the time domain does not correspond to a shift in the frequency domain."}, {"text": "A fast Fourier transform (FFT) is an algorithm that computes the discrete Fourier transform (DFT) of a sequence, or its inverse (IDFT). Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa."}, {"text": "Fourier Methods in Signal Processing The Fourier transform and discrete-time Fourier transform are mathematical analysis tools and cannot be evaluated exactly in a computer. The Fourier transform is used to analyze problems involving continuous-time signals or mixtures of continuous- and discrete-time signals."}, {"text": "First, after looking around on the web, it seems that there is no way to compute a (discrete) Fourier transform through a neural network. You can hack it by hard-coding the thing to include the Fourier constants for the transform and then get a decent result."}, {"text": "In mathematics, a Fourier series (/\u02c8f\u028arie\u026a, -i\u0259r/) is a periodic function composed of harmonically related sinusoids, combined by a weighted summation.  The discrete-time Fourier transform is an example of Fourier series. The process of deriving the weights that describe a given function is a form of Fourier analysis."}, {"text": "In mathematics, a Fourier transform (FT) is a mathematical transform that decomposes a function (often a function of time, or a signal) into its constituent frequencies, such as the expression of a musical chord in terms of the volumes and frequencies of its constituent notes."}]}, {"question": "What are statistically independent events in probability", "positive_ctxs": [{"text": "Two events are independent, statistically independent, or stochastically independent if the occurrence of one does not affect the probability of occurrence of the other (equivalently, does not affect the odds)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "In probability, two events are independent if the incidence of one event does not affect the probability of the other event. If the incidence of one event does affect the probability of the other event, then the events are dependent. There is a red 6-sided fair die and a blue 6-sided fair die."}, {"text": "In probability, we say two events are independent if knowing one event occurred doesn't change the probability of the other event.  So the result of a coin flip and the day being Tuesday are independent events; knowing it was a Tuesday didn't change the probability of getting \"heads.\""}, {"text": "Independent EventsTwo events A and B are said to be independent if the fact that one event has occurred does not affect the probability that the other event will occur.If whether or not one event occurs does affect the probability that the other event will occur, then the two events are said to be dependent."}, {"text": "Independent EventsTwo events A and B are said to be independent if the fact that one event has occurred does not affect the probability that the other event will occur.If whether or not one event occurs does affect the probability that the other event will occur, then the two events are said to be dependent."}, {"text": "Events A and B are independent if the equation P(A\u2229B) = P(A) \u00b7 P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together."}, {"text": "Events A and B are independent if the equation P(A\u2229B) = P(A) \u00b7 P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together."}]}, {"question": "How do you determine the intervals for a histogram", "positive_ctxs": [{"text": "2:194:05Suggested clip \u00b7 97 secondsChoosing Intervals for a Histogram - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition. The class intervals are the subsets into which the data is grouped. The width of the class intervals will be a compromise between having intervals short enough so that not all of the observations fall in the same interval, but long enough so that you do not end up with only one observation per interval."}, {"text": "Definition. The class intervals are the subsets into which the data is grouped. The width of the class intervals will be a compromise between having intervals short enough so that not all of the observations fall in the same interval, but long enough so that you do not end up with only one observation per interval."}, {"text": "An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value. By looking at the histogram for a specific image a viewer will be able to judge the entire tonal distribution at a glance."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Some practical uses of probability distributions are: To calculate confidence intervals for parameters and to calculate critical regions for hypothesis tests. For univariate data, it is often useful to determine a reasonable distributional model for the data."}, {"text": "Histograms are generally used to show the results of a continuous data set such as height, weight, time, etc. A bar graph has spaces between the bars, while a histogram does not. A histogram often shows the frequency that an event occurs within the defined range. It shows you how many times that event happens."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}]}, {"question": "In what way are artificial neurons functionally different from biological neurons", "positive_ctxs": [{"text": "So unlike biological neurons, artificial neurons don't just \u201cfire\u201d: they send continuous values instead of binary signals. Depending on their activation functions, they might somewhat fire all the time, but the strength of these signals varies."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Train the network. Initializing all the weights with zeros leads the neurons to learn the same features during training.  Thus, both neurons will evolve symmetrically throughout training, effectively preventing different neurons from learning different things."}, {"text": "A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation."}, {"text": "Hebbian learning is one of the oldest learning algorithms, and is based in large part on the dynamics of biological systems. A synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs."}, {"text": "The perceptron is a mathematical model of a biological neuron. While in actual neurons the dendrite receives electrical signals from the axons of other neurons, in the perceptron these electrical signals are represented as numerical values."}, {"text": "Neural Networks - Neuron. The perceptron is a mathematical model of a biological neuron. While in actual neurons the dendrite receives electrical signals from the axons of other neurons, in the perceptron these electrical signals are represented as numerical values."}, {"text": "In particular, three datasets are commonly used in different stages of the creation of the model. The model is initially fit on a training dataset, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model."}, {"text": "Each neuron in a layer receives an input from all the neurons present in the previous layer\u2014thus, they're densely connected. In other words, the dense layer is a fully connected layer, meaning all the neurons in a layer are connected to those in the next layer."}]}, {"question": "What is considered deep learning", "positive_ctxs": [{"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}]}, {"question": "What is the latent variable in variational autoencoders", "positive_ctxs": [{"text": "In the variational autoencoder model, there are only local latent variables (no datapoint shares its latent z with the latent variable of another datapoint). So we can decompose the ELBO into a sum where each term depends on a single datapoint."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "On a technical note, estimation of a latent variable is done by analyzing the variance and covariance of the indicators. The measurement model of a latent variable with effect indicators is the set of relationships (modeled as equations) in which the latent variable is set as the predictor of the indicators."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "The standard solution that psychologists take to measuring latent variables is to use a series of questions that are all designed to measure the latent variable. This is known as a multi-item scale, where an \u201citem\u201d is a question, and a \u201cscale\u201d is the resulting estimate of the latent variable."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Partial least squares discriminant analysis (PLS-DA) is a variant used when the Y is categorical. PLS is used to find the fundamental relations between two matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces."}, {"text": "The parameters are the ones that we specify a prior distribution for. The latent variables are usually the ones that we describe using a conditional distribution of the latent variable given the parameters."}]}, {"question": "How is the normal distribution related to the standard normal distribution", "positive_ctxs": [{"text": "A normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution.  Since the distribution has a mean of 0 and a standard deviation of 1, the Z column is equal to the number of standard deviations below (or above) the mean."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "Because the standard normal distribution is used to calculate critical values for the test, this test is often called the one-sample z-test."}, {"text": "Normal distributions are symmetric around their mean. The mean, median, and mode of a normal distribution are equal. The area under the normal curve is equal to 1.0.  Approximately 95% of the area of a normal distribution is within two standard deviations of the mean."}, {"text": "A normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution. Areas of the normal distribution are often represented by tables of the standard normal distribution.  For example, a Z of -2.5 represents a value 2.5 standard deviations below the mean."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean."}]}, {"question": "What is the symbol for null hypothesis in statistics", "positive_ctxs": [{"text": "In a hypothesis test, we: Evaluate the null hypothesis, typically denoted with H0. The null is not rejected unless the hypothesis test shows otherwise. The null statement must always contain some form of equality (=, \u2264 or \u2265)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Since p < 0.05 is enough to reject the null hypothesis (no association), p = 0.002 reinforce that rejection only. If the significance value that is p-value associated with chi-square statistics is 0.002, there is very strong evidence of rejecting the null hypothesis of no fit. It means good fit."}, {"text": "The range containing values that are consistent with the null hypothesis is the \"acceptance region\"; the other range, in which the null hypothesis is rejected, is the rejection region (or critical region)."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true."}, {"text": "In statistical hypothesis testing, the null distribution is the probability distribution of the test statistic when the null hypothesis is true. For example, in an F-test, the null distribution is an F-distribution. Null distribution is a tool scientists often use when conducting experiments."}, {"text": "In statistical hypothesis testing, the null distribution is the probability distribution of the test statistic when the null hypothesis is true. For example, in an F-test, the null distribution is an F-distribution. Null distribution is a tool scientists often use when conducting experiments."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive."}]}, {"question": "What is batch size", "positive_ctxs": [{"text": "Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration.  Usually, a number that can be divided into the total dataset size. stochastic mode: where the batch size is equal to one."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration. The batch size can be one of three options: batch mode: where the batch size is equal to the total dataset thus making the iteration and epoch values equivalent."}, {"text": "Given that very large datasets are often used to train deep learning neural networks, the batch size is rarely set to the size of the training dataset. Smaller batch sizes are used for two main reasons: Smaller batch sizes are noisy, offering a regularizing effect and lower generalization error."}, {"text": "The batch size limits the number of samples to be shown to the network before a weight update can be performed. This same limitation is then imposed when making predictions with the fit model. Specifically, the batch size used when fitting your model controls how many predictions you must make at a time."}, {"text": "Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration.  Usually, a number that can be divided into the total dataset size. stochastic mode: where the batch size is equal to one."}, {"text": "This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration. The sample is randomly shuffled and selected for performing the iteration."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process."}]}, {"question": "How is a variable distributed", "positive_ctxs": [{"text": "The distribution of a variable is a description of the relative numbers of times each possible outcome will occur in a number of trials.  If the measure is a Radon measure (which is usually the case), then the statistical distribution is a generalized function in the sense of a generalized function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The probability distribution for a random variable describes how the probabilities are distributed over the values of the random variable. For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x)."}, {"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}, {"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}, {"text": "In short, when a dependent variable is not distributed normally, linear regression remains a statistically sound technique in studies of large sample sizes. Figure 2 provides appropriate sample sizes (i.e., >3000) where linear regression techniques still can be used even if normality assumption is violated."}, {"text": "In short, when a dependent variable is not distributed normally, linear regression remains a statistically sound technique in studies of large sample sizes. Figure 2 provides appropriate sample sizes (i.e., >3000) where linear regression techniques still can be used even if normality assumption is violated."}, {"text": "A random variate is a variable generated from uniformly distributed pseudorandom numbers. Depending on how they are generated, a random variate can be uniformly or nonuniformly distributed. Random variates are frequently used as the input to simulation models (Neelamkavil 1987, p. 119)."}, {"text": "A discrete variable is a variable whose value is obtained by counting. A continuous variable is a variable whose value is obtained by measuring.  A discrete random variable X has a countable number of possible values."}]}, {"question": "Why do we use Spearman rank correlation", "positive_ctxs": [{"text": "When to use it Use Spearman rank correlation when you have two ranked variables, and you want to see whether the two variables covary; whether, as one variable increases, the other variable tends to increase or decrease."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Hello every one, We know that Pearson linear correlation coefficient gives the strength of linear relationship, while Spearman rank correlation coefficient gives the strength of monotonic relationship between two variables."}, {"text": "When to use it Use Spearman rank correlation when you have two ranked variables, and you want to see whether the two variables covary; whether, as one variable increases, the other variable tends to increase or decrease."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Spearman Rank Correlation: Worked Example (No Tied Ranks)The formula for the Spearman rank correlation coefficient when there are no tied ranks is:  Step 1: Find the ranks for each individual subject.  Step 2: Add a third column, d, to your data.  Step 5: Insert the values into the formula.More items\u2022"}, {"text": "Spearman Rank Correlation: Worked Example (No Tied Ranks)The formula for the Spearman rank correlation coefficient when there are no tied ranks is:  Step 1: Find the ranks for each individual subject.  Step 2: Add a third column, d, to your data.  Step 5: Insert the values into the formula.More items\u2022"}, {"text": "The Pearson correlation evaluates the linear relationship between two continuous variables.  The Spearman correlation coefficient is based on the ranked values for each variable rather than the raw data. Spearman correlation is often used to evaluate relationships involving ordinal variables."}, {"text": "The Pearson correlation evaluates the linear relationship between two continuous variables.  The Spearman correlation coefficient is based on the ranked values for each variable rather than the raw data. Spearman correlation is often used to evaluate relationships involving ordinal variables."}]}, {"question": "What does a correlation matrix show", "positive_ctxs": [{"text": "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "0:005:54Suggested clip \u00b7 111 secondsInterpreting correlation coefficients in a correlation matrix - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}, {"text": "Serial correlation is the relationship between a variable and a lagged version of itself over various time intervals. Repeating patterns often show serial correlation when the level of a variable affects its future level.  Serial correlation is also known as autocorrelation or lagged correlation."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Repeating patterns often show serial correlation when the level of a variable affects its future level. In finance, this correlation is used by technical analysts to determine how well the past price of a security predicts the future price. Serial correlation is also known as autocorrelation or lagged correlation."}, {"text": "A correlation matrix is a table showing correlation coefficients between sets of variables. Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj).  The diagonal of the table is always a set of ones, because the correlation between a variable and itself is always 1."}, {"text": "To perform principal component analysis using the correlation matrix using the prcomp() function, set the scale argument to TRUE . Plot the first two PCs of the correlation matrix using the autoplot() function."}]}, {"question": "Can ordinal variables be used in regression", "positive_ctxs": [{"text": "Traditionally in linear regression your predictors must either be continuous or binary. Ordinal variables are often inserted using a dummy coding scheme. This is equivalent to conducting an ANOVA and the baseline ordinal level will be represented by the intercept."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable."}, {"text": "Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable."}, {"text": "Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable."}, {"text": "3 Answers. Since your response is ordinal then you should use ordinal regression. At a very high level, the main difference ordinal regression and linear regression is that with linear regression the dependent variable is continuous and ordinal the dependent variable is ordinal."}, {"text": "In statistics, ordinal regression (also called \"ordinal classification\") is a type of regression analysis used for predicting an ordinal variable, i.e. a variable whose value exists on an arbitrary scale where only the relative ordering between different values is significant."}, {"text": "Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables."}, {"text": "Categorical variables are also known as discrete or qualitative variables. Categorical variables can be further categorized as either nominal, ordinal or dichotomous. Nominal variables are variables that have two or more categories, but which do not have an intrinsic order."}]}, {"question": "What does it mean when the covariance is 0", "positive_ctxs": [{"text": "The covariance is defined as the mean value of this product, calculated using each pair of data points xi and yi.  If the covariance is zero, then the cases in which the product was positive were offset by those in which it was negative, and there is no linear relationship between the two random variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Describe the scores in such a sample. If the standard deviation is 0 then the variance is 0 and the mean of the squared deviation scores must be 0.  Thus, when the standard deviation equals 0, all the scores are identical and equal to the mean."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "Logistic regression is a classification algorithm, used when the value of the target variable is categorical in nature. Logistic regression is most commonly used when the data in question has binary output, so when it belongs to one class or another, or is either a 0 or 1."}, {"text": "Now, three variable case it is less clear for me. An intuitive definition for covariance function would be Cov(X,Y,Z)=E[(x\u2212E[X])(y\u2212E[Y])(z\u2212E[Z])], but instead the literature suggests using covariance matrix that is defined as two variable covariance for each pair of variables."}]}, {"question": "Why is it important to examine a residual plot", "positive_ctxs": [{"text": "2. Why is it important to examine a residual plot even if a scatterplot appears to be linear? An examination of the of the residuals often leads us to discover groups of observations that are different from the rest."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The residual plot shows a fairly random pattern - the first residual is positive, the next two are negative, the fourth is positive, and the last residual is negative. This random pattern indicates that a linear model provides a decent fit to the data."}, {"text": "A residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a nonlinear model is more appropriate."}, {"text": "A residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a nonlinear model is more appropriate."}, {"text": "Mentor: Well, if the line is a good fit for the data then the residual plot will be random. However, if the line is a bad fit for the data then the plot of the residuals will have a pattern."}, {"text": "Mentor: Well, if the line is a good fit for the data then the residual plot will be random. However, if the line is a bad fit for the data then the plot of the residuals will have a pattern."}, {"text": "Simple linear regression is appropriate when the following conditions are satisfied. The dependent variable Y has a linear relationship to the independent variable X. To check this, make sure that the XY scatterplot is linear and that the residual plot shows a random pattern."}, {"text": "In statistics, a studentized residual is the quotient resulting from the division of a residual by an estimate of its standard deviation. It is a form of a Student's t-statistic, with the estimate of error varying between points. This is an important technique in the detection of outliers."}]}, {"question": "How do you calculate a priori probability", "positive_ctxs": [{"text": "Example 1: Fair Dice Roll The number of desired outcomes is 3 (rolling a 2, 4, or 6), and there are 6 outcomes in total. The a priori probability for this example is calculated as follows: A priori probability = 3 / 6 = 50%. Therefore, the a priori probability of rolling a 2, 4, or 6 is 50%."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Empirical and priori probabilities generally do not vary from person to person, and they are often grouped as objective probabilities. Subjective probability is a probability based on personal or subjective judgment."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Example 1: Fair Dice Roll The number of desired outcomes is 3 (rolling a 2, 4, or 6), and there are 6 outcomes in total. The a priori probability for this example is calculated as follows: A priori probability = 3 / 6 = 50%. Therefore, the a priori probability of rolling a 2, 4, or 6 is 50%."}, {"text": "Similar to the distinction in philosophy between a priori and a posteriori, in Bayesian inference a priori denotes general knowledge about the data distribution before making an inference, while a posteriori denotes knowledge that incorporates the results of making an inference."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Statistics can never \"prove\" anything. All a statistical test can do is assign a probability to the data you have, indicating the likelihood (or probability) that these numbers come from random fluctuations in sampling."}]}, {"question": "What is Optimizer neural network", "positive_ctxs": [{"text": "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.  Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.  Often the weights of a neural network are contained within the hidden layers of the network."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.  Often the weights of a neural network are contained within the hidden layers of the network."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value."}, {"text": "Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value."}, {"text": "A feedforward neural network is an artificial neural network wherein connections between the units do not form a cycle.  The simplest kind of neural network is a single-layer perceptron network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights."}]}, {"question": "What is good way to understand conditional entropy H y x", "positive_ctxs": [{"text": "A good property of conditional entropy is that if we know H(Y|X)=0, then Y=f(X) for a function f. To see another interest behind the conditional entropy, suppose that Y is an estimation of X and we are interested in probability of error Pe. If for Y=y, we can estimate X without error then H(Y|Y=y)=0."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Direct Application of AM-GM to an Inequality The simplest way to apply AM-GM is to apply it immediately on all of the terms. For example, we know that for non-negative values, x + y 2 \u2265 x y , x + y + z 3 \u2265 x y z 3 , w + x + y + z 4 \u2265 w x y z 4 ."}, {"text": "Joint entropy: H ( X , Y ) : = \u2212 \u03a3 x \u2208 J X \u03a3 y \u2208 J Y p ( x , y ) log p ( x , y ) . ."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The maximum entropy principle is defined as modeling a given set of data by finding the highest entropy to satisfy the constraints of our prior knowledge.  The maximum entropy model is a conditional probability model p(y|x) that allows us to predict class labels given a set of features for a given data point."}, {"text": "Properties. Unlike the classical conditional entropy, the conditional quantum entropy can be negative.  Positive conditional entropy of a state thus means the state cannot reach even the classical limit, while the negative conditional entropy provides for additional information."}, {"text": "The predicted value of y (\"\u02c6y \") is sometimes referred to as the \"fitted value\" and is computed as \u02c6yi=b0+b1xi y ^ i = b 0 + b 1 x i ."}, {"text": "joint entropy is the amount of information in two (or more) random variables; conditional entropy is the amount of information in one random variable given we already know the other."}]}, {"question": "How does the nocebo effect work", "positive_ctxs": [{"text": "A nocebo effect is said to occur when negative expectations of the patient regarding a treatment cause the treatment to have a more negative effect than it otherwise would have."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A new study suggests that the placebo effect may work in reverse. A new study suggests that the placebo effect may work in reverse. In the past, placebos have been given to participants in studies to detect whether the participant would still feel the effects of the \u201cdrug\u201d they thought they were being given."}, {"text": "In chaos theory, the butterfly effect is the sensitive dependence on initial conditions in which a small change in one state of a deterministic nonlinear system can result in large differences in a later state. The term butterfly effect is closely associated with the work of Edward Lorenz."}, {"text": "Additive interaction means the effect of two chemicals is equal to the sum of the effect of the two chemicals taken separately.  Synergistic interaction means that the effect of two chemicals taken together is greater than the sum of their separate effect at the same doses."}, {"text": "An experimental group, also known as a treatment group, receives the treatment whose effect researchers wish to study, whereas a control group does not. They should be identical in all other ways."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "In the design of experiments and analysis of variance, a main effect is the effect of an independent variable on a dependent variable averaged across the levels of any other independent variables.  Main effects are essentially the overall effect of a factor."}, {"text": "In statistics, main effect is the effect of one of just one of the independent variables on the dependent variable. There will always be the same number of main effects as independent variables. An interaction effect occurs if there is an interaction between the independent variables that affect the dependent variable."}]}, {"question": "What is the purpose of sampling frame", "positive_ctxs": [{"text": "A simple definition of a sampling frame is the set of source materials from which the sample is selected. The definition also encompasses the purpose of sampling frames, which is to provide a means for choosing the particular members of the target population that are to be interviewed in the survey."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A simple definition of a sampling frame is the set of source materials from which the sample is selected. The definition also encompasses the purpose of sampling frames, which is to provide a means for choosing the particular members of the target population that are to be interviewed in the survey."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific."}, {"text": "A sampling frame is a list or other device used to define a researcher's population of interest. The sampling frame defines a set of elements from which a researcher can select a sample of the target population."}, {"text": "A sampling frame is a list or other device used to define a researcher's population of interest. The sampling frame defines a set of elements from which a researcher can select a sample of the target population."}, {"text": "A sampling frame is a list or other device used to define a researcher's population of interest. The sampling frame defines a set of elements from which a researcher can select a sample of the target population."}]}, {"question": "What defines an outlier", "positive_ctxs": [{"text": "Definition of outliers. An outlier is an observation that lies an abnormal distance from other values in a random sample from a population."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, an outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analyses."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Some of the popular techniques are: Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept). Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "SYNONYMS FOR outlier 2 nonconformist, maverick; original, eccentric, bohemian; dissident, dissenter, iconoclast, heretic; outsider."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Definition of outliers. An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal."}]}, {"question": "What does it mean if a test is sensitive but not specific", "positive_ctxs": [{"text": "Sensitivity refers to a test's ability to designate an individual with disease as positive. A highly sensitive test means that there are few false negative results, and thus fewer cases of disease are missed. The specificity of a test is its ability to designate an individual who does not have a disease as negative."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "False negative would therefore mean that there was a object (result should be positive) but the algorithm did not detect it (and therefore returned negative). A true negative is simply the algorithm correctly stating that the area it checked does not hold an object."}, {"text": "This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction."}, {"text": "This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction."}, {"text": "This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction."}, {"text": "This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction."}]}, {"question": "How do you interpret a positively skewed distribution", "positive_ctxs": [{"text": "Interpreting. If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}, {"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}, {"text": "In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer."}, {"text": "In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right.  In a positively skewed distribution, the mode is always less than the mean and median."}, {"text": "If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical."}, {"text": "When p is greater than 0.5, the distribution will be positively skewed (the peak will be on the left side of the distribution, with relatively fewer observations on the right)."}, {"text": "Insufficient Data can cause a normal distribution to look completely scattered.  An extreme example: if you choose three random students and plot the results on a graph, you won't get a normal distribution. You might get a uniform distribution (i.e. 62 62 63) or you might get a skewed distribution (80 92 99)."}]}, {"question": "How do you find the weighted mean for grouped data", "positive_ctxs": [{"text": "9:1310:48Suggested clip \u00b7 69 secondsMean Mean of Grouped Data Weighted Mean - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The mean used here is referred to as the arithmetic mean \u2013 the sum of all values divided by the number of cases. When working with grouped data, this mean is sometimes referred to as the weighted mean or, more properly, the weighted arithmetic mean. Ungrouped and group methods."}, {"text": "How to Calculate VarianceFind the mean of the data set. Add all data values and divide by the sample size n.Find the squared difference from the mean for each data value. Subtract the mean from each data value and square the result.Find the sum of all the squared differences.  Calculate the variance."}, {"text": "How to Calculate VarianceFind the mean of the data set. Add all data values and divide by the sample size n.Find the squared difference from the mean for each data value. Subtract the mean from each data value and square the result.Find the sum of all the squared differences.  Calculate the variance."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "A mean can be determined for grouped data, or data that is placed in intervals.  The sum of the products divided by the total number of values will be the value of the mean."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}]}, {"question": "What is the difference between field experiment and quasi experiment", "positive_ctxs": [{"text": "In a true experiment, participants are randomly assigned to either the treatment or the control group, whereas they are not assigned randomly in a quasi-experiment.  Thus, the researcher must try to statistically control for as many of these differences as possible."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "As nouns the difference between trial and experiment is that trial is an opportunity to test something out; a test while experiment is a test under controlled conditions made to either demonstrate a known truth, examine the validity of a hypothesis, or determine the efficacy of something previously untried."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The sample space of a random experiment is the collection of all possible outcomes. An event associated with a random experiment is a subset of the sample space. The probability of any outcome is a number between 0 and 1. The probabilities of all the outcomes add up to 1."}, {"text": "The sample space of a random experiment is the collection of all possible outcomes. An event associated with a random experiment is a subset of the sample space. The probability of any outcome is a number between 0 and 1. The probabilities of all the outcomes add up to 1."}, {"text": "Definition : A random experiment is an experiment or a process for which the outcome cannot be predicted with certainty. Definition : The sample space (denoted S) of a random experiment is the set of all possible outcomes."}, {"text": "Definition : A random experiment is an experiment or a process for which the outcome cannot be predicted with certainty. Definition : The sample space (denoted S) of a random experiment is the set of all possible outcomes."}, {"text": "In probability, the set of outcomes from an experiment is known as an Event. So say for example you conduct an experiment by tossing a coin. The outcome of this experiment is the coin landing 'heads' or 'tails'. These can be said to be the events connected with the experiment."}]}, {"question": "What type of distributions is the binomial distribution", "positive_ctxs": [{"text": "In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes\u2013no question, and each with its own Boolean-valued outcome: success/yes/true/one (with probability p)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The negative binomial distribution is a probability distribution that is used with discrete random variables. This type of distribution concerns the number of trials that must occur in order to have a predetermined number of successes."}, {"text": "The negative binomial distribution is a probability distribution that is used with discrete random variables. This type of distribution concerns the number of trials that must occur in order to have a predetermined number of successes."}, {"text": "A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix \u201cbi\u201d means two, or twice)."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "The chi-square test is the most commonly used to test the goodness of fit tests and is used for discrete distributions like the binomial distribution and the Poisson distribution, whereas The Kolmogorov-Smirnov and Anderson-Darling goodness of fit tests are used for continuous distributions."}, {"text": "The probability mass function of the negative binomial distribution is. where r is the number of successes, k is the number of failures, and p is the probability of success."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution.  Suppose we flip a coin repeatedly and count the number of heads (successes)."}]}, {"question": "How do you find the scale of a histogram", "positive_ctxs": [{"text": "1:113:06Suggested clip \u00b7 115 secondsStatistics - How to make a histogram - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value. By looking at the histogram for a specific image a viewer will be able to judge the entire tonal distribution at a glance."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "A ratio scale is a quantitative scale where there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales."}, {"text": "A ratio scale is a quantitative scale where there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Histograms are generally used to show the results of a continuous data set such as height, weight, time, etc. A bar graph has spaces between the bars, while a histogram does not. A histogram often shows the frequency that an event occurs within the defined range. It shows you how many times that event happens."}]}, {"question": "How do you reject the null hypothesis in t test", "positive_ctxs": [{"text": "If the absolute value of the t-value is greater than the critical value, you reject the null hypothesis. If the absolute value of the t-value is less than the critical value, you fail to reject the null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How to Conduct Hypothesis TestsState the hypotheses. Every hypothesis test requires the analyst to state a null hypothesis and an alternative hypothesis.  Formulate an analysis plan. The analysis plan describes how to use sample data to accept or reject the null hypothesis.  Analyze sample data.  Interpret the results."}, {"text": "When you reject the null hypothesis with a t-test, you are saying that the means are statistically different. The difference is meaningful. Chi Square:  When you reject the null hypothesis with a Chi-Square, you are saying that there is a relationship between the two variables."}, {"text": "Compare the P-value to the \u03b1 significance level stated earlier. If it is less than \u03b1, reject the null hypothesis. If the result is greater than \u03b1, fail to reject the null hypothesis. If you reject the null hypothesis, this implies that your alternative hypothesis is correct, and that the data is significant."}, {"text": "It is easier to reject the null hypothesis with a one-tailed than with a two-tailed test as long as the effect is in the specified direction. Therefore, one-tailed tests have lower Type II error rates and more power than do two-tailed tests."}, {"text": "The critical region is the area that lies to the left of -1.645. If the z-value is less than -1.645 there we will reject the null hypothesis and accept the alternative hypothesis. If it is greater than -1.645, we will fail to reject the null hypothesis and say that the test was not statistically significant."}, {"text": "Statistical power, or the power of a hypothesis test is the probability that the test correctly rejects the null hypothesis. That is, the probability of a true positive result.  statistical power is the probability that a test will correctly reject a false null hypothesis."}]}, {"question": "What is the difference between binomial and geometric distribution", "positive_ctxs": [{"text": "In the binomial distribution, the number of trials is fixed, and we count the number of \"successes\". Whereas, in the geometric and negative binomial distributions, the number of \"successes\" is fixed, and we count the number of trials needed to obtain the desired number of \"successes\"."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "follows a negative binomial distribution with parameters r and p. The geometric distribution is a special case of discrete compound Poisson distribution."}, {"text": "In the binomial distribution, the number of trials is fixed, and we count the number of \"successes\". Whereas, in the geometric and negative binomial distributions, the number of \"successes\" is fixed, and we count the number of trials needed to obtain the desired number of \"successes\"."}, {"text": "The difference between the hypergeometric and the binomial distributions.  For the binomial distribution, the probability is the same for every trial. For the hypergeometric distribution, each trial changes the probability for each subsequent trial because there is no replacement."}, {"text": "The geometric distribution describes the probability of \"x trials are made before a success\", and the negative binomial distribution describes that of \"x trials are made before r successes are obtained\", where r is fixed. So you see that the latter is a particular case of the former, namely, when r=1."}, {"text": "This is in contrast to the Bernoulli, binomial, and hypergeometric distributions, where the number of possible values is finite.  Whereas, in the geometric and negative binomial distributions, the number of \"successes\" is fixed, and we count the number of trials needed to obtain the desired number of \"successes\"."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}]}, {"question": "What is considered a representative sample", "positive_ctxs": [{"text": "A representative sample is a subset of a population that seeks to accurately reflect the characteristics of the larger group. For example, a classroom of 30 students with 15 males and 15 females could generate a representative sample that might include six students: three males and three females."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Simply put, a random sample is a subset of individuals randomly selected by researchers to represent an entire group as a whole. The goal is to get a sample of people that is representative of the larger population."}, {"text": "Simply put, a random sample is a subset of individuals randomly selected by researchers to represent an entire group as a whole. The goal is to get a sample of people that is representative of the larger population."}, {"text": "A representative sample is a subset of a population that seeks to accurately reflect the characteristics of the larger group. For example, a classroom of 30 students with 15 males and 15 females could generate a representative sample that might include six students: three males and three females."}, {"text": "Probability sampling allows researchers to create a sample that is accurately representative of the real-life population of interest."}, {"text": "Quota sampling is different from stratified sampling, because in a stratified sample individuals within each stratum are selected at random. Quota sampling achieves a representative age distribution, but it isn't a random sample, because the sampling frame is unknown."}, {"text": "Generally, a value of r greater than 0.7 is considered a strong correlation. Anything between 0.5 and 0.7 is a moderate correlation, and anything less than 0.4 is considered a weak or no correlation."}, {"text": "No, because the sample is not representative of the whole population.  Find the\u200b range, variance, and standard deviation for the sample data."}]}, {"question": "What is cross validation in machine learning", "positive_ctxs": [{"text": "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.  That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A loss function is used to optimize a machine learning algorithm. The loss is calculated on training and validation and its interpretation is based on how well the model is doing in these two sets.  An accuracy metric is used to measure the algorithm's performance in an interpretable way."}, {"text": "Definition. In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived.  Model validation is carried out after model training."}, {"text": "The cross product is a calculation used in order to define the correlation coefficient between two variables. SP is the sum of all cross products between two variables."}, {"text": "1) Your model performs better on the training data than on the unknown validation data.  It can also happen when your training loss is calculated as a moving average over 1 epoch, whereas the validation loss is calculated after the learning phase of the same epoch."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The loss is calculated on training and validation and its interpretation is how well the model is doing for these two sets. Unlike accuracy, a loss is not a percentage. It is a sum of the errors made for each example in training or validation sets."}, {"text": "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived."}]}, {"question": "What are the disadvantages of using non standard units", "positive_ctxs": [{"text": "If we use non - standard units then we may not be able to express our measurement internationally as mainly standard units are used and accepted internationally. The non- standard units do not have the same dimensions all over the world."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Nonstandard units provide a good rationale for using standard units. It allows for a good transition into standard units because the students can understand the need for standard units if they have measured the same object but determined differing answers."}, {"text": "The standard error of the regression (S), also known as the standard error of the estimate, represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable."}, {"text": "The standard error of the regression (S), also known as the standard error of the estimate, represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable."}, {"text": "S is known both as the standard error of the regression and as the standard error of the estimate. S represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable."}, {"text": "Disadvantages include its \"black box\" nature, greater computational burden, proneness to overfitting, and the empirical nature of model development. An overview of the features of neural networks and logistic regression is presented, and the advantages and disadvantages of using this modeling technique are discussed."}, {"text": "The sum of squared errors is a 'total' and is, therefore, affected by the number of data points. The variance is the 'average' variability but in units squared. The standard deviation is the average variation but converted back to the original units of measurement."}, {"text": "The standard error of the regression provides the absolute measure of the typical distance that the data points fall from the regression line. S is in the units of the dependent variable. R-squared provides the relative measure of the percentage of the dependent variable variance that the model explains."}]}, {"question": "How does a neural network predict", "positive_ctxs": [{"text": "Neural networks work better at predictive analytics because of the hidden layers. Linear regression models use only input and output nodes to make predictions. Neural network also use the hidden layer to make predictions more accurate. That's because it 'learns' the way a human does."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A recurrent neural network (RNN) is a type of artificial neural network commonly used in speech recognition and natural language processing (NLP). RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario."}, {"text": "A recurrent neural network (RNN) is a type of artificial neural network commonly used in speech recognition and natural language processing (NLP). RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario."}, {"text": "A recurrent neural network (RNN) is a type of artificial neural network commonly used in speech recognition and natural language processing (NLP). RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario."}, {"text": "A recurrent neural network (RNN) is a type of neural network commonly used in speech recognition. RNNs are designed to recognize the sequential characteristics in data and use patterns to predict the next likely scenario."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "The main downside was that it was a pretty large network in terms of the number of parameters to be trained. VGG-19 neural network which is bigger then VGG-16, but because VGG-16 does almost as well as the VGG-19 a lot of people will use VGG-16."}, {"text": "A feedforward neural network is an artificial neural network wherein connections between the units do not form a cycle.  The simplest kind of neural network is a single-layer perceptron network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights."}]}, {"question": "What is cluster sampling technique", "positive_ctxs": [{"text": "Cluster sampling is a probability sampling method in which you divide a population into clusters, such as districts or schools, and then randomly select some of these clusters as your sample. The clusters should ideally each be mini-representations of the population as a whole."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "Cluster Sampling: Advantages and Disadvantages Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "In stratified sampling, a random sample is drawn from each of the strata, whereas in cluster sampling only the selected clusters are sampled. A common motivation of cluster sampling is to reduce costs by increasing sampling efficiency."}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).  In cluster sampling, the sampling unit is the whole cluster; Instead of sampling individuals from within each group, a researcher will study whole clusters."}]}, {"question": "What is a pipeline in ML", "positive_ctxs": [{"text": "A machine learning pipeline is used to help automate machine learning workflows. They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Generally, a machine learning pipeline describes or models your ML process: writing code, releasing it to production, performing data extractions, creating training models, and tuning the algorithm. An ML pipeline should be a continuous process as a team works on their ML platform."}, {"text": "There are many moving parts in a Machine Learning (ML) model that have to be tied together for an ML model to execute and produce results successfully. This process of tying together different pieces of the ML process is known as a pipeline. A pipeline is a generalized but very important concept for a Data Scientist."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "The difference is very slim between machine learning (ML) and optimization theory. In ML the idea is to learn a function that minimizes an error or one that maximizes reward over punishment.  The goal for ML is similarly to optimize the performance of a model given an objective and the training data."}, {"text": "This is machine learning in general and almost all ML algorithms are based on this optimization. Curve fitting, on the other hand, is a process of finding a mathematical function on the available data such that the function defines the best fit on the data points.  ML does the same but it needs to generalize it's fit."}, {"text": "A machine learning pipeline is used to help automate machine learning workflows. They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative."}]}, {"question": "How do you find the cumulative relative frequency", "positive_ctxs": [{"text": "To find the relative frequency, divide the frequency by the total number of data values. To find the cumulative relative frequency, add all of the previous relative frequencies to the relative frequency for the current row."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To find the relative frequency, divide the frequency by the total number of data values. To find the cumulative relative frequency, add all of the previous relative frequencies to the relative frequency for the current row."}, {"text": "The only difference between a relative frequency distribution graph and a frequency distribution graph is that the vertical axis uses proportional or relative frequency rather than simple frequency. Cumulative relative frequency (also called an ogive) is the accumulation of the previous relative frequencies."}, {"text": "As the area of a bar represents the frequency of its interval, the height of the bar represents the density. If you label the scare it is either frequency per unit or, if you divide by the total frequency, relative frequency per unit."}, {"text": "How to calculate the absolute error and relative errorTo find out the absolute error, subtract the approximated value from the real one: |1.41421356237 - 1.41| = 0.00421356237.Divide this value by the real value to obtain the relative error: |0.00421356237 / 1.41421356237| = 0.298%"}, {"text": "Step 1: Prepare a table containing less than type cumulative frequency with the help of given frequencies. belongs. Class-interval of this cumulative frequency is the median class-interval. Step 3 : Find out the frequency f and lower limit l of this median class."}, {"text": "An easy way to define the difference between frequency and relative frequency is that frequency relies on the actual values of each class in a statistical data set while relative frequency compares these individual values to the overall totals of all classes concerned in a data set."}, {"text": "A frequency table is a chart that shows the popularity or mode of a certain type of data. When we look at frequency, we are looking at the number of times an event occurs within a given scenario.  You can find the relative frequency by simply dividing the frequency number by the total number of values in the data set."}]}, {"question": "Why classification is important in machine learning", "positive_ctxs": [{"text": "Classification and prediction are two forms of data analysis that can be used to extract models describing important data classes or to predict future data trends [8]. Classification is a data mining (machine learning) technique used to predict group membership for data instances."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Another most important role of training data for machine learning is classifying the data sets into various categorized which is very much important for supervised machine learning.  It helps them to recognize and classify the similar objects in future, thus training data is very important for such classification."}, {"text": "The Gaussian Processes Classifier is a classification machine learning algorithm. Gaussian Processes are a generalization of the Gaussian probability distribution and can be used as the basis for sophisticated non-parametric machine learning algorithms for classification and regression."}, {"text": "Time series forecasting is an important area of machine learning that is often neglected. It is important because there are so many prediction problems that involve a time component.  Standard definitions of time series, time series analysis, and time series forecasting."}, {"text": "XGboost is the most widely used algorithm in machine learning, whether the problem is a classification or a regression problem. It is known for its good performance as compared to all other machine learning algorithms."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks.  In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly."}, {"text": "Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks.  In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly."}]}, {"question": "What is an example of systematic random sample", "positive_ctxs": [{"text": "Systematic random sampling is the random sampling method that requires selecting samples based on a system of intervals in a numbered population. For example, Lucas can give a survey to every fourth customer that comes in to the movie theater."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Despite the sample population being selected in advance, systematic sampling is still thought of as being random if the periodic interval is determined beforehand and the starting point is random."}, {"text": "Under simple random sampling, a sample of items is chosen randomly from a population, and each item has an equal probability of being chosen. Meanwhile, systematic sampling involves selecting items from an ordered population using a skip or sampling interval."}, {"text": "Under simple random sampling, a sample of items is chosen randomly from a population, and each item has an equal probability of being chosen. Meanwhile, systematic sampling involves selecting items from an ordered population using a skip or sampling interval."}, {"text": "Under simple random sampling, a sample of items is chosen randomly from a population, and each item has an equal probability of being chosen.  Meanwhile, systematic sampling involves selecting items from an ordered population using a skip or sampling interval."}, {"text": "Analysis of variance (ANOVA) is an analysis tool used in statistics that splits an observed aggregate variability found inside a data set into two parts: systematic factors and random factors. The systematic factors have a statistical influence on the given data set, while the random factors do not."}, {"text": "In probability theory, an event is an outcome or defined collection of outcomes of a random experiment. Since the collection of all possible outcomes to a random experiment is called the sample space, another definiton of event is any subset of a sample space."}, {"text": "A simple random sample is a subset of a statistical population in which each member of the subset has an equal probability of being chosen. A simple random sample is meant to be an unbiased representation of a group."}]}, {"question": "Is decision tree an ensemble method", "positive_ctxs": [{"text": "Mathematically speaking, a decision tree has low bias and high variance. Averaging the result of many decision trees reduces the variance while maintaining that low bias. Combining trees is known as an 'ensemble method'."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the"}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree learning is a supervised machine learning technique for inducing a decision tree from training data. A decision tree (also referred to as a classification tree or a reduction tree) is a predictive model which is a mapping from observations about an item to conclusions about its target value."}, {"text": "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value."}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes"}]}, {"question": "What is the difference between union and intersection in statistics", "positive_ctxs": [{"text": "The union of two sets is a new set that contains all of the elements that are in at least one of the two sets.  The intersection of two sets is a new set that contains all of the elements that are in both sets."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The probability of the intersection of Events A and B is denoted by P(A \u2229 B). If Events A and B are mutually exclusive, P(A \u2229 B) = 0. The probability that Events A or B occur is the probability of the union of A and B."}, {"text": "Look up the normal distribution in a statistics table. Statistics tables can be found online or in statistics textbooks. Find the value for the intersection of the correct degrees of freedom and alpha. If this value is less than or equal to the chi-square value, the data is statistically significant."}, {"text": "Key TakeawaysThe union of two or more sets is the set that contains all the elements of the two or more sets.  The general probability addition rule for the union of two events states that P(A\u222aB)=P(A)+P(B)\u2212P(A\u2229B) P ( A \u222a B ) = P ( A ) + P ( B ) \u2212 P ( A \u2229 B ) , where A\u2229B A \u2229 B is the intersection of the two sets.More items"}, {"text": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables."}, {"text": "Explain the difference between descriptive and inferential statistics. Descriptive statistics describes sets of data. Inferential statistics draws conclusions about the sets of data based on sampling.  A population is a set of units of interest to a study."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}]}, {"question": "What is it like when an academic culture is shaped by a hidden curriculum", "positive_ctxs": [{"text": "The hidden-curriculum concept is based on the recognition that students absorb lessons in school that may or may not be part of the formal course of study\u2014for example, how they should interact with peers, teachers, and other adults; how they should perceive different races, groups, or classes of people; or what ideas"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "It is a mathematical function having a characteristic that can take any real value and map it to between 0 to 1 shaped like the letter \u201cS\u201d. The sigmoid function also called a logistic function."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true."}, {"text": "In probability theory, a continuity correction is an adjustment that is made when a discrete distribution is approximated by a continuous distribution."}, {"text": "In probability theory, a continuity correction is an adjustment that is made when a discrete distribution is approximated by a continuous distribution."}, {"text": "Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive."}]}, {"question": "What is convolutional neural network algorithm", "positive_ctxs": [{"text": "A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery."}, {"text": "A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input."}, {"text": "A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input."}, {"text": "Deep NN is just a deep neural network, with a lot of layers. It can be CNN, or just a plain multilayer perceptron. CNN, or convolutional neural network, is a neural network using convolution layer and pooling layer."}, {"text": "Convolutional neural networks work because it's a good extension from the standard deep-learning algorithm. Given unlimited resources and money, there is no need for convolutional because the standard algorithm will also work. However, convolutional is more efficient because it reduces the number of parameters."}, {"text": "Convolutional neural networks work because it's a good extension from the standard deep-learning algorithm. Given unlimited resources and money, there is no need for convolutional because the standard algorithm will also work. However, convolutional is more efficient because it reduces the number of parameters."}]}, {"question": "What is exploratory structural equation modeling", "positive_ctxs": [{"text": "Exploratory structural equation modeling (ESEM) is an approach for analysis of latent variables using exploratory factor analysis to evaluate the measurement model.  ESEM is recommended when non-ignorable cross-factor loadings exist."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Structural equation modeling is a multivariate statistical analysis technique that is used to analyze structural relationships. This technique is the combination of factor analysis and multiple regression analysis, and it is used to analyze the structural relationship between measured variables and latent constructs."}, {"text": "A latent variable is a variable that is inferred using models from observed data.  Approaches to inferring latent variables from data include: using a single observed variable, multi-item scales, predictive models, dimension reduction techniques such as factor analysis, structural equation models, and mixture models."}, {"text": "There are two main differences between regression and structural equation modelling. The first is that SEM allows us to develop complex path models with direct and indirect effects. This allows us to more accurately model causal mechanisms we are interested in. The second key difference is to do with measurement."}, {"text": "In statistics, exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task."}, {"text": "Structural equation modeling (SEM) is a multivariate statistical framework that is used to model complex relationships between directly and indirectly observed (latent) variables.  Modeling the aggregate effects of common and rare variants in multiple potentially interesting genes using latent variable SEM."}, {"text": "Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables."}, {"text": "Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables."}]}, {"question": "When should I use batch normalization", "positive_ctxs": [{"text": "Batch normalization may be used on the inputs to the layer before or after the activation function in the previous layer. It may be more appropriate after the activation function if for s-shaped functions like the hyperbolic tangent and logistic function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling.  Others sustain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks."}, {"text": "Andrew Ng says that batch normalization should be applied immediately before the non-linearity of the current layer. The authors of the BN paper said that as well, but now according to Fran\u00e7ois Chollet on the keras thread, the BN paper authors use BN after the activation layer."}, {"text": "Andrew Ng says that batch normalization should be applied immediately before the non-linearity of the current layer. The authors of the BN paper said that as well, but now according to Fran\u00e7ois Chollet on the keras thread, the BN paper authors use BN after the activation layer."}, {"text": "To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation."}, {"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling."}, {"text": "When I calculate population variance, I then divide the sum of squared deviations from the mean by the number of items in the population (in example 1 I was dividing by 12). When I calculate sample variance, I divide it by the number of items in the sample less one. In our example 2, I divide by 99 (100 less 1)."}, {"text": "Using batch normalization makes the network more stable during training. This may require the use of much larger than normal learning rates, that in turn may further speed up the learning process. \u2014 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, 2015."}]}, {"question": "Is linear regression A GLM", "positive_ctxs": [{"text": "The term general linear model (GLM) usually refers to conventional linear regression models for a continuous response variable given continuous and/or categorical predictors. It includes multiple linear regression, as well as ANOVA and ANCOVA (with fixed effects only)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Thus logit regression is simply the GLM when describing it in terms of its link function, and logistic regression describes the GLM in terms of its activation function."}, {"text": ". Thus logit regression is simply the GLM when describing it in terms of its link function, and logistic regression describes the GLM in terms of its activation function."}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them."}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them. Sometimes this is incorrect."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "A GLM consists of three components: A random component, A systematic component, and. A link function."}, {"text": "As the name suggests, GLM models are the generalization of the linear regression model.  we mean that rather than forcing a linear relationship between the dependent and independent variables, it allows the dependent variable to be related with the independent variables through a link function."}]}, {"question": "Which algorithm is best for sentiment analysis", "positive_ctxs": [{"text": "Overall, Sentiment analysis may involve the following types of classification algorithms:Linear Regression.Naive Bayes.Support Vector Machines.RNN derivatives LSTM and GRU."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Clustering or cluster analysis is an unsupervised learning problem. It is often used as a data analysis technique for discovering interesting patterns in data, such as groups of customers based on their behavior. There are many clustering algorithms to choose from and no single best clustering algorithm for all cases."}, {"text": "Sentiment analysis is the automated process of analyzing text data and sorting it into sentiments positive, negative, or neutral. Using sentiment analysis tools to analyze opinions in Twitter data can help companies understand how people are talking about their brand."}, {"text": "Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability."}, {"text": "Kmeans clustering algorithm is applied to reduced datasets which is done by principal component analysis dimension reduction method. Cluster analysis is one of the major data analysis methods widely used for many practical applications in emerging areas[12]."}]}, {"question": "How does sensitivity and specificity relate to false positives negatives and true positives negatives", "positive_ctxs": [{"text": "In many tests, including diagnostic medical tests, sensitivity is the extent to which true positives are not overlooked, thus false negatives are few, and specificity is the extent to which true negatives are classified as such, thus false positives are few."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "These include: true positives, false positives (type 1 error), true negatives, and false negatives (type 2 error)."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "AUC and accuracy are fairly different things.  For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set. AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else."}, {"text": "AUC and accuracy are fairly different things.  For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set. AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else."}, {"text": "Measuring the Accuracy of a Test By calculating ratios between these values, we can quantitatively measure the accuracy of our tests. The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives)."}]}, {"question": "What is the difference between a normal distribution and a t distribution", "positive_ctxs": [{"text": "The normal distribution is used when the population distribution of data is assumed normal.  A sample of the population is used to estimate the mean and standard deviation. The t statistic is an estimate of the standard error of the mean of the population or how well known is the mean based on the sample size."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A major difference is in its shape: the normal distribution is symmetrical, whereas the lognormal distribution is not. Because the values in a lognormal distribution are positive, they create a right-skewed curve.  A further distinction is that the values used to derive a lognormal distribution are normally distributed."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}]}, {"question": "How is beta diversity measured", "positive_ctxs": [{"text": "Beta diversity measures the change in diversity of species from one environment to another. In simpler terms, it calculates the number of species that are not the same in two different environments. There are also indices which measure beta diversity on a normalized scale, usually from zero to one."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The beta value is used in measuring how effectively the predictor variable influences the criterion variable, it is measured in terms of standard deviation. R, is the measure of association between the observed value and the predicted value of the criterion variable."}, {"text": "Ridge regression has an additional factor called \u03bb (lambda) which is called the penalty factor which is added while estimating beta coefficients. This penalty factor penalizes high value of beta which in turn shrinks beta coefficients thereby reducing the mean squared error and predicted error."}, {"text": "The beta function has the formula. B(\\alpha,\\beta) = \\int_{0}^{1} {t^{\\alpha-1}(1-t)^{\\beta-1}dt} The case where a = 0 and b = 1 is called the standard beta distribution. The equation for the standard beta distribution is. f(x) = \\frac{x^{p-1}(1-x)^{q-1}}{B(p,q)} \\hspace{.3in} 0 \\le x \\le 1; p, q > 0."}, {"text": "Alpha levels and beta levels are related: An alpha level is the probability of a type I error, or rejecting the null hypothesis when it is true. A beta level, usually just called beta(\u03b2), is the opposite; the probability of of accepting the null hypothesis when it's false."}, {"text": "The beta distribution is a continuous probability distribution that can be used to represent proportion or probability outcomes. For example, the beta distribution might be used to find how likely it is that your preferred candidate for mayor will receive 70% of the vote."}, {"text": "The beta distribution is a continuous probability distribution that can be used to represent proportion or probability outcomes. For example, the beta distribution might be used to find how likely it is that your preferred candidate for mayor will receive 70% of the vote."}, {"text": "The second reason you may see validation loss lower than training loss is due to how the loss value are measured and reported: Training loss is measured during each epoch. While validation loss is measured after each epoch."}]}, {"question": "What logit means", "positive_ctxs": [{"text": "In statistics, the logit (/\u02c8lo\u028ad\u0292\u026at/ LOH-jit) function or the log-odds is the logarithm of the odds where p is a probability. It is a type of function that creates a map of probability values from to. ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Logistic regression, also called a logit model, is used to model dichotomous outcome variables. In the logit model the log odds of the outcome is modeled as a linear combination of the predictor variables."}, {"text": "Logistic regression, also called a logit model, is used to model dichotomous outcome variables. In the logit model the log odds of the outcome is modeled as a linear combination of the predictor variables."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "If p is a probability, then p/(1 \u2212 p) is the corresponding odds; the logit of the probability is the logarithm of the odds, i.e.  For each choice of base, the logit function takes values between negative and positive infinity."}, {"text": "To convert a logit ( glm output) to probability, follow these 3 steps:Take glm output coefficient (logit)compute e-function on the logit using exp() \u201cde-logarithimize\u201d (you'll get odds then)convert odds to probability using this formula prob = odds / (1 + odds) ."}]}, {"question": "How do you find connected components in an image", "positive_ctxs": [{"text": "Two pixels, p and q, are connected if there is a path from p to q of pixels with property V. A path is an ordered sequence of pixels such that any two adjacent pixels in the sequence are neighbors. An example of an image with a connected component is shown at the right."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Connected components labeling scans an image and groups its pixels into components based on pixel connectivity, i.e. all pixels in a connected component share similar pixel intensity values and are in some way connected with each other."}, {"text": "Kosaraju's algorithm finds the strongly connected components of a graph.  - For each vertex u of the graph do Visit(u), where Visit(u) is the recursive subroutine: - If u is unvisited then: - Mark u as visited. - For each out-neighbour v of u, do Visit(v)."}, {"text": "A Blob is a group of connected pixels in an image that share some common property ( E.g grayscale value ). In the image above, the dark connected regions are blobs, and the goal of blob detection is to identify and mark these regions."}, {"text": "A Blob is a group of connected pixels in an image that share some common property ( E.g grayscale value ). In the image above, the dark connected regions are blobs, and the goal of blob detection is to identify and mark these regions."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Connected components, in a 2D image, are clusters of pixels with the same value, which are connected to each other through either 4-pixel, or 8-pixel connectivity.  We offer several user-friendly ways to segment, and then rapidly calculate and display the connected components of 2D and 3D segmentations."}]}, {"question": "Classification machine learning When should I use a K NN classifier over a Naive Bayes classifier", "positive_ctxs": [{"text": "1. If having conditional independence will highly negative affect classification, you'll want to choose K-NN over Naive Bayes. Naive Bayes can suffer from the zero probability problem; when a particular attribute's conditional probability equals zero, Naive Bayes will completely fail to produce a valid prediction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Naive Bayes is a linear classifier Naive Bayes leads to a linear decision boundary in many common cases.  The red decision line indicates the decision boundary where P(y=1|x)=P(y=2|x)."}, {"text": "The computational efficiency of Naive Bayes lies in the fact that the runtime complexity of Naive Bayes classifier is O(nK), where n is the number of features and K is the number of label classes."}, {"text": "a. If your data is labeled, but you only have a limited amount, you should use a classifier with high bias (for example, Naive Bayes). I'm guessing this is because a higher-bias classifier will have lower variance, which is good because of the small amount of data."}, {"text": "And the Machine Learning \u2013 The Na\u00efve Bayes Classifier. It is a classification technique based on Bayes' theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature."}, {"text": "It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature."}, {"text": "It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature."}, {"text": "It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature."}]}, {"question": "What is a histogram in image processing", "positive_ctxs": [{"text": "An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value.  The vertical axis represents the size of the area (total number of pixels) that is captured in each one of these zones."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value. By looking at the histogram for a specific image a viewer will be able to judge the entire tonal distribution at a glance."}, {"text": "The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts occurrences of gradient orientation in localized portions of an image."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}, {"text": "Image processing is often viewed as arbitrarily manipulating an image to achieve an aesthetic standard or to support a preferred reality. However, image processing is more accurately defined as a means of translation between the human visual system and digital imaging devices."}, {"text": "Image processing is often viewed as arbitrarily manipulating an image to achieve an aesthetic standard or to support a preferred reality. However, image processing is more accurately defined as a means of translation between the human visual system and digital imaging devices."}, {"text": "A SIFT descriptor is a 3-D spatial histogram of the image gradients in characterizing the appearance of a keypoint. The gradient at each pixel is regarded as a sample of a three-dimensional elementary feature vector, formed by the pixel location and the gradient orientation."}]}, {"question": "What do you understand by true positive rate and false positive rate", "positive_ctxs": [{"text": "Measuring the Accuracy of a Test The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives).  The true positive rate (TPR, also called sensitivity) is calculated as TP/TP+FN."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "AUC and accuracy are fairly different things.  For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set. AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else."}, {"text": "AUC and accuracy are fairly different things.  For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set. AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else."}, {"text": "The false discovery rate is the ratio of the number of false positive results to the number of total positive test results. Out of 10,000 people given the test, there are 450 true positive results (box at top right) and 190 false positive results (box at bottom right) for a total of 640 positive results."}, {"text": "The receiver operating characteristic (ROC) curve is a two dimensional graph in which the false positive rate is plotted on the X axis and the true positive rate is plotted on the Y axis. The ROC curves are useful to visualize and compare the performance of classifier methods (see Figure 1)."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}]}, {"question": "How do you identify stock patterns", "positive_ctxs": [{"text": "Just having them in your face each and every day will subconsciously help you learn to recognize them in live trading.Pennant.Cup And Handle.Ascending Triangle.Triple Bottom.Descending Triangle.Inverse Head And Shoulders.Bullish Symmetric Triangle.Rounding Bottom.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How to read a stock chartIdentify the trend line. This is that blue line you see every time you hear about a stock\u2014it's either going up or down right?  Look for lines of support and resistance.  Know when dividends and stock splits occur.  Understand historic trading volumes."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Machine learning algorithms find natural patterns in data that generate insight and help you make better decisions and predictions. They are used every day to make critical decisions in medical diagnosis, stock trading, energy load forecasting, and more."}, {"text": "Descriptive analytics is a statistical method that is used to search and summarize historical data in order to identify patterns or meaning."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "What is the difference between linear models and generalized linear models", "positive_ctxs": [{"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Linear models, generalized linear models, and nonlinear models are examples of parametric regression models because we know the function that describes the relationship between the response and explanatory variables.  If the relationship is unknown and nonlinear, nonparametric regression models should be used."}, {"text": "In statistics, a generalized linear mixed model (GLMM) is an extension to the generalized linear model (GLM) in which the linear predictor contains random effects in addition to the usual fixed effects. They also inherit from GLMs the idea of extending linear mixed models to non-normal data."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}, {"text": "The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions."}]}, {"question": "Is a decision tree a model", "positive_ctxs": [{"text": "Decision trees: Are popular among non-statisticians as they produce a model that is very easy to interpret. Each leaf node is presented as an if/then rule."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Decision tree learning is a supervised machine learning technique for inducing a decision tree from training data. A decision tree (also referred to as a classification tree or a reduction tree) is a predictive model which is a mapping from observations about an item to conclusions about its target value."}, {"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision trees are a classic machine learning technique. The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree. By Narendra Nath Joshi, Carnegie Mellon.  The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree."}, {"text": "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value."}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes"}]}, {"question": "What is Softmax in deep learning", "positive_ctxs": [{"text": "You likely have run into the Softmax function, a wonderful activation function that turns numbers aka logits into probabilities that sum to one. Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In short, Softmax Loss is actually just a Softmax Activation plus a Cross-Entropy Loss. Softmax is an activation function that outputs the probability for each class and these probabilities will sum up to one. Cross Entropy loss is just the sum of the negative logarithm of the probabilities."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}]}, {"question": "Where are AI used", "positive_ctxs": [{"text": "Currently AI is Used is Following Things/Fields:Virtual Assistant or Chatbots.Agriculture and Farming.Autonomous Flying.Retail, Shopping and Fashion.Security and Surveillance.Sports Analytics and Activities.Manufacturing and Production.Live Stock and Inventory Management.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Artificial intelligence (AI) is a branch of computer science.  Most AI programs are not used to control robots. Even when AI is used to control robots, the AI algorithms are only part of the larger robotic system, which also includes sensors, actuators, and non-AI programming."}, {"text": "AI Strategy is a road plan for the adoption and implementation of artificial intelligence, machine learning, or deep learning technologies within your organization. An AI Strategy defines your AI priorities, goals, milestones, mission and vision.  AI Strategies are being used in corporations around the world."}, {"text": "Values range from 0 to 1, where 0 is perfect disagreement and 1 is perfect agreement. Krippendorff suggests: \u201c[I]t is customary to require \u03b1 \u2265 . 800. Where tentative conclusions are still acceptable, \u03b1 \u2265 ."}, {"text": "Difference between rule-based AI and machine learning Machine learning systems are probabilistic and rule-based AI models are deterministic.  Machine learning systems require more data as compared to rule-based models. Rule-based AI models can operate with simple basic information and data."}, {"text": "Optimizing Neural Networks \u2014 Where to Start?Start with learning rate;Then try number of hidden units, mini-batch size and momentum term;Lastly, tune number of layers and learning rate decay."}, {"text": "The term cognitive computing is typically used to describe AI systems that aim to simulate human thought.  A number of AI technologies are required for a computer system to build cognitive models that mimic human thought processes, including machine learning, deep learning, neural networks, NLP and sentiment analysis."}, {"text": "To analyze data and reporting speed AI can be very helpful in improving the data analyzing speed and also to increase the reporting time. The data are analyzed more accurately and the reporting time is also increased. AI can be used to analyze large amounts of data to draw conclusive reports."}]}, {"question": "What is an example of skewed data", "positive_ctxs": [{"text": "The mean is also to the left of the peak. A right-skewed distribution has a long right tail.  Next, you'll see a fair amount of negatively skewed distributions. For example, household income in the U.S. is negatively skewed with a very long left tail."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical."}, {"text": "Data skewed to the right is usually a result of a lower boundary in a data set (whereas data skewed to the left is a result of a higher boundary). So if the data set's lower bounds are extremely low relative to the rest of the data, this will cause the data to skew right. Another cause of skewness is start-up effects."}, {"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}, {"text": "An example of statistics is a report of numbers saying how many followers of each religion there are in a particular country. An example of statistics is a math class offered in high schools and colleges. The definition of a statistic is a number, or a person who is an unnamed piece of data to be studied."}, {"text": "The definition of an ensemble is two or more people or things that function together as a whole. An example of an ensemble is a string quartet. An example of an ensemble is a group of actors in a play.  A small group of musicians playing or singing together."}, {"text": "The log transformation can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics. Figure 1 shows an example of how a log transformation can make patterns more visible."}]}, {"question": "What is law of averages 1", "positive_ctxs": [{"text": "The law of averages is the commonly held belief that a particular outcome or event will over certain periods of time occur at a frequency that is similar to its probability. Depending on context or application it can be considered a valid common-sense observation or a misunderstanding of probability."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The law of averages is often mistaken by many people as the law of large numbers, but there is a big difference. The law of averages is a spurious belief that any deviation in expected probability will have to average out in a small sample of consecutive experiments, but this is not necessarily true."}, {"text": "The law of averages is a false belief, sometimes known as the 'gambler's fallacy,' that is derived from the law of large numbers.  The law of averages is a misconception that probability occurs with a small number of consecutive experiments so they will certainly have to 'average out' sooner rather than later."}, {"text": "The law of averages is not a mathematical principle, whereas the law of large numbers is. In probability theory, the law of large numbers is a theorem that describes the result of performing the same experiment a large number of times."}, {"text": "The law of averages is sometimes known as \u201cGambler's Fallacy. \u201d It evokes the idea that an event is \u201cdue\u201d to happen.  The law of averages says it's due to land on black! \u201d Of course, the wheel has no memory and its probabilities do not change according to past results."}, {"text": "The law of averages is not a mathematical principle, whereas the law of large numbers is.  According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed."}, {"text": "The law of averages is the commonly held belief that a particular outcome or event will over certain periods of time occur at a frequency that is similar to its probability. Depending on context or application it can be considered a valid common-sense observation or a misunderstanding of probability."}, {"text": "Functions of Random Variables One law is called the \u201cweak\u201d law of large numbers, and the other is called the \u201cstrong\u201d law of large numbers. The weak law describes how a sequence of probabilities converges, and the strong law describes how a sequence of random variables behaves in the limit."}]}, {"question": "What are some methods of time series regression analysis", "positive_ctxs": [{"text": "Time series regression is a statistical method for predicting a future response based on the response history (known as autoregressive dynamics) and the transfer of dynamics from relevant predictors.  Time series regression is commonly used for modeling and forecasting of economic, financial, and biological systems."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.  Interrupted time series analysis is the analysis of interventions on a single time series. Time series data have a natural temporal ordering."}, {"text": "Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values."}, {"text": "Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components. Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting."}, {"text": "Generally, we use linear regression for time series analysis, it is used for predicting the result for time series as its trends. For example, If we have a dataset of time series with the help of linear regression we can predict the sales with the time."}, {"text": "Time series analysis involves developing models that best capture or describe an observed time series in order to understand the underlying causes. This field of study seeks the \u201cwhy\u201d behind a time series dataset."}, {"text": "Time series analysis is a statistical technique that deals with time series data, or trend analysis. Time series data means that data is in a series of particular time periods or intervals.  Time series data: A set of observations on the values that a variable takes at different times."}, {"text": "Time series forecasting is an important area of machine learning that is often neglected. It is important because there are so many prediction problems that involve a time component.  Standard definitions of time series, time series analysis, and time series forecasting."}]}, {"question": "What is Z scores in statistics", "positive_ctxs": [{"text": "A Z-score is a numerical measurement that describes a value's relationship to the mean of a group of values. Z-score is measured in terms of standard deviations from the mean.  A Z-Score is a statistical measurement of a score's relationship to the mean in a group of scores."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution."}, {"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}, {"text": "The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution."}, {"text": "The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution."}, {"text": "The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution."}, {"text": "The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution."}]}, {"question": "What is rule based learning in AI", "positive_ctxs": [{"text": "Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "An artificial neural network's learning rule or learning process is a method, mathematical logic or algorithm which improves the network's performance and/or training time. Usually, this rule is applied repeatedly over the network."}, {"text": "AI taking into account many levels of abstraction, embodied AI and multimodal interaction is also DAI. Distributed AI means AI solved by multiple smart or reasoning agents (communicant object, physical or software) where size of agents can be a simple rule or can be a human or more ambient or pervasive structure."}, {"text": "The process of adjusting the weights and threshold of the ADALINE network is based on a learning algorithm named the Delta rule (Widrow and Hoff 1960) or Widrow-Hoff learning rule, also known as LMS (Least Mean Square ) algorithm or Gradient Descent method."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm."}, {"text": "In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm."}, {"text": "Deep learning is an AI function that mimics the workings of the human brain in processing data for use in detecting objects, recognizing speech, translating languages, and making decisions. Deep learning AI is able to learn without human supervision, drawing from data that is both unstructured and unlabeled."}]}, {"question": "Can artificial intelligence contribute in sustainable development", "positive_ctxs": [{"text": "The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals.  Failure to do so could result in gaps in transparency, safety, and ethical standards."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Java, Python, Lisp, Prolog, and C++ are major AI programming language used for artificial intelligence capable of satisfying different needs in the development and designing of different software."}, {"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "Java, Python, Lisp, Prolog, and C++ are major AI programming language used for artificial intelligence capable of satisfying different needs in the development and designing of different software.  It answers the question, 'what is the language used for artificial intelligence?"}, {"text": "Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves."}, {"text": "Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "Artificial Intelligence (AI) is the branch of computer sciences that emphasizes the development of intelligence machines, thinking and working like humans. For example, speech recognition, problem-solving, learning and planning."}]}, {"question": "What does the interquartile range mean", "positive_ctxs": [{"text": "When a data set has outliers or extreme values, we summarize a typical value using the median as opposed to the mean. When a data set has outliers, variability is often summarized by a statistic called the interquartile range, which is the difference between the first and third quartiles."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are 5 values above the median (upper half), the middle value is 77 which is the third quartile. The interquartile range is 77 \u2013 64 = 13; the interquartile range is the range of the middle 50% of the data.  When the sample size is odd, the median and quartiles are determined in the same way."}, {"text": "The interquartile range is the difference between the third quartile and the first quartile in a data set, giving the middle 50%. The interquartile range is a measure of spread; it's used to build box plots, determine normal distributions and as a way to determine outliers."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "We can use the median with the interquartile range, or we can use the mean with the standard deviation."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "To find the interquartile range (IQR), \u200bfirst find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1."}, {"text": "To find the interquartile range (IQR), \u200bfirst find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1."}]}, {"question": "What is Large Scale Machine Learning", "positive_ctxs": [{"text": "This issue calls for the need of {Large-scale Machine Learning} (LML), which aims to learn patterns from big data with comparable performance efficiently."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep Learning is the evolution of Machine Learning and it will definitely help in making machines better than what Machine Learning does. But one thing to note is that Deep Learning models require a very large amount of data to train the model otherwise it won't work as expected."}, {"text": "Naive Bayes is a Supervised Machine Learning algorithm based on the Bayes Theorem that is used to solve classification problems by following a probabilistic approach. It is based on the idea that the predictor variables in a Machine Learning model are independent of each other."}, {"text": "Usually, Deep Learning takes more time to train as compared to Machine Learning. The main reason is that there are so many parameters in a Deep Learning algorithm. Whereas Machine Learning takes much less time to train, ranging from a few seconds to a few hours."}, {"text": "Tensorflow is the more popular of the two. Tensorflow is typically used more in Deep Learning and Neural Networks. SciKit learn is more general Machine Learning."}, {"text": "Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability."}, {"text": "Neural Networks are essentially a part of Deep Learning, which in turn is a subset of Machine Learning. So, Neural Networks are nothing but a highly advanced application of Machine Learning that is now finding applications in many fields of interest."}, {"text": "Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models."}]}, {"question": "What is Concept drift in machine learning", "positive_ctxs": [{"text": "In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways.  The term concept refers to the quantity to be predicted."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The ability to detect and adapt to changes in the distribution of examples is paramount for data stream mining algorithms. The shift in the underlying distribution of examples arriving from a data stream is referred to as concept drift. Concept drift occurs over time and the rate at which the drifts occurs varies."}, {"text": "Data Drift Defined Data drift is unexpected and undocumented changes to data structure, semantics, and infrastructure that is a result of modern data architectures. Data drift breaks processes and corrupts data, but can also reveal new opportunities for data use."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "A Formal Definition for Concept Learning: Inferring a boolean-valued function from training examples of its input and output. \u2022 An example for concept-learning is the learning of bird-concept from the given examples of birds (positive examples) and non-birds (negative examples). \u2022"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}]}, {"question": "What are the types of classification in statistics", "positive_ctxs": [{"text": "There are four types of classification. They are Geographical classification, Chronological classification, Qualitative classification, Quantitative classification."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population."}, {"text": "Frequency distribution in statistics provides the information of the number of occurrences (frequency) of distinct values distributed within a given period of time or interval, in a list, table, or graphical representation. Grouped and Ungrouped are two types of Frequency Distribution."}, {"text": "Overall, Sentiment analysis may involve the following types of classification algorithms: Linear Regression. Naive Bayes. Support Vector Machines."}, {"text": "The most significant difference between regression vs classification is that while regression helps predict a continuous quantity, classification predicts discrete class labels. There are also some overlaps between the two types of machine learning algorithms."}, {"text": "Confusion matrix not only gives you insight into the errors being made by your classifier but also types of errors that are being made. This breakdown helps you to overcomes the limitation of using classification accuracy alone. Every column of the confusion matrix represents the instances of that predicted class."}, {"text": "Definition. Multivariate statistics refers to methods that examine the simultaneous effect of multiple variables. Traditional classification of multivariate statistical methods suggested by Kendall is based on the concept of dependency between variables (Kendall 1957)."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What are the forces of motivation", "positive_ctxs": [{"text": "Motivation involves the biological, emotional, social, and cognitive forces that activate behavior. In everyday usage, the term \"motivation\" is frequently used to describe why a person does something."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The distribution becomes normal when you have several different forces of varying magnitude acting together. Generally, the more forces then the more normal the distribution will become. This occurs a lot in nature which is why the normal distribution is so prevalent."}, {"text": "Vectors have many real-life applications, including situations involving force or velocity. For example, consider the forces acting on a boat crossing a river. The boat's motor generates a force in one direction, and the current of the river generates a force in another direction. Both forces are vectors."}, {"text": "In stratified sampling, a random sample is drawn from each of the strata, whereas in cluster sampling only the selected clusters are sampled. A common motivation of cluster sampling is to reduce costs by increasing sampling efficiency."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Omitted variable bias occurs when a regression model leaves out relevant independent variables, which are known as confounding variables. This condition forces the model to attribute the effects of omitted variables to variables that are in the model, which biases the coefficient estimates."}, {"text": "The mean of the negative binomial distribution with parameters r and p is rq / p, where q = 1 \u2013 p. The variance is rq / p2. The simplest motivation for the negative binomial is the case of successive random trials, each having a constant probability P of success."}, {"text": "A significant advantage of a decision tree is that it forces the consideration of all possible outcomes of a decision and traces each path to a conclusion. It creates a comprehensive analysis of the consequences along each branch and identifies decision nodes that need further analysis."}]}, {"question": "What are adversarial examples", "positive_ctxs": [{"text": "Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake; they're like optical illusions for machines."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "One-shot learning is a classification task where one, or a few, examples are used to classify many new examples in the future.  One-shot learning are classification tasks where many predictions are required given one (or a few) examples of each class, and face recognition is an example of one-shot learning."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Typical examples are the linear operator of multiplication by and differentiation in ."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "What is difference between linear and logistic regression", "positive_ctxs": [{"text": "Linear regression is used for predicting the continuous dependent variable using a given set of independent features whereas Logistic Regression is used to predict the categorical. Linear regression is used to solve regression problems whereas logistic regression is used to solve classification problems.26\u200f/04\u200f/2020"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Logistic regression is quite different than linear regression in that it does not make several of the key assumptions that linear and general linear models (as well as other ordinary least squares algorithm based models) hold so close: (1) logistic regression does not require a linear relationship between the dependent"}, {"text": "3 Answers. Since your response is ordinal then you should use ordinal regression. At a very high level, the main difference ordinal regression and linear regression is that with linear regression the dependent variable is continuous and ordinal the dependent variable is ordinal."}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}, {"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}]}, {"question": "What does false positive rate mean", "positive_ctxs": [{"text": "False positive rate (FPR) is a measure of accuracy for a test: be it a medical diagnostic test, a machine learning model, or something else. In technical terms, the false positive rate is defined as the probability of falsely rejecting the null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The false discovery rate is the ratio of the number of false positive results to the number of total positive test results. Out of 10,000 people given the test, there are 450 true positive results (box at top right) and 190 false positive results (box at bottom right) for a total of 640 positive results."}, {"text": "The test does not produce false negatives (that is, anyone who has the disease will test positive for it), but the false positive rate is 5% (that is, about 5% of people who take the test will test positive, even though they do not have the disease). Suppose a randomly selected person takes the test and tests positive."}, {"text": "Recall and True Positive Rate (TPR) are exactly the same. So the difference is in the precision and the false positive rate.  While precision measures the probability of a sample classified as positive to actually be positive, the false positive rate measures the ratio of false positives within the negative samples."}, {"text": "False positive rate (FPR) is a measure of accuracy for a test: be it a medical diagnostic test, a machine learning model, or something else. In technical terms, the false positive rate is defined as the probability of falsely rejecting the null hypothesis."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}]}, {"question": "Where can I find large data sets", "positive_ctxs": [{"text": "11 websites to find free, interesting datasetsFiveThirtyEight.  BuzzFeed News.  Kaggle.  Socrata.  Awesome-Public-Datasets on Github.  Google Public Datasets.  UCI Machine Learning Repository.  Data.gov.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Association rules mining is another key unsupervised data mining method, after clustering, that finds interesting associations (relationships, dependencies) in large sets of data items."}, {"text": "Association rules are \"if-then\" statements, that help to show the probability of relationships between data items, within large data sets in various types of databases."}, {"text": "Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. A characteristic of these large data sets is a large number of variables that require a lot of computing resources to process."}, {"text": "Ridge regression has two main benefits. First, adding a penalty term reduces overfitting. Second, the penalty term guarantees that we can find a solution. I think the second part is easier to explain."}, {"text": "A residual plot is typically used to find problems with regression. Some data sets are not good candidates for regression, including: Heteroscedastic data (points at widely varying distances from the line). Data that is non-linearly associated."}, {"text": "How big data analytics worksdata mining, which sift through data sets in search of patterns and relationships;predictive analytics, which build models to forecast customer behavior and other future developments;machine learning, which taps algorithms to analyze large data sets; and.More items"}, {"text": "Every time you conduct a t-test there is a chance that you will make a Type I error.  An ANOVA controls for these errors so that the Type I error remains at 5% and you can be more confident that any statistically significant result you find is not just running lots of tests."}]}, {"question": "How do you construct a less than cumulative frequency distribution", "positive_ctxs": [{"text": "1:314:30Suggested clip \u00b7 120 secondsCumulative Frequency Distribution (Less than and More than YouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The distribution function , also called the cumulative distribution function (CDF) or cumulative frequency function, describes the probability that a variate takes on a value less than or equal to a number . The distribution function is sometimes also denoted. (Evans et al. 2000, p."}, {"text": "Step 1: Prepare a table containing less than type cumulative frequency with the help of given frequencies. belongs. Class-interval of this cumulative frequency is the median class-interval. Step 3 : Find out the frequency f and lower limit l of this median class."}, {"text": "The cumulative distribution function (CDF) calculates the cumulative probability for a given x-value. Use the CDF to determine the probability that a random observation that is taken from the population will be less than or equal to a certain value."}, {"text": "A curve that represents the cumulative frequency distribution of grouped data on a graph is called a Cumulative Frequency Curve or an Ogive. Representing cumulative frequency data on a graph is the most efficient way to understand the data and derive results."}, {"text": "A curve that represents the cumulative frequency distribution of grouped data on a graph is called a Cumulative Frequency Curve or an Ogive."}, {"text": "Given a probability density function, we define the cumulative distribution function (CDF) as follows. The cumulative distribution function (CDF) of a random variable X is denoted by F(x), and is defined as F(x) = Pr(X \u2264 x). where xn is the largest possible value of X that is less than or equal to x."}, {"text": "To convert a frequency distribution to a probability distribution, divide area of the bar or interval of x by the total area of all the Bars. A simpler formula is: , N is the total Frequency and w is the interval of x. Example (From a frequency distribution table construct a probability plot)."}]}, {"question": "What is the difference between mutual information and correlation", "positive_ctxs": [{"text": "Mutual information is a distance between two probability distributions. Correlation is a linear distance between two random variables.  If you are working with variables that are smooth, correlation may tell you more about them; for instance if their relationship is monotonic."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Pointwise mutual information (PMI), or point mutual information, is a measure of association used in information theory and statistics. In contrast to mutual information (MI) which builds upon PMI, it refers to single events, whereas MI refers to the average of all possible events."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In computational linguistics, second-order co-occurrence pointwise mutual information is a semantic similarity measure. To assess the degree of association between two given words, it uses pointwise mutual information (PMI) to sort lists of important neighbor words of the two target words from a large corpus."}, {"text": "Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.  The mutual information between two random variables X and Y can be stated formally as follows: I(X ; Y) = H(X) \u2013 H(X | Y)"}, {"text": "Thus, the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the"}, {"text": "The difference between these two statistical measurements is that correlation measures the degree of a relationship between two variables (x and y), whereas regression is how one variable affects another."}, {"text": "Information gain can also be used for feature selection, by evaluating the gain of each variable in the context of the target variable. In this slightly different usage, the calculation is referred to as mutual information between the two random variables."}]}, {"question": "How do you respond to your best", "positive_ctxs": [{"text": "A few common responses to compliments are \"you're welcome\", \"no problem\", \"my pleasure\" or \"glad I could help\". The best of all is \" My Pleasure\"."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How to optimize your meta tags: A checklistCheck whether all your pages and your content have title tags and meta descriptions.Start paying more attention to your headings and how you structure your content.Don't forget to mark up your images with alt text.More items\u2022"}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use."}, {"text": "How to Get Started with AIPick a topic you are interested in.Find a quick solution.Improve your simple solution.Share your solution.Repeat steps 1-4 for different problems.Complete a Kaggle competition.Use machine learning professionally."}, {"text": "How to train a Machine Learning model in 5 minutesModel Naming \u2014 Give Your Model a Name: Let's start with giving your model a name, describe your model and attach tags to your model.  Data Type Selection \u2014 Choose data type(Images/Text/CSV): It's time to tell us about the type of data you want to train your model.More items"}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "It will be easier to learn and use. If you are in the industry where you need to deploy models in production, Tensorflow is your best choice. You can use Keras/Pytorch for prototyping if you want. But you don't need to switch as Tensorflow is here to stay."}]}, {"question": "How do you do multiple logistic regression", "positive_ctxs": [{"text": "0:012:32Suggested clip \u00b7 101 secondsMultiple Logistic Regression - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Multinomial logistic regression is used when you have a categorical dependent variable with two or more unordered levels (i.e. two or more discrete outcomes). It is practically identical to logistic regression, except that you have multiple possible outcomes instead of just one."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}, {"text": "Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)\u2208{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits."}, {"text": "When observed outcome of dependent variable can have multiple possible types then logistic regression will be multinomial."}]}, {"question": "What is N gram in NLP", "positive_ctxs": [{"text": "\u00b74 min read N-gram is probably the easiest concept to understand in the whole machine learning space, I guess. An N-gram means a sequence of N words. So for example, \u201cMedium blog\u201d is a 2-gram (a bigram), \u201cA Medium blog post\u201d is a 4-gram, and \u201cWrite on Medium\u201d is a 3-gram (trigram)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Top N accuracy \u2014 Top N accuracy is when you measure how often your predicted class falls in the top N values of your softmax distribution."}, {"text": "A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "For the coin flip example, N = 2 and \u03c0 = 0.5. The formula for the binomial distribution is shown below: where P(x) is the probability of x successes out of N trials, N is the number of trials, and \u03c0 is the probability of success on a given trial.Number of HeadsProbability21/42 more rows"}, {"text": "We use the following formula to compute variance.Var(X) = \u03a3 ( Xi - X )2 / N = \u03a3 xi2 / N.N is the number of scores in a set of scores. X is the mean of the N scores.  Cov(X, Y) = \u03a3 ( Xi - X ) ( Yi - Y ) / N = \u03a3 xiyi / N.N is the number of scores in each set of data. X is the mean of the N scores in the first data set."}, {"text": "Gram matrix is simply the matrix of the inner product of each vector and its corresponding vectors in same. It found use in the current machine learning is due to deep learning loss where while style transferring the loss function is computed using the gram matrix."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "How do you find the relative frequency in a normal distribution", "positive_ctxs": [{"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The only difference between a relative frequency distribution graph and a frequency distribution graph is that the vertical axis uses proportional or relative frequency rather than simple frequency. Cumulative relative frequency (also called an ogive) is the accumulation of the previous relative frequencies."}, {"text": "A marginal distribution is a frequency or relative frequency distribution of either the row or column variable in a contingency table.  A conditional distribution lists the relative frequency of each category of the response variable, given a specific value of the explanatory variable in a contingency table."}, {"text": "A marginal distribution is a frequency or relative frequency distribution of either the row or column variable in a contingency table.  A conditional distribution lists the relative frequency of each category of the response variable, given a specific value of the explanatory variable in a contingency table."}, {"text": "To find the relative frequency, divide the frequency by the total number of data values. To find the cumulative relative frequency, add all of the previous relative frequencies to the relative frequency for the current row."}, {"text": "As the area of a bar represents the frequency of its interval, the height of the bar represents the density. If you label the scare it is either frequency per unit or, if you divide by the total frequency, relative frequency per unit."}, {"text": "An easy way to define the difference between frequency and relative frequency is that frequency relies on the actual values of each class in a statistical data set while relative frequency compares these individual values to the overall totals of all classes concerned in a data set."}, {"text": "A frequency table is a chart that shows the popularity or mode of a certain type of data. When we look at frequency, we are looking at the number of times an event occurs within a given scenario.  You can find the relative frequency by simply dividing the frequency number by the total number of values in the data set."}]}, {"question": "What is exogenous variation", "positive_ctxs": [{"text": "The variables used to explain variations in the level of education are called exogenous. More generally, the variables that show differences we wish to explain are called endogenous, while the variables used to explain the differences are called exogenous. Often this goes along with a causal imagery."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In an economic model, an exogenous variable is one whose value is determined outside the model and is imposed on the model, and an exogenous change is a change in an exogenous variable. In contrast, an endogenous variable is a variable whose value is determined by the model."}, {"text": "The concept of exclusion restrictions denotes that some of the exogenous variables are not in some of the equations. Often this idea is expressed by saying the coefficient next to that exogenous variable is zero."}, {"text": "An exogenous variable is a variable that is not affected by other variables in the system. For example, take a simple causal system like farming. Variables like weather, farmer skill, pests, and availability of seed are all exogenous to crop production."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Inter-observer variation is the amount of variation between the results obtained by two or more observers examining the same material. Intra-observer variation is the amount of variation one observer experiences when observing the same material more than once."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "What is the applicability of association rules", "positive_ctxs": [{"text": "In data science, association rules are used to find correlations and co-occurrences between data sets. They are ideally used to explain patterns in data from seemingly independent information repositories, such as relational databases and transactional databases."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In data mining, association rules are useful for analyzing and predicting customer behavior. They play an important part in customer analytics, market basket analysis, product clustering, catalog design and store layout. Programmers use association rules to build programs capable of machine learning."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Data Mining is a process of discovering hidden patterns and rules from the existing data. It uses relatively simple rules such as association, correlation rules for the decision-making process, etc. Deep Learning is used for complex problem processing such as voice recognition etc."}, {"text": "Finding and Making the RulesFrequent Itemset Generation:- find all itemsets whose support is greater than or equal to the minimum support threshold.Rule generation: generate strong association rules from the frequent itemset whose confidence greater than or equal to minimum confidence threshold."}, {"text": "The beta value is used in measuring how effectively the predictor variable influences the criterion variable, it is measured in terms of standard deviation. R, is the measure of association between the observed value and the predicted value of the criterion variable."}, {"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}]}, {"question": "What is a random walk in statistics", "positive_ctxs": [{"text": "Statistics Definitions > A random walk is a sequence of discrete, fixed-length steps in random directions. Random walks may be 1-dimensional, 2-dimensional, or n-dimensional for any n. A random walk can also be confined to a lattice."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Statistics Definitions > A random walk is a sequence of discrete, fixed-length steps in random directions. Random walks may be 1-dimensional, 2-dimensional, or n-dimensional for any n. A random walk can also be confined to a lattice."}, {"text": "Statistics Definitions > A random walk is a sequence of discrete, fixed-length steps in random directions. Random walks may be 1-dimensional, 2-dimensional, or n-dimensional for any n. A random walk can also be confined to a lattice."}, {"text": "In mathematics, a random walk is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers."}, {"text": "The random walk is simple if Xk = \u00b11, with P(Xk = 1) = p and P(Xk = \u22121) = 1\u2212p = q. Imagine a particle performing a random walk on the integer points of the real line, where it in each step moves to one of its neighboring points; see Figure 1. Remark 1. You can also study random walks in higher dimensions."}, {"text": "One of the simplest and yet most important models in time series forecasting is the random walk model. This model assumes that in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (\u201ci.i.d.\u201d)."}, {"text": "By Paul King on Ap in Probability. A random walk refers to any process in which there is no observable pattern or trend; that is, where the movements of an object, or the values taken by a certain variable, are completely random."}, {"text": "Statistical inference consists in the use of statistics to draw conclusions about some unknown aspect of a population based on a random sample from that population.  Point estimation is discussed in the statistics section of the encyclopedia."}]}, {"question": "How was Graham's law discovered", "positive_ctxs": [{"text": "This result is known as Graham's law of diffusion after Thomas Graham (1805 to 1869), a Scottish chemist, who discovered it by observing effusion of gases through a thin plug of plaster of paris.  Calculate the relative rates of effusion of He(g) and O2(g) ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Graham's law states that the rate of diffusion or of effusion of a gas is inversely proportional to the square root of its molecular weight.  In the same conditions of temperature and pressure, the molar mass is proportional to the mass density."}, {"text": "The t distributions were discovered by William S. Gosset was a statistician employed by the Guinness brewing company which had stipulated that he not publish under his own name.  He therefore wrote under the pen name ``Student."}, {"text": "Functions of Random Variables One law is called the \u201cweak\u201d law of large numbers, and the other is called the \u201cstrong\u201d law of large numbers. The weak law describes how a sequence of probabilities converges, and the strong law describes how a sequence of random variables behaves in the limit."}, {"text": "The t distributions were discovered by William S.  Gosset was a statistician employed by the Guinness brewing company which had stipulated that he not publish under his own name. He therefore wrote under the pen name ``Student. '' These distributions arise in the following situation."}, {"text": "The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss.  It is often called the bell curve, because the graph of its probability density looks like a bell. Many values follow a normal distribution."}, {"text": "The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss.  It is often called the bell curve, because the graph of its probability density looks like a bell. Many values follow a normal distribution."}, {"text": "The logistic function was discovered anew in 1920 by Pearl and Reed in a study of the population growth of the United States. They were unaware of Verhulst's work (though not of the curves for autocatalytic reactions dis0 cussed presently), and they arrived independently at the logistic curve of (10)."}]}, {"question": "What is model calibration", "positive_ctxs": [{"text": "Model calibration is the process of adjustment of the model parameters and forcing within the margins of the uncertainties (in model parameters and / or model forcing) to obtain a model representation of the processes of interest that satisfies pre-agreed criteria (Goodness-of-Fit or Cost Function)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A vital aspect of the model construction process is the calibration phase.  In fact, a model's predictive uncertainty will only be reduced by calibration if the information content of the calibration data set is able to constrain those parameters that have a significant bearing on that prediction."}, {"text": "Model calibration is the process of adjustment of the model parameters and forcing within the margins of the uncertainties (in model parameters and / or model forcing) to obtain a model representation of the processes of interest that satisfies pre-agreed criteria (Goodness-of-Fit or Cost Function)."}, {"text": "Model calibration is the process of adjustment of the model parameters and forcing within the margins of the uncertainties (in model parameters and / or model forcing) to obtain a model representation of the processes of interest that satisfies pre-agreed criteria (Goodness-of-Fit or Cost Function)."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In this blog we will learn what is calibration and why and when we should use it. We calibrate our model when the probability estimate of a data point belonging to a class is very important. Calibration is comparison of the actual output and the expected output given by a system."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "How can I improve my gan training", "positive_ctxs": [{"text": "Balance between discriminator & generator We can improve GAN by turning our attention in balancing the loss between the generator and the discriminator. Unfortunately, the solution seems elusive. We can maintain a static ratio between the number of gradient descent iterations on the discriminator and the generator."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "This post is about various evaluation metrics and how and when to use them.Accuracy, Precision, and Recall: A.  F1 Score: This is my favorite evaluation metric and I tend to use this a lot in my classification projects.  Log Loss/Binary Crossentropy.  Categorical Crossentropy.  AUC."}, {"text": "According to my POV model accuracy is more important and its all depends on the training data.  Model performance can be improved using distributed computing and parallelizing over the scored assets, whereas accuracy has to be carefully built during the model training process."}, {"text": "ABSTRACT. We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization."}, {"text": "We present a freely available open-source toolkit for training recurrent neural network based language models. It can be easily used to improve existing speech recognition and machine translation systems."}, {"text": "Gradient boosting is a greedy algorithm and can overfit a training dataset quickly. It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting."}, {"text": "FastText uses a simple and efficient baseline for sentence classification( represent sentences as bag of words (BoW) and train a linear classifier). It uses negative sampling , hierarchical softmax and N-gram features to reduce computational cost and improve efficiency. Have to say, all of the terms made my head spin."}, {"text": "Precision and recall at k: Definition Precision at k is the proportion of recommended items in the top-k set that are relevant. Its interpretation is as follows. Suppose that my precision at 10 in a top-10 recommendation problem is 80%. This means that 80% of the recommendation I make are relevant to the user."}]}, {"question": "What is an intuitive explanation for the log loss function", "positive_ctxs": [{"text": "To me an intuitive explanation is that minimizing the log loss equals minimizing the Kullback-Leibler divergence (Kullback\u2013Leibler divergence - Wikipedia ) between the function you want to optimize (for example a neural network) and the true function that generates the data (from which you have samples in the form of a"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "For a binary classification like our example, the typical loss function is the binary cross-entropy / log loss."}, {"text": "Loss function for Logistic Regression The loss function for linear regression is squared loss. The loss function for logistic regression is Log Loss, which is defined as follows: Log Loss = \u2211 ( x , y ) \u2208 D \u2212 y log \u2061 ( y \u2032 ) \u2212 ( 1 \u2212 y ) log \u2061 where: ( x , y ) \u2208 D."}, {"text": "In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs). For an intended output t = \u00b11 and a classifier score y, the hinge loss of the prediction y is defined as."}, {"text": "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.  As the predicted probability decreases, however, the log loss increases rapidly."}, {"text": "The terms cost and loss functions almost refer to the same meaning. But, loss function mainly applies for a single training set as compared to the cost function which deals with a penalty for a number of training sets or the complete batch.  The cost function is calculated as an average of loss functions."}, {"text": "Softmax is an activation function that outputs the probability for each class and these probabilities will sum up to one. Cross Entropy loss is just the sum of the negative logarithm of the probabilities.  Therefore, Softmax loss is just these two appended together."}, {"text": "In mathematical optimization and decision theory, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event.  In optimal control, the loss is the penalty for failing to achieve a desired value."}]}, {"question": "What is the output range of ReLU activation function", "positive_ctxs": [{"text": "The range of ReLu is [0, inf). This means it can blow up the activation.  Imagine a network with random initialized weights ( or normalised ) and almost 50% of the network yields 0 activation because of the characteristic of ReLu ( output 0 for negative values of x )."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "An activation function is defined by and defines the output of a neuron in terms of its input (aka induced local field) . There are three types of activation functions. Threshhold function an example of which is. This function is also termed the Heaviside function. Piecewise Linear."}, {"text": "ReLU has become the darling activation function of the neural network world. Short for Rectified Linear Unit, it is a piecewise linear function that is defined to be 0 for all negative values of x and equal to a \u00d7 x otherwise, where a is a learnable parameter."}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "An activation function is a node that you add to the output layer or between two layers of any neural network. It is also known as the transfer function. It is used to determine the output of neural network layer in between 0 to 1 or -1 to 1 etc."}, {"text": "An activation function is a node that you add to the output layer or between two layers of any neural network. It is also known as the transfer function. It is used to determine the output of neural network layer in between 0 to 1 or -1 to 1 etc."}]}, {"question": "What is the use of ReLU in CNN", "positive_ctxs": [{"text": "A Gentle Introduction to the Rectified Linear Unit (ReLU) In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "The role of a fully connected layer in a CNN architecture The objective of a fully connected layer is to take the results of the convolution/pooling process and use them to classify the image into a label (in a simple classification example)."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig."}, {"text": "ReLu refers to the Rectifier Unit, the most commonly deployed activation function for the outputs of the CNN neurons. Mathematically, it's described as: Unfortunately, the ReLu function is not differentiable at the origin, which makes it hard to use with backpropagation training."}, {"text": "Dense CNN is a type of Deep CNN in which each layer is connected with another layer deeper than itself."}]}, {"question": "What is reactive management", "positive_ctxs": [{"text": "Reactive management is the polar opposite, and usually a follow-up, of proactive management. When a proactive leader gets swarmed enough with problems long enough, they turn reactive. Reactive management is an approach to management when the company leadership cannot or does not plan ahead for potential problems."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In project management terms, an s-curve is a mathematical graph that depicts relevant cumulative data for a project\u2014such as cost or man-hours\u2014plotted against time.  An s-curve in project management is typically used to track the progress of a project."}, {"text": "In project management terms, an s-curve is a mathematical graph that depicts relevant cumulative data for a project\u2014such as cost or man-hours\u2014plotted against time.  An s-curve in project management is typically used to track the progress of a project."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "High Pass RL Filter An inductor, like a capacitor, is a reactive device.  And this is why this circuit is a high-pass filter circuit. Low frequency signals, however, will go through the inductor, because inductors offer very low resistance to low-frequency, or Dc, signals."}]}, {"question": "What is the relationship between hypothesis testing and confidence interval", "positive_ctxs": [{"text": "Confidence intervals and hypothesis tests are similar in that they are both inferential methods that rely on an approximated sampling distribution. Confidence intervals use data from a sample to estimate a population parameter. Hypothesis tests use data from a sample to test a specified hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the \"Other Resources\" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"}, {"text": "So, if your significance level is 0.05, the corresponding confidence level is 95%. If the P value is less than your significance (alpha) level, the hypothesis test is statistically significant. If the confidence interval does not contain the null hypothesis value, the results are statistically significant."}, {"text": "This is why it is important to distinguish between the statistical significance of a result and the practical significance of that result.  Null hypothesis testing is a formal approach to deciding whether a statistical relationship in a sample reflects a real relationship in the population or is just due to chance."}, {"text": "The correct interpretation of a 95% confidence interval is that \"we are 95% confident that the population parameter is between X and X.\""}, {"text": "Null hypothesis are never accepted. We either reject them or fail to reject them. The distinction between \u201cacceptance\u201d and \u201cfailure to reject\u201d is best understood in terms of confidence intervals. Failing to reject a hypothesis means a confidence interval contains a value of \u201cno difference\u201d."}, {"text": "A 95% confidence interval for \u03b2i has two equivalent definitions: The interval is the set of values for which a hypothesis test to the level of 5% cannot be rejected. The interval has a probability of 95% to contain the true value of \u03b2i ."}, {"text": "Statistical significance is a determination that a relationship between two or more variables is caused by something other than chance.  Statistical hypothesis testing is used to determine whether the result of a data set is statistically significant."}]}, {"question": "What is a multinomial variable", "positive_ctxs": [{"text": "Multinomial logistic regression is used when you have a categorical dependent variable with two or more unordered levels (i.e. two or more discrete outcomes).  One level of the dependent variable is chosen as the reference category. This is typically the most common or the most frequent category."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Dirichlet distribution is a conjugate prior for the multinomial distribution. This means that if the prior distribution of the multinomial parameters is Dirichlet then the posterior distribution is also a Dirichlet distribution (with parameters different from those of the prior)."}, {"text": "The Dirichlet distribution is a conjugate prior for the multinomial distribution. This means that if the prior distribution of the multinomial parameters is Dirichlet then the posterior distribution is also a Dirichlet distribution (with parameters different from those of the prior)."}, {"text": "A multinomial experiment is almost identical with one main difference: a binomial experiment can have two outcomes, while a multinomial experiment can have multiple outcomes.  A binomial experiment will have a binomial distribution."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "While the trials are independent, their outcomes X are dependent because they must be summed to n. ; in this form, a categorical distribution is equivalent to a multinomial distribution over a single trial."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "Which function is used to load a model in spaCy", "positive_ctxs": [{"text": "load_model functionv2. 0. Load a model from a shortcut link, package or data path. If called with a shortcut link or package name, spaCy will assume the model is a Python package and import and call its load() method."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "According to this link LDA is a generative classifier. Also, the motto of LDA is to model a discriminant function to classify."}, {"text": "Linear regression is a classical model for predicting a numerical quantity.  Coefficients of a linear regression model can be estimated using a negative log-likelihood function from maximum likelihood estimation. The negative log-likelihood function can be used to derive the least squares solution to linear regression."}, {"text": "3.4. 1 The Logit Link Function. The logit link function is used to model the probability of 'success' as a function of covariates (e.g., logistic regression)."}, {"text": "To predict a continuous value, you need to adjust your model (regardless whether it is Recurrent or Not) to the following conditions:Use a linear activation function for the final layer.Chose an appropriate cost function (square error loss is typically used to measure the error of predicting real values)"}, {"text": "To predict a continuous value, you need to adjust your model (regardless whether it is Recurrent or Not) to the following conditions:Use a linear activation function for the final layer.Chose an appropriate cost function (square error loss is typically used to measure the error of predicting real values)"}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}]}, {"question": "What are the assumptions of discriminant analysis", "positive_ctxs": [{"text": "Assumptions. The assumptions of discriminant analysis are the same as those for MANOVA. The analysis is quite sensitive to outliers and the size of the smallest group must be larger than the number of predictor variables. Multivariate normality: Independent variables are normal for each level of the grouping variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Assumptions. The assumptions of discriminant analysis are the same as those for MANOVA. The analysis is quite sensitive to outliers and the size of the smallest group must be larger than the number of predictor variables. Multivariate normality: Independent variables are normal for each level of the grouping variable."}, {"text": "Linear discriminant function analysis (i.e., discriminant analysis) performs a multivariate test of differences between groups.  In addition, discriminant analysis is used to determine the minimum number of dimensions needed to describe these differences."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and other fields, to find a linear combination of features that characterizes or separates two or more classes"}, {"text": "Cluster analysis tries to maximize in-group homogeneity and maximize between group heterogeneity. Multiple discriminant analysis is different. It starts with a discrete DV and tries to determine how much the levels of the IV's distinguish the members of the groups."}, {"text": "Canonical discriminant analysis is a dimension-reduction technique related to principal component analysis and canonical correlation.  This maximal multiple correlation is called the first canonical correlation. The coefficients of the linear combination are the canonical coefficients or canonical weights."}, {"text": "Canonical discriminant analysis is a dimension-reduction technique related to principal component analysis and canonical correlation.  This maximal multiple correlation is called the first canonical correlation. The coefficients of the linear combination are the canonical coefficients or canonical weights."}]}, {"question": "What is the Multilayer Perceptron neural network algorithm", "positive_ctxs": [{"text": "A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN).  MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multilayer Perceptron (MLP) MLP is a deep learning method. A multilayer perceptron is a neural network connecting multiple layers in a directed graph, which means that the signal path through the nodes only goes one way. Each node, apart from the input nodes, has a nonlinear activation function."}, {"text": "A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features or business intelligence in the input data. Perceptron was introduced by Frank Rosenblatt in 1957.  A Perceptron is an algorithm for supervised learning of binary classifiers."}, {"text": "For example, a network with two variables in the input layer, one hidden layer with eight nodes, and an output layer with one node would be described using the notation: 2/8/1. I recommend using this notation when describing the layers and their size for a Multilayer Perceptron neural network."}, {"text": "Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks. Perceptron is a linear classifier (binary). Also, it is used in supervised learning. It helps to classify the given input data."}, {"text": "1. Why is the XOR problem exceptionally interesting to neural network researchers?  Explanation: Linearly separable problems of interest of neural network researchers because they are the only class of problem that Perceptron can solve successfully."}, {"text": "A perceptron is a simple model of a biological neuron in an artificial neural network. Perceptron is also the name of an early algorithm for supervised learning of binary classifiers."}, {"text": "The general algorithm is The Backpropagation algorithm is suitable for the feed forward neural network on fixed sized input-output pairs. The Backpropagation Through Time is the application of Backpropagation training algorithm which is applied to the sequence data like the time series."}]}, {"question": "What is the goal of a generative adversarial network GAN )", "positive_ctxs": [{"text": "The generator is a convolutional neural network and the discriminator is a deconvolutional neural network. The goal of the generator is to artificially manufacture outputs that could easily be mistaken for real data. The goal of the discriminator is to identify which outputs it receives have been artificially created."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The deep convolutional generative adversarial network, or DCGAN for short, is an extension of the GAN architecture for using deep convolutional neural networks for both the generator and discriminator models and configurations for the models and training that result in the stable training of a generator model."}, {"text": "A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss)."}, {"text": "A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss)."}, {"text": "A GAN is a generative model that is trained using two neural network models. One model is called the \u201cgenerator\u201d or \u201cgenerative network\u201d model that learns to generate new plausible samples.  After training, the generative model can then be used to create new plausible samples on demand."}, {"text": "Minibatch Discrimination is a discriminative technique for generative adversarial networks where we discriminate between whole minibatches of samples rather than between individual samples. This is intended to avoid collapse of the generator."}, {"text": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks."}, {"text": "The discriminator in a GAN is simply a classifier. It tries to distinguish real data from the data created by the generator. It could use any network architecture appropriate to the type of data it's classifying. Figure 1: Backpropagation in discriminator training."}]}, {"question": "How do you find percent in stratified random sampling", "positive_ctxs": [{"text": "To implement stratified sampling, first find the total number of members in the population, and then the number of members of each stratum. For each stratum, divide the number of members by the total number in the entire population to get the percentage of the population represented by that stratum."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "The difference between quota sampling and stratified sampling is: although both \"group\" participants by an important characteristic, stratified sampling relies on random selection within each group, while quota sampling relies on convenience sampling within each group."}, {"text": "Cluster sampling is best used when the clusters occur naturally in a population, when you don't have access to the entire population, and when the clusters are geographically convenient. However, cluster sampling is not as precise as simple random sampling or stratified random sampling."}, {"text": "Quota sampling is different from stratified sampling, because in a stratified sample individuals within each stratum are selected at random. Quota sampling achieves a representative age distribution, but it isn't a random sample, because the sampling frame is unknown."}, {"text": "Compared to simple random sampling, stratified sampling has two main disadvantages.Advantages and DisadvantagesA stratified sample can provide greater precision than a simple random sample of the same size.Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money.More items"}, {"text": "The main types of probability sampling methods are simple random sampling, stratified sampling, cluster sampling, multistage sampling, and systematic random sampling."}, {"text": "A sample may be selected from a population through a number of ways, one of which is the stratified random sampling method. A stratified random sampling involves dividing the entire population into homogeneous groups called strata (plural for stratum). Random samples are then selected from each stratum."}]}, {"question": "What is the difference between Sarsa and Q learning", "positive_ctxs": [{"text": "The Sarsa algorithm is an On-Policy algorithm for TD-Learning. The major difference between it and Q-Learning, is that the maximum reward for the next state is not necessarily used for updating the Q-values."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Sarsa algorithm is an On-Policy algorithm for TD-Learning. The major difference between it and Q-Learning, is that the maximum reward for the next state is not necessarily used for updating the Q-values."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Q-Learning is a value-based reinforcement learning algorithm which is used to find the optimal action-selection policy using a Q function. Our goal is to maximize the value function Q. The Q table helps us to find the best action for each state.  Initially we explore the environment and update the Q-Table."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables."}, {"text": "The Q statistic is used to try to partition the variability we see between studies into variability that is due to random variation, and variability that is due to potential differences between the studies. This is really similar to the way we partition variance when doing an ANOVA."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}]}, {"question": "Why is matrix decomposition useful", "positive_ctxs": [{"text": "Matrix decomposition refers to the transformation of a given matrix into a product of matrices. They are used to implement efficient matrix algorithms. Decomposing a matrix breaks it up into two matrix factors. This can be helpful when solving equations of the form Ax=b for x when multiple b vectors are to be used."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler."}, {"text": "It is often useful to view an image as a random process.  An eigenvalue/eigenvector decomposition of the covariance matrix reveals the principal directions of variation between images in the collection. This has applications in image coding, image classification, object recognition, and more."}, {"text": "Rank-reduced singular value decomposition T is a computed m by r matrix of term vectors where r is the rank of A\u2014a measure of its unique dimensions \u2264 min(m,n). S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors."}, {"text": "SVD is the decomposition of a matrix A into 3 matrices \u2013 U, S, and V. S is the diagonal matrix of singular values. Think of singular values as the importance values of different features in the matrix. The rank of a matrix is a measure of the unique information stored in a matrix."}, {"text": "The matrix norm is similar to the magnitude of a vector. It is useful whenever a system/problem can be formulated into a matrix that has some physical meaning."}, {"text": "The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler. For the case of simplicity we will focus on the SVD for real-valued matrices and ignore the case for complex numbers."}, {"text": "The singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values.  The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning."}]}, {"question": "What is the difference between z score and T score", "positive_ctxs": [{"text": "Z score is used when: the data follows a normal distribution, when you know the standard deviation of the population and your sample size is above 30. T-Score - is used when you have a smaller sample <30 and you have an unknown population standard deviation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30)."}, {"text": "Definition. A score that is derived from an individual's raw score within a distribution of scores. The standard score describes the difference of the raw score from a sample mean, expressed in standard deviations. Standard scores preserve the absolute differences between scores."}, {"text": "To reject the null, the tail used for the rejection region should cover the extreme values of the alternative hypothesis - the area in red. The z or t score is negative and less than the score set for the rejection condition."}, {"text": "The formula for calculating a z-score is is z = (x-\u03bc)/\u03c3, where x is the raw score, \u03bc is the population mean, and \u03c3 is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation."}]}, {"question": "What does activation function do in neural network", "positive_ctxs": [{"text": "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron."}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "Logistic Regression is a special case of a Neural Network with no hidden layers, that uses the sigmoid activation function and uses the softmax with cross entropy loss.  neural network and logistic regressions are different techniques or algorithms to do the same thing, classification of data."}]}, {"question": "What are the most common probability distributions", "positive_ctxs": [{"text": "Perhaps the most common probability distribution is the normal distribution, or \"bell curve,\" although several distributions exist that are commonly used. Typically, the data generating process of some phenomenon will dictate its probability distribution. This process is called the probability density function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Examples of Discrete Distribution The most common discrete probability distributions include binomial, Poisson, Bernoulli, and multinomial."}, {"text": "One common method of consolidating two probability distributions is to simply average them - for every set of values A, set If the distributions both have densities, for example, averaging the probabilities results in a probability distribution with density the average of the two input densities (Figure 1)."}, {"text": "The Beta distribution is a continuous probability distribution having two parameters. One of its most common uses is to model one's uncertainty about the probability of success of an experiment."}, {"text": "For distributions that are strongly skewed or have outliers, the median is often the most appropriate measure of central tendency because in skewed distributions the mean is pulled out toward the tail. The median is more resistant to outliers compared to the mean."}, {"text": "In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution.  The most common measures of central tendency are the arithmetic mean, the median, and the mode."}, {"text": "Confidence intervals measure the degree of uncertainty or certainty in a sampling method. They can take any number of probability limits, with the most common being a 95% or 99% confidence level. Confidence intervals are conducted using statistical methods, such as a t-test."}, {"text": "A common pattern is the bell-shaped curve known as the \"normal distribution.\" In a normal or \"typical\" distribution, points are as likely to occur on one side of the average as on the other. Note that other distributions look similar to the normal distribution."}]}, {"question": "What is meant by membership function in fuzzy logic", "positive_ctxs": [{"text": "In mathematics, the membership function of a fuzzy set is a generalization of the indicator function for classical sets. In fuzzy logic, it represents the degree of truth as an extension of valuation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In mathematics, the membership function of a fuzzy set is a generalization of the indicator function for classical sets. In fuzzy logic, it represents the degree of truth as an extension of valuation."}, {"text": "Two main types of fuzzy inference systems can be implemented: Mamdani-type (1977) and Sugeno-type (1985). These two types of inference systems vary somewhat in the way outputs are determined. Mamdani-type inference expects the output membership functions to be fuzzy sets."}, {"text": "Abstract: The k-Nearest Neighbors (kNN) classifier is one of the most effective methods in supervised learning problems. It classifies unseen cases comparing their similarity with the training data.  Fuzzy-kNN computes a fuzzy degree of membership of each instance to the classes of the problem."}, {"text": "Inductive logic programming is the subfield of machine learning that uses first-order logic to represent hypotheses and data. Because first-order logic is expressive and declarative, inductive logic programming specifically targets problems involving structured data and background knowledge."}, {"text": "Neural networks and fuzzy logic systems are parameterised computational nonlinear algorithms for numerical processing of data (signals, images, stimuli). \u2022 These algorithms can be either implemented of a general-purpose computer or built into a dedicated hardware."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Computational Logic is the process of designing and analyzing logic in computer applications. In this lesson, we'll discuss creating logic based on the statements and constraints provided. Logic in relation to computers is mainly of two types: Propositional Logic and First Order Logic(FOL)."}]}, {"question": "What is the area under the normal curve between z", "positive_ctxs": [{"text": "The area percentage (proportion, probability) calculated using a z-score will be a decimal value between 0 and 1, and will appear in a Z-Score Table. The total area under any normal curve is 1 (or 100%)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. The area under the normal distribution curve represents probability and the total area under the curve sums to one."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Normal distributions are symmetric around their mean. The mean, median, and mode of a normal distribution are equal. The area under the normal curve is equal to 1.0.  Approximately 95% of the area of a normal distribution is within two standard deviations of the mean."}, {"text": "Probability and the Normal Curve The normal distribution is a continuous probability distribution.  The total area under the normal curve is equal to 1. The probability that a normal random variable X equals any particular value is 0."}, {"text": "The area under the normal curve is equal to 1.0. Normal distributions are denser in the center and less dense in the tails. Normal distributions are defined by two parameters, the mean (\u03bc) and the standard deviation (\u03c3). 68% of the area of a normal distribution is within one standard deviation of the mean."}, {"text": "To find the area between two positive z scores takes a couple of steps. First use the standard normal distribution table to look up the areas that go with the two z scores. Next subtract the smaller area from the larger area. For example, to find the area between z1 = ."}, {"text": "Properties of a normal distributionThe mean, mode and median are all equal.The curve is symmetric at the center (i.e. around the mean, \u03bc).Exactly half of the values are to the left of center and exactly half the values are to the right.The total area under the curve is 1."}]}, {"question": "What are 3 ways of reducing dimensionality", "positive_ctxs": [{"text": "Here is a brief review of our original seven techniques for dimensionality reduction:Missing Values Ratio.  Low Variance Filter.  High Correlation Filter.  Random Forests/Ensemble Trees.  Principal Component Analysis (PCA).  Backward Feature Elimination.  Forward Feature Construction."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Feature extraction involves reducing the number of resources required to describe a large set of data.GeneralIndependent component analysis.Isomap.Kernel PCA.Latent semantic analysis.Partial least squares.Principal component analysis.Multifactor dimensionality reduction.Nonlinear dimensionality reduction.More items"}, {"text": "There are 3 main ways of describing the intensity of an activity \u2013 vigorous, moderate, and gentle. Vigorous activities tend to make you \u201chuff and puff\u201d."}, {"text": "Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance."}, {"text": "Dimensionality reduction refers to techniques for reducing the number of input variables in training data. When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the \u201cessence\u201d of the data."}, {"text": "The number of units is a parameter in the LSTM, referring to the dimensionality of the hidden state and dimensionality of the output state (they must be equal). a LSTM comprises an entire layer."}, {"text": "The number of units is a parameter in the LSTM, referring to the dimensionality of the hidden state and dimensionality of the output state (they must be equal). a LSTM comprises an entire layer."}, {"text": "Max pooling is a sample-based discretization process. The objective is to down-sample an input representation (image, hidden-layer output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned."}]}, {"question": "Does dependent variable need to be normally distributed", "positive_ctxs": [{"text": "You do need distributional assumptions about the response variable in order to make inferences (e.g, confidence intervals), but it is not necessary that the response variable be normallhy distributed."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are NO assumptions in any linear model about the distribution of the independent variables. Yes, you only get meaningful parameter estimates from nominal (unordered categories) or numerical (continuous or discrete) independent variables.  They do not need to be normally distributed or continuous."}, {"text": "There are NO assumptions in any linear model about the distribution of the independent variables. Yes, you only get meaningful parameter estimates from nominal (unordered categories) or numerical (continuous or discrete) independent variables.  They do not need to be normally distributed or continuous."}, {"text": "There are NO assumptions in any linear model about the distribution of the independent variables. Yes, you only get meaningful parameter estimates from nominal (unordered categories) or numerical (continuous or discrete) independent variables.  They do not need to be normally distributed or continuous."}, {"text": "A normality test is used to determine whether sample data has been drawn from a normally distributed population (within some tolerance). A number of statistical tests, such as the Student's t-test and the one-way and two-way ANOVA require a normally distributed sample population."}, {"text": "Because when you are constructing a linear regression model you are assuming that your dependent variable \"Y\" is normally distributed. But when you have a binary dependent variable, this assumption is heavily violated. Thus, it doesn't makes sense to use linear regression when your dependent variable is binary."}, {"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}, {"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}]}, {"question": "How do you implement CNN image classification", "positive_ctxs": [{"text": "The basic steps to build an image classification model using a neural network are:Flatten the input image dimensions to 1D (width pixels x height pixels)Normalize the image pixel values (divide by 255)One-Hot Encode the categorical column.Build a model architecture (Sequential) with Dense layers.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "The role of a fully connected layer in a CNN architecture The objective of a fully connected layer is to take the results of the convolution/pooling process and use them to classify the image into a label (in a simple classification example)."}, {"text": "CNNs are used for image classification and recognition because of its high accuracy.  The CNN follows a hierarchical model which works on building a network, like a funnel, and finally gives out a fully-connected layer where all the neurons are connected to each other and the output is processed."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "CNNs can be used in tons of applications from image and video recognition, image classification, and recommender systems to natural language processing and medical image analysis.  This is the way that a CNN works! Image by NatWhitePhotography on Pixabay. CNNs have an input layer, and output layer, and hidden layers."}, {"text": "4.1 Input Layer Input layer in CNN should contain image data. Image data is represented by three dimensional matrix as we saw earlier. You need to reshape it into a single column.  If you have \u201cm\u201d training examples then dimension of input will be (784, m)."}, {"text": "Multi-class Classification using Decision Tree, Random Forest and Extra Trees Algorithm in Python: An End-To-End Data Science Recipe \u2014 016. a) Different types of Machine Learning problems.  i) How to implement Decision Tree, Random Forest and Extra Tree Algorithms for Multiclass Classification in Python."}]}, {"question": "How do I start preparing for data structures and algorithms", "positive_ctxs": [{"text": "Pre-Interview PreparationDevelop a deep knowledge of data structures. You should understand and be able to talk about different data structures and their strengths, weaknesses, and how they compare to each other.  Understand Big O notation.  Know the major sorting algorithms."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Probabilistic data structures are a group of data structures that are extremely useful for big data and streaming applications. Generally speaking, these data structures use hash functions to randomize and compactly represent a set of items."}, {"text": "Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models."}, {"text": "Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Getting and preparing the data Each line of the text file contains a list of labels, followed by the corresponding document. All the labels start by the __label__ prefix, which is how fastText recognize what is a label or what is a word. The model is then trained to predict the labels given the word in the document."}]}, {"question": "What is the difference between null and alternative hypothesis", "positive_ctxs": [{"text": "The null hypothesis is a general statement that states that there is no relationship between two phenomenons under consideration or that there is no association between two groups. An alternative hypothesis is a statement that describes that there is a relationship between two selected variables in a study."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In research, there is a convention that the hypothesis is written in two forms, the null hypothesis, and the alternative hypothesis (called the experimental hypothesis when the method of investigation is an experiment)."}, {"text": "The null hypothesis is the one to be tested and the alternative is everything else. In our example, The null hypothesis would be: The mean data scientist salary is 113,000 dollars. While the alternative: The mean data scientist salary is not 113,000 dollars."}, {"text": "The null hypothesis is the one to be tested and the alternative is everything else. In our example, The null hypothesis would be: The mean data scientist salary is 113,000 dollars. While the alternative: The mean data scientist salary is not 113,000 dollars."}, {"text": "The null hypothesis (H0) for a one tailed test is that the mean is greater (or less) than or equal to \u00b5, and the alternative hypothesis is that the mean is < (or >, respectively) \u00b5."}, {"text": "Rejecting or failing to reject the null hypothesis If our statistical analysis shows that the significance level is below the cut-off value we have set (e.g., either 0.05 or 0.01), we reject the null hypothesis and accept the alternative hypothesis."}, {"text": "A null hypothesis is a type of conjecture used in statistics that proposes that there is no difference between certain characteristics of a population or data-generating process. The alternative hypothesis proposes that there is a difference."}, {"text": "A null hypothesis is a type of conjecture used in statistics that proposes that there is no difference between certain characteristics of a population or data-generating process. The alternative hypothesis proposes that there is a difference."}]}, {"question": "How do you find the partial derivative of fxy", "positive_ctxs": [{"text": "Verify that the partial derivative Fxy is correct by calculating its equivalent, Fyx, taking the derivatives in the opposite order (d/dy first, then d/dx). In the above example, the derivative d/dy of the function f(x,y) = 3x^2*y - 2xy is 3x^2 - 2x."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "Essentially, backpropagation evaluates the expression for the derivative of the cost function as a product of derivatives between each layer from left to right \u2013 \"backwards\" \u2013 with the gradient of the weights between each layer being a simple modification of the partial products (the \"backwards propagated error\")."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "The derivative of sigmoid(x) is defined as sigmoid(x)*(1-sigmoid(x)).  Short answer : The derivative of the sigmoid function at any is implemented as because calculating the derivative this way is computationally effective."}, {"text": "How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression."}]}, {"question": "What does the regression equation tell us", "positive_ctxs": [{"text": "A regression equation is used in stats to find out what relationship, if any, exists between sets of data. For example, if you measure a child's height every year you might find that they grow about 3 inches a year. That trend (growing three inches a year) can be modeled with a regression equation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "The sampling distribution of the sample mean is very useful because it can tell us the probability of getting any specific mean from a random sample."}, {"text": "We use factorials when we look at permutations and combinations. Permutations tell us how many different ways we can arrange things if their order matters. Combinations tells us how many ways we can choose k item from n items if their order does not matter."}, {"text": "Characteristics of a Relationship. Correlations have three important characterstics. They can tell us about the direction of the relationship, the form (shape) of the relationship, and the degree (strength) of the relationship between two variables."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}]}, {"question": "Where is a bootstrap distribution centered", "positive_ctxs": [{"text": "3.1 . Each bootstrap distribution is centered at the statistic from the corresponding sample rather than at the population mean \u03bc."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "A normal distribution has a bell-shaped density curve described by its mean and standard deviation . The density curve is symmetrical, centered about its mean, with its spread determined by its standard deviation."}, {"text": "The idea behind bootstrap is to use the data of a sample study at hand as a \u201csurrogate population\u201d, for the purpose of approximating the sampling distribution of a statistic; i.e. to resample (with replacement) from the sample data at hand and create a large number of \u201cphantom samples\u201d known as bootstrap samples."}, {"text": "The number of bootstrap samples can be indicated with B (e.g. if you resample 10 times then B = 10). A star next to a statistic, like s* or x\u0304* indicates the statistic was calculated by resampling. A bootstrap statistic is sometimes denoted with a T, where T*b would be the Bth bootstrap sample statistic T."}, {"text": "The bootstrap is a tool, which allows us to obtain better finite sample approximation of estimators. The bootstrap is used all over the place to estimate the variance, correct bias and construct CIs etc. There are many, many different types of bootstraps."}, {"text": "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It can be used to estimate summary statistics such as the mean or standard deviation.  That when using the bootstrap you must choose the size of the sample and the number of repeats."}, {"text": "In the \u201cCompute Variable\u201d dialog box that opens, enter a name for the new centered variable in the \u201cTarget Variable:\u201d text box at the top right. In the \u201cNumeric Expression:\u201d box, write \u201cmath-52.65\u201d as shown in Figure 2. Press OK to create the centered variable."}]}, {"question": "What are eigenvectors of a matrix", "positive_ctxs": [{"text": "Eigenvectors are a special set of vectors associated with a linear system of equations (i.e., a matrix equation) that are sometimes also known as characteristic vectors, proper vectors, or latent vectors (Marcus and Minc 1988, p.  Each eigenvector is paired with a corresponding so-called eigenvalue."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The function scipy. linalg. eig computes eigenvalues and eigenvectors of a square matrix ."}, {"text": "The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal. Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA.  If the matrix A is a real matrix, then U and V are also real."}, {"text": "0:001:38Suggested clip \u00b7 98 secondsFind the matrix A given the eigenvalues and eigenvectors - YouTubeYouTubeStart of suggested clipEnd of suggested clip"}, {"text": "Principle Component Analysis (PCA) is a common feature extraction method in data science. Technically, PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues and then uses those to project the data into a new subspace of equal or less dimensions."}, {"text": "Principle Component Analysis (PCA) is a common feature extraction method in data science. Technically, PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues and then uses those to project the data into a new subspace of equal or less dimensions."}, {"text": "The eigenvalues and eigenvectors of a matrix are often used in the analysis of financial data and are integral in extracting useful information from the raw data. They can be used for predicting stock prices and analyzing correlations between various stocks, corresponding to different companies."}, {"text": "The eigenvalues and eigenvectors of a matrix are often used in the analysis of financial data and are integral in extracting useful information from the raw data. They can be used for predicting stock prices and analyzing correlations between various stocks, corresponding to different companies."}]}, {"question": "What is the importance of probability distribution", "positive_ctxs": [{"text": "This type of distribution is useful when you need to know which outcomes are most likely, the spread of potential values, and the likelihood of different results. In this blog post, you'll learn about probability distributions for both discrete and continuous variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "You can get the feature importance of each feature of your dataset by using the feature importance property of the model. Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable."}, {"text": "In statistics, importance sampling is a general technique for estimating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest. It is related to umbrella sampling in computational physics."}, {"text": "The probability mass function of the negative binomial distribution is. where r is the number of successes, k is the number of failures, and p is the probability of success."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side.  The normal distribution is often called the bell curve because the graph of its probability density looks like a bell."}, {"text": "Normal Distribution is a probability distribution where probability of x is highest at centre and lowest in the ends whereas in Uniform Distribution probability of x is constant. Uniform Distribution is a probability distribution where probability of x is constant."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. The area under the normal distribution curve represents probability and the total area under the curve sums to one."}]}, {"question": "How do you predict using machine learning", "positive_ctxs": [{"text": "With the LassoCV, RidgeCV, and Linear Regression machine learning algorithms.Define the problem.Gather the data.Clean & Explore the data.Model the data.Evaluate the model.Answer the problem."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data."}, {"text": "How to Get Started with AIPick a topic you are interested in.Find a quick solution.Improve your simple solution.Share your solution.Repeat steps 1-4 for different problems.Complete a Kaggle competition.Use machine learning professionally."}, {"text": "You do not need to learn linear algebra before you get started in machine learning, but at some time you may wish to dive deeper.  It will give you the tools to help you with the other areas of mathematics required to understand and build better intuitions for machine learning algorithms."}, {"text": "Definition. Predictive analytics is an area of statistics that deals with extracting information from data and using it to predict trends and behavior patterns.  Predictive analytics statistical techniques include data modeling, machine learning, AI, deep learning algorithms and data mining."}, {"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}]}, {"question": "When training error is very low and testing error is high that is called", "positive_ctxs": [{"text": "The training and testing error is the score that your train and test sets score using your error metrics. If your train error is low and test error high, you are likely overfitting to your train data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Summary. Probably approximately correct (PAC) learning is a theoretical framework for analyzing the generalization error of a learning algorithm in terms of its error on a training set and some measure of complexity. The goal is typically to show that an algorithm achieves low generalization error with high probability"}, {"text": "The clear sign of a machine learning overfitting is if its error on testing set is much greater than the error on training set.  For instance if the model accuracy for train data is 85% and the accuracy for test/validation data is 65% then its very obvious that the model has overlearned and you should check that."}, {"text": "Training loss is the error on the training set of data. Validation loss is the error after running the validation set of data through the trained network. Train/valid is the ratio between the two. Unexpectedly, as the epochs increase both validation and training error drop."}, {"text": "approximation of the expected error called the empirical error which is the average error on the. training set. Given a function f, a loss function V , and a training set S consisting of n data points, the empirical error of f is: IS[f] ="}, {"text": "When a population is finite, the formula that determines the standard error of the mean \u03c3\u00afx. needs to be adjusted. If N is the size of the population and n is the size of the sample ( where n\u22650.05N) , then the standard error of the mean is \u03c3\u00afx=\u03c3\u221an\u221aN\u2212nN\u22121."}, {"text": "If our model is too simple and has very few parameters then it may have high bias and low variance.  This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can't be more complex and less complex at the same time."}, {"text": "The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs. In other words, model with high bias pays very little attention to the training data and oversimplifies the model."}]}, {"question": "What is classification tree in data mining", "positive_ctxs": [{"text": "A Classification tree labels, records, and assigns variables to discrete classes.  A Classification tree is built through a process known as binary recursive partitioning. This is an iterative process of splitting the data into partitions, and then splitting it up further on each of the branches."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves)."}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}, {"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree learning is a supervised machine learning technique for inducing a decision tree from training data. A decision tree (also referred to as a classification tree or a reduction tree) is a predictive model which is a mapping from observations about an item to conclusions about its target value."}]}, {"question": "How K means algorithm works", "positive_ctxs": [{"text": "The k-means clustering algorithm attempts to split a given anonymous data set (a set containing no information as to class identity) into a fixed number (k) of clusters.  The resulting classifier is used to classify (using k = 1) the data and thereby produce an initial randomized set of clusters."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.  The results of the K-means clustering algorithm are: The centroids of the K clusters, which can be used to label new data. Labels for the training data (each data point is assigned to a single cluster)"}, {"text": "K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups).  The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided."}, {"text": "Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows\u2022"}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "In general, K-means is a heuristic algorithm that partitions a data set into K clusters by minimizing the sum of squared distance in each cluster.  In this paper, the simulation of basic k-means algorithm is done, which is implemented using Euclidian distance metric."}, {"text": "How to Handle Imbalanced DatasetChange the evaluation matrix. If we apply the wrong evaluation matrix on the imbalanced dataset, it can give us misleading results.  Resample the dataset. Resample means to change the distribution of the imbalance classes in the dataset.  Change the algorithm and approach to the problem."}, {"text": "Chebyshev's inequality says that at least 1\u22121K2 of data from a sample must fall within K standard deviations from the mean, where K is any positive real number greater than one."}]}, {"question": "How many different outcomes are possible for 6 rolls of a die", "positive_ctxs": [{"text": "The Fundamental Counting Principle If one event has p possible outcomes, and another event has m possible outcomes, then there are a total of p \u2022 m possible outcomes for the two events. Rolling two six-sided dice: Each die has 6 equally likely outcomes, so the sample space is 6 \u2022 6 or 36 equally likely outcomes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There is only one way to roll two 6's on a pair of dice: the first die must be a 6 and the second die must be a 6. The probability is 1/6 \u00d7 1/6 = 1/36. There are 3 ways in which to get at least one 6 in the roll of two dice. The first is to roll 6 on both dice, which we already determined has a probability of 1/36."}, {"text": "For example, a random variable could be the outcome of the roll of a die or the flip of a coin. A probability distribution is a list of all of the possible outcomes of a random variable along with their corresponding probability values."}, {"text": "Example 1: Fair Dice Roll The number of desired outcomes is 3 (rolling a 2, 4, or 6), and there are 6 outcomes in total. The a priori probability for this example is calculated as follows: A priori probability = 3 / 6 = 50%. Therefore, the a priori probability of rolling a 2, 4, or 6 is 50%."}, {"text": "Stochastic vs. For example, a stochastic variable is a random variable. A stochastic process is a random process. Typically, random is used to refer to a lack of dependence between observations in a sequence. For example, the rolls of a fair die are random, so are the flips of a fair coin."}, {"text": "First, make a list of the possible outcomes for each flip. Next, count the number of the possible outcomes for each flip. There are two outcomes for each flip of a coin: heads or tails. Then, multiply the number of outcomes by the number of flips."}, {"text": "If outcomes are equally likely, then the probability of an event occurring is the number in the event divided by the number in the sample space. The probability of rolling a six on a single roll of a die is 1/6 because there is only 1 way to roll a six out of 6 ways it could be rolled."}, {"text": "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment."}]}, {"question": "What is discrete distributions and continuous distributions", "positive_ctxs": [{"text": "A discrete distribution is one in which the data can only take on certain values, for example integers. A continuous distribution is one in which data can take on any value within a specified range (which may be infinite)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A discrete distribution is a statistical distribution that shows the probabilities of discrete (countable) outcomes, such as 1, 2, 3  Overall, the concepts of discrete and continuous probability distributions and the random variables they describe are the underpinnings of probability theory and statistical analysis."}, {"text": "The chi-square test is the most commonly used to test the goodness of fit tests and is used for discrete distributions like the binomial distribution and the Poisson distribution, whereas The Kolmogorov-Smirnov and Anderson-Darling goodness of fit tests are used for continuous distributions."}, {"text": "Hybrid Bayesian networks contain both discrete and continuous conditional probability distributions as numerical inputs. A commonly used type of hybrid Bayesian network is the conditional linear Gaussian (CLG) model [Lauritzen 1992, Cowell et al."}, {"text": "For discrete data key distributions are: Bernoulli, Binomial, Poisson and Multinomial."}, {"text": "Examples of Discrete Distribution The most common discrete probability distributions include binomial, Poisson, Bernoulli, and multinomial."}, {"text": "In a normal distribution the mean is zero and the standard deviation is 1. It has zero skew and a kurtosis of 3. Normal distributions are symmetrical, but not all symmetrical distributions are normal."}, {"text": "A random variable with a Gaussian distribution is said to be normally distributed, and is called a normal deviate. Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known."}]}, {"question": "Does SVM use gradient descent", "positive_ctxs": [{"text": "They are used for different purposes. Gradient descent, in its vanilla form, minimizes an unconstrained optimization problem. To handle constraints, you can use some modifications like projected gradient descent."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}, {"text": "Gradient descent is a simple optimization procedure that you can use with many machine learning algorithms.  Stochastic gradient descent refers to calculating the derivative from each training data instance and calculating the update immediately."}, {"text": "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning."}]}, {"question": "What is Agent function in artificial intelligence", "positive_ctxs": [{"text": "The agent function is a mathematical function that maps a sequence of perceptions into action. The function is implemented as the agent program. The part of the agent taking an action is called an actuator. environment -> sensors -> agent function -> actuators -> environment."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}, {"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}, {"text": "Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subtopic of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem."}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}, {"text": "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception."}]}, {"question": "What is fuzzy sets in artificial intelligence", "positive_ctxs": [{"text": "Definition A.I (fuzzy set) A fuzzy set A on universe (domain) X is defined by the membership function ILA{X) which is a mapping from the universe X into the unit interval:  F{X) denotes the set of all fuzzy sets on X. Fuzzy set theory allows for a partial membership of an element in a set."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subtopic of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem."}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}, {"text": "In mathematics, the membership function of a fuzzy set is a generalization of the indicator function for classical sets. In fuzzy logic, it represents the degree of truth as an extension of valuation."}, {"text": "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}]}, {"question": "Why is squared of l2 norm preferred in ML than just l2 norm", "positive_ctxs": [{"text": "The reason for preferring L2 norm is that it corresponds to Hilbert space ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "30.4. Introduction. A matrix norm is a number defined in terms of the entries of the matrix. The norm is a useful quantity which can give important information about a matrix."}, {"text": "Lasso Regression Another Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather than the L2 norm as a measure of complexity."}, {"text": "In order to choose the support vectors, we want to maximize the margin m and that implies we reduce the magnitude or norm of the vector that's perpendicular to the hyperplanes(s) and closest to a datapoint. which implies that the lower the norm of vector w, then greater is the margin."}, {"text": "n = norm( v ) returns the Euclidean norm of vector v . This norm is also called the 2-norm, vector magnitude, or Euclidean length. n = norm( v , p ) returns the generalized vector p-norm. n = norm( X ) returns the 2-norm or maximum singular value of matrix X , which is approximately max(svd(X)) ."}, {"text": "The matrix norm is similar to the magnitude of a vector. It is useful whenever a system/problem can be formulated into a matrix that has some physical meaning."}, {"text": "At a higher level, the chief difference between the L1 and the L2 terms is that the L2 term is proportional to the square of the \u03b2 values, while the L1 norm is proportional the absolute value of the values in \u03b2."}, {"text": "At a higher level, the chief difference between the L1 and the L2 terms is that the L2 term is proportional to the square of the \u03b2 values, while the L1 norm is proportional the absolute value of the values in \u03b2."}]}, {"question": "What is Gamma in statistics", "positive_ctxs": [{"text": "Gamma is defined as the difference between the number of concordant pairs and the number of discordant pairs divided by the total number of concordant and discordant pairs, and it ranges from 0 to 1."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Gamma is a measure of association for ordinal variables. Gamma ranges from -1.00 to 1.00. Again, a Gamma of 0.00 reflects no association; a Gamma of 1.00 reflects a positive perfect relationship between variables; a Gamma of -1.00 reflects a negative perfect relationship between those variables.28\u200f/06\u200f/2020"}, {"text": "In SWedge, the Gamma distribution can be useful for any variable which is always positive, such as cohesion or shear strength for example. The Gamma distribution has the following probability density function: where G(a) is the Gamma function, and the parameters a and b are both positive, i.e. a > 0 and b > 0."}, {"text": "The Gamma distribution is widely used in engineering, science, and business, to model continuous variables that are always positive and have skewed distributions. In SWedge, the Gamma distribution can be useful for any variable which is always positive, such as cohesion or shear strength for example."}, {"text": "The Gamma distribution is widely used in engineering, science, and business, to model continuous variables that are always positive and have skewed distributions. In SWedge, the Gamma distribution can be useful for any variable which is always positive, such as cohesion or shear strength for example."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The Gamma distribution can be thought of as a generalization of the Chi-square distribution. If a random variable has a Chi-square distribution with degrees of freedom and is a strictly positive constant, then the random variable defined as has a Gamma distribution with parameters and ."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is the equation for logistic function", "positive_ctxs": [{"text": "A more accurate model postulates that the relative growth rate P /P decreases when P approaches the carrying capacity K of the environment. The corre- sponding equation is the so called logistic differential equation: dP dt = kP ( 1 \u2212 P K ) ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Mean Squared Error, commonly used for linear regression models, isn't convex for logistic regression. This is because the logistic function isn't always convex. The logarithm of the likelihood function is however always convex."}, {"text": "The logistic function is the inverse of the natural logit function and so can be used to convert the logarithm of odds into a probability. In mathematical notation the logistic function is sometimes written as expit in the same form as logit."}, {"text": "The logistic function is the inverse of the natural logit function and so can be used to convert the logarithm of odds into a probability. In mathematical notation the logistic function is sometimes written as expit in the same form as logit."}, {"text": "The Fourier Transform is a mathematical technique that transforms a function of time, x(t), to a function of frequency, X(\u03c9).  Making these substitutions in the previous equation yields the analysis equation for the Fourier Transform (also called the Forward Fourier Transform)."}, {"text": "In linear regression, the function is a linear (straight-line) equation. In power or exponential regression, the function is a power (polynomial) equation of the form or an exponential function in the form ."}, {"text": "Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative distribution function of logistic distribution."}, {"text": "Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true ."}]}, {"question": "Why mean square error is used", "positive_ctxs": [{"text": "The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the \u201cerrors\u201d) and squaring them. The squaring is necessary to remove any negative signs. It also gives more weight to larger differences."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "From Wikipedia, the free encyclopedia. In statistics and signal processing, a minimum mean square error (MMSE) estimator is an estimation method which minimizes the mean square error (MSE), which is a common measure of estimator quality, of the fitted values of a dependent variable."}, {"text": "While the variance and the standard error of the mean are different estimates of variability, one can be derived from the other. Multiply the standard error of the mean by itself to square it. This step assumes that the standard error is a known quantity."}, {"text": "A kind of average sometimes used in statistics and engineering, often abbreviated as RMS. To find the root mean square of a set of numbers, square all the numbers in the set and then find the arithmetic mean of the squares. Take the square root of the result. This is the root mean square."}, {"text": "MSE is the average of the squared error that is used as the loss function for least squares regression: It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE."}, {"text": "The difference is pretty simple: in squared error, you are penalizing large deviations more.  The mean absolute error is a common measure of forecast error in time [2]series analysis, where the terms \"mean absolute deviation\" is sometimes used in confusion with the more standard definition of mean absolute deviation."}, {"text": "Definition 1. A statistic d is called an unbiased estimator for a function of the parameter g(\u03b8) provided that for every choice of \u03b8, E\u03b8d(X) = g(\u03b8). Any estimator that not unbiased is called biased.  Note that the mean square error for an unbiased estimator is its variance."}, {"text": "The standard deviation of this set of mean values is the standard error. In lieu of taking many samples one can estimate the standard error from a single sample. This estimate is derived by dividing the standard deviation by the square root of the sample size."}]}, {"question": "Why is sigmoid function used", "positive_ctxs": [{"text": "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The derivative of the sigmoid function is the sigmoid function times one minus itself."}, {"text": "Definition. A sigmoid function is a bounded, differentiable, real function that is defined for all real input values and has a non-negative derivative at each point and exactly one inflection point. A sigmoid \"function\" and a sigmoid \"curve\" refer to the same object."}, {"text": "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable."}, {"text": "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable."}, {"text": "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable."}, {"text": "sigmoid activation function"}, {"text": "Logistic regression is a powerful machine learning algorithm that utilizes a sigmoid function and works best on binary classification problems, although it can be used on multi-class classification problems through the \u201cone vs. all\u201d method."}]}, {"question": "What is K in the K nearest neighbors algorithm in Python", "positive_ctxs": [{"text": "k in kNN algorithm represents the number of nearest neighbor points which are voting for the new test data's class. If k=1, then test examples are given the same label as the closest example in the training set."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Here is step by step on how to compute K-nearest neighbors KNN algorithm:Determine parameter K = number of nearest neighbors.Calculate the distance between the query-instance and all the training samples.Sort the distance and determine nearest neighbors based on the K-th minimum distance.More items"}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.  The results of the K-means clustering algorithm are: The centroids of the K clusters, which can be used to label new data. Labels for the training data (each data point is assigned to a single cluster)"}, {"text": "Chebyshev's inequality says that at least 1\u22121K2 of data from a sample must fall within K standard deviations from the mean, where K is any positive real number greater than one."}, {"text": "There is a popular method known as elbow method which is used to determine the optimal value of K to perform the K-Means Clustering Algorithm. The basic idea behind this method is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer elements in the cluster."}, {"text": "In general, K-means is a heuristic algorithm that partitions a data set into K clusters by minimizing the sum of squared distance in each cluster.  In this paper, the simulation of basic k-means algorithm is done, which is implemented using Euclidian distance metric."}, {"text": "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers."}]}, {"question": "Why is logistic regression considered a linear model", "positive_ctxs": [{"text": "The short answer is: Logistic regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable."}, {"text": "Multinomial logistic regression (often just called 'multinomial regression') is used to predict a nominal dependent variable given one or more independent variables. It is sometimes considered an extension of binomial logistic regression to allow for a dependent variable with more than two categories."}, {"text": "Multinomial logistic regression (often just called 'multinomial regression') is used to predict a nominal dependent variable given one or more independent variables. It is sometimes considered an extension of binomial logistic regression to allow for a dependent variable with more than two categories."}, {"text": "Linear regression attempts to model the relationship between two variables by fitting a linear equation (= a straight line) to the observed data. One variable is considered to be an explanatory variable (e.g. your income), and the other is considered to be a dependent variable (e.g. your expenses)."}, {"text": "In statistics, Poisson regression is a generalized linear model form of regression analysis used to model count data and contingency tables.  A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables."}, {"text": "Log-Log linear regression A regression model where the outcome and at least one predictor are log transformed is called a log-log linear model."}]}, {"question": "What do box plots tell you", "positive_ctxs": [{"text": "A boxplot is a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d).  It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Benefits of Usability TestingUsability testing provides an unbiased, accurate, and direct examination of your product or website's user experience.  Usability testing is convenient.  Usability testing can tell you what your users do on your site or product and why they take these actions.More items\u2022"}, {"text": "Extreme Value AnalysisFocus on univariate methods.Visualize the data using scatterplots, histograms and box and whisker plots and look for extreme values.Assume a distribution (Gaussian) and look for values more than 2 or 3 standard deviations from the mean or 1.5 times from the first or third quartile.More items\u2022"}, {"text": "The very first is a Box Plot. A box plot is a graphical display for describing the distribution of data. Box plots use the median and the lower and upper quartiles. An outlier can easily be detected via Box plot where any point above or below the whiskers represent an outlier."}, {"text": "To test for non-time-series violations of independence, you can look at plots of the residuals versus independent variables or plots of residuals versus row number in situations where the rows have been sorted or grouped in some way that depends (only) on the values of the independent variables."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}]}, {"question": "When would you use systematic sampling", "positive_ctxs": [{"text": "Use systematic sampling when there's low risk of data manipulation. Systematic sampling is the preferred method over simple random sampling when a study maintains a low risk of data manipulation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "The sampling distribution assumes that the null hypothesis is true. When we compare an obtained test statistic to the sampling distribution, we're asking how likely it is that we would get that statistic if we were sampling from a population that has the null hypothesis characteristics (e.g., P = 0.50)."}, {"text": "In simple random sampling, each data point has an equal probability of being chosen. Meanwhile, systematic sampling chooses a data point per each predetermined interval. While systematic sampling is easier to execute than simple random sampling, it can produce skewed results if the data set exhibits patterns."}, {"text": "Stochastic Gradient Descent: you would randomly select one of those training samples at each iteration to update your coefficients. Online Gradient Descent: you would use the \"most recent\" sample at each iteration. There is no stochasticity as you deterministically select your sample."}, {"text": "The main types of probability sampling methods are simple random sampling, stratified sampling, cluster sampling, multistage sampling, and systematic random sampling."}, {"text": "In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).  In cluster sampling, the sampling unit is the whole cluster; Instead of sampling individuals from within each group, a researcher will study whole clusters."}, {"text": "Steps in selecting a systematic random sample:Calculate the sampling interval (the number of households in the population divided by the number of households needed for the sample)Select a random start between 1 and sampling interval.Repeatedly add sampling interval to select subsequent households."}]}, {"question": "Is logistic regression only for binary classification", "positive_ctxs": [{"text": "Logistic regression is a powerful machine learning algorithm that utilizes a sigmoid function and works best on binary classification problems, although it can be used on multi-class classification problems through the \u201cone vs. all\u201d method. Logistic regression (despite its name) is not fit for regression tasks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}, {"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression."}, {"text": "Logistic regression can be binomial, ordinal or multinomial. Binomial or binary logistic regression deals with situations in which the observed outcome for a dependent variable can have only two possible types, \"0\" and \"1\" (which may represent, for example, \"dead\" vs. \"alive\" or \"win\" vs. \"loss\")."}, {"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'."}, {"text": "Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'."}]}, {"question": "What is the difference between latent variable models and structural equation models", "positive_ctxs": [{"text": "The main difference between the two types of models is that path analysis assumes that all variables are measured without error. SEM uses latent variables to account for measurement error."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A latent variable is a variable that is inferred using models from observed data.  Approaches to inferring latent variables from data include: using a single observed variable, multi-item scales, predictive models, dimension reduction techniques such as factor analysis, structural equation models, and mixture models."}, {"text": "Structural equation modeling is a multivariate statistical analysis technique that is used to analyze structural relationships. This technique is the combination of factor analysis and multiple regression analysis, and it is used to analyze the structural relationship between measured variables and latent constructs."}, {"text": "Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables."}, {"text": "Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables."}, {"text": "There are two main differences between regression and structural equation modelling. The first is that SEM allows us to develop complex path models with direct and indirect effects. This allows us to more accurately model causal mechanisms we are interested in. The second key difference is to do with measurement."}, {"text": "\"The difference between discrete choice models and conjoint models is that discrete choice models present experimental replications of the market with the focus on making accurate predictions regarding the market, while conjoint models do not, using product profiles to estimate underlying utilities (or partworths)"}, {"text": "\"The difference between discrete choice models and conjoint models is that discrete choice models present experimental replications of the market with the focus on making accurate predictions regarding the market, while conjoint models do not, using product profiles to estimate underlying utilities (or partworths)"}]}, {"question": "What is the definition of entropy", "positive_ctxs": [{"text": "Entropy, the measure of a system's thermal energy per unit temperature that is unavailable for doing useful work. Because work is obtained from ordered molecular motion, the amount of entropy is also a measure of the molecular disorder, or randomness, of a system."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Explanation: Entropy (S) by the modern definition is the amount of energy dispersal in a system. Therefore, the system entropy will increase when the amount of motion within the system increases. For example, the entropy increases when ice (solid) melts to give water (liquid)."}, {"text": "joint entropy is the amount of information in two (or more) random variables; conditional entropy is the amount of information in one random variable given we already know the other."}, {"text": "A simple definition of a sampling frame is the set of source materials from which the sample is selected. The definition also encompasses the purpose of sampling frames, which is to provide a means for choosing the particular members of the target population that are to be interviewed in the survey."}, {"text": "Definition: Entropy is a measure of uncertainty of a random variable. The entropy of a discrete random variable X with alphabet X is H(X) = -) p(x) log p(2) DEX When the base of the logarithm is 2, entropy is measured in bits."}, {"text": "The intuition for entropy is that it is the average number of bits required to represent or transmit an event drawn from the probability distribution for the random variable. \u2026 the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution."}, {"text": "In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes.  The minimum surprise is when p = 0 or p = 1, when the event is known and the entropy is zero bits."}, {"text": "In information theory, the entropy of a random variable is the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes.  The minimum surprise is when p = 0 or p = 1, when the event is known and the entropy is zero bits."}]}, {"question": "What is marginal effects in probit model", "positive_ctxs": [{"text": "Marginal probability effects are the partial effects of each explanatory variable on. the probability that the observed dependent variable Yi = 1, where in probit. models."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The random effects assumption is that the individual-specific effects are uncorrelated with the independent variables. The fixed effect assumption is that the individual-specific effects are correlated with the independent variables."}, {"text": "Ordered probit, like ordered logit, is a particular method of ordinal regression.  The ordered probit model provides an appropriate fit to these data, preserving the ordering of response options while making no assumptions of the interval distances between options."}, {"text": "A marginal distribution is the percentages out of totals, and conditional distribution is the percentages out of some column."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In many applications including econometrics and biostatistics a fixed effects model refers to a regression model in which the group means are fixed (non-random) as opposed to a random effects model in which the group means are a random sample from a population."}, {"text": "In many applications including econometrics and biostatistics a fixed effects model refers to a regression model in which the group means are fixed (non-random) as opposed to a random effects model in which the group means are a random sample from a population."}]}, {"question": "Is reinforcement learning considered a branch of semi supervised learning", "positive_ctxs": [{"text": "Semi-supervised learning takes a middle ground. It uses a small amount of labeled data bolstering a larger set of unlabeled data. And reinforcement learning trains an algorithm with a reward system, providing feedback when an artificial intelligence agent performs the best action in a particular situation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "It's a form of machine learning and therefore a branch of artificial intelligence. Depending on the complexity of the problem, reinforcement learning algorithms can keep adapting to the environment over time if necessary in order to maximize the reward in the long-term."}, {"text": "Semi-structured data is information that doesn't reside in a relational database but that does have some organizational properties that make it easier to analyze.  Examples of semi-structured : CSV but XML and JSON documents are semi structured documents, NoSQL databases are considered as semi structured."}, {"text": "Some of the practical applications of reinforcement learning are:Manufacturing. In Fanuc, a robot uses deep reinforcement learning to pick a device from one box and putting it in a container.  Inventory Management.  Delivery Management.  Power Systems.  Finance Sector."}, {"text": "In the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available.  An algorithm that implements classification, especially in a concrete implementation, is known as a classifier."}, {"text": "In the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available.  An algorithm that implements classification, especially in a concrete implementation, is known as a classifier."}, {"text": "In the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available.  An algorithm that implements classification, especially in a concrete implementation, is known as a classifier."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}]}, {"question": "How do you create a structural equation model in SPSS", "positive_ctxs": [{"text": "2:1812:46Suggested clip 110 secondsStructural Equation Modeling with SPSS AMOS PART1: by G N YouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables."}, {"text": "Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "Structural equation modeling is a multivariate statistical analysis technique that is used to analyze structural relationships. This technique is the combination of factor analysis and multiple regression analysis, and it is used to analyze the structural relationship between measured variables and latent constructs."}, {"text": "There are two main differences between regression and structural equation modelling. The first is that SEM allows us to develop complex path models with direct and indirect effects. This allows us to more accurately model causal mechanisms we are interested in. The second key difference is to do with measurement."}, {"text": "A latent variable is a variable that is inferred using models from observed data.  Approaches to inferring latent variables from data include: using a single observed variable, multi-item scales, predictive models, dimension reduction techniques such as factor analysis, structural equation models, and mixture models."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}]}, {"question": "What is an eigenvector of a covariance matrix", "positive_ctxs": [{"text": "One of the most intuitive explanations of eigenvectors of a covariance matrix is that they are the directions in which the data varies the most.  The eigenvectors of the covariance matrix of these data samples are the vectors u and v; u, longer arrow, is the first eigenvector and v, the shorter arrow, is the second."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A covariance matrix is a square matrix which gives two types of information. If you are looking at the population covariance matrix then. each diagonal element is the variance of the corresponding random variable. each off-diagonal element is the covariance of the corresponding pair of random variables."}, {"text": "In probability theory and statistics, a covariance matrix (also known as auto-covariance matrix, dispersion matrix, variance matrix, or variance\u2013covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector."}, {"text": "The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of observed values of two of the variables and whose i, i element is the sample variance of the observed values of one of the variables."}, {"text": "The SVD represents an expansion of the original data in a coordinate system where the covariance matrix is diagonal. Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA.  If the matrix A is a real matrix, then U and V are also real."}, {"text": "Weighted least squares (WLS), also known as weighted linear regression, is a generalization of ordinary least squares and linear regression in which the errors covariance matrix is allowed to be different from an identity matrix."}, {"text": "Now, three variable case it is less clear for me. An intuitive definition for covariance function would be Cov(X,Y,Z)=E[(x\u2212E[X])(y\u2212E[Y])(z\u2212E[Z])], but instead the literature suggests using covariance matrix that is defined as two variable covariance for each pair of variables."}, {"text": "Eigenvectors can be used to represent a large dimensional matrix. This means that a matrix M and a vector o can be replaced by a scalar n and a vector o. In this instance, o is the eigenvector and n is the eigenvalue and our target is to find o and n."}]}, {"question": "What are the types of activation function", "positive_ctxs": [{"text": "Types of Activation FunctionsSigmoid Function. In an ANN, the sigmoid function is a non-linear AF used primarily in feedforward neural networks.  Hyperbolic Tangent Function (Tanh)  Softmax Function.  Softsign Function.  Rectified Linear Unit (ReLU) Function.  Exponential Linear Units (ELUs) Function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An activation function is defined by and defines the output of a neuron in terms of its input (aka induced local field) . There are three types of activation functions. Threshhold function an example of which is. This function is also termed the Heaviside function. Piecewise Linear."}, {"text": "It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.  The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better."}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function."}, {"text": "The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time."}, {"text": "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input."}, {"text": "In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input."}]}, {"question": "What is neural network architecture", "positive_ctxs": [{"text": "Neural Networks are complex structures made of artificial neurons that can take in multiple inputs to produce a single output. Usually, a Neural Network consists of an input and output layer with one or multiple hidden layers within."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Advertisements. Multi-Layer perceptron defines the most complicated architecture of artificial neural networks. It is substantially formed from multiple layers of perceptron."}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.  LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series."}, {"text": "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.  LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series."}, {"text": "LSTM stands for long short term memory. It is a model or architecture that extends the memory of recurrent neural networks. Typically, recurrent neural networks have 'short term memory' in that they use persistent previous information to be used in the current neural network."}, {"text": "Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods.  The rise of lambda architecture is correlated with the growth of big data, real-time analytics, and the drive to mitigate the latencies of map-reduce."}]}, {"question": "What is positive matrix factorization", "positive_ctxs": [{"text": "EPA's Positive Matrix Factorization (PMF) Model is a mathematical receptor model developed by EPA scientists that provides scientific support for the development and review of air and water quality standards, exposure research and environmental forensics."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Non negative matrix factorization only takes positive values as input while SVD can take both positive and negative values.  SVD and NMF are both matrix decomposition techniques but they are very different and are generally used for different purposes. SVD helps in giving Eigen vectors of the input matrix."}, {"text": "Matrix factorization using the alternating least squares algorithm for collaborative filtering. Alternating least squares (ALS) is an optimization technique to solve the matrix factorization problem. This technique achieves good performance and has proven relatively easy to implement."}, {"text": "Regular Markov Chains. \u25cb A transition matrix P is regular if some power of P has only positive entries. A Markov chain is a regular Markov chain if its transition matrix is regular. For example, if you take successive powers of the matrix D, the entries of D will always be positive (or so it appears)."}, {"text": "Low-rank matrix factorization (MF) is an important technique in data science. The key idea of MF is that there exists latent structures in the data, by uncovering which we could obtain a compressed representation of the data.  By properly adapting MF, we can go beyond the problem of clustering and matrix completion."}, {"text": "In mathematics, a nonnegative matrix, written. is a matrix in which all the elements are equal to or greater than zero, that is, A positive matrix is a matrix in which all the elements are strictly greater than zero."}]}, {"question": "What is the difference between a training set and a test set", "positive_ctxs": [{"text": "In a dataset a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In a dataset a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}, {"text": "In a dataset, a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}, {"text": "Hold-out is when you split up your dataset into a 'train' and 'test' set. The training set is what the model is trained on, and the test set is used to see how well that model performs on unseen data."}, {"text": "The essential difference between the set and the multiset is that in a set the keys must be unique, while a multiset permits duplicate keys.  In both sets and multisets, the sort order of components is the sort order of the keys, so the components in a multiset that have duplicate keys may appear in any order."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived."}, {"text": "Overfitting is a significant practical difficulty for decision tree models and many other predictive models. Overfitting happens when the learning algorithm continues to develop hypotheses that reduce training set error at the cost of an. increased test set error."}]}, {"question": "What are the uses of the range in statistics and what are the areas that we use range for calculations in statistics", "positive_ctxs": [{"text": "The range can also be used to estimate another measure of spread, the standard deviation. Rather than go through a fairly complicated formula to find the standard deviation, we can instead use what is called the range rule. The range is fundamental in this calculation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Descriptive statistics uses the data to provide descriptions of the population, either through numerical calculations or graphs or tables. Inferential statistics makes inferences and predictions about a population based on a sample of data taken from the population in question."}, {"text": "Descriptive statistics uses the data to provide descriptions of the population, either through numerical calculations or graphs or tables. Inferential statistics makes inferences and predictions about a population based on a sample of data taken from the population in question."}, {"text": "Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures.  Descriptive statistics are typically distinguished from inferential statistics. With descriptive statistics you are simply describing what is or what the data shows."}, {"text": "In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one fewer quantile than the number of groups created."}, {"text": "In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one fewer quantile than the number of groups created."}, {"text": "The median is a robust measure of central tendency.  The median absolute deviation and interquartile range are robust measures of statistical dispersion, while the standard deviation and range are not. Trimmed estimators and Winsorised estimators are general methods to make statistics more robust."}, {"text": "Inferential statistics are often used to compare the differences between the treatment groups. Inferential statistics use measurements from the sample of subjects in the experiment to compare the treatment groups and make generalizations about the larger population of subjects."}]}, {"question": "When would one use Random Forests over Gradient Boosted Machines GBMs", "positive_ctxs": [{"text": "So, if you are constrained either by the size of the data or the number of trials you want to try, you may have to go with random forests. There is one fundamental difference in performance between the two that may force you to choose Random Forests over Gradient Boosted Machines (GBMs)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Time Series Model. The time series model comprises a sequence of data points captured, using time as the input parameter.  Random Forest. Random Forest is perhaps the most popular classification algorithm, capable of both classification and regression.  Gradient Boosted Model (GBM)  K-Means.  Prophet."}, {"text": "Stochastic Gradient Descent: you would randomly select one of those training samples at each iteration to update your coefficients. Online Gradient Descent: you would use the \"most recent\" sample at each iteration. There is no stochasticity as you deterministically select your sample."}, {"text": "Random Forests / Ensemble Trees. One approach to dimensionality reduction is to generate a large and carefully constructed set of trees against a target attribute and then use each attribute's usage statistics to find the most informative subset of features."}, {"text": "While in Gradient Descent (GD) the whole Training Set is considered before taking one Model Parameters Update Step, in Stochastic Gradient Descent (SGD) only one Data Point is considered for each Model Parameters Update Step, cycling over the Training Set."}, {"text": "Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression.  The advantages of Stochastic Gradient Descent are: Efficiency."}, {"text": "Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression."}, {"text": "So far we've looked at GBMs that use two different direction vectors, the residual vector (Gradient boosting: Distance to target) and the sign vector (Gradient boosting: Heading in the right direction)."}]}, {"question": "How do you tell if a vector field is a gradient field", "positive_ctxs": [{"text": "The converse of Theorem 1 is the following: Given vector field F = Pi + Qj on D with C1 coefficients, if Py = Qx, then F is the gradient of some function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In vector calculus and physics, a vector field is an assignment of a vector to each point in a subset of space. For instance, a vector field in the plane can be visualised as a collection of arrows with a given magnitude and direction, each attached to a point in the plane."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "A vector space is a space of vectors, ie. each element is a vector. A vector field is, at its core, a function between some space and some vector space, so every point in our base space has a vector assigned to it. A good example would be wind direction maps you see on weather reports."}, {"text": "Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms."}, {"text": "Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms."}, {"text": "Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms."}, {"text": "The receptive field size of a unit can be increased in a number of ways. One option is to stack more layers to make the network deeper, which increases the receptive field size linearly by theory, as each extra layer increases the receptive field size by the kernel size."}]}, {"question": "What is difference between standard deviation and mean deviation", "positive_ctxs": [{"text": "The difference between the two norms is that the standard deviation is calculating the square of the difference whereas the mean absolute deviation is only looking at the absolute difference. Hence large outliers will create a higher dispersion when using the standard deviation instead of the other method."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The difference between the two norms is that the standard deviation is calculating the square of the difference whereas the mean absolute deviation is only looking at the absolute difference. Hence large outliers will create a higher dispersion when using the standard deviation instead of the other method."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "Mean, variance, and standard deviation The mean of the sampling distribution of the sample mean will always be the same as the mean of the original non-normal distribution. In other words, the sample mean is equal to the population mean. where \u03c3 is population standard deviation and n is sample size."}, {"text": "Describe the scores in such a sample. If the standard deviation is 0 then the variance is 0 and the mean of the squared deviation scores must be 0.  Thus, when the standard deviation equals 0, all the scores are identical and equal to the mean."}, {"text": "The coefficient of variation is a better risk measure than the standard deviation alone because the CV adjusts for the size of the project. The CV measures the standard deviation divided by the mean and therefore puts the standard deviation into context."}, {"text": "Definition of 'average deviation' 1. the difference between an observed value of a variable and its mean. 2. Also: mean deviation from the mean, mean deviation from the median, average deviation."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean."}]}, {"question": "What does denying the antecedent mean", "positive_ctxs": [{"text": "inverse error"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "An association rule has two parts: an antecedent (if) and a consequent (then). An antecedent is an item found within the data.  Support is an indication of how frequently the items appear in the data. Confidence indicates the number of times the if-then statements are found true."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "How do you balance classes in machine learning", "positive_ctxs": [{"text": "A simple way to fix imbalanced data-sets is simply to balance them, either by oversampling instances of the minority class or undersampling instances of the majority class. This simply allows us to create a balanced data-set that, in theory, should not lead to classifiers biased toward one class or the other."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to Get Started with AIPick a topic you are interested in.Find a quick solution.Improve your simple solution.Share your solution.Repeat steps 1-4 for different problems.Complete a Kaggle competition.Use machine learning professionally."}, {"text": "You do not need to learn linear algebra before you get started in machine learning, but at some time you may wish to dive deeper.  It will give you the tools to help you with the other areas of mathematics required to understand and build better intuitions for machine learning algorithms."}, {"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "Machine learning algorithms can minimize forecasting error and do the forecast much faster and with the usage of more data. What's more, machine learning algorithms can analyze many alternative models at the same time, when in traditional econometrics you can analyze just one model at a time."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}]}, {"question": "What is the importance of the P value in a hypothesis test such as the Wilcoxon rank sum test", "positive_ctxs": [{"text": "For the Wilcoxon test, a p-value is the probability of getting a test statistic as large or larger assuming both distributions are the same. In addition to a p-value we would like some estimated measure of how these distributions differ. The wilcox. test function provides this information when we set conf.int = TRUE ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Wilcoxon signed rank test is a nonparametric test that compares the median of a set of numbers against a hypothetical median. The Wilcoxon rank sum test is a nonparametric test to compare two unmatched groups. It is equivalent to the Mann-Whitney test."}, {"text": "The Wilcoxon signed rank test is a nonparametric test that compares the median of a set of numbers against a hypothetical median. The Wilcoxon rank sum test is a nonparametric test to compare two unmatched groups. It is equivalent to the Mann-Whitney test. The Gehan-Wilcoxon test is a method to compare survival curves."}, {"text": "The Wilcoxon rank sum test is a nonparametric test that may be used to assess whether the distributions of observations obtained between two separate groups on a dependent variable are systematically different from one another."}, {"text": "P > 0.05 is the probability that the null hypothesis is true. 1 minus the P value is the probability that the alternative hypothesis is true. A statistically significant test result (P \u2264 0.05) means that the test hypothesis is false or should be rejected. A P value greater than 0.05 means that no effect was observed."}, {"text": "P > 0.05 is the probability that the null hypothesis is true. 1 minus the P value is the probability that the alternative hypothesis is true. A statistically significant test result (P \u2264 0.05) means that the test hypothesis is false or should be rejected. A P value greater than 0.05 means that no effect was observed."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "P > 0.05 is the probability that the null hypothesis is true.  A statistically significant test result (P \u2264 0.05) means that the test hypothesis is false or should be rejected. A P value greater than 0.05 means that no effect was observed."}]}, {"question": "What is the exponential smoothing formula", "positive_ctxs": [{"text": "The component form of simple exponential smoothing is given by: Forecast equation^yt+h|t=\u2113tSmoothing equation\u2113t=\u03b1yt+(1\u2212\u03b1)\u2113t\u22121, Forecast equation y ^ t + h | t = \u2113 t Smoothing equation \u2113 t = \u03b1 y t + ( 1 \u2212 \u03b1 ) \u2113 t \u2212 1 , where \u2113t is the level (or the smoothed value) of the series at time t ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The exponential smoothing method takes this into account and allows for us to plan inventory more efficiently on a more relevant basis of recent data. Another benefit is that spikes in the data aren't quite as detrimental to the forecast as previous methods."}, {"text": "It is very much like the exponential distribution, with \u03bb corresponding to 1/p, except that the geometric distribution is discrete while the exponential distribution is continuous."}, {"text": "In linear regression, the function is a linear (straight-line) equation. In power or exponential regression, the function is a power (polynomial) equation of the form or an exponential function in the form ."}, {"text": "It is called Laplace smoothing because the smoothing proceeds from a logic of slightly correcting the observed proportions (in the case of categorical variables) in the direction of a uniform distribution among the categories (i.e., injecting a bit of equi-probability among them)."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The exponential distribution is a continuous probability distribution used to model the time we need to wait before a given event occurs. It is the continuous counterpart of the geometric distribution, which is instead discrete. Sometimes it is also called negative exponential distribution."}, {"text": "The exponential distribution is one of the widely used continuous distributions. It is often used to model the time elapsed between events. We will now mathematically define the exponential distribution, and derive its mean and expected value."}]}, {"question": "What is the Markov assumption aka Markov property in a Bayesian network", "positive_ctxs": [{"text": "The Markov condition, sometimes called the Markov assumption, is an assumption made in Bayesian probability theory, that every node in a Bayesian network is conditionally independent of its nondescendents, given its parents. Stated loosely, it is assumed that a node has no bearing on nodes which do not descend from it."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic , whereas Markov networks are undirected and may be cyclic.  The underlying graph of a Markov random field may be finite or infinite."}, {"text": "A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic , whereas Markov networks are undirected and may be cyclic.  The underlying graph of a Markov random field may be finite or infinite."}, {"text": "In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph.  The underlying graph of a Markov random field may be finite or infinite."}, {"text": "Bayesian networks encode the dependencies and independencies between variables. Under the causal Markov assumption, each variable in a Bayesian network is independent of its ancestors given the values of its parents."}, {"text": "Markovian is an adjective that may describe: In probability theory and statistics, subjects named for Andrey Markov: A Markov chain or Markov process, a stochastic model describing a sequence of possible events. The Markov property, the memoryless property of a stochastic process."}, {"text": "A Bayesian network is a directed graphical model. (A Markov random field is a undirected graphical model.) A graphical model captures the conditional independence, which can be different from the Markovian property."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}]}, {"question": "Is kNN a linear classifier", "positive_ctxs": [{"text": "An example of a nonlinear classifier is kNN.  The decision boundaries of kNN (the double lines in Figure 14.6 ) are locally linear segments, but in general have a complex shape that is not equivalent to a line in 2D or a hyperplane in higher dimensions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Figure 14.11: A nonlinear problem. An example of a nonlinear classifier is kNN.  Linear classifiers misclassify the enclave, whereas a nonlinear classifier like kNN will be highly accurate for this type of problem if the training set is large enough."}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them."}, {"text": "Though SVM is a linear classifier which learns an (n \u2013 1)-dimensional classifier for classification of data into two classes. But SVM it can be used for classifying a non-linear dataset."}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them. Sometimes this is incorrect."}, {"text": "Naive Bayes is a linear classifier Naive Bayes leads to a linear decision boundary in many common cases.  The red decision line indicates the decision boundary where P(y=1|x)=P(y=2|x)."}, {"text": "A linear classifier that the perceptron is categorized as is a classification algorithm, which relies on a linear predictor function to make predictions. Its predictions are based on a combination that includes weights and feature vector."}, {"text": "An example of a nonlinear classifier is kNN.  If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier."}]}, {"question": "What is latent class segmentation", "positive_ctxs": [{"text": "Latent Class (LC) segmentation operates under the assumption that there are groups underlying the data that give rise to segments. These groups are \"latent\" or not directly observable. LC techniques use formal statistical modeling to get at these segments, unlike most other segmentation methods."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects).  Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The standard solution that psychologists take to measuring latent variables is to use a series of questions that are all designed to measure the latent variable. This is known as a multi-item scale, where an \u201citem\u201d is a question, and a \u201cscale\u201d is the resulting estimate of the latent variable."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "On a technical note, estimation of a latent variable is done by analyzing the variance and covariance of the indicators. The measurement model of a latent variable with effect indicators is the set of relationships (modeled as equations) in which the latent variable is set as the predictor of the indicators."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (\"hidden units\"), with connections between the layers but not between units within each layer."}]}, {"question": "What is randomized quicksort analyze the expected running time of randomized quicksort with the help of a suitable example", "positive_ctxs": [{"text": "Therefore, the average running time of QUICKSORT on uniformly distributed permutations (random data) and the expected running time of randomized QUICKSORT are both O(n + n lg n) = O(n lg n). This is the same growth rate as merge sort and heap sort."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A clinical trial is a randomized controlled trial only when participants are randomly allocated to the group receiving the treatment and a control group. What participants are allocated among groups receiving different treatments the clinical trial is simply called a randomized trial."}, {"text": "Three of the more widely used experimental designs are the completely randomized design, the randomized block design, and the factorial design. In a completely randomized experimental design, the treatments are randomly assigned to the experimental units."}, {"text": "Researchers can take a number of steps to account for regression to the mean and avoid making incorrect conclusions. The best way is to remove the effect of regression to the mean during the design stage by conducting a randomized controlled trial (RCT)."}, {"text": "Definition. A study design that randomly assigns participants into an experimental group or a control group. As the study is conducted, the only expected difference between the control and experimental groups in a randomized controlled trial (RCT) is the outcome variable being studied."}, {"text": "Definition. A study design that randomly assigns participants into an experimental group or a control group. As the study is conducted, the only expected difference between the control and experimental groups in a randomized controlled trial (RCT) is the outcome variable being studied."}, {"text": "Typical well-designed randomized controlled trials set at 0.10 or 0.20. Related to is the statistical power (), the probability of declaring the two treatments different when the true difference is exactly ."}, {"text": "Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with\u200b 10-point bins."}]}, {"question": "What is the alpha state of mind", "positive_ctxs": [{"text": "The alpha state of mind is when you reach a very relaxed state while awake. Your brain begins to emit alpha waves instead of beta, which is what you emit when you're fully awake."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Medical Definition of alpha state : a state of wakeful relaxation that is associated with increased alpha wave activity When electroencephalograms show a brain wave pattern of 9 to 12 cycles per second, the subject is said to be in alpha state, usually described as relaxed, peaceful, or floating.\u2014"}, {"text": "Why is an alpha level of . 05 commonly used? Seeing as the alpha level is the probability of making a Type I error, it seems to make sense that we make this area as tiny as possible.  The smaller the alpha level, the smaller the area where you would reject the null hypothesis."}, {"text": "Mathematically test efficiency is calculated as a percentage of the number of alpha testing (in-house or on-site) defects divided by sum of a number of alpha testing and a number of beta testing (off-site) defects."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. For a robot that is learning to walk, the state is the position of its two legs. For a Go program, the state is the positions of all the pieces on the board."}, {"text": "There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. For a robot that is learning to walk, the state is the position of its two legs. For a Go program, the state is the positions of all the pieces on the board."}, {"text": "There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. For a robot that is learning to walk, the state is the position of its two legs. For a Go program, the state is the positions of all the pieces on the board."}]}, {"question": "What effect size is appropriate for a chi square test", "positive_ctxs": [{"text": "Most recent answer. Three different measures of effect size for chi-squared test and Fisher's exact test predominantly used are Phi, Cramer's V, and Odds Ratio. Phi and Odds Ratio are only suitable for a 2x2 contingency table and Cramer's V is suitable for larger contingency tables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship."}, {"text": "The larger the sample size is the smaller the effect size that can be detected. The reverse is also true; small sample sizes can detect large effect sizes.  Thus an appropriate determination of the sample size used in a study is a crucial step in the design of a study."}, {"text": "The rank-sum test is a non-parametric hypothesis test that can be used to determine if there is a statistically significant association between categorical survey responses provided for two different survey questions. The use of this test is appropriate even when survey sample size is small."}, {"text": "The effect size is the main finding of a quantitative study. While a P value can inform the reader whether an effect exists, the P value will not reveal the size of the effect."}, {"text": "Abstract. Dunn's test is the appropriate nonparametric pairwise multiple- comparison. procedure when a Kruskal\u2013Wallis test is rejected, and it is now im- plemented for Stata in the dunntest command. dunntest produces multiple com- parisons following a Kruskal\u2013Wallis k-way test by using Stata's built-in kwallis command."}, {"text": "This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction."}, {"text": "This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction."}]}, {"question": "What is the difference between measures of central tendency and standard deviation", "positive_ctxs": [{"text": "2 Answers. measures of central tendency are mean, mode and median , whereas measures of dispersion are variance, standard deviation and interquartile range (it explains the extent to which distribution stretched or squeezed)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deviation means change or distance.  Hence standard deviation is a measure of change or the distance from a measure of central tendency - which is normally the mean. Hence, standard deviation is different from a measure of central tendency."}, {"text": "Deviation means change or distance. But change is always followed by the word 'from'. Hence standard deviation is a measure of change or the distance from a measure of central tendency - which is normally the mean. Hence, standard deviation is different from a measure of central tendency."}, {"text": "In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution.  The most common measures of central tendency are the arithmetic mean, the median, and the mode."}, {"text": "All descriptive statistics are either measures of central tendency or measures of variability, also known as measures of dispersion.  Range, quartiles, absolute deviation and variance are all examples of measures of variability. Consider the following data set: 5, 19, 24, 62, 91, 100."}, {"text": "These measures indicate where most values in a distribution fall and are also referred to as the central location of a distribution. You can think of it as the tendency of data to cluster around a middle value. In statistics, the three most common measures of central tendency are the mean, median, and mode."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "Quartile deviation is the difference between \u201cfirst and third quartiles\u201d in any distribution. Standard deviation measures the \u201cdispersion of the data set\u201d that is relative to its mean."}]}, {"question": "How do you calculate the mean square error", "positive_ctxs": [{"text": "General steps to calculate the mean squared error from a set of X and Y values:Find the regression line.Insert your X values into the linear regression equation to find the new Y values (Y').Subtract the new Y value from the original to get the error.Square the errors.Add up the errors.Find the mean."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "From Wikipedia, the free encyclopedia. In statistics and signal processing, a minimum mean square error (MMSE) estimator is an estimation method which minimizes the mean square error (MSE), which is a common measure of estimator quality, of the fitted values of a dependent variable."}, {"text": "While the variance and the standard error of the mean are different estimates of variability, one can be derived from the other. Multiply the standard error of the mean by itself to square it. This step assumes that the standard error is a known quantity."}, {"text": "You take the sum of the squares of the terms in the distribution, and divide by the number of terms in the distribution (N). From this, you subtract the square of the mean (\u03bc2). It's a lot less work to calculate the standard deviation this way."}, {"text": "To calculate the standard deviation of those numbers:Work out the Mean (the simple average of the numbers)Then for each number: subtract the Mean and square the result.Then work out the mean of those squared differences.Take the square root of that and we are done!"}, {"text": "To calculate the standard deviation of those numbers:Work out the Mean (the simple average of the numbers)Then for each number: subtract the Mean and square the result.Then work out the mean of those squared differences.Take the square root of that and we are done!"}, {"text": "The name tells you how to calculate it. You subtract the regression-predicted values from the actual values, square them (to get rid of directionality), take their average, then take the square root of the average."}]}, {"question": "What is the shortcoming of content based recommender systems", "positive_ctxs": [{"text": "The model can only make recommendations based on existing interests of the user. In other words, the model has limited ability to expand on the users' existing interests."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Content-based recommendation systems uses their knowledge about each product to recommend new ones. Recommendations are based on attributes of the item. Content-based recommender systems work well when descriptive data on the content is provided beforehand. \u201cSimilarity\u201d is measured against product attributes."}, {"text": "Offline evaluations test the effectiveness of recommender system algorithms on a certain dataset. Online evaluation attempts to evaluate recommender systems by a method called A/B testing where a part of users are served by recommender system A and the another part of users by recommender system B."}, {"text": "Adaptive learning systems are designed to dynamically adjust to the level or type of course content based on an individual student's abilities or skill attainment, in ways that accelerate a learner's performance with both automated and instructor interventions."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "An Inverted file is an index data structure that maps content to its location within a database file, in a document or in a set of documents.  The inverted file is the most popular data structure used in document retrieval systems to support full text search."}, {"text": "Enthalpy is the measure of total heat present in the thermodynamic system where the pressure is constant. Entropy is the measure of disorder in a thermodynamic system.  It is represented as \\Delta S=\\Delta Q/T where Q is the heat content and T is the temperature."}]}, {"question": "What is spatio temporal model", "positive_ctxs": [{"text": "Spatiotemporal models arise when data are collected across time as well as space and has at least one spatial and one temporal property. An event in a spatiotemporal dataset describes a spatial and temporal phenomenon that exists at a certain time t and location x."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Local interactions in space can give rise to large scale spatio temporal patterns (e.g. (spiral) waves, spatio-temporal chaos (turbulence), stationary (Turing-type) patterns and transitions between these modes). Their occurrence and properties are largely independent of the precise interaction structure."}, {"text": "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior.  Both classes of networks exhibit temporal dynamic behavior."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Use temporal data types to store date, time, and time-interval information. Although you can store this data in character strings, it is better to use temporal types for consistency and validation. An hour, minute, and second to six decimal places (microseconds), and the time zone offset from GMT."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "How is the concept of probability related to the normal distribution", "positive_ctxs": [{"text": "The normal distribution is a probability distribution. As with any probability distribution, the proportion of the area that falls under the curve between two points on a probability distribution plot indicates the probability that a value will fall within that interval."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. The area under the normal distribution curve represents probability and the total area under the curve sums to one."}, {"text": "In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side.  The normal distribution is often called the bell curve because the graph of its probability density looks like a bell."}, {"text": "A univariate distribution refers to the distribution of a single random variable.  On the other hand, a multivariate distribution refers to the probability distribution of a group of random variables. For example, a multivariate normal distribution is used to specify the probabilities of returns of a group of n stocks."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What does regression model mean", "positive_ctxs": [{"text": "Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Ridge regression does not really select variables in the many predictors situation.  Both ridge regression and the LASSO can outperform OLS regression in some predictive situations \u2013 exploiting the tradeoff between variance and bias in the mean square error."}, {"text": "The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the \u201cerrors\u201d) and squaring them. The squaring is necessary to remove any negative signs."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}]}, {"question": "Is Weka good for machine learning", "positive_ctxs": [{"text": "Weka has a lot of machine learning algorithms. This is great, it is one of the large benefits of using Weka as a platform for machine learning. A down side is that it can be a little overwhelming to know which algorithms to use, and when."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "XGboost is the most widely used algorithm in machine learning, whether the problem is a classification or a regression problem. It is known for its good performance as compared to all other machine learning algorithms."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "If you want to solve some real-world problems and design a cool product or algorithm, then having machine learning skills is not enough. You would need good working knowledge of data structures.  So you've decided to move beyond canned algorithms and start to code your own machine learning methods."}, {"text": "Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2."}, {"text": "Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called empirical risk minimization."}, {"text": "Relative Frequency Of A Class Is The Percentage Of The Data That Falls In That Class, While Cumulative Frequency Of A Class Is The Sum Of The Frequencies Of That Class And All Previous Classes."}, {"text": "A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem. \u2014 Practical recommendations for gradient-based training of deep architectures, 2012."}]}, {"question": "Are regression coefficients normally distributed", "positive_ctxs": [{"text": "More precisely, if we consider repeated sampling from our population, for large sample sizes, the distribution (across repeated samples) of the ordinary least squares estimates of the regression coefficients follow a normal distribution."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A multivariate normal distribution is a vector in multiple normally distributed variables, such that any linear combination of the variables is also normally distributed."}, {"text": "A normality test is used to determine whether sample data has been drawn from a normally distributed population (within some tolerance). A number of statistical tests, such as the Student's t-test and the one-way and two-way ANOVA require a normally distributed sample population."}, {"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}, {"text": "The statistic used to estimate the mean of a population, \u03bc, is the sample mean, . If X has a distribution with mean \u03bc, and standard deviation \u03c3, and is approximately normally distributed or n is large, then is approximately normally distributed with mean \u03bc and standard error .."}, {"text": "OLS cannot be used because the regression function is not a linear function of the regression coefficients (the coefficients appear inside the nonlinear functions \u03a6 or \u039b)."}, {"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}]}, {"question": "How do you do linear discriminant analysis", "positive_ctxs": [{"text": "5:1515:11Suggested clip \u00b7 109 secondsStatQuest: Linear Discriminant Analysis (LDA) clearly explained YouTubeStart of suggested clipEnd of suggested clip"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and other fields, to find a linear combination of features that characterizes or separates two or more classes"}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Discriminant or discriminant function analysis is a. parametric technique to determine which weightings of. quantitative variables or predictors best discriminate. between 2 or more than 2 groups of cases and do so."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}, {"text": "Yes, you should check normality of errors AFTER modeling. In linear regression, errors are assumed to follow a normal distribution with a mean of zero. Let's do some simulations and see how normality influences analysis results and see what could be consequences of normality violation."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}]}, {"question": "What is the difference between the z test and the t test", "positive_ctxs": [{"text": "Z-tests are statistical calculations that can be used to compare population means to a sample's. T-tests are calculations used to test a hypothesis, but they are most useful when we need to determine if there is a statistically significant difference between two independent sample groups."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "If the level of significance is \u03b1 = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28. For a two tailed test, use \u03b1/2 = 0.05 and the critical region is below z = -1.645 and above z = 1.645."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "To apply the linear regression t-test to sample data, we require the standard error of the slope, the slope of the regression line, the degrees of freedom, the t statistic test statistic, and the P-value of the test statistic.  Therefore, the P-value is 0.0121 + 0.0121 or 0.0242."}]}, {"question": "How do you describe a scatter plot with no correlation", "positive_ctxs": [{"text": "If the points on the scatter plot seem to form a line that slants down from left to right, there is a negative relationship or negative correlation between the variables. If the points on the scatter plot seem to be scattered randomly, there is no relationship or no correlation between the variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "You can use a scatter plot to analyze trends in your data and to help you to determine whether or not there is a relationship between two variables.  If the points on the scatter plot seem to form a line that slants down from left to right, there is a negative relationship or negative correlation between the variables."}, {"text": "A scatter plot is a special type of graph designed to show the relationship between two variables. With regression analysis, you can use a scatter plot to visually inspect the data to see whether X and Y are linearly related."}, {"text": "A weak correlation means that as one variable increases or decreases, there is a lower likelihood of there being a relationship with the second variable.  Earthquake magnitude and the depth at which it was measured is therefore weakly correlated, as you can see the scatter plot is nearly flat."}, {"text": "How to Read a Correlation Matrix-1 indicates a perfectly negative linear correlation between two variables.0 indicates no linear correlation between two variables.1 indicates a perfectly positive linear correlation between two variables."}, {"text": "Scatter plots' primary uses are to observe and show relationships between two numeric variables. The dots in a scatter plot not only report the values of individual data points, but also patterns when the data are taken as a whole.  A scatter plot can also be useful for identifying other patterns in data."}, {"text": "Method comparisonCorrelation coefficient. A correlation coefficient measures the association between two methods.Scatter plot. A scatter plot shows the relationship between two methods.Fit Y on X.  Linearity.  Residual plot.  Average bias.  Difference plot (Bland-Altman plot)  Fit differences.More items\u2022"}, {"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}]}, {"question": "What are the uses of least square method", "positive_ctxs": [{"text": "The least squares method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve. Least squares regression is used to predict the behavior of dependent variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "MSE is the average of the squared error that is used as the loss function for least squares regression: It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE."}, {"text": "To see the accuracy of clustering process by using K-Means clustering method then calculated the square error value (SE) of each data in cluster 2. The value of square error is calculated by squaring the difference of the quality score or GPA of each student with the value of centroid cluster 2."}, {"text": "To see the accuracy of clustering process by using K-Means clustering method then calculated the square error value (SE) of each data in cluster 2. The value of square error is calculated by squaring the difference of the quality score or GPA of each student with the value of centroid cluster 2."}, {"text": "Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable; the method estimates the relationship by minimizing the sum of the squares in the difference between the observed and predicted values of the"}]}, {"question": "Where can I make a decision tree", "positive_ctxs": [{"text": "Simply head on over to www.canva.com to start creating your decision tree design. You don't need to download Canva, just create an account and log in."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature."}, {"text": "Decision tree learning is a supervised machine learning technique for inducing a decision tree from training data. A decision tree (also referred to as a classification tree or a reduction tree) is a predictive model which is a mapping from observations about an item to conclusions about its target value."}, {"text": "Decision tree classifier \u2013 Decision tree classifier is a systematic approach for multiclass classification. It poses a set of questions to the dataset (related to its attributes/features). The decision tree classification algorithm can be visualized on a binary tree."}, {"text": "Decision tree classifier \u2013 Decision tree classifier is a systematic approach for multiclass classification. It poses a set of questions to the dataset (related to its attributes/features). The decision tree classification algorithm can be visualized on a binary tree."}, {"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value."}]}, {"question": "Why is squared loss bad for classification", "positive_ctxs": [{"text": "There are two reasons why Mean Squared Error(MSE) is a bad choice for binary classification problems:  If we use maximum likelihood estimation(MLE), assuming that the data is from a normal distribution(a wrong assumption, by the way), we get the MSE as a Cost function for optimizing our model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater."}, {"text": "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater."}, {"text": "In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. A variant for classification is also sometimes used."}, {"text": "MSE loss is used for regression tasks. As the name suggests, this loss is calculated by taking the mean of squared differences between actual(target) and predicted values."}, {"text": "Binary, multi-class and multi-label classification Cross-entropy is a commonly used loss function for classification tasks."}, {"text": "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1.  So predicting a probability of . 012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0."}, {"text": "It's a cost function that is used as loss for machine learning models, telling us how bad it's performing, the lower the better. Also it's much easier to reason about the loss this way, to be consistent with the rule of loss functions approaching 0 as the model gets better."}]}, {"question": "What is the significance of AlphaGo Zero in AI research", "positive_ctxs": [{"text": "Training artificial intelligence (AI) without datasets derived from human experts has significant implications for the development of AI with superhuman skills because expert data is \"often expensive, unreliable or simply unavailable.\" Demis Hassabis, the co-founder and CEO of DeepMind, said that AlphaGo Zero was so"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "AlphaGo Zero is a version of DeepMind's Go software AlphaGo.  By playing games against itself, AlphaGo Zero surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0, reached the level of AlphaGo Master in 21 days, and exceeded all the old versions in 40 days."}, {"text": "Training. AlphaGo Zero's neural network was trained using TensorFlow, with 64 GPU workers and 19 CPU parameter servers.  In the first three days AlphaGo Zero played 4.9 million games against itself in quick succession."}, {"text": "It is able to do this by using a novel form of reinforcement learning, in which AlphaGo Zero becomes its own teacher. The system starts off with a neural network that knows nothing about the game of Go. It then plays games against itself, by combining this neural network with a powerful search algorithm."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "Many problems in AI can be modeled as constraint satisfaction problems (CSPs). Hence the development of effective solution techniques for CSPs is an important research problem.  Each constraint is defined over some subset of the original set of variables and restricts the values these variables can simultaneously take."}, {"text": "The significance level is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference."}]}, {"question": "How does skew affect standard deviation", "positive_ctxs": [{"text": "In a skewed distribution, the upper half and the lower half of the data have a different amount of spread, so no single number such as the standard deviation could describe the spread very well."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In a normal distribution the mean is zero and the standard deviation is 1. It has zero skew and a kurtosis of 3. Normal distributions are symmetrical, but not all symmetrical distributions are normal."}, {"text": "Events are independent if the outcome of one event does not affect the outcome of another. For example, if you throw a die and a coin, the number on the die does not affect whether the result you get on the coin."}, {"text": "How to Calculate a CorrelationFind the mean of all the x-values.Find the standard deviation of all the x-values (call it sx) and the standard deviation of all the y-values (call it sy).  For each of the n pairs (x, y) in the data set, take.Add up the n results from Step 3.Divide the sum by sx \u2217 sy.More items"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "In probability, two events are independent if the incidence of one event does not affect the probability of the other event. If the incidence of one event does affect the probability of the other event, then the events are dependent. There is a red 6-sided fair die and a blue 6-sided fair die."}]}, {"question": "How does being multilingual change brain structure and function", "positive_ctxs": [{"text": "It has been found that multilingualism affects the structure, and essentially, the cytoarchitecture of the brain.  Language learning boosts brain plasticity and the brain's ability to code new information. Early language learning plays a significant role in the formation of memory circuits for learning new information."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Events A and B are independent if: knowing whether A occured does not change the probability of B. Mathematically, can say in tw. Page 1. Events A and B are independent if: knowing whether A occured does not change the probability of B."}, {"text": "How to optimize your meta tags: A checklistCheck whether all your pages and your content have title tags and meta descriptions.Start paying more attention to your headings and how you structure your content.Don't forget to mark up your images with alt text.More items\u2022"}, {"text": "Decision tree is unstable because training a tree with a slightly different sub-sample causes the structure of the tree to change drastically. It overfits by learning from noise data as well and optimises for that particular sample, which causes its variable importance order to change significantly."}, {"text": "Stationarity. A common assumption in many time series techniques is that the data are stationary. A stationary process has the property that the mean, variance and autocorrelation structure do not change over time."}, {"text": "As you experiment with your algorithm to try and improve your model, your loss function will tell you if you're getting(or reaching) anywhere. At its core, a loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value)."}, {"text": "A loss function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. A most commonly used method of finding the minimum point of function is \u201cgradient descent\u201d.  Loss functions can be broadly categorized into 2 types: Classification and Regression Loss."}, {"text": "Disadvantages of decision trees:They are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree.They are often relatively inaccurate.More items"}]}, {"question": "What is deep learning in image processing", "positive_ctxs": [{"text": "Deep learning is a type of machine learning in which a model learns to perform classification tasks directly from images, text or sound. Deep learning is usually implemented using neural network architecture. The term deep refers to the number of layers in the network\u2014the more the layers, the deeper the network."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "jobs. deep learning performs better when sequential processing is used."}, {"text": "We discuss some wonders in the field of image processing with machine learning advancements. Image processing can be defined as the technical analysis of an image by using complex algorithms. Here, image is used as the input, where the useful information returns as the output."}, {"text": "Dictionary learning is learning a set of atoms so that a given image can be well approximated by a sparse linear combination of these learned atoms, while deep learning methods aim at extracting deep semantic feature representations via a deep network."}, {"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}, {"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}, {"text": "One method of processing images is via face detection. Face detection is a branch of image processing that uses machine learning to detect faces in images. A Haar Cascade is an object detection method used to locate an object of interest in images."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}]}, {"question": "What is a simple linear regression model", "positive_ctxs": [{"text": "Simple linear regression is a regression model that estimates the relationship between one independent variable and one dependent variable using a straight line. Both variables should be quantitative."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A linear regression model extended to include more than one independent variable is called a multiple regression model. It is more accurate than to the simple regression.  The principal adventage of multiple regression model is that it gives us more of the information available to us who estimate the dependent variable."}, {"text": "In statistics, Poisson regression is a generalized linear model form of regression analysis used to model count data and contingency tables.  A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables."}, {"text": "In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression."}, {"text": "It is used to determine the extent to which there is a linear relationship between a dependent variable and one or more independent variables.  In simple linear regression a single independent variable is used to predict the value of a dependent variable."}, {"text": "Log-Log linear regression A regression model where the outcome and at least one predictor are log transformed is called a log-log linear model."}, {"text": "Log-Log linear regression A regression model where the outcome and at least one predictor are log transformed is called a log-log linear model."}, {"text": "Yes, although 'linear regression' refers to any approach to model the relationship between one or more variables, OLS is the method used to find the simple linear regression of a set of data."}]}, {"question": "Which model helps SVM to implement the algorithm in high dimensional space", "positive_ctxs": [{"text": "Kernel method is used by SVM to perform a non-linear classification. They take low dimensional input space and convert them into high dimensional input space. It converts non-separable classes into the separable one, it finds out a way to separate the data on the basis of the data labels defined by us."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The SVM typically tries to use a \"kernel function\" to project the sample points to high dimension space to make them linearly separable, while the perceptron assumes the sample points are linearly separable."}, {"text": "HYPERPLANE. Now that we understand the SVM logic lets formally define the hyperplane . A hyperplane in an n-dimensional Euclidean space is a flat, n-1 dimensional subset of that space that divides the space into two disconnected parts."}, {"text": "You can see SVM as an instance-based learning algorithm because you need to memorize the support vectors if you cannot represent the feature space and hence the discriminating hyperplane in this space explicitly."}, {"text": "Another common model for classification is the support vector machine (SVM). An SVM works by projecting the data into a higher dimensional space and separating it into different classes by using a single (or set of) hyperplanes. A single SVM does binary classification and can differentiate between two classes."}, {"text": "Dimensionality reduction refers to techniques for reducing the number of input variables in training data. When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the \u201cessence\u201d of the data."}, {"text": "PCA is designed to model linear variabilities in high-dimensional data. However, many high dimensional data sets have a nonlinear nature. In these cases the high-dimensional data lie on or near a nonlinear manifold (not a linear subspace) and therefore PCA can not model the variability of the data correctly."}, {"text": "Variable screening is the process of filtering out irrelevant variables, with the aim to reduce the dimensionality from ultrahigh to high while retaining all important variables.  The main theme of this thesis is to develop variable screening and variable selection methods for high dimensional data analysis."}]}, {"question": "What is Parametric vs nonparametric", "positive_ctxs": [{"text": "Parametric tests assume underlying statistical distributions in the data. Nonparametric tests do not rely on any distribution.  They can thus be applied even if parametric conditions of validity are not met."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "For multi class classification using SVM; It is NOT (one vs one) and NOT (one vs REST). Instead learn a two-class classifier where the feature vector is (x, y) where x is data and y is the correct label associated with the data."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The Wilcoxon signed rank test is a nonparametric test that compares the median of a set of numbers against a hypothetical median. The Wilcoxon rank sum test is a nonparametric test to compare two unmatched groups. It is equivalent to the Mann-Whitney test."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "They are continuous vs discrete distributions. A first difference is that multinomial distribution M(N,p) is discrete (it generalises binomial disrtibution) whereas Dirichlet distribution is continuous (it generalizes Beta distribution)."}]}, {"question": "Does the Law of Averages exist", "positive_ctxs": [{"text": "The law of averages typically assumes that unnatural short-term \u201cbalance\u201d must occur. This can also be known as \u201cGambler's Fallacy\u201d and is not a real mathematical principle.  The law of large numbers is important because it \u201cguarantees\u201d stable long-term results for the averages of random events."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Averages have less variation than individual observations.  For any sample size n, the sampling distribution of Picture is normal if the population from which the sample is drawn is normally distributed."}, {"text": "Example of Law of Large Numbers Let's say you rolled the dice three times and the outcomes were 6, 6, 3. The average of the results is 5. According to the law of the large numbers, if we roll the dice a large number of times, the average result will be closer to the expected value of 3.5."}, {"text": "The law of large numbers, in probability and statistics, states that as a sample size grows, its mean gets closer to the average of the whole population. In the 16th century, mathematician Gerolama Cardano recognized the Law of Large Numbers but never proved it."}, {"text": "The law of large numbers, in probability and statistics, states that as a sample size grows, its mean gets closer to the average of the whole population. In the 16th century, mathematician Gerolama Cardano recognized the Law of Large Numbers but never proved it."}, {"text": "Law of large numbers, in statistics, the theorem that, as the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean. The law of large numbers was first proved by the Swiss mathematician Jakob Bernoulli in 1713."}, {"text": "Does not affect R2 or adjusted R2 (since these estimate the POPULATION variances which are not conditional on X)"}, {"text": "Disadvantages of Sampling Since choice of sampling method is a judgmental task, there exist chances of biasness as per the mindset of the person who chooses it. Improper selection of sampling techniques may cause the whole process to defunct. Selection of proper size of samples is a difficult job."}]}, {"question": "What is random in Random Forest algorithm", "positive_ctxs": [{"text": "Random forest has nearly the same hyperparameters as a decision tree or a bagging classifier.  Random forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Random Forest Algorithm The Random Forest ML Algorithm is a versatile supervised learning algorithm that's used for both classification and regression analysis tasks."}, {"text": "Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression.  A Random Forest operates by constructing several decision trees during training time and outputting the mean of the classes as the prediction of all the trees."}, {"text": "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging."}, {"text": "Random Forest"}, {"text": "Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class. For multiclass problem you will need to reduce it into multiple binary classification problems. Random Forest works well with a mixture of numerical and categorical features."}, {"text": "Random Forest uses bootstrap sampling and feature sampling, i.e row sampling and column sampling. Therefore Random Forest is not affected by multicollinearity that much since it is picking different set of features for different models and of course every model sees a different set of data points."}, {"text": "Random Forest is perhaps the most popular classification algorithm, capable of both classification and regression. It can accurately classify large volumes of data. The name \u201cRandom Forest\u201d is derived from the fact that the algorithm is a combination of decision trees."}]}, {"question": "Why is pooling used in convolutional neural network", "positive_ctxs": [{"text": "This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep NN is just a deep neural network, with a lot of layers. It can be CNN, or just a plain multilayer perceptron. CNN, or convolutional neural network, is a neural network using convolution layer and pooling layer."}, {"text": "Convolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input. A pooling layer is a new layer added after the convolutional layer."}, {"text": "A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input."}, {"text": "A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input."}, {"text": "1. Why is the XOR problem exceptionally interesting to neural network researchers?  Explanation: Linearly separable problems of interest of neural network researchers because they are the only class of problem that Perceptron can solve successfully."}, {"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery."}]}, {"question": "Why is the decision forest better than the random forest", "positive_ctxs": [{"text": "Random forests consist of multiple single trees each based on a random sample of the training data. They are typically more accurate than single decision trees. The following figure shows the decision boundary becomes more accurate and stable as more trees are added."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The fundamental reason to use a random forest instead of a decision tree is to combine the predictions of many decision trees into a single model. The logic is that a single even made up of many mediocre models will still be better than one good model."}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case."}, {"text": "A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results."}, {"text": "A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results."}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}, {"text": "The random forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a \u201cforest\u201d), this model uses two key concepts that gives it the name random: Random sampling of training data points when building trees."}]}, {"question": "What is the difference between correlated and uncorrelated subquery", "positive_ctxs": [{"text": "Correlated vs. A correlated subquery can be thought of as a filter on the table that it refers to, as if the subquery were evaluated on each row of the table in the outer query. An uncorrelated subquery has no such external column references."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A correlated subquery is much slower than the non-correlated subquery because in former, the inner query executes for each row of the outer query."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The random effects assumption is that the individual-specific effects are uncorrelated with the independent variables. The fixed effect assumption is that the individual-specific effects are correlated with the independent variables."}, {"text": "A noncorrelated (simple) subquery obtains its results independently of its containing (outer) statement. A correlated subquery requires values from its outer query in order to execute."}, {"text": "A subquery is a select statement that is embedded in a clause of another select statement.  A Correlated subquery is a subquery that is evaluated once for each row processed by the outer query or main query."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Hebbian learning is one of the oldest learning algorithms, and is based in large part on the dynamics of biological systems. A synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs."}]}, {"question": "Is NLP part of data science", "positive_ctxs": [{"text": "Natural Language Processing (NLP) is the sub-branch of Data Science that attempts to extract insights from \u201ctext.\u201d Thus, NLP is assuming an important role in Data Science."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep Learning is a part of Machine Learning which is applied to larger data-sets and based on ANN (Artificial Neural Networks). The main technology used in NLP (Natural Language Processing) which mainly focuses on teaching natural/human language to computers.  NLP is a part of AI which overlaps with ML & DL."}, {"text": "Unsupervised or undirected data science uncovers hidden patterns in unlabeled data. In unsupervised data science, there are no output variables to predict. The objective of this class of data science techniques, is to find patterns in data based on the relationship between data points themselves."}, {"text": "Natural Language Processing (NLP) is the part of AI that studies how machines interact with human language.  Combined with machine learning algorithms, NLP creates systems that learn to perform tasks on their own and get better through experience."}, {"text": "Data science is an umbrella term for a group of fields that are used to mine large datasets. Data analytics software is a more focused version of this and can even be considered part of the larger process. Analytics is devoted to realizing actionable insights that can be applied immediately based on existing queries."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Big data analysis caters to a large amount of data set which is also known as data mining, but data science makes use of the machine learning algorithms to design and develop statistical models to generate knowledge from the pile of big data."}, {"text": "Big data analysis caters to a large amount of data set which is also known as data mining, but data science makes use of the machine learning algorithms to design and develop statistical models to generate knowledge from the pile of big data."}]}, {"question": "What is inter and intra observer reliability", "positive_ctxs": [{"text": "intra-observer (or within observer) reliability; the degree to which measurements taken by the same observer are consistent, \u2022 inter-observer (or between observers) reliability; the degree to which measurements taken by different observers are similar."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Split-half reliability is a statistical method used to measure the consistency of the scores of a test. It is a form of internal consistency reliability and had been commonly used before the coefficient \u03b1 was invented."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Because it arises from consistency between parts of a test, split-half reliability is an \u201cinternal consistency\u201d approach to estimating reliability. This result is an estimate of the reliability of the test scores, and it provides some support for the quality of the test scores."}, {"text": "Alternate-form reliability is the consistency of test results between two different \u2013 but equivalent \u2013 forms of a test. Alternate-form reliability is used when it is necessary to have two forms of the same tests.  \u2013 Alternative-form reliability is needed whenever two test forms are being used to measure the same thing."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Inter-observer variation is the amount of variation between the results obtained by two or more observers examining the same material. Intra-observer variation is the amount of variation one observer experiences when observing the same material more than once."}]}, {"question": "What is the nocebo response", "positive_ctxs": [{"text": "In the strictest sense, a nocebo response is where a drug-trial's subject's symptoms are worsened by the administration of an inert, sham, or dummy (simulator) treatment, called a placebo."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "If you have both a response variable and an explanatory variable, the explanatory variable is always plotted on the x-axis (the horizontal axis). The response variable is always plotted on the y-axis (the vertical axis)."}, {"text": "The independent variable is called the Explanatory variable (or better known as the predictor) - the variable which influences or predicts the values. i.e. if the explanatory variable changes then it affects the response variable. Here Y is the Dependent variable or response variable."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "So, assuming a 15% survey response rate, we see that you should send your NPS survey to 1,700 customers. What if you're a smaller company and don't have enough customers to send the recommended number of invitations?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}]}, {"question": "What is the difference between stratified random sampling and cluster sampling", "positive_ctxs": [{"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  With stratified random sampling, these breaks may not exist*, so you divide your target population into groups (more formally called \"strata\")."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "Cluster Sampling: Advantages and Disadvantages Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "In stratified sampling, a random sample is drawn from each of the strata, whereas in cluster sampling only the selected clusters are sampled. A common motivation of cluster sampling is to reduce costs by increasing sampling efficiency."}, {"text": "Cluster sampling is best used when the clusters occur naturally in a population, when you don't have access to the entire population, and when the clusters are geographically convenient. However, cluster sampling is not as precise as simple random sampling or stratified random sampling."}]}, {"question": "Why do we use Lasso regression", "positive_ctxs": [{"text": "The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. The lasso does this by imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink toward zero."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Below are the different regression techniques: Ridge Regression. Lasso Regression. Polynomial Regression. Bayesian Linear Regression."}, {"text": "Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data."}, {"text": "You can use the Lasso or elastic net regularization for generalized linear model regression which can be used for classification problems. Here data is the data matrix with rows as observations and columns as features. group is the labels."}, {"text": "Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients.  On the other hand, L2 regularization (e.g. Ridge regression) doesn't result in elimination of coefficients or sparse models. This makes the Lasso far easier to interpret than the Ridge."}, {"text": "Generally, we use linear regression for time series analysis, it is used for predicting the result for time series as its trends. For example, If we have a dataset of time series with the help of linear regression we can predict the sales with the time."}, {"text": "Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters)."}]}, {"question": "Is income a discrete variable", "positive_ctxs": [{"text": "Income, although you may consider it to be technically discrete, would likely be treated as a continuous variable. Other discrete variables (such as the number of ER visits per year for a sample of hospitals) may also be treated as continuous even though they are technically discrete."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A discrete variable is a variable whose value is obtained by counting. A continuous variable is a variable whose value is obtained by measuring.  A discrete random variable X has a countable number of possible values."}, {"text": "A Latent Class regression model: Is used to predict a dependent variable as a function of predictor variables (Regression model). Includes a K-category latent variable X to cluster cases (LC model)  Each case may contain multiple records (Regression with repeated measurements)."}, {"text": "So by the definition of discrete and continuous random variables, a random variable cannot be both discrete and continuous. No. For a random variable to be discrete, there must a countable sequence such that ."}, {"text": "Discrete Probability Distributions If a random variable is a discrete variable, its probability distribution is called a discrete probability distribution. An example will make this clear. Suppose you flip a coin two times."}, {"text": "An embedding is a mapping of a discrete \u2014 categorical \u2014 variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables."}, {"text": "The optimal binning is the optimal discretization of a variable into bins given a discrete or continuous numeric target."}, {"text": "Mean of General discrete uniform distribution The expected value of discrete uniform random variable is E ( X ) = a + b 2 ."}]}, {"question": "What does AUC stand for", "positive_ctxs": [{"text": "area under the curve"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The area under the ROC curve (AUC) results were considered excellent for AUC values between 0.9-1, good for AUC values between 0.8-0.9, fair for AUC values between 0.7-0.8, poor for AUC values between 0.6-0.7 and failed for AUC values between 0.5-0.6."}, {"text": "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). Figure 5. AUC (Area under the ROC Curve). AUC provides an aggregate measure of performance across all possible classification thresholds."}, {"text": "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). Figure 5. AUC (Area under the ROC Curve). AUC provides an aggregate measure of performance across all possible classification thresholds."}, {"text": "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). Figure 5. AUC (Area under the ROC Curve)."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively."}]}, {"question": "When do you apply regression analysis and analysis of variance", "positive_ctxs": [{"text": "Regression is the statistical model that you use to predict a continuous outcome on the basis of one or more continuous predictor variables. In contrast, ANOVA is the statistical model that you use to predict a continuous outcome on the basis of one or more categorical predictor variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "\"Degrees of freedom\" is commonly abbreviated to df.  When this principle of restriction is applied to regression and analysis of variance, the general result is that you lose one degree of freedom for each parameter estimated prior to estimating the (residual) standard deviation."}, {"text": "A simple linear regression plot for amount of rainfall. Regression analysis is used in stats to find trends in data. For example, you might guess that there's a connection between how much you eat and how much you weigh; regression analysis can help you quantify that."}, {"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}, {"text": "When comparing more than two sets of numerical data, a multiple group comparison test such as one-way analysis of variance (ANOVA) or Kruskal-Wallis test should be used first."}, {"text": "In robust statistics, robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Regression analysis seeks to find the relationship between one or more independent variables and a dependent variable."}, {"text": "The term that does not apply to cluster analysis is factorization. Cluster analysis is a way of grouping data, based on obvious similarities. It is also called as classification analysis or numerical taxonomy.  Hierarchical cluster analysis tends to build a hierarchy within clusters."}, {"text": "Use regression analysis to describe the relationships between a set of independent variables and the dependent variable. Regression analysis produces a regression equation where the coefficients represent the relationship between each independent variable and the dependent variable."}]}, {"question": "How do you find the estimated variance", "positive_ctxs": [{"text": "Assuming 0<\u03c32<\u221e, by definition \u03c32=E[(X\u2212\u03bc)2]. Thus, the variance itself is the mean of the random variable Y=(X\u2212\u03bc)2. This suggests the following estimator for the variance \u02c6\u03c32=1nn\u2211k=1(Xk\u2212\u03bc)2."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The variance estimated as the average squared difference from the sample mean will always be less than the variance estimated as the average squared difference from the population mean unless the sample mean equals the population mean in which case they will be the same."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "The finite population correction (fpc) factor is used to adjust a variance estimate for an estimated mean or total, so that this variance only applies to the portion of the population that is not in the sample."}, {"text": "One way to measure multicollinearity is the variance inflation factor (VIF), which assesses how much the variance of an estimated regression coefficient increases if your predictors are correlated. If no factors are correlated, the VIFs will all be 1."}, {"text": "In statistics and machine learning, the bias\u2013variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters."}, {"text": "The asymptotic variance-covariance matrix can be used to calculate confidence intervals and to test hypotheses about the variance components. In this example, the variance for the estimated Var(STOREID) is 65787.226. The positive square root of this number gives the standard error for Var(STOREID), which is 256.49."}]}, {"question": "Is Bayesian statistics useful for machine learning", "positive_ctxs": [{"text": "(It is also possible to integrate existing knowledge with data - one way of doing that is using Bayesian statistics, in fact!)  Since Bayesian statistics provides a framework for updating \"knowledge\", it is in fact used a whole lot in machine learning."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bayesian inference is a machine learning model not as widely used as deep learning or regression models."}, {"text": "The most popular supervised NLP machine learning algorithms are: Support Vector Machines. Bayesian Networks. Maximum Entropy."}, {"text": "Key concepts include probability distributions, statistical significance, hypothesis testing, and regression. Furthermore, machine learning requires understanding Bayesian thinking."}, {"text": "Bayesian networks (BN) and Bayesian classifiers (BC) are traditional probabilistic techniques that have been successfully used by various machine learning methods to help solving a variety of problems in many different domains."}, {"text": "Statistical machine learning merges statistics with the computational sciences---computer science, systems science and optimization.  Moreover, by its interdisciplinary nature, statistical machine learning helps to forge new links among these fields."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Created by the Google Brain team, TensorFlow is an open source library for numerical computation and large-scale machine learning. TensorFlow bundles together a slew of machine learning and deep learning (aka neural networking) models and algorithms and makes them useful by way of a common metaphor."}]}, {"question": "What is a single layer Perceptron", "positive_ctxs": [{"text": "A single layer perceptron (SLP) is a feed-forward network based on a threshold transfer function. SLP is the simplest type of artificial neural networks and can only classify linearly separable cases with a binary target (1 , 0)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks. Perceptron is a linear classifier (binary). Also, it is used in supervised learning. It helps to classify the given input data."}, {"text": "A Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer). While a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non \u2013 linear functions. Figure 4 shows a multi layer perceptron with a single hidden layer."}, {"text": "A Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer). While a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non \u2013 linear functions."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "The original LSTM model is comprised of a single hidden LSTM layer followed by a standard feedforward output layer. The Stacked LSTM is an extension to this model that has multiple hidden LSTM layers where each layer contains multiple memory cells."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "There is no need to use LINEAR hidden layer in a neural network. Because two (or three or four) linear layers can't provide more intelligence than a single linear layer."}]}, {"question": "What is the relationship between the chi square statistic and the p value", "positive_ctxs": [{"text": "The P-value is the probability that a chi-square statistic having 2 degrees of freedom is more extreme than 19.58. We use the Chi-Square Distribution Calculator to find P(\u03a72 > 19.58) = 0.0001. Interpret results. Since the P-value (0.0001) is less than the significance level (0.05), we cannot accept the null hypothesis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The test statistic is a z-score (z) defined by the following equation. z=(p\u2212P)\u03c3 where P is the hypothesized value of population proportion in the null hypothesis, p is the sample proportion, and \u03c3 is the standard deviation of the sampling distribution."}, {"text": "Test statistic. The test statistic is a z-score (z) defined by the following equation. where P is the hypothesized value of population proportion in the null hypothesis, p is the sample proportion, and \u03c3 is the standard deviation of the sampling distribution."}, {"text": "Graphically, the p value is the area in the tail of a probability distribution. It's calculated when you run hypothesis test and is the area to the right of the test statistic (if you're running a two-tailed test, it's the area to the left and to the right)."}, {"text": "There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}]}, {"question": "What is p value in backward elimination", "positive_ctxs": [{"text": "The first step in backward elimination is pretty simple, you just select a significance level, or select the P-value. Usually, in most cases, a 5% significance level is selected. This means the P-value will be 0.05. You can change this value depending on the project."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The first step in backward elimination is pretty simple, you just select a significance level, or select the P-value. Usually, in most cases, a 5% significance level is selected. This means the P-value will be 0.05. You can change this value depending on the project."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Backward elimination (or backward deletion) is the reverse process. All the independent variables are entered into the equation first and each one is deleted one at a time if they do not contribute to the regression equation. Stepwise selection is considered a variation of the previous two methods."}, {"text": "Definition: Given data the maximum likelihood estimate (MLE) for the parameter p is the value of p that maximizes the likelihood P(data |p). That is, the MLE is the value of p for which the data is most likely. 100 P(55 heads|p) = ( 55 ) p55(1 \u2212 p)45."}, {"text": "Definition: Given data the maximum likelihood estimate (MLE) for the parameter p is the value of p that maximizes the likelihood P(data |p). That is, the MLE is the value of p for which the data is most likely. 100 P(55 heads|p) = ( 55 ) p55(1 \u2212 p)45."}, {"text": "n = norm( X ) returns the 2-norm or maximum singular value of matrix X , which is approximately max(svd(X)) . n = norm( X , p ) returns the p-norm of matrix X , where p is 1 , 2 , or Inf : If p = 1 , then n is the maximum absolute column sum of the matrix. If p = 2 , then n is approximately max(svd(X)) ."}, {"text": "Seriously, the p value is literally a confounded index because it reflects both the size of the underlying effect and the size of the sample. Hence any information included in the p value is ambiguous (Lang et al. 1998).  The smaller the sample, the less likely the result will be statistically significant."}]}, {"question": "How do you find the probability of a binomial random variable", "positive_ctxs": [{"text": "Key TakeawaysA Bernoulli (success-failure) experiment is performed n times, and the trials are independent.The probability of success on each trial is a constant p ; the probability of failure is q=1\u2212p q = 1 \u2212 p .The random variable X counts the number of successes in the n trials."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution.  Suppose we flip a coin repeatedly and count the number of heads (successes)."}, {"text": "The direct approximation of the binomial by the Poisson says that a binomial(n,p) random variable has approximately the same distribution as a Poisson(np) random variable when np is large."}, {"text": "The cumulative density function gives you the probability of a random variable being on or below a certain value. The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value.  A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75."}, {"text": "Similarly, the probability density function of a continuous random variable can be obtained by differentiating the cumulative distribution. The c.d.f. can be used to find out the probability of a random variable being between two values: P(s \u2264 X \u2264 t) = the probability that X is between s and t."}, {"text": "To find the expected value, E(X), or mean \u03bc of a discrete random variable X, simply multiply each value of the random variable by its probability and add the products. The formula is given as E(X)=\u03bc=\u2211xP(x)."}, {"text": "For example, a random variable could be the outcome of the roll of a die or the flip of a coin. A probability distribution is a list of all of the possible outcomes of a random variable along with their corresponding probability values."}]}, {"question": "What are Markov models used for", "positive_ctxs": [{"text": "Markov models are often used to model the probabilities of different states and the rates of transitions among them. The method is generally used to model systems. Markov models can also be used to recognize patterns, make predictions and to learn the statistics of sequential data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In other words, discriminative models are used to specify outputs based on inputs (by models such as Logistic regression, Neural networks and Random forests), while generative models generate both inputs and outputs (for example, by Hidden Markov model, Bayesian Networks and Gaussian mixture model)."}, {"text": "In other words, discriminative models are used to specify outputs based on inputs (by models such as Logistic regression, Neural networks and Random forests), while generative models generate both inputs and outputs (for example, by Hidden Markov model, Bayesian Networks and Gaussian mixture model)."}, {"text": "Hidden Markov models (HMMs) have been extensively used in biological sequence analysis.  We especially focus on three types of HMMs: the profile-HMMs, pair-HMMs, and context-sensitive HMMs."}, {"text": "Markov chains are used in a broad variety of academic fields, ranging from biology to economics. When predicting the value of an asset, Markov chains can be used to model the randomness. The price is set by a random factor which can be determined by a Markov chain."}, {"text": "Markov chains are used in a broad variety of academic fields, ranging from biology to economics. When predicting the value of an asset, Markov chains can be used to model the randomness. The price is set by a random factor which can be determined by a Markov chain."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}]}, {"question": "What are the methods to avoid overfitting in decision trees", "positive_ctxs": [{"text": "There are several approaches to avoiding overfitting in building decision trees. Pre-pruning that stop growing the tree earlier, before it perfectly classifies the training set. Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood."}, {"text": "In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration."}, {"text": "Even with the use of pre-pruning, they tend to overfit and provide poor generalization performance. Therefore, in most applications, by aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of decision trees can be substantially improved."}, {"text": "A random forest is simply a collection of decision trees whose results are aggregated into one final result. Their ability to limit overfitting without substantially increasing error due to bias is why they are such powerful models. One way Random Forests reduce variance is by training on different samples of the data."}, {"text": "A random forest is simply a collection of decision trees whose results are aggregated into one final result. Their ability to limit overfitting without substantially increasing error due to bias is why they are such powerful models. One way Random Forests reduce variance is by training on different samples of the data."}, {"text": "Neural networks are often compared to decision trees because both methods can model data that has nonlinear relationships between variables, and both can handle interactions between variables."}, {"text": "There are two main types of decision trees that are based on the target variable, i.e., categorical variable decision trees and continuous variable decision trees.Categorical variable decision tree.  Continuous variable decision tree.  Assessing prospective growth opportunities.More items"}]}, {"question": "How is analysis of variance calculated", "positive_ctxs": [{"text": "Steps for Using ANOVAStep 1: Compute the Variance Between. First, the sum of squares (SS) between is computed:  Step 2: Compute the Variance Within. Again, first compute the sum of squares within.  Step 3: Compute the Ratio of Variance Between and Variance Within. This is called the F-ratio."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data. Due to this value of denominator in the formula for variance in case of sample data is 'n-1', and it is 'n' for population data."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data."}, {"text": "Unlike range and quartiles, the variance combines all the values in a data set to produce a measure of spread.  It is calculated as the average squared deviation of each number from the mean of a data set. For example, for the numbers 1, 2, and 3 the mean is 2 and the variance is 0.667."}, {"text": "The variance (symbolized by S2) and standard deviation (the square root of the variance, symbolized by S) are the most commonly used measures of spread. We know that variance is a measure of how spread out a data set is. It is calculated as the average squared deviation of each number from the mean of a data set."}, {"text": "Analysis of variance (ANOVA) is an analysis tool used in statistics that splits an observed aggregate variability found inside a data set into two parts: systematic factors and random factors.  1\ufeff\ufeff2\ufeff ANOVA is also called the Fisher analysis of variance, and it is the extension of the t- and z-tests."}, {"text": "The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups."}, {"text": "Factorial analysis of variance (ANOVA) is a statistical procedure that allows researchers to explore the influence of two or more independent variables (factors) on a single dependent variable."}]}, {"question": "Why is stepwise regression bad", "positive_ctxs": [{"text": "A fundamental problem with stepwise regression is that some real explanatory variables that have causal effects on the dependent variable may happen to not be statistically significant, while nuisance variables may be coincidentally significant."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The reality is that stepwise regression is less effective the larger the number of potential explanatory variables. Stepwise regression does not solve the Big-Data problem of too many explanatory variables. Big Data exacerbates the failings of stepwise regression."}, {"text": "Stepwise regression is an appropriate analysis when you have many variables and you're interested in identifying a useful subset of the predictors. In Minitab, the standard stepwise regression procedure both adds and removes predictors one at a time."}, {"text": "Stepwise regression is an appropriate analysis when you have many variables and you're interested in identifying a useful subset of the predictors. In Minitab, the standard stepwise regression procedure both adds and removes predictors one at a time."}, {"text": "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater."}, {"text": "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater."}, {"text": "BACKWARD STEPWISE REGRESSION is a stepwise regression approach that begins with a full (saturated) model and at each step gradually eliminates variables from the regression model to find a reduced model that best explains the data. Also known as Backward Elimination regression."}, {"text": "Findings. A fundamental problem with stepwise regression is that some real explanatory variables that have causal effects on the dependent variable may happen to not be statistically significant, while nuisance variables may be coincidentally significant."}]}, {"question": "What is maximum entropy model in NLP", "positive_ctxs": [{"text": "The maximum entropy principle is defined as modeling a given set of data by finding the highest entropy to satisfy the constraints of our prior knowledge.  The maximum entropy model is a conditional probability model p(y|x) that allows us to predict class labels given a set of features for a given data point."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The maximum entropy principle is defined as modeling a given set of data by finding the highest entropy to satisfy the constraints of our prior knowledge.  The maximum entropy model is a conditional probability model p(y|x) that allows us to predict class labels given a set of features for a given data point."}, {"text": "Properties. The normal distribution is the only distribution whose cumulants beyond the first two (i.e., other than the mean and variance) are zero. It is also the continuous distribution with the maximum entropy for a specified mean and variance."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Among all continuous probability distributions with support [0, \u221e) and mean \u03bc, the exponential distribution with \u03bb = 1/\u03bc has the largest differential entropy. In other words, it is the maximum entropy probability distribution for a random variate X which is greater than or equal to zero and for which E[X] is fixed."}, {"text": "joint entropy is the amount of information in two (or more) random variables; conditional entropy is the amount of information in one random variable given we already know the other."}, {"text": "Maximum entropy is the state of a physical system at greatest disorder or a statistical model of least encoded information, these being important theoretical analogs."}, {"text": "The gamma distribution is the maximum entropy probability distribution (both with respect to a uniform base measure and with respect to a 1/x base measure) for a random variable X for which E[X] = k\u03b8 = \u03b1/\u03b2 is fixed and greater than zero, and E[ln(X)] = \u03c8(k) + ln(\u03b8) = \u03c8(\u03b1) \u2212 ln(\u03b2) is fixed (\u03c8 is the digamma function)."}]}, {"question": "Why F score is harmonic mean", "positive_ctxs": [{"text": "Precision and recall both have true positives in the numerator, and different denominators. To average them it really only makes sense to average their reciprocals, thus the harmonic mean. Because it punishes extreme values more.  With the harmonic mean, the F1-measure is 0."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The harmonic mean is a type of numerical average. It is calculated by dividing the number of observations by the reciprocal of each number in the series. Thus, the harmonic mean is the reciprocal of the arithmetic mean of the reciprocals."}, {"text": "The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time. A large F ratio means that the variation among group means is more than you'd expect to see by chance."}, {"text": "The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time. A large F ratio means that the variation among group means is more than you'd expect to see by chance."}, {"text": "The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time.  The P value is determined from the F ratio and the two values for degrees of freedom shown in the ANOVA table."}, {"text": "F statistic is a statistic that is determined by an ANOVA test. It determines the significance of the groups of variables. The F critical value is also known as the F \u2013statistic. The F \u2013 statistic value is obtained from the F-distribution table."}, {"text": "The test statistic used in ANOVA is Student's t. One characteristic of the F distribution is that F cannot be negative. One characteristic of the F distribution is that the computed F can only range between -1 and +1."}, {"text": "Find the F Statistic (the critical value for this test). The F statistic formula is: F Statistic = variance of the group means / mean of the within group variances. You can find the F Statistic in the F-Table."}]}, {"question": "What is Bayesian network in machine learning", "positive_ctxs": [{"text": "A Bayesian network is a compact, flexible and interpretable representation of a joint probability distribution. It is also an useful tool in knowledge discovery as directed acyclic graphs allow representing causal relations between variables. Typically, a Bayesian network is learned from data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bayesian inference is a machine learning model not as widely used as deep learning or regression models."}, {"text": "A Bayesian network (also known as a Bayes network, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).  Efficient algorithms can perform inference and learning in Bayesian networks."}, {"text": "For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms can perform inference and learning in Bayesian networks."}, {"text": "Bayesian deep learning is a field at the intersection between deep learning and Bayesian probability theory.  Bayesian deep learning models typically form uncertainty estimates by either placing distributions over model weights, or by learning a direct mapping to probabilistic outputs."}, {"text": "Bayesian networks encode the dependencies and independencies between variables. Under the causal Markov assumption, each variable in a Bayesian network is independent of its ancestors given the values of its parents."}, {"text": "The most popular supervised NLP machine learning algorithms are: Support Vector Machines. Bayesian Networks. Maximum Entropy."}, {"text": "\"A Bayesian network is a probabilistic graphical model which represents a set of variables and their conditional dependencies using a directed acyclic graph.\"  It is also called a Bayes network, belief network, decision network, or Bayesian model."}]}, {"question": "What is the meaning of estimation in statistics", "positive_ctxs": [{"text": "Estimation, in statistics, any of numerous procedures used to calculate the value of some property of a population from observations of a sample drawn from the population.  A point estimate, for example, is the single number most likely to express the value of the property."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Statistical inference consists in the use of statistics to draw conclusions about some unknown aspect of a population based on a random sample from that population.  Point estimation is discussed in the statistics section of the encyclopedia."}, {"text": "Estimation is a division of statistics and signal processing that determines the values of parameters through measured and observed empirical data. The process of estimation is carried out in order to measure and diagnose the true value of a function or a particular set of populations."}, {"text": "From Wikipedia, the free encyclopedia. In statistics and signal processing, a minimum mean square error (MMSE) estimator is an estimation method which minimizes the mean square error (MSE), which is a common measure of estimator quality, of the fitted values of a dependent variable."}, {"text": "There are two types of estimations used: point and interval. A point estimation is a type of estimation that uses a single value, a sample statistic, to infer information about the population.  Interval estimation is the range of numbers in which a population parameter lies considering margin of error."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Geometrical meaning of integration is a statement so it must be true. Another way of analysing this statement is area of a curve or the volume of a curve of revolution or area of an implicit equation of x and y.  The geometrical meaning of integration is to find the area under the corresponding curve."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}]}, {"question": "What are latent variables in research", "positive_ctxs": [{"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "The parameters are the ones that we specify a prior distribution for. The latent variables are usually the ones that we describe using a conditional distribution of the latent variable given the parameters."}, {"text": "In qualitative research no hypotheses or relationships of variables are tested. Because variables must be defined numerically in hypothesis-testing research, they cannot reflect subjective experience. This leads to hypothesis-generating research using the grounded theory method to study subjective experience directly."}, {"text": "The standard solution that psychologists take to measuring latent variables is to use a series of questions that are all designed to measure the latent variable. This is known as a multi-item scale, where an \u201citem\u201d is a question, and a \u201cscale\u201d is the resulting estimate of the latent variable."}, {"text": "Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables."}, {"text": "Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables."}]}, {"question": "What is sum rule of probability", "positive_ctxs": [{"text": "The addition law of probability (sometimes referred to as the addition rule or sum rule), states that the probability that A or B will occur is the sum of the probabilities that A will happen and that B will happen, minus the probability that both A and B will happen."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The confidence of an association rule is a percentage value that shows how frequently the rule head occurs among all the groups containing the rule body.  Thus, the confidence of a rule is the percentage equivalent of m/n, where the values are: m. The number of groups containing the joined rule head and rule body."}, {"text": "Softmax is an activation function that outputs the probability for each class and these probabilities will sum up to one. Cross Entropy loss is just the sum of the negative logarithm of the probabilities.  Therefore, Softmax loss is just these two appended together."}, {"text": "The probability of each value of the discrete random variable is between 0 and\u200b 1, inclusive, and the sum of all the probabilities is 1.  It is the expected value of a discrete random variable."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Softmax is an activation function that outputs the probability for each class and these probabilities will sum up to one.  Cross Entropy loss is just the sum of the negative logarithm of the probabilities. They are both commonly used together in classifications."}, {"text": "In short, Softmax Loss is actually just a Softmax Activation plus a Cross-Entropy Loss. Softmax is an activation function that outputs the probability for each class and these probabilities will sum up to one. Cross Entropy loss is just the sum of the negative logarithm of the probabilities."}]}, {"question": "When should I use ridge regression", "positive_ctxs": [{"text": "Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "When I calculate population variance, I then divide the sum of squared deviations from the mean by the number of items in the population (in example 1 I was dividing by 12). When I calculate sample variance, I divide it by the number of items in the sample less one. In our example 2, I divide by 99 (100 less 1)."}, {"text": "Logistic regression is a pretty flexible method. It can readily use as independent variables categorical variables. Most software that use Logistic regression should let you use categorical variables.  A single column in your model can handle as many categories as needed for a single categorical variable."}, {"text": "Basically, predicting a continuous variable is termed as regression. There are a no of regression algorithms like ridge and lasso regression you may want to check out.Linear Regression.Logistic Regression.Polynomial Regression.Stepwise Regression.Ridge Regression.Lasso Regression.ElasticNet Regression,"}, {"text": "Ridge regression does not really select variables in the many predictors situation.  Both ridge regression and the LASSO can outperform OLS regression in some predictive situations \u2013 exploiting the tradeoff between variance and bias in the mean square error."}, {"text": "3 Answers. Since your response is ordinal then you should use ordinal regression. At a very high level, the main difference ordinal regression and linear regression is that with linear regression the dependent variable is continuous and ordinal the dependent variable is ordinal."}, {"text": "Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity.  By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. It is hoped that the net effect will be to give estimates that are more reliable."}, {"text": "Ridge regression is a term used to refer to a linear regression model whose coefficients are not estimated by ordinary least squares (OLS), but by an estimator, called ridge estimator, that is biased but has lower variance than the OLS estimator."}]}, {"question": "What are different types of machine learning algorithms", "positive_ctxs": [{"text": "List of Common Machine Learning AlgorithmsLinear Regression.Logistic Regression.Decision Tree.SVM.Naive Bayes.kNN.K-Means.Random Forest.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model."}, {"text": "The different types of regression in machine learning techniques are explained below in detail:Linear Regression. Linear regression is one of the most basic types of regression in machine learning.  Logistic Regression.  Ridge Regression.  Lasso Regression.  Polynomial Regression.  Bayesian Linear Regression."}, {"text": "SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types."}, {"text": "Some common types of problems built on top of classification and regression include recommendation and time series prediction respectively. Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems. Random forest for classification and regression problems."}, {"text": "Uncertainty is a popular phenomenon in machine learning and a variety of methods to model uncertainty at different levels has been developed.  Different types of uncertainty can be observed: (i) Input data are subject to noise, outliers, and errors."}, {"text": "Unsupervised machine learning helps you to finds all kind of unknown patterns in data. Clustering and Association are two types of Unsupervised learning. Four types of clustering methods are 1) Exclusive 2) Agglomerative 3) Overlapping 4) Probabilistic."}, {"text": "Unsupervised machine learning helps you to finds all kind of unknown patterns in data. Clustering and Association are two types of Unsupervised learning. Four types of clustering methods are 1) Exclusive 2) Agglomerative 3) Overlapping 4) Probabilistic."}]}, {"question": "How do you analyze a confusion matrix", "positive_ctxs": [{"text": "How to Calculate a Confusion MatrixStep 1) First, you need to test dataset with its expected outcome values.Step 2) Predict all the rows in the test dataset.Step 3) Calculate the expected predictions and outcomes:"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing."}, {"text": "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing."}, {"text": "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing."}, {"text": "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing."}, {"text": "A confusion matrix, also known as error matrix is a table layout that is used to visualize the performance of a classification model where the true values are already known."}, {"text": "Machine learning algorithms can minimize forecasting error and do the forecast much faster and with the usage of more data. What's more, machine learning algorithms can analyze many alternative models at the same time, when in traditional econometrics you can analyze just one model at a time."}]}, {"question": "How do you protect yourself from confirmation bias", "positive_ctxs": [{"text": "How to Protect Against Confirmation Bias Find someone who disagrees with a decision you're about to make. Ask them why they disagree with you. Carefully listen to what they have to say.   Continue listening until you can honestly say, \u201cI now understand why you believe that.\u201d"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Confirmation bias can make people less likely to engage with information which challenges their views.  Even when people do get exposed to challenging information, confirmation bias can cause them to reject it and, perversely, become even more certain that their own beliefs are correct."}, {"text": "Five tips to prevent confirmation bias Encourage and carefully consider critical views on the working hypothesis. Ensure that all stakeholders examine the primary data. Do not rely on analysis and summary from a single individual. Design experiments to actually test the hypothesis."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "If you restrict yourself to linear kernels, both SVMs and LR will give almost identical performance and in some cases, LR will beat SVM.  If we compare logistic regression with SVMs with non-linear kernels, then SVMs beat LRs hands down."}, {"text": "Nonresponse bias occurs when some respondents included in the sample do not respond. The key difference here is that the error comes from an absence of respondents instead of the collection of erroneous data.  Most often, this form of bias is created by refusals to participate or the inability to reach some respondents."}]}, {"question": "How do you find the sample standard deviation of the differences", "positive_ctxs": [{"text": "To calculate the standard deviation of those numbers:Work out the Mean (the simple average of the numbers)Then for each number: subtract the Mean and square the result.Then work out the mean of those squared differences.Take the square root of that and we are done!"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The standard deviation of the sample mean \u02c9X that we have just computed is the standard deviation of the population divided by the square root of the sample size: \u221a10=\u221a20/\u221a2."}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "How to calculate margin of errorGet the population standard deviation (\u03c3) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:"}, {"text": "The population mean of the distribution of sample means is the same as the population mean of the distribution being sampled from.  Thus as the sample size increases, the standard deviation of the means decreases; and as the sample size decreases, the standard deviation of the sample means increases."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}]}, {"question": "What is latent representation", "positive_ctxs": [{"text": "The latent space representation of our data contains all the important information needed to represent our original data point. This representation must then represent the features of the original data. In other words, the model learns the data features and simplifies its representation to make it easier to analyze."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The latent space is simply a representation of compressed data in which similar data points are closer together in space. Latent space is useful for learning data features and for finding simpler representations of data for analysis."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The standard solution that psychologists take to measuring latent variables is to use a series of questions that are all designed to measure the latent variable. This is known as a multi-item scale, where an \u201citem\u201d is a question, and a \u201cscale\u201d is the resulting estimate of the latent variable."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "On a technical note, estimation of a latent variable is done by analyzing the variance and covariance of the indicators. The measurement model of a latent variable with effect indicators is the set of relationships (modeled as equations) in which the latent variable is set as the predictor of the indicators."}, {"text": "Low-rank matrix factorization (MF) is an important technique in data science. The key idea of MF is that there exists latent structures in the data, by uncovering which we could obtain a compressed representation of the data.  By properly adapting MF, we can go beyond the problem of clustering and matrix completion."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "What test should you use to determine the equality of the two sample means when the population standard deviation is unknown", "positive_ctxs": [{"text": "Aspin-Welch t-test"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "On this site, we use the normal distribution when the population standard deviation is known and the sample size is large. We might use either distribution when standard deviation is unknown and the sample size is very large."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "The population mean of the distribution of sample means is the same as the population mean of the distribution being sampled from.  Thus as the sample size increases, the standard deviation of the means decreases; and as the sample size decreases, the standard deviation of the sample means increases."}, {"text": "The t-distribution describes the standardized distances of sample means to the population mean when the population standard deviation is not known, and the observations come from a normally distributed population."}, {"text": "Particular distributions are associated with hypothesis testing. Perform tests of a population mean using a normal distribution or a Student's t-distribution. (Remember, use a Student's t-distribution when the population standard deviation is unknown and the distribution of the sample mean is approximately normal.)"}, {"text": "Mean, variance, and standard deviation The mean of the sampling distribution of the sample mean will always be the same as the mean of the original non-normal distribution. In other words, the sample mean is equal to the population mean. where \u03c3 is population standard deviation and n is sample size."}]}, {"question": "Which algorithm is used for face detection", "positive_ctxs": [{"text": "Eigenface"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Face detection is a broader term than face recognition. Face detection just means that a system is able to identify that there is a human face present in an image or video.  Face recognition can confirm identity. It is therefore used to control access to sensitive areas."}, {"text": "Face detection is a broader term than face recognition. Face detection just means that a system is able to identify that there is a human face present in an image or video.  Face recognition can confirm identity. It is therefore used to control access to sensitive areas."}, {"text": "One method of processing images is via face detection. Face detection is a branch of image processing that uses machine learning to detect faces in images. A Haar Cascade is an object detection method used to locate an object of interest in images."}, {"text": "Abstract: The Local Binary Pattern Histogram(LBPH) algorithm is a simple solution on face recognition problem, which can recognize both front face and side face.  To solve this problem, a modified LBPH algorithm based on pixel neighborhood gray median(MLBPH) is proposed."}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "Outlier detection is extensively used in a wide variety of applications such as military surveillance for enemy activities to prevent attacks, intrusion detection in cyber security, fraud detection for credit cards, insurance or health care and fault detection in safety critical systems and in various kind of images."}, {"text": "k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification."}]}, {"question": "Does sample mean affect margin of error", "positive_ctxs": [{"text": "Three things influence the margin of error in a confidence interval estimate of a population mean: sample size, variability in the population, and confidence level.  Answer: As sample size increases, the margin of error decreases. As the variability in the population increases, the margin of error increases."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The relationship between margin of error and sample size is simple: As the sample size increases, the margin of error decreases.  If you think about it, it makes sense that the more information you have, the more accurate your results are going to be (in other words, the smaller your margin of error will get)."}, {"text": "SOLUTION: sample siae =400; sample mean = 44; sample standard deviation =16. what is the margin of error? I have: 44-400/16=356/16=22.30 E=2.16/400=32/400=. 2E-4 margin or error."}, {"text": "Does not affect R2 or adjusted R2 (since these estimate the POPULATION variances which are not conditional on X)"}, {"text": "The margin of error is a statistic expressing the amount of random sampling error in the results of a survey. The larger the margin of error, the less confidence one should have that a poll result would reflect the result of a survey of the entire population."}, {"text": "When using a sample to estimate a measure of a population, statisticians do so with a certain level of confidence and with a possible margin of error. For example, if the mean of our sample is 20, we can say the true mean of the population is 20 plus-or-minus 2 with 95% confidence."}, {"text": "The standard error is a statistical term that measures the accuracy with which a sample distribution represents a population by using standard deviation. In statistics, a sample mean deviates from the actual mean of a population\u2014this deviation is the standard error of the mean."}, {"text": "The standard error is a statistical term that measures the accuracy with which a sample distribution represents a population by using standard deviation. In statistics, a sample mean deviates from the actual mean of a population\u2014this deviation is the standard error of the mean."}]}, {"question": "What is an agent and environment in artificial intelligence", "positive_ctxs": [{"text": "An agent is anything that can perceive its environment through sensors and acts upon that environment through effectors. A human agent has sensory organs such as eyes, ears, nose, tongue and skin parallel to the sensors, and other organs such as hands, legs, mouth, for effectors."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "An environment is everything in the world which surrounds the agent, but it is not a part of an agent itself. An environment can be described as a situation in which an agent is present. The environment is where agent lives, operate and provide the agent with something to sense and act upon it."}, {"text": "In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent)."}, {"text": "In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent)."}, {"text": "In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats\u2014artificial systems that exhibit complex behaviour in an agent environment.  The term is also sometimes used in ethology or animal behavior."}, {"text": "Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences."}, {"text": "An autonomous agent is an intelligent agent operating on an owner's behalf but without any interference of that ownership entity.  Non-biological examples include intelligent agents, autonomous robots, and various software agents, including artificial life agents, and many computer viruses."}, {"text": "Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation."}]}, {"question": "How do you explain linear regression in interview", "positive_ctxs": [{"text": "In technical terms, linear regression is a machine learning algorithm that finds the best linear-fit relationship on any given data, between independent and dependent variables. It is mostly done by the Sum of Squared Residuals Method."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  After fitting a linear regression model, you need to determine how well the model fits the data."}, {"text": "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  After fitting a linear regression model, you need to determine how well the model fits the data."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively."}, {"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "Simple linear regression is commonly used in forecasting and financial analysis\u2014for a company to tell how a change in the GDP could affect sales, for example. Microsoft Excel and other software can do all the calculations, but it's good to know how the mechanics of simple linear regression work."}]}, {"question": "What is least square regression analysis", "positive_ctxs": [{"text": "The \"least squares\" method is a form of mathematical regression analysis used to determine the line of best fit for a set of data, providing a visual demonstration of the relationship between the data points."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Test for Significance of Regression. The test for significance of regression in the case of multiple linear regression analysis is carried out using the analysis of variance. The test is used to check if a linear statistical relationship exists between the response variable and at least one of the predictor variables."}, {"text": "Logistic regression works like ordinary least squares regression but on the logit of the dependent variable. Discriminant analysis is really used only for categorization. Logistic regression is often used when we aren't even interested in categorization but in getting the odds ratios for each variable."}, {"text": "1 Introduction. The partial least squares (PLS) algorithm was first introduced for regression tasks and then evolved into a classification method that is well known as PLS-discriminant analysis (PLS-DA)."}, {"text": "Robust regression is an alternative to least squares regression when data is contaminated with outliers or influential observations and it can also be used for the purpose of detecting influential observations. Please note: The purpose of this page is to show how to use various data analysis commands."}]}, {"question": "How is Arima model implemented", "positive_ctxs": [{"text": "Implementing time series ARIMABrief description about ARMA, ARIMA:Step-by-step general approach of implementing ARIMA:Step 1: Load the dataset and plot the source data. (  Step 2: Apply the Augmented Dickey Fuller Test (to confirm the stationarity of data)Step 3: Run ETS Decomposition on data (To check the seasonality in data)More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In a dataset a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}, {"text": "In a dataset, a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set."}, {"text": "Inverted Dropout is how Dropout is implemented in practice in the various deep learning frameworks because it helps to define the model once and just change a parameter (the keep/drop probability) to run train and test on the same model."}, {"text": "The derivative of sigmoid(x) is defined as sigmoid(x)*(1-sigmoid(x)).  Short answer : The derivative of the sigmoid function at any is implemented as because calculating the derivative this way is computationally effective."}, {"text": "Sequential minimal optimization (SMO) is an algorithm for solving the quadratic programming (QP) problem that arises during the training of support-vector machines (SVM).  SMO is widely used for training support vector machines and is implemented by the popular LIBSVM tool."}, {"text": "In general, K-means is a heuristic algorithm that partitions a data set into K clusters by minimizing the sum of squared distance in each cluster.  In this paper, the simulation of basic k-means algorithm is done, which is implemented using Euclidian distance metric."}, {"text": "Specifically, you learned: Machine learning algorithms are procedures that are implemented in code and are run on data. Machine learning models are output by algorithms and are comprised of model data and a prediction algorithm."}]}, {"question": "How do you find the accuracy of a ROC curve", "positive_ctxs": [{"text": "Sample ROC plot: x-axis = (1-specificity), y-axis = sensitivity. The area under the ROC curve represents accuracy of a trial test. ROC curve AUC is determined by multiple cut-points of the trial test, it gives better estimate of accuracy."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "As the area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test, the areas under ROC curves are used to compare the usefulness of tests. The term ROC stands for Receiver Operating Characteristic."}, {"text": "Interpreting the ROC curve Classifiers that give curves closer to the top-left corner indicate a better performance. As a baseline, a random classifier is expected to give points lying along the diagonal (FPR = TPR). The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test."}, {"text": "The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 \u2013 FPR). Classifiers that give curves closer to the top-left corner indicate a better performance.  The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test."}, {"text": "AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability.  By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease."}, {"text": "AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability.  By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease."}, {"text": "ROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests.  In addition, the area under the ROC curve gives an idea about the benefit of using the test(s) in question."}, {"text": "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate."}]}, {"question": "Is a Poisson process a Markov chain", "positive_ctxs": [{"text": "The Poisson counting process can be viewed as a continuous-time Markov chain. Suppose that takes values in and is independent of . Define X t = X 0 + N t for t \u2208 [ 0 , \u221e ) ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain."}, {"text": "A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed."}, {"text": "A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed."}, {"text": "An (ordinary) Poisson process is a special Markov process [ref. to Stadje in this volume], in continuous time, in which the only possible jumps are to the next higher state. A Poisson process may also be viewed as a counting process that has particular, desirable, properties."}, {"text": "In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain."}, {"text": "In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain."}]}, {"question": "Is conditional probability dependent", "positive_ctxs": [{"text": "Conditional probability is probability of a second event given a first event has already occurred.  This is conditional probability with two dependent events. A dependent event is when one event influences the outcome of another event in a probability scenario."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them."}, {"text": "Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them. Sometimes this is incorrect."}, {"text": "A Generative Model \u200clearns the joint probability distribution p(x,y). It predicts the conditional probability with the help of Bayes Theorem. A Discriminative model \u200clearns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems."}, {"text": "Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2."}, {"text": "The posterior probability is one of the quantities involved in Bayes' rule. It is the conditional probability of a given event, computed after observing a second event whose conditional and unconditional probabilities were known in advance."}, {"text": "A conditional probability estimate is a probability estimate that we make given or assuming the occurrence of some other event. In this case we might start with an estimate that the probability of rain is 30% and then make a conditional probability estimate that the probability of rain given a cloudy sky is 65%."}]}, {"question": "What happens to uncertainty when you average", "positive_ctxs": [{"text": "The average value becomes more and more precise as the number of measurements N increases. Although the uncertainty of any single measurement is always \u0394\ufffd\ufffd, the uncertainty in the mean \u0394\ufffd\ufffdavg becomes smaller (by a factor of 1/ N) as more measurements are made. You measure the length of an object five times."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the best way to store data used for Natural Language Processing?stream data from disk when you can.  inverted indexes each get their own text file (more relevant for search, maybe not what you're doing)use sparse data structures (e.g. sparse matrix) as much as possible when you need to load stuff into memory."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Underfitting in Neural Networks Underfitting happens when the network is not able to generate accurate predictions on the training set\u2014not to mention the validation set."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language. NLG processes turn structured data into text."}, {"text": "Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}]}, {"question": "What is neurons and its function", "positive_ctxs": [{"text": "The neuron is the basic working unit of the brain, a specialized cell designed to transmit information to other nerve cells, muscle, or gland cells. Neurons are cells within the nervous system that transmit information to other nerve cells, muscle, or gland cells. Most neurons have a cell body, an axon, and dendrites."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "The Hidden layer of the neural network is the intermediate layer between Input and Output layer. Activation function applies on hidden layer if it is available.  Hidden nodes or hidden neurons are the neurons that are neither in the input layer nor the output layer [3]."}, {"text": "The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.  Due to this reason, during the backpropogation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated."}, {"text": "What they are & why they matter. Neural networks are computing systems with interconnected nodes that work much like neurons in the human brain. Using algorithms, they can recognize hidden patterns and correlations in raw data, cluster and classify it, and \u2013 over time \u2013 continuously learn and improve."}, {"text": "Hebbian learning is one of the oldest learning algorithms, and is based in large part on the dynamics of biological systems. A synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs."}, {"text": "Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum."}]}, {"question": "Which selection algorithm has better best case performance", "positive_ctxs": [{"text": "Selection sortClassSorting algorithmWorst-case performance\u041e(n2) comparisons, \u041e(n) swapsBest-case performance\u041e(n2) comparisons, O(1) swapsAverage performance\u041e(n2) comparisons, \u041e(n) swapsWorst-case space complexityO(1) auxiliary1 more row"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Q17. Which of the following is true about \u201cRidge\u201d or \u201cLasso\u201d regression methods in case of feature selection? \u201cRidge regression\u201d will use all predictors in final model whereas \u201cLasso regression\u201d can be used for feature selection because coefficient values can be zero."}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)"}, {"text": "Machine learning usually has to achieve multiple targets, which are often conflicting with each other. Multi-objective model selection to improve the performance of learning models, such as neural networks, support vector machines, decision trees, and fuzzy systems."}, {"text": "Deviance is a measure of error; lower deviance means better fit to data. The greater the deviance, the worse the model fits compared to the best case (saturated). Deviance is a quality-of-fit statistic for a model that is often used for statistical hypothesis testing."}, {"text": "The purpose of such selection is to determine a set of variables that will provide the best fit for the model so that accurate predictions can be made. Variable selection is one of the most difficult aspects of model building."}]}, {"question": "What does ROC mean in statistics", "positive_ctxs": [{"text": "receiver operating characteristic curve"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population."}, {"text": "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). Figure 5. AUC (Area under the ROC Curve)."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}]}, {"question": "What is a linear model used for", "positive_ctxs": [{"text": "Linear models describe a continuous response variable as a function of one or more predictor variables. They can help you understand and predict the behavior of complex systems or analyze experimental, financial, and biological data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, Poisson regression is a generalized linear model form of regression analysis used to model count data and contingency tables.  A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables."}, {"text": "Linear regression is a classical model for predicting a numerical quantity.  Coefficients of a linear regression model can be estimated using a negative log-likelihood function from maximum likelihood estimation. The negative log-likelihood function can be used to derive the least squares solution to linear regression."}, {"text": "In statistics, Poisson regression is a generalized linear model form of regression analysis used to model count data and contingency tables.  This model is popular because it models the Poisson heterogeneity with a gamma distribution."}, {"text": "Logistic regression, also called a logit model, is used to model dichotomous outcome variables. In the logit model the log odds of the outcome is modeled as a linear combination of the predictor variables."}, {"text": "Logistic regression, also called a logit model, is used to model dichotomous outcome variables. In the logit model the log odds of the outcome is modeled as a linear combination of the predictor variables."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Alternative procedures include: Different linear model: fitting a linear model with additional X variable(s) Nonlinear model: fitting a nonlinear model when the linear model is inappropriate.  Weighted least squares linear regression: dealing with unequal variances in Y by performing a weighted least squares fit."}]}, {"question": "What is the difference between word2vec and Doc2Vec", "positive_ctxs": [{"text": "1 Answer. In word2vec, you train to find word vectors and then run similarity queries between words. In doc2vec, you tag your text and you also get tag vectors.  If two authors generally use the same words then their vector will be closer."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Word2Vec, Doc2Vec and Glove are semi-supervised learning algorithms and they are Neural Word Embeddings for the sole purpose of Natural Language Processing. Specifically Word2vec is a two-layer neural net that processes text."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "The Range is the difference between the lowest and highest values. Example: In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9. So the range is 9 \u2212 3 = 6."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}, {"text": "The chief difference between MEMM and CRF is that MEMM is locally renormalized and suffers from the label bias problem, while CRFs are globally renormalized."}]}, {"question": "What is sparsity constraint", "positive_ctxs": [{"text": "A sparsity penalty term is included in the loss function to prevent the identity mapping by keeping only a selected set of neurons active at any instance.  The constraint forces the AE to represent each input using only a small number of hidden neurons."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The sparsity of a matrix can be quantified with a score, which is the number of zero values in the matrix divided by the total number of elements in the matrix. sparsity = count zero elements / total elements. 1. sparsity = count zero elements / total elements. Below is an example of a small 3 x 6 sparse matrix."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "A problem is an issue you can resolve while a constraint is an issue you cannot resolve. That is the simplest definition of these two terms. You can also define it in terms of your control over the situation. A problem is an issue where you have control over while a constraint is one where you do not have control over."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Many problems in AI can be modeled as constraint satisfaction problems (CSPs). Hence the development of effective solution techniques for CSPs is an important research problem.  Each constraint is defined over some subset of the original set of variables and restricts the values these variables can simultaneously take."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}]}, {"question": "What type of variable must the criterion be in a multiple linear regression", "positive_ctxs": [{"text": "Multiple linear regression requires at least two independent variables, which can be nominal, ordinal, or interval/ratio level variables. A rule of thumb for the sample size is that regression analysis requires at least 20 cases per independent variable in the analysis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In simple linear regression a single independent variable is used to predict the value of a dependent variable. In multiple linear regression two or more independent variables are used to predict the value of a dependent variable. The difference between the two is the number of independent variables."}, {"text": "In simple linear regression a single independent variable is used to predict the value of a dependent variable. In multiple linear regression two or more independent variables are used to predict the value of a dependent variable. The difference between the two is the number of independent variables."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}]}, {"question": "Why is the coefficient of variation better than standard deviation", "positive_ctxs": [{"text": "Advantages. The coefficient of variation is useful because the standard deviation of data must always be understood in the context of the mean of the data. In contrast, the actual value of the CV is independent of the unit in which the measurement has been taken, so it is a dimensionless number."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The coefficient of variation is a better risk measure than the standard deviation alone because the CV adjusts for the size of the project. The CV measures the standard deviation divided by the mean and therefore puts the standard deviation into context."}, {"text": "The coefficient of variation (CV) is a measure of relative variability. It is the ratio of the standard deviation to the mean (average). For example, the expression \u201cThe standard deviation is 15% of the mean\u201d is a CV."}, {"text": "The coefficient of variation (CV) is a measure of relative variability. It is the ratio of the standard deviation to the mean (average). For example, the expression \u201cThe standard deviation is 15% of the mean\u201d is a CV."}, {"text": "The coefficient of variation (CV) is the ratio of the standard deviation to the mean. The higher the coefficient of variation, the greater the level of dispersion around the mean.  The lower the value of the coefficient of variation, the more precise the estimate."}, {"text": "The coefficient of variation (CV) is the ratio of the standard deviation to the mean. The higher the coefficient of variation, the greater the level of dispersion around the mean. It is generally expressed as a percentage.  The lower the value of the coefficient of variation, the more precise the estimate."}, {"text": "The coefficient of variation represents the ratio of the standard deviation to the mean, and it is a useful statistic for comparing the degree of variation from one data series to another, even if the means are drastically different from one another."}, {"text": "The coefficient of variation represents the ratio of the standard deviation to the mean, and it is a useful statistic for comparing the degree of variation from one data series to another, even if the means are drastically different from one another."}]}, {"question": "What two ways can we do statistical inference", "positive_ctxs": [{"text": "Statistical inference can be divided into two areas: estimation and hypothesis testing. In estimation, the goal is to describe an unknown aspect of a population, for example, the average scholastic aptitude test (SAT) writing score of all examinees in the State of California in the USA."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Variational Bayesian methods are primarily used for two purposes: To provide an analytical approximation to the posterior probability of the unobserved variables, in order to do statistical inference over these variables."}, {"text": "The fundamental counting principle states that if there are p ways to do one thing, and q ways to do another thing, then there are p\u00d7q ways to do both things. possible outcomes of the experiment. The counting principle can be extended to situations where you have more than 2 choices."}, {"text": "We use factorials when we look at permutations and combinations. Permutations tell us how many different ways we can arrange things if their order matters. Combinations tells us how many ways we can choose k item from n items if their order does not matter."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Bayesian inference refers to statistical inference where uncertainty in inferences is quantified using probability.  Statistical models specify a set of statistical assumptions and processes that represent how the sample data is generated. Statistical models have a number of parameters that can be modified."}, {"text": "Causal inference is a statistical tool that enables our AI and machine learning algorithms to reason in similar ways.  We're interested in understanding how changes in our network settings affect latency, so we use causal inference to proactively choose our settings based on this knowledge."}, {"text": "Two main types of fuzzy inference systems can be implemented: Mamdani-type (1977) and Sugeno-type (1985). These two types of inference systems vary somewhat in the way outputs are determined. Mamdani-type inference expects the output membership functions to be fuzzy sets."}]}, {"question": "What is control strategy in artificial intelligence", "positive_ctxs": [{"text": "Control Strategy in Artificial Intelligence scenario is a technique or strategy, tells us about which rule has to be applied next while searching for the solution of a problem within problem space. It helps us to decide which rule has to apply next without getting stuck at any point."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}, {"text": "Artificial intelligence (AI) is a branch of computer science.  Most AI programs are not used to control robots. Even when AI is used to control robots, the AI algorithms are only part of the larger robotic system, which also includes sensors, actuators, and non-AI programming."}, {"text": "A clinical trial is a randomized controlled trial only when participants are randomly allocated to the group receiving the treatment and a control group. What participants are allocated among groups receiving different treatments the clinical trial is simply called a randomized trial."}, {"text": "Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subtopic of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem."}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}, {"text": "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception."}]}, {"question": "Can artificial intelligence replace the human intelligence", "positive_ctxs": [{"text": "Coming to the debate of Artificial Intelligence Vs Human Intelligence, recent AI achievements imitate human intelligence more closely than before, however, machines are still way beyond what human brains are capable of doing.  Meanwhile, real-world scenarios need a holistic human approach."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "To put put it bluntly, Artificial intelligence (AI) relies on machines, whereas Collective Intelligence (CI) relies on people. AI stands for the simulation of human intelligence by machines, computers or software systems.  In fact, artificial and collective intelligence can -and should \u2013 reinforce each other."}, {"text": "An artificial neural network (ANN) is the piece of a computing system designed to simulate the way the human brain analyzes and processes information. It is the foundation of artificial intelligence (AI) and solves problems that would prove impossible or difficult by human or statistical standards."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}]}, {"question": "How do you train a generative model", "positive_ctxs": [{"text": "To train a generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.) and then train a model to generate data like it. The intuition behind this approach follows a famous quote from Richard Feynman: \u201cWhat I cannot create, I do not understand.\u201d"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How to train a Machine Learning model in 5 minutesModel Naming \u2014 Give Your Model a Name: Let's start with giving your model a name, describe your model and attach tags to your model.  Data Type Selection \u2014 Choose data type(Images/Text/CSV): It's time to tell us about the type of data you want to train your model.More items"}, {"text": "A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words."}, {"text": "A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words."}, {"text": "Generative Adversarial Networks, or GANs, are a deep-learning-based generative model. More generally, GANs are a model architecture for training a generative model, and it is most common to use deep learning models in this architecture."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "A GAN is a generative model that is trained using two neural network models. One model is called the \u201cgenerator\u201d or \u201cgenerative network\u201d model that learns to generate new plausible samples.  After training, the generative model can then be used to create new plausible samples on demand."}, {"text": "According to this link LDA is a generative classifier. Also, the motto of LDA is to model a discriminant function to classify."}]}, {"question": "How do you know if a classification model is accurate", "positive_ctxs": [{"text": "Classification Accuracy It is the ratio of number of correct predictions to the total number of input samples. It works well only if there are equal number of samples belonging to each class. For example, consider that there are 98% samples of class A and 2% samples of class B in our training set."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Markov model is a state machine with the state changes being probabilities. In a hidden Markov model, you don't know the probabilities, but you know the outcomes."}, {"text": "Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "According to Investopedia, a model is considered to be robust if its output dependent variable (label) is consistently accurate even if one or more of the input independent variables (features) or assumptions are drastically changed due to unforeseen circumstances."}, {"text": "A probability-predicting regression model can be used as part of a classifier by imposing a decision rule - for example, if the probability is 50% or more, decide it's a cat.  There are also \"true\" classification algorithms, such as SVM, which only predict an outcome and do not provide a probability."}, {"text": "The determinant of a matrix is a special value that is calculated from a square matrix. It can help you determine whether a matrix has an inverse, find the area of a triangle, and let you know if the system of equations has a unique solution. Determinants are also used in calculus and linear algebra."}, {"text": "Heisenberg's uncertainty principle is a key principle in quantum mechanics. Very roughly, it states that if we know everything about where a particle is located (the uncertainty of position is small), we know nothing about its momentum (the uncertainty of momentum is large), and vice versa."}]}, {"question": "What is factor loading SEM", "positive_ctxs": [{"text": "Factor loading is basically the correlation coefficient for the variable and factor. Factor loading shows the variance explained by the variable on that particular factor. In the SEM approach, as a rule of thumb, 0.7 or higher factor loading represents that the factor extracts sufficient variance from that variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Communality value is also a deciding factor to include or exclude a variable in the factor analysis. A value of above 0.5 is considered to be ideal. But in a study, it is seen that a variable with low community value (<0.5), is contributing to a well defined factor, though loading is low."}, {"text": "Answer. A negative path loading is basically the same as a negative regression coefficient. I.e., For a path loading from X to Y it is the predicted increase in Y for a one unit increase on X holding all other variables constant. So a negative coefficient just means that as X increases, Y is predicted to decrease."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Path analysis is a special case of SEM.  Most of the models that you will see in the literature are SEM rather than path analyses. The main difference between the two types of models is that path analysis assumes that all variables are measured without error. SEM uses latent variables to account for measurement error."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "There are two types of factor analyses, exploratory and confirmatory. Exploratory factor analysis (EFA) is method to explore the underlying structure of a set of observed variables, and is a crucial step in the scale development process. The first step in EFA is factor extraction."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "What is the difference between first order logic and propositional logic", "positive_ctxs": [{"text": "Propositional Logic converts a complete sentence into a symbol and makes it logical whereas in First-Order Logic relation of a particular sentence will be made that involves relations, constants, functions, and constants."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Inductive logic programming is the subfield of machine learning that uses first-order logic to represent hypotheses and data. Because first-order logic is expressive and declarative, inductive logic programming specifically targets problems involving structured data and background knowledge."}, {"text": "Computational Logic is the process of designing and analyzing logic in computer applications. In this lesson, we'll discuss creating logic based on the statements and constraints provided. Logic in relation to computers is mainly of two types: Propositional Logic and First Order Logic(FOL)."}, {"text": "Fuzzy logic is useful for commercial and practical purposes. It can control machines and consumer products. It may not give accurate reasoning, but acceptable reasoning. Fuzzy logic helps to deal with the uncertainty in engineering."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The main difference is obviously that, in a first order reaction, the order of reaction is one by nature. A pseudo first-order reaction is second order reaction by nature but has been altered to make it a first order reaction."}, {"text": "An artificial neural network's learning rule or learning process is a method, mathematical logic or algorithm which improves the network's performance and/or training time. Usually, this rule is applied repeatedly over the network."}, {"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}]}, {"question": "How do you calculate the median", "positive_ctxs": [{"text": "MedianArrange your numbers in numerical order.Count how many numbers you have.If you have an odd number, divide by 2 and round up to get the position of the median number.If you have an even number, divide by 2. Go to the number in that position and average it with the number in the next higher position to get the median."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Arrange your set of numbers from smallest to largest. Determine which measure of central tendency you wish to calculate. The three types are mean, median and mode. To calculate the mean, add all your data and divide the result by the number of data."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How to calculate the absolute error and relative errorTo find out the absolute error, subtract the approximated value from the real one: |1.41421356237 - 1.41| = 0.00421356237.Divide this value by the real value to obtain the relative error: |0.00421356237 / 1.41421356237| = 0.298%"}, {"text": "To calculate the Sharpe ratio on a portfolio or individual investment, you first calculate the expected return for the investment. You then subtract the risk free rate from the expected return, then divide this sum by the standard deviation of the of the portfolio or individual investment. This gives you the ratio."}]}, {"question": "What is spatio temporal data mining", "positive_ctxs": [{"text": "Spatiotemporal data mining refers to the process of discovering patterns and knowledge from spatiotemporal data.  Other examples of moving-object data mining include mining periodic patterns for one or a set of moving objects, and mining trajectory patterns, clusters, models, and outliers."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Local interactions in space can give rise to large scale spatio temporal patterns (e.g. (spiral) waves, spatio-temporal chaos (turbulence), stationary (Turing-type) patterns and transitions between these modes). Their occurrence and properties are largely independent of the precise interaction structure."}, {"text": "Association rules mining is another key unsupervised data mining method, after clustering, that finds interesting associations (relationships, dependencies) in large sets of data items."}, {"text": "A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior.  Both classes of networks exhibit temporal dynamic behavior."}, {"text": "Use temporal data types to store date, time, and time-interval information. Although you can store this data in character strings, it is better to use temporal types for consistency and validation. An hour, minute, and second to six decimal places (microseconds), and the time zone offset from GMT."}, {"text": "The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or"}, {"text": "Definition: In simple words, data mining is defined as a process used to extract usable data from a larger set of any raw data. It implies analysing data patterns in large batches of data using one or more software.  Data mining is also known as Knowledge Discovery in Data (KDD)."}, {"text": "Definition: In simple words, data mining is defined as a process used to extract usable data from a larger set of any raw data. It implies analysing data patterns in large batches of data using one or more software.  Data mining is also known as Knowledge Discovery in Data (KDD)."}]}, {"question": "What is homogeneous data", "positive_ctxs": [{"text": "Homogeneous data are drawn from a single population. In other words, all outside processes that could potentially affect the data must remain constant for the complete time period of the sample."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A data set is homogeneous if it is made up of things (i.e. people, cells or traits) that are similar to each other. For example a data set made up of 20-year-old college students enrolled in Physics 101 is a homogeneous sample."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Definition: Quota sampling is a sampling methodology wherein data is collected from a homogeneous group. It involves a two-step process where two variables can be used to filter information from the population. It can easily be administered and helps in quick comparison."}, {"text": "Definition: Quota sampling is a sampling methodology wherein data is collected from a homogeneous group. It involves a two-step process where two variables can be used to filter information from the population. It can easily be administered and helps in quick comparison."}]}, {"question": "What error metric would you use to evaluate how good a binary classifier is", "positive_ctxs": [{"text": "Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "1). Now the difference is that Confusion Matrix is used to evaluate the performance of a classifier, and it tells how accurate a classifier is in making predictions about classification, and contingency table is used to evaluate association rules."}, {"text": "An ROC (Receiver Operating Characteristic) curve is a useful graphical tool to evaluate the performance of a binary classifier as its discrimination threshold is varied.  In binary classification, a collection of objects is given, and the task is to classify the objects into two groups based on their features."}, {"text": "a. If your data is labeled, but you only have a limited amount, you should use a classifier with high bias (for example, Naive Bayes). I'm guessing this is because a higher-bias classifier will have lower variance, which is good because of the small amount of data."}, {"text": "A metric is a function that is used to judge the performance of your model. Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that you may use any loss function as a metric."}, {"text": "Option D is correct. Q25. Instead of trying to achieve absolute zero error, we set a metric called bayes error which is the error we hope to achieve."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "Decision tree classifier \u2013 Decision tree classifier is a systematic approach for multiclass classification. It poses a set of questions to the dataset (related to its attributes/features). The decision tree classification algorithm can be visualized on a binary tree."}]}, {"question": "How can hidden Markov models be used to recognize cursive handwriting", "positive_ctxs": [{"text": "A method for the off-line recognition of cursive handwriting based on hidden Markov models (HMMs) is described. The features used in the HMMs are based on the arcs of skeleton graphs of the words to be recognized. An algorithm is applied to the skeleton graph of a word that extracts the edges in a particular order."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Hidden Markov Model (HMM) is a relatively simple way to model sequential data. A hidden Markov model implies that the Markov Model underlying the data is hidden or unknown to you. More specifically, you only know observational data and not information about the states."}, {"text": "Markov chains are used in a broad variety of academic fields, ranging from biology to economics. When predicting the value of an asset, Markov chains can be used to model the randomness. The price is set by a random factor which can be determined by a Markov chain."}, {"text": "Markov chains are used in a broad variety of academic fields, ranging from biology to economics. When predicting the value of an asset, Markov chains can be used to model the randomness. The price is set by a random factor which can be determined by a Markov chain."}, {"text": "The primary advantage of CRFs over hidden Markov models is their conditional nature, resulting in the relaxation of the independence assumptions required by HMMs in order to ensure tractable inference."}, {"text": "Unlikely to CNN, RNN learns to recognize image features across time. Although RNN can be used for image classification theoretically, only a few researches about RNN image classifier can be found."}, {"text": "A hidden Markov model (HMM) is a statistical model that can be used to describe the evolution of observable events that depend on internal factors, which are not directly observable. We call the observed event a `symbol' and the invisible factor underlying the observation a `state'."}, {"text": "2. HIDDEN MARKOV MODELS. A hidden Markov model (HMM) is a statistical model that can be used to describe the evolution of observable events that depend on internal factors, which are not directly observable. We call the observed event a `symbol' and the invisible factor underlying the observation a `state'."}]}, {"question": "What are the consequences of Heteroscedasticity for regression analysis using the OLS estimator", "positive_ctxs": [{"text": "Heteroskedasticity has serious consequences for the OLS estimator. Although the OLS estimator remains unbiased, the estimated SE is wrong. Because of this, confidence intervals and hypotheses tests cannot be relied on. In addition, the OLS estimator is no longer BLUE."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Consequences of Heteroscedasticity The OLS estimators and regression predictions based on them remains unbiased and consistent. The OLS estimators are no longer the BLUE (Best Linear Unbiased Estimators) because they are no longer efficient, so the regression predictions will be inefficient too."}, {"text": "In ordinary least squares, the relevant assumption of the classical linear regression model is that the error term is uncorrelated with the regressors. The presence of omitted-variable bias violates this particular assumption. The violation causes the OLS estimator to be biased and inconsistent."}, {"text": "The consequences of autocorrelated disturbances are that the t, F and chi-squared distributions are invalid; there is inefficient estimation and prediction of the regression vector; the usual formulae often underestimate the sampling variance of the regression vector; and the regression vector is biased and"}, {"text": "In the presence of heteroskedasticity, there are two main consequences on the least squares estimators: The least squares estimator is still a linear and unbiased estimator, but it is no longer best. That is, there is another estimator with a smaller variance."}, {"text": "Introduction. Linear regression and logistic regression are two types of regression analysis techniques that are used to solve the regression problem using machine learning. They are the most prominent techniques of regression."}, {"text": "Test for Significance of Regression. The test for significance of regression in the case of multiple linear regression analysis is carried out using the analysis of variance. The test is used to check if a linear statistical relationship exists between the response variable and at least one of the predictor variables."}, {"text": "By Jim Frost 45 Comments. Heteroscedasticity means unequal scatter. In regression analysis, we talk about heteroscedasticity in the context of the residuals or error term. Specifically, heteroscedasticity is a systematic change in the spread of the residuals over the range of measured values."}]}, {"question": "What is the target variable", "positive_ctxs": [{"text": "Target variable, in the machine learning context is the variable that is or should be the output. For example it could be binary 0 or 1 if you are classifying or it could be a continuous variable if you are doing a regression. In statistics you also refer to it as the response variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values. Below is a plot of an MSE function where the true target value is 100, and the predicted values range between -10,000 to 10,000."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}, {"text": "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable)."}]}, {"question": "What model is best for forecasting", "positive_ctxs": [{"text": "A causal model is the most sophisticated kind of forecasting tool. It expresses mathematically the relevant causal relationships, and may include pipeline considerations (i.e., inventories) and market survey information. It may also directly incorporate the results of a time series analysis."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A moving average is a technique to get an overall idea of the trends in a data set; it is an average of any subset of numbers. The moving average is extremely useful for forecasting long-term trends. You can calculate it for any period of time.  Moving averages are usually plotted and are best visualized."}, {"text": "Predictive analytics is the process of using data analytics to make predictions based on data. This process uses data along with analysis, statistics, and machine learning techniques to create a predictive model for forecasting future events."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures."}, {"text": "An autoregressive (AR) model predicts future behavior based on past behavior. It's used for forecasting when there is some correlation between values in a time series and the values that precede and succeed them.  Where simple linear regression and AR models differ is that Y is dependent on X and previous values for Y."}, {"text": "Deviance is a measure of error; lower deviance means better fit to data. The greater the deviance, the worse the model fits compared to the best case (saturated). Deviance is a quality-of-fit statistic for a model that is often used for statistical hypothesis testing."}, {"text": "Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values."}]}, {"question": "How do you find the discrete random variable", "positive_ctxs": [{"text": "Means and Variances of Random Variables: The mean of a discrete random variable, X, is its weighted average. Each value of X is weighted by its probability. To find the mean of X, multiply each value of X by its probability, then add all the products. The mean of a random variable X is called the expected value of X."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To find the expected value, E(X), or mean \u03bc of a discrete random variable X, simply multiply each value of the random variable by its probability and add the products. The formula is given as E(X)=\u03bc=\u2211xP(x)."}, {"text": "Note that the CDF gives us P(X\u2264x). To find P(X<x), for a discrete random variable, we can simply write P(X<x)=P(X\u2264x)\u2212P(X=x)=FX(x)\u2212PX(x). Let X be a discrete random variable with range RX={1,2,3,}. Suppose the PMF of X is given by PX(k)=12k for k=1,2,3,"}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "So by the definition of discrete and continuous random variables, a random variable cannot be both discrete and continuous. No. For a random variable to be discrete, there must a countable sequence such that ."}, {"text": "The probability of each value of the discrete random variable is between 0 and\u200b 1, inclusive, and the sum of all the probabilities is 1.  It is the expected value of a discrete random variable."}, {"text": "Now, let the random variable X represent the number of Heads that result from this experiment. The random variable X can only take on the values 0, 1, or 2, so it is a discrete random variable."}]}, {"question": "What is the intuition behind linear regression", "positive_ctxs": [{"text": "Linear Regression, intuitively is a regression algorithm with a Linear approach. We try to predict a continuous value of a given data point by generalizing on the data that we have in hand. The linear part indicates that we are using a linear approach in generalizing over the data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Decision trees are a classic machine learning technique. The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree. By Narendra Nath Joshi, Carnegie Mellon.  The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree."}, {"text": "Usually, people use the cosine similarity as a similarity metric between vectors. Now, the distance can be defined as 1-cos_similarity. The intuition behind this is that if 2 vectors are perfectly the same then similarity is 1 (angle=0) and thus, distance is 0 (1-1=0)."}, {"text": "The intuition is simple projection. This picture is from wiki. is the observed response and we predict by the linear combination of the explanatory variables which in inside the vector space ."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "We all know the forward pass of a Convolutional layer uses Convolutions. But, the backward pass during Backpropagation also uses Convolutions! So, let us dig in and start with understanding the intuition behind Backpropagation."}, {"text": "Consider that:You choose door 1. Monty shows you a goat behind door 2.If the car is behind door 1, Monty will not choose it.  If the car is behind door 2, Monty will always open door 3, as he never reveals the car.If the car is behind door 3, Monty will open door 2 100% of the time."}, {"text": "In statistics, Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference."}]}, {"question": "Which techniques can be used for normalization in text mining", "positive_ctxs": [{"text": "Lemmatization and stemming are the techniques of keyword normalization, while Levenshtein and Soundex are techniques of string matching."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Important Data mining techniques are Classification, clustering, Regression, Association rules, Outer detection, Sequential Patterns, and prediction. R-language and Oracle Data mining are prominent data mining tools. Data mining technique helps companies to get knowledge-based information."}, {"text": "Before getting features, various image preprocessing techniques like binarization, thresholding, resizing, normalization etc. are applied on the sampled image. After that, feature extraction techniques are applied to get features that will be useful in classifying and recognition of images."}, {"text": "Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection. It can also be thought of as a form of text mining \u2013 a way to obtain recurring patterns of words in textual material."}, {"text": "Q17. Which of the following is true about \u201cRidge\u201d or \u201cLasso\u201d regression methods in case of feature selection? \u201cRidge regression\u201d will use all predictors in final model whereas \u201cLasso regression\u201d can be used for feature selection because coefficient values can be zero."}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "Text mining (also referred to as text analytics) is an artificial intelligence (AI) technology that uses natural language processing (NLP) to transform the free (unstructured) text in documents and databases into normalized, structured data suitable for analysis or to drive machine learning (ML) algorithms."}, {"text": "N-grams of texts are extensively used in text mining and natural language processing tasks. They are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios)."}]}, {"question": "What is the difference between decision tree and logistic regression", "positive_ctxs": [{"text": "Decision Trees bisect the space into smaller and smaller regions, whereas Logistic Regression fits a single line to divide the space exactly into two.  A single linear boundary can sometimes be limiting for Logistic Regression."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes."}, {"text": "Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The general regression tree building methodology allows input variables to be a mixture of continuous and categorical variables. A decision tree is generated when each decision node in the tree contains a test on some input variable's value. The terminal nodes of the tree contain the predicted output variable values."}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}]}, {"question": "Can test error be lower than training error", "positive_ctxs": [{"text": "You description is confusing, but it is totally possible to have test error both lower and higher than training error. A lower training error is expected when a method easily overfits to the training data, yet, poorly generalizes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Since is more degree 4 will be more complex(overfit the data) than the degree 3 model so it will again perfectly fit the data. In such case training error will be zero but test error may not be zero."}, {"text": "It is easier to reject the null hypothesis with a one-tailed than with a two-tailed test as long as the effect is in the specified direction. Therefore, one-tailed tests have lower Type II error rates and more power than do two-tailed tests."}, {"text": "approximation of the expected error called the empirical error which is the average error on the. training set. Given a function f, a loss function V , and a training set S consisting of n data points, the empirical error of f is: IS[f] ="}, {"text": "in a test involving multiple comparisons, the probability of making at least one Type I error over an entire research study. The experiment-wise error rate differs from the testwise error rate, which is the probability of making a Type I error when performing a specific test or comparison."}, {"text": "Training loss is the error on the training set of data. Validation loss is the error after running the validation set of data through the trained network. Train/valid is the ratio between the two. Unexpectedly, as the epochs increase both validation and training error drop."}, {"text": "Random forest will reduce variance part of error rather than bias part, so on a given training data set decision tree may be more accurate than a random forest. But on an unexpected validation data set, Random forest always wins in terms of accuracy."}, {"text": "A type II error produces a false negative, also known as an error of omission. For example, a test for a disease may report a negative result, when the patient is, in fact, infected. This is a type II error because we accept the conclusion of the test as negative, even though it is incorrect."}]}, {"question": "What are the 3 laws of probability", "positive_ctxs": [{"text": "There are three basic rules associated with probability: the addition, multiplication, and complement rules. The addition rule is used to calculate the probability of event A or event B happening; we express it as: P(A or B) = P(A) + P(B) - P(A and B)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Divide the number of events by the number of possible outcomes. This will give us the probability of a single event occurring. In the case of rolling a 3 on a die, the number of events is 1 (there's only a single 3 on each die), and the number of outcomes is 6."}, {"text": "A discrete distribution is a statistical distribution that shows the probabilities of discrete (countable) outcomes, such as 1, 2, 3  Overall, the concepts of discrete and continuous probability distributions and the random variables they describe are the underpinnings of probability theory and statistical analysis."}, {"text": "Example 1: Fair Dice Roll The number of desired outcomes is 3 (rolling a 2, 4, or 6), and there are 6 outcomes in total. The a priori probability for this example is calculated as follows: A priori probability = 3 / 6 = 50%. Therefore, the a priori probability of rolling a 2, 4, or 6 is 50%."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Laws that concern data are highly relevant for AI, since those laws can impact the use and growth of AI systems.  However, no countries yet have specific laws in place around ethical and responsible AI. Time will tell whether or not companies will self-monitor or if governments will step in to more formally regulate."}, {"text": "How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together."}]}, {"question": "What are the odds of a false positive test", "positive_ctxs": [{"text": "Most home pregnancy tests are reliable, for example Clearblue's tests have an accuracy of over 99% from the day you expect your period, and while it's possible a test showing a negative result is wrong, particularly if you're testing early, getting a false positive is extremely rare."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The false discovery rate is the ratio of the number of false positive results to the number of total positive test results. Out of 10,000 people given the test, there are 450 true positive results (box at top right) and 190 false positive results (box at bottom right) for a total of 640 positive results."}, {"text": "An example of a false positive is when a particular test designed to detect melanoma, a type of skin cancer , tests positive for the disease, even though the person does not have cancer."}, {"text": "Statistical power, or the power of a hypothesis test is the probability that the test correctly rejects the null hypothesis. That is, the probability of a true positive result.  statistical power is the probability that a test will correctly reject a false null hypothesis."}, {"text": "The test does not produce false negatives (that is, anyone who has the disease will test positive for it), but the false positive rate is 5% (that is, about 5% of people who take the test will test positive, even though they do not have the disease). Suppose a randomly selected person takes the test and tests positive."}, {"text": "Recall and True Positive Rate (TPR) are exactly the same. So the difference is in the precision and the false positive rate.  While precision measures the probability of a sample classified as positive to actually be positive, the false positive rate measures the ratio of false positives within the negative samples."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}, {"text": "The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative."}]}, {"question": "What are neurons in artificial neural networks", "positive_ctxs": [{"text": "Within an artificial neural network, a neuron is a mathematical function that model the functioning of a biological neuron. Typically, a neuron compute the weighted average of its input, and this sum is passed through a nonlinear function, often called activation function, such as the sigmoid."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation."}, {"text": "Unlike humans, artificial neural networks are fed with massive amount of data to learn.  Also, real neurons do not stay on until the inputs change and the outputs may encode information using complex pulse arrangements."}, {"text": "Google uses artificial neural networks to power voice search."}, {"text": "7.2. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems. Radial basis function networks are distinguished from other neural networks due to their universal approximation and faster learning speed."}, {"text": "The ways in which they function Another fundamental difference between traditional computers and artificial neural networks is the way in which they function. While computers function logically with a set of rules and calculations, artificial neural networks can function via images, pictures, and concepts."}, {"text": "What they are & why they matter. Neural networks are computing systems with interconnected nodes that work much like neurons in the human brain. Using algorithms, they can recognize hidden patterns and correlations in raw data, cluster and classify it, and \u2013 over time \u2013 continuously learn and improve."}, {"text": "Adaptive artificial neural networks are a class of networks used in dynamic environments. They are characterized by online learning. A number of techniques are used to provide adaptability to neural networks: adaptation by weight modification, by neuronal property modification, and by network structure modification."}]}, {"question": "When did AI start and what did it involve", "positive_ctxs": [{"text": "It began with the \u201cheartless\u201d Tin man from the Wizard of Oz and continued with the humanoid robot that impersonated Maria in Metropolis. By the 1950s, we had a generation of scientists, mathematicians, and philosophers with the concept of artificial intelligence (or AI) culturally assimilated in their minds."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The top-1 error:- The percentage of time that the classifier did not give the correct class the highest probability score. The top-5 error:- The percentage of time that the classifier did not involve the correct class among the top 5 probabilities or guesses."}, {"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for\u2014tasks that involve creativity and empathy among others."}, {"text": "Examples of sampling bias include self-selection, pre-screening of trial participants, discounting trial subjects/tests that did not run to completion and migration bias by excluding subjects who have recently moved into or out of the study area."}]}, {"question": "What is meant by training data set", "positive_ctxs": [{"text": "A training dataset is a dataset of examples used during the learning process and is used to fit the parameters (e.g., weights) of, for example, a classifier."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived."}, {"text": "Definition. In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived.  Model validation is carried out after model training."}, {"text": "The training data is an initial set of data used to help a program understand how to apply technologies like neural networks to learn and produce sophisticated results.  Training data is also known as a training set, training dataset or learning set."}, {"text": "The performance of deep learning neural networks often improves with the amount of data available. Data augmentation is a technique to artificially create new training data from existing training data. This means, variations of the training set images that are likely to be seen by the model."}, {"text": "Training loss is the error on the training set of data. Validation loss is the error after running the validation set of data through the trained network. Train/valid is the ratio between the two. Unexpectedly, as the epochs increase both validation and training error drop."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Using the entire training set is just using a very large minibatch size, where the size of your minibatch is limited by the amount you spend on data collection, rather than the amount you spend on computation."}]}, {"question": "What factors inhibit collective intelligence", "positive_ctxs": [{"text": "What factors inhibit collective intelligence?In-group bias.  Out-group homogeneity bias.  Groupthink, bandwagon effect, herd behavior.  Facilitation and loafing .  Group polarization.  Biased use of information and the common knowledge effect.  Risky shift.  Distortions in multi-level group decisions."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Collective intelligence (CI) is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals and appears in consensus decision making."}, {"text": "To put put it bluntly, Artificial intelligence (AI) relies on machines, whereas Collective Intelligence (CI) relies on people. AI stands for the simulation of human intelligence by machines, computers or software systems.  In fact, artificial and collective intelligence can -and should \u2013 reinforce each other."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Artificial intelligence has close connections with philosophy because both use concepts that have the same names and these include intelligence, action, consciousness, epistemology, and even free will.  These factors contributed to the emergence of the philosophy of artificial intelligence."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Regression analysis is a reliable method of identifying which variables have impact on a topic of interest. The process of performing a regression allows you to confidently determine which factors matter most, which factors can be ignored, and how these factors influence each other."}, {"text": "Groupthink can lead collective rationalization, lack of personal accountability and pressure to acquiesce. Groupthink is a common factor in bad decision-making and serious ethical breaches.  They take precautions to prevent groupthink from taking hold."}]}, {"question": "What is artificial intelligence machine learning and deep learning", "positive_ctxs": [{"text": "Artificial intelligence is imparting a cognitive ability to a machine.  The idea behind machine learning is that the machine can learn without human intervention. The machine needs to find a way to learn how to solve a task given the data. Deep learning is the breakthrough in the field of artificial intelligence."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}, {"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}, {"text": "Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network."}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}, {"text": "\"AI is a computer system able to perform tasks that ordinarily require human intelligence Many of these artificial intelligence systems are powered by machine learning, some of them are powered by deep learning and some of them are powered by very boring things like rules.\""}, {"text": "Deep learning (sometimes known as deep structured learning) is a subset of machine learning, where machines employ artificial neural networks to process information. Inspired by biological nodes in the human body, deep learning helps computers to quickly recognize and process images and speech."}]}, {"question": "Why do we use t distribution instead of Z", "positive_ctxs": [{"text": "Like z-scores, t-scores are also a conversion of individual scores into a standard form. However, t-scores are used when you don't know the population standard deviation; You make an estimate by using your sample."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "Lets do this step by step:Step 1: find the mean.Step 2: fin the standard deviation of the mean (using the population SD)Step 3: find the Z score.Step 4: compare to the critical Z score. From the stated hypothesis, we know that we are dealing with a 1-tailed hypothesis test.  Step 4 : compare to the critical Z score."}, {"text": "The shape of the t distribution depends on the degrees of freedom (df) that went into the estimate of the standard deviation. With very few degrees of freedom, the t distribution is very leptokurtic. With 100 or more degrees of freedom, the t distribution is almost indistinguishable from the normal distribution."}, {"text": "For a large sample size, Sample Variance will be a better estimate of Population variance so even if population variance is unknown, we can use the Z test using sample variance. Similarly, for a Large Sample, we have a high degree of freedom."}, {"text": "The exponential distribution is a continuous probability distribution used to model the time we need to wait before a given event occurs. It is the continuous counterpart of the geometric distribution, which is instead discrete."}, {"text": "Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point.  Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood)."}, {"text": "The decision rule is: Reject H0 if Z < 1.645. The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960. The complete table of critical values of Z for upper, lower and two-tailed tests can be found in the table of Z values to the right in \"Other Resources.\""}]}, {"question": "What is the need of dimensionality reduction in data mining", "positive_ctxs": [{"text": "Dimensionality reduction is the process of reducing the number of random variables or attributes under consideration. High-dimensionality data reduction, as part of a data pre-processing-step, is extremely important in many real-world applications."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A relatively new method of dimensionality reduction is the autoencoder. Autoencoders are a branch of neural network which attempt to compress the information of the input variables into a reduced dimensional space and then recreate the input data set.  This is where the information from the input has been compressed."}, {"text": "I daresay that dimensionality reduction is necessary when we are lacking an acceptable balance between bias and variance. Some learning algorithms have some kind of 'built in' dimensionality reduction like the Relevance Vector Machine or Random Forests (to name two that are widely used)."}, {"text": "Abstract. The goal of statistical pattern feature extraction (SPFE) is 'low loss dimension reduction'. As the key link of pattern recognition, dimension reduction has become the research hot spot and difficulty in the fields of pattern recognition, machine learning, data mining and so on."}, {"text": "Dimensionality reduction refers to techniques for reducing the number of input variables in training data. When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the \u201cessence\u201d of the data."}, {"text": "Feature extraction is a type of dimensionality reduction where a large number of pixels of the image are efficiently represented in such a way that interesting parts of the image are captured effectively. From: Sensors for Health Monitoring, 2019."}, {"text": "Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. A characteristic of these large data sets is a large number of variables that require a lot of computing resources to process."}, {"text": "The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or"}]}, {"question": "What is singular value decomposition of a matrix", "positive_ctxs": [{"text": "In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix that generalizes the eigendecomposition of a square normal matrix to any. matrix via an extension of the polar decomposition."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Rank-reduced singular value decomposition T is a computed m by r matrix of term vectors where r is the rank of A\u2014a measure of its unique dimensions \u2264 min(m,n). S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors."}, {"text": "The singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values.  The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning."}, {"text": "SVD is the decomposition of a matrix A into 3 matrices \u2013 U, S, and V. S is the diagonal matrix of singular values. Think of singular values as the importance values of different features in the matrix. The rank of a matrix is a measure of the unique information stored in a matrix."}, {"text": "The purpose of singular value decomposition is to reduce a dataset containing a large number of values to a dataset containing significantly fewer values, but which still contains a large fraction of the variability present in the original data."}, {"text": "Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text."}, {"text": "A local minimum is a suboptimal equilibrium point at which system error is non-zero and the hidden output matrix is singular [12]. The complex problem which has a large number of patterns needs as many hidden nodes as patterns in order not to cause a singular hidden output matrix."}, {"text": "A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is zero.  Non-square matrices (m-by-n matrices for which m \u2260 n) do not have an inverse. However, in some cases such a matrix may have a left inverse or right inverse."}]}, {"question": "What is the difference between LDA and PCA", "positive_ctxs": [{"text": "Both LDA and PCA are linear transformation techniques: LDA is a supervised whereas PCA is unsupervised \u2013 PCA ignores class labels.  Remember that LDA makes assumptions about normally distributed classes and equal class covariances."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Both PLS and PCA are used for dimension reduction. Partial Least Squares, use the annotated label to maximize inter-class variance.  Principal components are focus on maximize correlation. The main difference is that the PCA is unsupervised method and PLS is supervised method."}, {"text": "LDA (Linear Discriminant Analysis) is used when a linear boundary is required between classifiers and QDA (Quadratic Discriminant Analysis) is used to find a non-linear boundary between classifiers. LDA and QDA work better when the response classes are separable and distribution of X=x for all class is normal."}, {"text": "LDA is a parametric model, and the parameter is number of topics."}, {"text": "Some of the algorithms used in image recognition (Object Recognition, Face Recognition) are SIFT (Scale-invariant Feature Transform), SURF (Speeded Up Robust Features), PCA (Principal Component Analysis), and LDA (Linear Discriminant Analysis)."}, {"text": "The difference between factor analysis and principal component analysis.  Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}]}, {"question": "How do you explain gradient descent", "positive_ctxs": [{"text": "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used."}, {"text": "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning."}, {"text": "Gradient descent is a simple optimization procedure that you can use with many machine learning algorithms.  Stochastic gradient descent refers to calculating the derivative from each training data instance and calculating the update immediately."}, {"text": "Delta learning does this using the difference between a target activation and an actual obtained activation. Using a linear activation function, network connections are adjusted. Another way to explain the Delta rule is that it uses an error function to perform gradient descent learning."}, {"text": "According to a senior data scientist, one of the distinct advantages of using Stochastic Gradient Descent is that it does the calculations faster than gradient descent and batch gradient descent. However, gradient descent is the best approach if one wants a speedier result."}, {"text": "The delta rule is a straight-forward application of gradient descent (i.e. hill climbing), and is easy to do because in a neural network with a single hidden layer, the neurons have direct access to the error signal."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}]}, {"question": "Which algorithm is used for classification", "positive_ctxs": [{"text": "When most dependent variables are numeric, logistic regression and SVM should be the first try for classification. These models are easy to implement, their parameters easy to tune, and the performances are also pretty good. So these models are appropriate for beginners."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability."}, {"text": "\u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems.  Support Vectors are simply the co-ordinates of individual observation."}, {"text": "KNN algorithm is one of the simplest classification algorithm. Even with such simplicity, it can give highly competitive results. KNN algorithm can also be used for regression problems."}, {"text": "datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris."}, {"text": "Random Forest Algorithm The Random Forest ML Algorithm is a versatile supervised learning algorithm that's used for both classification and regression analysis tasks."}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}, {"text": "A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems \u2014 yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions."}]}, {"question": "How do you interpret the coefficient in Poisson regression", "positive_ctxs": [{"text": "In the discussion above, Poisson regression coefficients were interpreted as the difference between the log of expected counts, where formally, this can be written as \u03b2 = log( \u03bcx+1) \u2013 log( \u03bcx ), where \u03b2 is the regression coefficient, \u03bc is the expected count and the subscripts represent where the predictor variable, say"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "We can interpret the Poisson regression coefficient as follows: for a one unit change in the predictor variable, the difference in the logs of expected counts is expected to change by the respective regression coefficient, given the other predictor variables in the model are held constant."}, {"text": "Analysis methods you might considerNegative binomial regression \u2013 Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean.  Poisson regression \u2013 Poisson regression is often used for modeling count data.More items"}, {"text": "But severe multicollinearity is a major problem, because it increases the variance of the regression coefficients, making them unstable. The more variance they have, the more difficult it is to interpret the coefficients.  You see a positive regression coefficient when the response should decrease as X increases."}, {"text": "Poisson regression \u2013 Poisson regression is often used for modeling count data. Poisson regression has a number of extensions useful for count models. Negative binomial regression \u2013 Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean."}, {"text": "Poisson regression \u2013 Poisson regression is often used for modeling count data. Poisson regression has a number of extensions useful for count models. Negative binomial regression \u2013 Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "Introduction to Poisson Regression Poisson regression is also a type of GLM model where the random component is specified by the Poisson distribution of the response variable which is a count. When all explanatory variables are discrete, log-linear model is equivalent to poisson regression model."}]}, {"question": "What is minimum variance of an estimator", "positive_ctxs": [{"text": "In statistics a minimum-variance unbiased estimator (MVUE) or uniformly minimum-variance unbiased estimator (UMVUE) is an unbiased estimator that has lower variance than any other unbiased estimator for all possible values of the parameter."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Cram\u00e9r-Rao Inequality provides a lower bound for the variance of an unbiased estimator of a parameter. It allows us to conclude that an unbiased estimator is a minimum variance unbiased estimator for a parameter."}, {"text": "The Cram\u00e9r-Rao Inequality provides a lower bound for the variance of an unbiased estimator of a parameter. It allows us to conclude that an unbiased estimator is a minimum variance unbiased estimator for a parameter."}, {"text": "Sample variance Concretely, the naive estimator sums the squared deviations and divides by n, which is biased.  The sample mean, on the other hand, is an unbiased estimator of the population mean \u03bc. Note that the usual definition of sample variance is. , and this is an unbiased estimator of the population variance."}, {"text": "From Wikipedia, the free encyclopedia. In statistics and signal processing, a minimum mean square error (MMSE) estimator is an estimation method which minimizes the mean square error (MSE), which is a common measure of estimator quality, of the fitted values of a dependent variable."}, {"text": "The variance of a set of numbers is the mean squared deviation from the mean. It is a measure of how spread out the set of numbers is.  The estimation variance is the variance of that large set of values. It measures how much, well, variance there is in an estimator from sample to sample."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}, {"text": "The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance."}]}, {"question": "What are generative models in deep learning", "positive_ctxs": [{"text": "Generative model is a class of models for Unsupervised learning where given training data our goal is to try and generate new samples from the same distribution. To train a Generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.)"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Generative Adversarial Networks, or GANs, are a deep-learning-based generative model. More generally, GANs are a model architecture for training a generative model, and it is most common to use deep learning models in this architecture."}, {"text": "The deep convolutional generative adversarial network, or DCGAN for short, is an extension of the GAN architecture for using deep convolutional neural networks for both the generator and discriminator models and configurations for the models and training that result in the stable training of a generator model."}, {"text": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks."}, {"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "Bayesian deep learning is a field at the intersection between deep learning and Bayesian probability theory.  Bayesian deep learning models typically form uncertainty estimates by either placing distributions over model weights, or by learning a direct mapping to probabilistic outputs."}, {"text": "A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  Restricted Boltzmann machines can also be used in deep learning networks."}, {"text": "A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  Restricted Boltzmann machines can also be used in deep learning networks."}]}, {"question": "Is Chi square bivariate analysis", "positive_ctxs": [{"text": "The chi-square test is a hypothesis test designed to test for a statistically significant relationship between nominal and ordinal variables organized in a bivariate table. In other words, it tells us whether two variables are independent of one another.  The chi-square test is sensitive to sample size."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "All Answers (6) Chi square test requires 2 categorical variables. T test requires 1 categorical and 1 continuous variables. You can't use them interchangeably."}, {"text": "Chi Square distributions are positively skewed, with the degree of skew decreasing with increasing degrees of freedom. As the degrees of freedom increases, the Chi Square distribution approaches a normal distribution. Figure 1 shows density functions for three Chi Square distributions."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Chi Square distributions are positively skewed, with the degree of skew decreasing with increasing degrees of freedom. As the degrees of freedom increases, the Chi Square distribution approaches a normal distribution."}, {"text": "Bivariate analysis means the analysis of bivariate data. It is one of the simplest forms of statistical analysis, used to find out if there is a relationship between two sets of values. It usually involves the variables X and Y. Univariate analysis is the analysis of one (\u201cuni\u201d) variable."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.  Like univariate analysis, bivariate analysis can be descriptive or inferential."}, {"text": "Bivariate analysis means the analysis of bivariate data. It is one of the simplest forms of statistical analysis, used to find out if there is a relationship between two sets of values. It usually involves the variables X and Y."}]}, {"question": "What is the kurtosis of a normal distribution", "positive_ctxs": [{"text": "The kurtosis of any univariate normal distribution is 3. It is common to compare the kurtosis of a distribution to this value. Distributions with kurtosis less than 3 are said to be platykurtic, although this does not imply the distribution is \"flat-topped\" as is sometimes stated."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The normal curve is called Mesokurtic curve. If the curve of a distribution is peaked than a normal or mesokurtic curve then it is referred to as a Leptokurtic curve. If a curve is less peaked than a normal curve, it is called as a Platykurtic curve. That's why kurtosis of normal distribution equal to three."}, {"text": "The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution."}, {"text": "The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution."}, {"text": "The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "In a normal distribution the mean is zero and the standard deviation is 1. It has zero skew and a kurtosis of 3. Normal distributions are symmetrical, but not all symmetrical distributions are normal."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}]}, {"question": "How do you calculate Hyperplane", "positive_ctxs": [{"text": "The equation of a hyperplane is w \u00b7 x + b = 0, where w is a vector normal to the hyperplane and b is an offset."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Generally a cosine similarity between two documents is used as a similarity measure of documents. In Java, you can use Lucene (if your collection is pretty large) or LingPipe to do this. The basic concept would be to count the terms in every document and calculate the dot product of the term vectors."}, {"text": "2 Answers. If M is your matrix, then it represents a linear f:Rn\u2192Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus \u2202M\u2202T is just the derivative of the vector MT, which you do component-wise."}, {"text": "To assess which word2vec model is best, simply calculate the distance for each pair, do it 200 times, sum up the total distance, and the smallest total distance will be your best model."}]}, {"question": "What are latent semantic keywords", "positive_ctxs": [{"text": "LSI keywords are simply words that are frequently found together because they share the same context. For example, \u201cApple\u201d and \u201ciTunes\u201d are LSI keywords because they share the same context and are frequently found together. But they are not synonyms."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Latent semantic indexing (LSI) is a concept used by search engines to discover how a term and content work together to mean the same thing, even if they do not share keywords or synonyms.  Basically, though, you often need specific keywords on your pages to boost your website traffic."}, {"text": "Latent semantic indexing (LSI) is a concept used by search engines to discover how a term and content work together to mean the same thing, even if they do not share keywords or synonyms.  Basically, though, you often need specific keywords on your pages to boost your website traffic."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The parameters are the ones that we specify a prior distribution for. The latent variables are usually the ones that we describe using a conditional distribution of the latent variable given the parameters."}, {"text": "The standard solution that psychologists take to measuring latent variables is to use a series of questions that are all designed to measure the latent variable. This is known as a multi-item scale, where an \u201citem\u201d is a question, and a \u201cscale\u201d is the resulting estimate of the latent variable."}]}, {"question": "How do you find the mode of a chi square distribution", "positive_ctxs": [{"text": "As the df increase, the chi square distribution approaches a normal distribution. The mean of a chi square distribution is its df. The mode is df - 2 and the median is approximately df - 0 ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship."}, {"text": "A kind of average sometimes used in statistics and engineering, often abbreviated as RMS. To find the root mean square of a set of numbers, square all the numbers in the set and then find the arithmetic mean of the squares. Take the square root of the result. This is the root mean square."}, {"text": "The determinant of a matrix is a special value that is calculated from a square matrix. It can help you determine whether a matrix has an inverse, find the area of a triangle, and let you know if the system of equations has a unique solution. Determinants are also used in calculus and linear algebra."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "For a perfectly normal distribution the mean, median and mode will be the same value, visually represented by the peak of the curve. The normal distribution is often called the bell curve because the graph of its probability density looks like a bell."}, {"text": "Distance Learning Off-line is a mode of delivery that does not require online participation. You do not have to come to campus. Course materials may be available through the internet, but they can also be mailed to you if you prefer."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}]}, {"question": "What is meant by asymptotic behavior", "positive_ctxs": [{"text": "Asymptotic behavior describes a function or expression with a defined limit or asymptote. Your function may approach this limit, getting closer and closer as you change the function's input, but will never reach it. Unbounded behavior is when you have a function or expression without any limits."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A p-value that is calculated using an approximation to the true distribution is called an asymptotic p-value.  A p-value calculated using the true distribution is called an exact p-value. For large sample sizes, the exact and asymptotic p-values are very similar."}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "In mathematics and statistics, an asymptotic distribution is a probability distribution that is in a sense the \"limiting\" distribution of a sequence of distributions."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "There is a direct relationship between the coefficients produced by logit and the odds ratios produced by logistic. First, let's define what is meant by a logit: A logit is defined as the log base e (log) of the odds. : [1] logit(p) = log(odds) = log(p/q) The range is negative infinity to positive infinity."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Abstract: The generalized likelihood ratio test (GLRT), which is commonly used in composite hypothesis testing problems, is investigated. Conditions for asymptotic optimality of the GLRT in the Neyman-Pearson sense are studied and discussed."}]}, {"question": "What are statistical models used for", "positive_ctxs": [{"text": "A statistical model is a mathematical representation (or mathematical model) of observed data. When data analysts apply various statistical models to the data they are investigating, they are able to understand and interpret the information more strategically."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Vector autoregression (VAR) is a statistical model used to capture the relationship between multiple quantities as they change over time.  VAR models generalize the single-variable (univariate) autoregressive model by allowing for multivariate time series. VAR models are often used in economics and the natural sciences."}, {"text": "Vector autoregression (VAR) is a statistical model used to capture the relationship between multiple quantities as they change over time.  VAR models generalize the single-variable (univariate) autoregressive model by allowing for multivariate time series. VAR models are often used in economics and the natural sciences."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "Analysis of variance (ANOVA) is a collection of statistical models used to analyze the differences among group means and their associated procedures (such as \"variation\" among and between groups).  ANOVAs are useful for comparing (testing) three or more means (groups or variables) for statistical significance."}, {"text": "Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model."}, {"text": "In machine learning, the term \"ground truth\" refers to the accuracy of the training set's classification for supervised learning techniques. This is used in statistical models to prove or disprove research hypotheses."}, {"text": "Bayesian model comparison is a method of model selection based on Bayes factors. The models under consideration are statistical models. The aim of the Bayes factor is to quantify the support for a model over another, regardless of whether these models are correct."}]}, {"question": "What is the difference between linear regression and least squares", "positive_ctxs": [{"text": "In short, linear regression is one of the mathematical models to describe the (linear) relationship between input and output. Least squares, on the other hand, is a method to metric and estimate models, in which the optimal parameters have been found."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Weighted least squares (WLS), also known as weighted linear regression, is a generalization of ordinary least squares and linear regression in which the errors covariance matrix is allowed to be different from an identity matrix."}, {"text": "Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable; the method estimates the relationship by minimizing the sum of the squares in the difference between the observed and predicted values of the"}, {"text": "Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable; the method estimates the relationship by minimizing the sum of the squares in the difference between the observed and predicted values of the"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Logistic regression is quite different than linear regression in that it does not make several of the key assumptions that linear and general linear models (as well as other ordinary least squares algorithm based models) hold so close: (1) logistic regression does not require a linear relationship between the dependent"}, {"text": "Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the"}, {"text": "The least squares approach limits the distance between a function and the data points that the function explains. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data. Mathematicians use the least squares method to arrive at a maximum-likelihood estimate."}]}, {"question": "What is divisive clustering", "positive_ctxs": [{"text": "This variant of hierarchical clustering is called top-down clustering or divisive clustering . We start at the top with all documents in one cluster. The cluster is split using a flat clustering algorithm. This procedure is applied recursively until each document is in its own singleton cluster."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Divisive Clustering: The divisive clustering algorithm is a top-down clustering approach, initially, all the points in the dataset belong to one cluster and split is performed recursively as one moves down the hierarchy."}, {"text": "Answer: Agglomerative Hierarchical clustering method allows the clusters to be read from bottom to top and it follows this approach so that the program always reads from the sub-component first then moves to the parent whereas, divisive uses top-bottom approach in which the parent is visited first then the child."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2)."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Supervised clustering is applied on classified examples with the objective of identifying clusters that have high probability density to a single class.  Semi-supervised clustering is to enhance a clustering algorithm by using side information in clustering process."}]}, {"question": "How is fuzzy logic used in artificial intelligence", "positive_ctxs": [{"text": "Fuzzy logic is used in Natural language processing and various intensive applications in Artificial Intelligence. It is extensively used in modern control systems such as expert systems. Fuzzy Logic mimics how a person would make decisions, only much faster. Thus, you can use it with Neural Networks."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."}, {"text": "Java, Python, Lisp, Prolog, and C++ are major AI programming language used for artificial intelligence capable of satisfying different needs in the development and designing of different software.  It answers the question, 'what is the language used for artificial intelligence?"}, {"text": "Inductive logic programming is the subfield of machine learning that uses first-order logic to represent hypotheses and data. Because first-order logic is expressive and declarative, inductive logic programming specifically targets problems involving structured data and background knowledge."}, {"text": "An artificial neural network's learning rule or learning process is a method, mathematical logic or algorithm which improves the network's performance and/or training time. Usually, this rule is applied repeatedly over the network."}, {"text": "Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence \u2014 to search a space of potential solutions to find one which solves the problem."}, {"text": "Neural networks and fuzzy logic systems are parameterised computational nonlinear algorithms for numerical processing of data (signals, images, stimuli). \u2022 These algorithms can be either implemented of a general-purpose computer or built into a dedicated hardware."}, {"text": "An artificial neural network (ANN) is the component of artificial intelligence that is meant to simulate the functioning of a human brain. Processing units make up ANNs, which in turn consist of inputs and outputs."}]}, {"question": "When should logistic regression be used for data analysis", "positive_ctxs": [{"text": "Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used."}, {"text": "Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used."}, {"text": "When observed outcome of dependent variable can have multiple possible types then logistic regression will be multinomial."}, {"text": "When comparing more than two sets of numerical data, a multiple group comparison test such as one-way analysis of variance (ANOVA) or Kruskal-Wallis test should be used first."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}, {"text": "Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable."}, {"text": "Introduction. Linear regression and logistic regression are two types of regression analysis techniques that are used to solve the regression problem using machine learning. They are the most prominent techniques of regression."}]}, {"question": "When Should relative frequency tables be used", "positive_ctxs": [{"text": "1 Answer. A relative frequency table is a table that records counts of data in percentage form, aka relative frequency. It is used when you are trying to compare categories within the table."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "When a table shows relative frequencies for different categories of a categorical variable, it is called a relative frequency table. The first table shows relative frequencies as a proportion, and the second table shows relative frequencies as a percentage."}, {"text": "The only difference between a relative frequency distribution graph and a frequency distribution graph is that the vertical axis uses proportional or relative frequency rather than simple frequency. Cumulative relative frequency (also called an ogive) is the accumulation of the previous relative frequencies."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "To find the relative frequency, divide the frequency by the total number of data values. To find the cumulative relative frequency, add all of the previous relative frequencies to the relative frequency for the current row."}, {"text": "An easy way to define the difference between frequency and relative frequency is that frequency relies on the actual values of each class in a statistical data set while relative frequency compares these individual values to the overall totals of all classes concerned in a data set."}, {"text": "A frequency table is a chart that shows the popularity or mode of a certain type of data. When we look at frequency, we are looking at the number of times an event occurs within a given scenario.  You can find the relative frequency by simply dividing the frequency number by the total number of values in the data set."}, {"text": "A marginal distribution is a frequency or relative frequency distribution of either the row or column variable in a contingency table.  A conditional distribution lists the relative frequency of each category of the response variable, given a specific value of the explanatory variable in a contingency table."}]}, {"question": "What do you mean by randomized algorithms", "positive_ctxs": [{"text": "A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic.  In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A clinical trial is a randomized controlled trial only when participants are randomly allocated to the group receiving the treatment and a control group. What participants are allocated among groups receiving different treatments the clinical trial is simply called a randomized trial."}, {"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "Researchers can take a number of steps to account for regression to the mean and avoid making incorrect conclusions. The best way is to remove the effect of regression to the mean during the design stage by conducting a randomized controlled trial (RCT)."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Three of the more widely used experimental designs are the completely randomized design, the randomized block design, and the factorial design. In a completely randomized experimental design, the treatments are randomly assigned to the experimental units."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}]}, {"question": "What is variance in statistics", "positive_ctxs": [{"text": "Variance (\u03c32) in statistics is a measurement of the spread between numbers in a data set. That is, it measures how far each number in the set is from the mean and therefore from every other number in the set."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data. Due to this value of denominator in the formula for variance in case of sample data is 'n-1', and it is 'n' for population data."}, {"text": "In statistics and machine learning, the bias\u2013variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Analysis of variance (ANOVA) is an analysis tool used in statistics that splits an observed aggregate variability found inside a data set into two parts: systematic factors and random factors.  1\ufeff\ufeff2\ufeff ANOVA is also called the Fisher analysis of variance, and it is the extension of the t- and z-tests."}]}, {"question": "How graphically represents multivariate data", "positive_ctxs": [{"text": "Another way of visualizing multivariate data for multiple attributes together is to use parallel coordinates. Basically, in this visualization as depicted above, points are represented as connected line segments. Each vertical line represents one data attribute."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": ": having or involving a number of independent mathematical or statistical variables multivariate calculus multivariate data analysis."}, {"text": "How TensorFlow works. TensorFlow allows developers to create dataflow graphs\u2014structures that describe how data moves through a graph, or a series of processing nodes. Each node in the graph represents a mathematical operation, and each connection or edge between nodes is a multidimensional data array, or tensor."}, {"text": "How to conduct a multivariate testIdentify a problem.  Formulate a hypothesis.  Create variations.  Determine your sample size.  Test your tools.  Start driving traffic.  Analyze your results.  Learn from your results."}, {"text": "Univariate and multivariate represent two approaches to statistical analysis. Univariate involves the analysis of a single variable while multivariate analysis examines two or more variables. Most multivariate analysis involves a dependent variable and multiple independent variables."}, {"text": "Univariate and multivariate represent two approaches to statistical analysis. Univariate involves the analysis of a single variable while multivariate analysis examines two or more variables. Most multivariate analysis involves a dependent variable and multiple independent variables."}, {"text": "As the name implies, multivariate regression is a technique that estimates a single regression model with more than one outcome variable. When there is more than one predictor variable in a multivariate regression model, the model is a multivariate multiple regression."}, {"text": "The multivariate normal distribution has two or more random variables \u2014 so the bivariate normal distribution is actually a special case of the multivariate normal distribution."}]}, {"question": "What is the application of principal component analysis", "positive_ctxs": [{"text": "Applications of Principal Component Analysis PCA is predominantly used as a dimensionality reduction technique in domains like facial recognition, computer vision and image compression. It is also used for finding patterns in data of high dimension in the field of finance, data mining, bioinformatics, psychology, etc."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Image compression with principal component analysis is a frequently occurring application of the dimension reduction technique.  As the number of principal components used to project the new data increases, the quality and representation compared to the original image improve."}, {"text": "Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors."}, {"text": "Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors."}, {"text": "Kmeans clustering algorithm is applied to reduced datasets which is done by principal component analysis dimension reduction method. Cluster analysis is one of the major data analysis methods widely used for many practical applications in emerging areas[12]."}, {"text": "Canonical discriminant analysis is a dimension-reduction technique related to principal component analysis and canonical correlation.  This maximal multiple correlation is called the first canonical correlation. The coefficients of the linear combination are the canonical coefficients or canonical weights."}, {"text": "Canonical discriminant analysis is a dimension-reduction technique related to principal component analysis and canonical correlation.  This maximal multiple correlation is called the first canonical correlation. The coefficients of the linear combination are the canonical coefficients or canonical weights."}, {"text": "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  Factor analysis is related to principal component analysis (PCA), but the two are not identical."}]}, {"question": "Is signal processing related to machine learning", "positive_ctxs": [{"text": "We see that machine learning can do what signal processing can, but has inherently higher complexity, with the benefit of being generalizable to different problems. The signal processing algorithms are optimal for the job in terms of complexity, but are specific to the particular problems they solve."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between these two approaches is the goals (not the methods used). Therefore, Image processing is related to enhancing the image and play with features like colors. While computer vision is related to \"Image Understanding\" and it can use machine learning as well."}, {"text": "Filters typically are applied to data in the data processing stage or the preprocessing stage. Filters enhance the clarity of the signal that's used for machine learning."}, {"text": "Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks."}, {"text": "Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks."}, {"text": "Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks."}, {"text": "One method of processing images is via face detection. Face detection is a branch of image processing that uses machine learning to detect faces in images. A Haar Cascade is an object detection method used to locate an object of interest in images."}, {"text": "When part of the memory network is activated, activation spreads along the associative pathways to related areas in memory. This spread of activation serves to make these related areas of the memory network more available for further cognitive processing (Balota & Lorch, 1986)."}]}, {"question": "What is the difference between a chi square test and t test", "positive_ctxs": [{"text": "A t-test tests a null hypothesis about two means; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero.  A chi-square test tests a null hypothesis about the relationship between two variables."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes."}, {"text": "As nouns the difference between trial and experiment is that trial is an opportunity to test something out; a test while experiment is a test under controlled conditions made to either demonstrate a known truth, examine the validity of a hypothesis, or determine the efficacy of something previously untried."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction."}]}, {"question": "What is the discrete random variable", "positive_ctxs": [{"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The probability of each value of the discrete random variable is between 0 and\u200b 1, inclusive, and the sum of all the probabilities is 1.  It is the expected value of a discrete random variable."}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "Now, let the random variable X represent the number of Heads that result from this experiment. The random variable X can only take on the values 0, 1, or 2, so it is a discrete random variable."}, {"text": "To find the expected value, E(X), or mean \u03bc of a discrete random variable X, simply multiply each value of the random variable by its probability and add the products. The formula is given as E(X)=\u03bc=\u2211xP(x)."}, {"text": "A discrete variable is a variable whose value is obtained by counting. A continuous variable is a variable whose value is obtained by measuring.  A discrete random variable X has a countable number of possible values."}, {"text": "Let X be a discrete random variable with the Bernoulli distribution with parameter p: X\u223cBern(p) Then the variance of X is given by: var(X)=p(1\u2212p)"}]}, {"question": "What is confounders in statistics", "positive_ctxs": [{"text": "In statistics, a confounder (also confounding variable, confounding factor, or lurking variable) is a variable that influences both the dependent variable and independent variable, causing a spurious association. Confounding is a causal concept, and as such, cannot be described in terms of correlations or associations."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "The outcome variable is also called the response or dependent variable, and the risk factors and confounders are called the predictors, or explanatory or independent variables. In regression analysis, the dependent variable is denoted \"Y\" and the independent variables are denoted by \"X\"."}]}, {"question": "What is meant by likelihood", "positive_ctxs": [{"text": "the state of being likely or probable; probability. a probability or chance of something: There is a strong likelihood of his being elected."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To put simply, likelihood is \"the likelihood of \u03b8 having generated D\" and posterior is essentially \"the likelihood of \u03b8 having generated D\" further multiplied by the prior distribution of \u03b8."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The likelihood function is given by: L(p|x) \u221dp4(1 \u2212 p)6. The likelihood of p=0.5 is 9.77\u00d710\u22124, whereas the likelihood of p=0.1 is 5.31\u00d710\u22125."}, {"text": "The likelihood function is given by: L(p|x) \u221dp4(1 \u2212 p)6. The likelihood of p=0.5 is 9.77\u00d710\u22124, whereas the likelihood of p=0.1 is 5.31\u00d710\u22125."}, {"text": "There is a direct relationship between the coefficients produced by logit and the odds ratios produced by logistic. First, let's define what is meant by a logit: A logit is defined as the log base e (log) of the odds. : [1] logit(p) = log(odds) = log(p/q) The range is negative infinity to positive infinity."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable."}]}, {"question": "Are neural networks intelligent", "positive_ctxs": [{"text": "A neural network is either a system software or hardware that works similar to the tasks performed by neurons of human brain. Neural networks include various technologies like deep learning, and machine learning as a part of Artificial Intelligence (AI)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?"}, {"text": "Bayesian networks and neural networks are not exclusive of each other. In fact, Bayesian networks are just another term for \"directed graphical model\".  A neural networks is used to implemented p(x|z) and an approximation to its inverse: q(z|x)\u2248p(z|x)."}, {"text": "We demonstrated that convolutional neural networks are primarily utilized for text classification tasks while recurrent neural networks are commonly used for natural language generation or machine translation."}, {"text": "7.2. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems. Radial basis function networks are distinguished from other neural networks due to their universal approximation and faster learning speed."}, {"text": "Google uses artificial neural networks to power voice search."}, {"text": "Neural networks are sets of algorithms intended to recognize patterns and interpret data through clustering or labeling. In other words, neural networks are algorithms. A training algorithm is the method you use to execute the neural network's learning process."}, {"text": "An activation function is a function used in artificial neural networks which outputs a small value for small inputs, and a larger value if its inputs exceed a threshold.  Activation functions are useful because they add non-linearities into neural networks, allowing the neural networks to learn powerful operations."}]}, {"question": "What do coefficients tell you in regression", "positive_ctxs": [{"text": "In linear regression, coefficients are the values that multiply the predictor values.  The sign of each coefficient indicates the direction of the relationship between a predictor variable and the response variable. A positive sign indicates that as the predictor variable increases, the response variable also increases."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Simple linear regression is commonly used in forecasting and financial analysis\u2014for a company to tell how a change in the GDP could affect sales, for example. Microsoft Excel and other software can do all the calculations, but it's good to know how the mechanics of simple linear regression work."}, {"text": "Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion."}, {"text": "OLS cannot be used because the regression function is not a linear function of the regression coefficients (the coefficients appear inside the nonlinear functions \u03a6 or \u039b)."}, {"text": "If there are other predictor variables, all coefficients will be changed.  All the coefficients are jointly estimated, so every new variable changes all the other coefficients already in the model. This is one reason we do multiple regression, to estimate coefficient B1 net of the effect of variable Xm."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}, {"text": "Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target."}]}, {"question": "What is binomial example", "positive_ctxs": [{"text": "Binomial is defined as a math term meaning two expressions connected by a plus or minus sign. An example of a binomial is x \u2013 y.  An example of a binomial is Canis familiaris, the scientific name for dog."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution. The negative binomial distribution is also known as the Pascal distribution."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution.  Suppose we flip a coin repeatedly and count the number of heads (successes)."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "A multinomial experiment is almost identical with one main difference: a binomial experiment can have two outcomes, while a multinomial experiment can have multiple outcomes.  A binomial experiment will have a binomial distribution."}, {"text": "The binomial theorem is valid more generally for any elements x and y of a semiring satisfying xy = yx. The theorem is true even more generally: alternativity suffices in place of associativity. The binomial theorem can be stated by saying that the polynomial sequence {1, x, x2, x3, } is of binomial type."}]}, {"question": "Is batch normalization used in inference", "positive_ctxs": [{"text": "Batch Normalization during inference During testing or inference phase we can't apply the same batch-normalization as we did during training because we might pass only sample at a time so it doesn't make sense to find mean and variance on a single sample."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling.  Others sustain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks."}, {"text": "Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling."}, {"text": "To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Using batch normalization makes the network more stable during training. This may require the use of much larger than normal learning rates, that in turn may further speed up the learning process. \u2014 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, 2015."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics."}]}, {"question": "What is difference between VR and AR", "positive_ctxs": [{"text": "Augmented reality (AR) adds digital elements to a live view often by using the camera on a smartphone. Virtual reality (VR) implies a complete immersion experience that shuts out the physical world."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "An autoregressive (AR) model predicts future behavior based on past behavior. It's used for forecasting when there is some correlation between values in a time series and the values that precede and succeed them.  Where simple linear regression and AR models differ is that Y is dependent on X and previous values for Y."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "The chief difference between MEMM and CRF is that MEMM is locally renormalized and suffers from the label bias problem, while CRFs are globally renormalized."}, {"text": "The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows\u2022"}]}, {"question": "What is weight sharing in CNN", "positive_ctxs": [{"text": "A CNN has multiple layers. Weight sharing happens across the receptive field of the neurons(filters) in a particular layer. Weights are the numbers within each filter.  These filters act on a certain receptive field/ small section of the image. When the filter moves through the image, the filter does not change."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Dense CNN is a type of Deep CNN in which each layer is connected with another layer deeper than itself."}, {"text": "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus."}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig. 2."}, {"text": "A CNN has multiple layers. Weight sharing happens across the receptive field of the neurons(filters) in a particular layer. Weights are the numbers within each filter.  These filters act on a certain receptive field/ small section of the image. When the filter moves through the image, the filter does not change."}, {"text": "In 1D CNN, kernel moves in 1 direction. Input and output data of 1D CNN is 2 dimensional. Mostly used on Time-Series data. In 2D CNN, kernel moves in 2 directions. Input and output data of 2D CNN is 3 dimensional."}]}, {"question": "Whats the difference between matrix factorization and collaborative filtering", "positive_ctxs": [{"text": "collaborative filtering: user-based for example, CF calculates users' similarities in the item space.  matrix factorization amounts to mapping features of user and item via linear combination to latent factor space respectively."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices."}, {"text": "Matrix factorization using the alternating least squares algorithm for collaborative filtering. Alternating least squares (ALS) is an optimization technique to solve the matrix factorization problem. This technique achieves good performance and has proven relatively easy to implement."}, {"text": "Model-based collaborative filtering algorithms provide item recommendation by first developing a model of user ratings. Algorithms in this category take a probabilistic approach and envision the collaborative filtering process as computing the expected value of a user prediction, given his/her ratings on other items."}, {"text": "Firstly, the basic difference between the two is that Market basket analysis is a representation for the whole population (to understand the fact that what products are purchased together as a bunch by the users) whereas the collaborative filtering on the other side restricts itself only to a particular user to"}, {"text": "Non negative matrix factorization only takes positive values as input while SVD can take both positive and negative values.  SVD and NMF are both matrix decomposition techniques but they are very different and are generally used for different purposes. SVD helps in giving Eigen vectors of the input matrix."}, {"text": "Most websites like Amazon, YouTube, and Netflix use collaborative filtering as a part of their sophisticated recommendation systems. You can use this technique to build recommenders that give suggestions to a user on the basis of the likes and dislikes of similar users."}]}, {"question": "What is a good representative sample size", "positive_ctxs": [{"text": "A good maximum sample size is usually 10% as long as it does not exceed 1000. A good maximum sample size is usually around 10% of the population, as long as this does not exceed 1000. For example, in a population of 5000, 10% would be 500. In a population of 200,000, 10% would be 20,000."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A good maximum sample size is usually 10% as long as it does not exceed 1000. A good maximum sample size is usually around 10% of the population, as long as this does not exceed 1000. For example, in a population of 5000, 10% would be 500. In a population of 200,000, 10% would be 20,000."}, {"text": "In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others."}, {"text": "Simply put, a random sample is a subset of individuals randomly selected by researchers to represent an entire group as a whole. The goal is to get a sample of people that is representative of the larger population."}, {"text": "Simply put, a random sample is a subset of individuals randomly selected by researchers to represent an entire group as a whole. The goal is to get a sample of people that is representative of the larger population."}, {"text": "A representative sample is a subset of a population that seeks to accurately reflect the characteristics of the larger group. For example, a classroom of 30 students with 15 males and 15 females could generate a representative sample that might include six students: three males and three females."}, {"text": "Probability sampling allows researchers to create a sample that is accurately representative of the real-life population of interest."}, {"text": "The larger the sample size is the smaller the effect size that can be detected. The reverse is also true; small sample sizes can detect large effect sizes.  Thus an appropriate determination of the sample size used in a study is a crucial step in the design of a study."}]}, {"question": "What is non response bias in stats", "positive_ctxs": [{"text": "Non-response bias is a type of bias that occurs when people are unwilling or unable to respond to a survey due to a factor that makes them differ greatly from people who respond. The difference between non-respondents and respondents is usually an influencing factor for the lack of response."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Nonresponse bias is the bias that results when respondents differ in meaningful ways from nonrespondents.  Response rate is often low, making mail surveys vulnerable to nonresponse bias. Voluntary response bias. Voluntary response bias occurs when sample members are self-selected volunteers, as in voluntary samples."}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "A low response rate can give rise to sampling bias if the nonresponse is unequal among the participants regarding exposure and/or outcome.  For many years, a survey's response rate was viewed as an important indicator of survey quality."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Any LTI filter with output and input can be represented by a difference equation in the form: If at least one of the is not null, the filter is recursive. If the are all zero, it is a non recursive filter usually called FIR (Finite Input Response) filter.  This happens both to recursive and non recursive filters."}, {"text": "Standard error is used in inferential stats to see whether the sample stat that we get from one sample is larger or smaller than the average differences of the stat (variance or error) of certain stat due to chance."}, {"text": "A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function."}]}, {"question": "Which classification algorithms is easiest to start with for prediction", "positive_ctxs": [{"text": "1 \u2014 Linear Regression.  2 \u2014 Logistic Regression.  3 \u2014 Linear Discriminant Analysis.  4 \u2014 Classification and Regression Trees.  5 \u2014 Naive Bayes.  6 \u2014 K-Nearest Neighbors.  7 \u2014 Learning Vector Quantization.  8 \u2014 Support Vector Machines.More items\u2022"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Voting and Averaging Based Ensemble Methods Voting and averaging are two of the easiest ensemble methods.  Voting is used for classification and averaging is used for regression. In both methods, the first step is to create multiple classification/regression models using some training dataset."}, {"text": "The cross-entropy compares the model's prediction with the label which is the true probability distribution. The cross-entropy goes down as the prediction gets more and more accurate. It becomes zero if the prediction is perfect. As such, the cross-entropy can be a loss function to train a classification model."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.  Explicit regression gradient boosting algorithms were subsequently developed by Jerome H."}, {"text": "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.  Explicit regression gradient boosting algorithms were subsequently developed by Jerome H."}, {"text": "In this context, a neural network is one of several machine learning algorithms that can help solve classification problems. Its unique strength is its ability to dynamically create complex prediction functions, and emulate human thinking, in a way that no other algorithm can."}, {"text": "Random forests is a robust algorithm that can be used for remotely sensed data classification and regression. Performance of random forests is on par with other machine learning algorithms but it is much easier to use and more forgiving with regard to over fitting and outliers than other algorithms."}, {"text": "TensorFlow is more of a low-level library; basically, we can think of TensorFlow as the Lego bricks (similar to NumPy and SciPy) that we can use to implement machine learning algorithms whereas scikit-learn comes with off-the-shelf algorithms, e.g., algorithms for classification such as SVMs, Random Forests, Logistic"}]}, {"question": "What is AI in computing", "positive_ctxs": [{"text": "Artificial intelligence (AI) is wide-ranging branch of computer science concerned with building smart machines capable of performing tasks that typically require human intelligence.  It is the endeavor to replicate or simulate human intelligence in machines."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "AI works by combining large amounts of data with fast, iterative processing and intelligent algorithms, allowing the software to learn automatically from patterns or features in the data.  Cognitive computing is a subfield of AI that strives for a natural, human-like interaction with machines."}, {"text": "AI works by combining large amounts of data with fast, iterative processing and intelligent algorithms, allowing the software to learn automatically from patterns or features in the data.  Cognitive computing is a subfield of AI that strives for a natural, human-like interaction with machines."}, {"text": "The term cognitive computing is typically used to describe AI systems that aim to simulate human thought.  A number of AI technologies are required for a computer system to build cognitive models that mimic human thought processes, including machine learning, deep learning, neural networks, NLP and sentiment analysis."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}]}, {"question": "Is the output layer a hidden layer", "positive_ctxs": [{"text": "The Neural Network is constructed from 3 type of layers: Input layer \u2014 initial data for the neural network. Hidden layers \u2014 intermediate layer between input and output layer and place where all the computation is done. Output layer \u2014 produce the result for given inputs."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Hidden layer of the neural network is the intermediate layer between Input and Output layer. Activation function applies on hidden layer if it is available.  Hidden nodes or hidden neurons are the neurons that are neither in the input layer nor the output layer [3]."}, {"text": "What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer."}, {"text": "A two layer (one input layer, one output layer; no hidden layer) neural network can represent the XOR function. We must compose multiple logical operations by using a hidden layer to represent the XOR function."}, {"text": "MLP With Batch Normalization A new BatchNormalization layer can be added to the model after the hidden layer before the output layer. Specifically, after the activation function of the prior hidden layer."}, {"text": "Artificial neural networks (ANN) is the key tool of machine learning.  Neural networks (NN) constitute both input & output layer, as well as a hidden layer containing units that change input into output so that output layer can utilise the value."}, {"text": "One hidden layer is sufficient for the large majority of problems. Usually, each hidden layer contains the same number of neurons. The larger the number of hidden layers in a neural network, the longer it will take for the neural network to produce the output and the more complex problems the neural network can solve."}, {"text": "A Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer). While a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non \u2013 linear functions. Figure 4 shows a multi layer perceptron with a single hidden layer."}]}, {"question": "Can Chi Square be used for categorical data", "positive_ctxs": [{"text": "The Chi square test is used to compare a group with a value, or to compare two or more groups, always using categorical data."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Chi Square distributions are positively skewed, with the degree of skew decreasing with increasing degrees of freedom. As the degrees of freedom increases, the Chi Square distribution approaches a normal distribution. Figure 1 shows density functions for three Chi Square distributions."}, {"text": "Chi Square distributions are positively skewed, with the degree of skew decreasing with increasing degrees of freedom. As the degrees of freedom increases, the Chi Square distribution approaches a normal distribution."}, {"text": "The following types of inferential statistics are extensively used and relatively easy to interpret: One sample test of difference/One sample hypothesis test. Confidence Interval. Contingency Tables and Chi Square Statistic."}, {"text": "All Answers (6) Chi square test requires 2 categorical variables. T test requires 1 categorical and 1 continuous variables. You can't use them interchangeably."}, {"text": "A one hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical."}, {"text": "Mean Square Error, Quadratic loss, L2 Loss Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values."}, {"text": "The probability distribution associated with a random categorical variable is called a categorical distribution. Categorical data is the statistical data type consisting of categorical variables or of data that has been converted into that form, for example as grouped data."}]}, {"question": "What is the difference between probability distribution and probability density", "positive_ctxs": [{"text": "A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable."}, {"text": "2 Answers. By definition the probability density function is the derivative of the distribution function. But distribution function is an increasing function on R thus its derivative is always positive. Assume that probability density of X is -ve in the interval (a, b)."}, {"text": "A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function."}, {"text": "A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}]}, {"question": "What are decision trees in machine learning", "positive_ctxs": [{"text": "Decision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood."}, {"text": "Decision trees are a classic machine learning technique. The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree. By Narendra Nath Joshi, Carnegie Mellon.  The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree."}, {"text": "There are two main types of decision trees that are based on the target variable, i.e., categorical variable decision trees and continuous variable decision trees.Categorical variable decision tree.  Continuous variable decision tree.  Assessing prospective growth opportunities.More items"}, {"text": "Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning."}, {"text": "Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning."}, {"text": "Decision trees can help organizations structure and automate (complex) information. Decision trees are decision models that answer a specific question based on a question structure and certain conditions."}, {"text": "Gradient boosting classifiers are a group of machine learning algorithms that combine many weak learning models together to create a strong predictive model. Decision trees are usually used when doing gradient boosting."}]}, {"question": "What is non stratified sampling", "positive_ctxs": [{"text": "A sampling technique where a group of subjects (a sample) for study is selected from a larger group (a population).  A non-stratified sample does not take separate samples from strata or sub-groups of a population."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "Connection to stratified sampling Quota sampling is the non-probability version of stratified sampling. In stratified sampling, subsets of the population are created so that each subset has a common characteristic, such as gender."}, {"text": "Quota sampling is different from stratified sampling, because in a stratified sample individuals within each stratum are selected at random. Quota sampling achieves a representative age distribution, but it isn't a random sample, because the sampling frame is unknown."}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "The difference between quota sampling and stratified sampling is: although both \"group\" participants by an important characteristic, stratified sampling relies on random selection within each group, while quota sampling relies on convenience sampling within each group."}]}, {"question": "Why do we learn residual", "positive_ctxs": [{"text": "The residual learning framework eases the training of these networks, and enables them to be substantially deeper \u2014 leading to improved performance in both visual and non-visual tasks. These residual networks are much deeper than their 'plain' counterparts, yet they require a similar number of parameters (weights)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?"}, {"text": "If adjacent residuals are correlated, one residual can predict the next residual. In statistics, this is known as autocorrelation. This correlation represents explanatory information that the independent variables do not describe. Models that use time-series data are susceptible to this problem."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}, {"text": "As we optimize the squared residuals to estimate the regression parameters, so we need commonly known normal situation. In Statistics, normal means that everything has equal probability.  So equal probability for each value of regression residual is only possible through Normal Distribution."}]}, {"question": "What is hashing and its advantages", "positive_ctxs": [{"text": "Hashing provides a more reliable and flexible method of data retrieval than any other data structure. It is faster than searching arrays and lists. In the same space it can retrieve in 1.5 probes anything stored in a tree that will otherwise take log n probes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "Disadvantages include its \"black box\" nature, greater computational burden, proneness to overfitting, and the empirical nature of model development. An overview of the features of neural networks and logistic regression is presented, and the advantages and disadvantages of using this modeling technique are discussed."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "In computer science, locality-sensitive hashing (LSH) is an algorithmic technique that hashes similar input items into the same \"buckets\" with high probability.  It differs from conventional hashing techniques in that hash collisions are maximized, not minimized."}, {"text": "Major advantages include its simplicity and lack of bias. Among the disadvantages are difficulty gaining access to a list of a larger population, time, costs, and that bias can still occur under certain circumstances."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "\u201cThe advantages of bootstrapping are that it is a straightforward way to derive the estimates of standard errors and confidence intervals, and it is convenient since it avoids the cost of repeating the experiment to get other groups of sampled data."}]}, {"question": "How do you get the best K value in Knn in R", "positive_ctxs": [{"text": "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Coefficient of correlation is \u201cR\u201d value which is given in the summary table in the Regression output. R square is also called coefficient of determination. Multiply R times R to get the R square value. In other words Coefficient of Determination is the square of Coefficeint of Correlation."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "Simply put, R is the correlation between the predicted values and the observed values of Y. R square is the square of this coefficient and indicates the percentage of variation explained by your regression line out of the total variation. This value tends to increase as you include additional predictors in the model."}, {"text": "Kinesthetic learners are the most hands-on learning type. They learn best by doing and may get fidgety if forced to sit for long periods of time. Kinesthetic learners do best when they can participate in activities or solve problems in a hands-on manner."}, {"text": "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers."}, {"text": "The disadvantages: Convenience samples do not produce representative results. If you need to extrapolate to the target population, convenience samples aren't going to get you there."}, {"text": "There is a popular method known as elbow method which is used to determine the optimal value of K to perform the K-Means Clustering Algorithm. The basic idea behind this method is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer elements in the cluster."}]}, {"question": "What is an agent artificial intelligence", "positive_ctxs": [{"text": "In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation."}, {"text": "In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats\u2014artificial systems that exhibit complex behaviour in an agent environment.  The term is also sometimes used in ethology or animal behavior."}, {"text": "Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation.  Its goal is to maximize the total reward."}, {"text": "Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation.  Its goal is to maximize the total reward."}, {"text": "An autonomous agent is an intelligent agent operating on an owner's behalf but without any interference of that ownership entity.  Non-biological examples include intelligent agents, autonomous robots, and various software agents, including artificial life agents, and many computer viruses."}, {"text": "In the real world, knowledge plays a vital role in intelligence as well as creating artificial intelligence. It demonstrates the intelligent behavior in AI agents or systems. It is possible for an agent or system to act accurately on some input only when it has the knowledge or experience about the input."}, {"text": "In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent)."}]}, {"question": "What does population in statistics mean", "positive_ctxs": [{"text": "In statistics, a population is the entire pool from which a statistical sample is drawn. A population may refer to an entire group of people, objects, events, hospital visits, or measurements. A population can thus be said to be an aggregate observation of subjects grouped together by a common feature."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "Descriptive statistics uses the data to provide descriptions of the population, either through numerical calculations or graphs or tables. Inferential statistics makes inferences and predictions about a population based on a sample of data taken from the population in question."}, {"text": "Descriptive statistics uses the data to provide descriptions of the population, either through numerical calculations or graphs or tables. Inferential statistics makes inferences and predictions about a population based on a sample of data taken from the population in question."}, {"text": "The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It can be used to estimate summary statistics such as the mean or standard deviation.  That when using the bootstrap you must choose the size of the sample and the number of repeats."}, {"text": "If an overestimate or underestimate does happen, the mean of the difference is called a \u201cbias.\u201d That's just saying if the estimator (i.e. the sample mean) equals the parameter (i.e. the population mean), then it's an unbiased estimator."}]}, {"question": "How do we know whether a model is overfitting", "positive_ctxs": [{"text": "Overfitting can be identified by checking validation metrics such as accuracy and loss. The validation metrics usually increase until a point where they stagnate or start declining when the model is affected by overfitting."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The One-Sample z-test is used when we want to know whether the difference between the mean of a sample mean and the mean of a population is large enough to be statistically significant, that is, if it is unlikely to have occurred by chance."}, {"text": "In Kalman filtering the \"process noise\" represents the idea/feature that the state of the system changes over time, but we do not know the exact details of when/how those changes occur, and thus we need to model them as a random process."}, {"text": "Collaborative filtering is an unsupervised learning which we make predictions from ratings supplied by people. Each rows represents the ratings of movies from a person and each column indicates the ratings of a movie. In Collaborative Filtering, we do not know the feature set before hands."}, {"text": "Heisenberg's uncertainty principle is a key principle in quantum mechanics. Very roughly, it states that if we know everything about where a particle is located (the uncertainty of position is small), we know nothing about its momentum (the uncertainty of momentum is large), and vice versa."}, {"text": "Markov model is a state machine with the state changes being probabilities. In a hidden Markov model, you don't know the probabilities, but you know the outcomes."}, {"text": "Answer. The low value of loss function determines whether a model is a good fit for the datasets."}, {"text": "Regularization is a set of techniques that can prevent overfitting in neural networks and thus improve the accuracy of a Deep Learning model when facing completely new data from the problem domain."}]}, {"question": "What is sampling and non sampling errors", "positive_ctxs": [{"text": "\u201cNon-sampling error is the error that arises in a data collection process as a result of factors other than taking a sample. Non-sampling errors have the potential to cause bias in polls, surveys or samples. Examples of non-sampling errors are generally more useful than using names to describe them."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling)."}, {"text": "Uses. Quota sampling is useful when time is limited, a sampling frame is not available, the research budget is very tight or detailed accuracy is not important. Subsets are chosen and then either convenience or judgment sampling is used to choose people from each subset."}, {"text": "Cluster Sampling: Advantages and Disadvantages Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling."}, {"text": "The difference between nonprobability and probability sampling is that nonprobability sampling does not involve random selection and probability sampling does. At least with a probabilistic sample, we know the odds or probability that we have represented the population well."}, {"text": "The difference between nonprobability and probability sampling is that nonprobability sampling does not involve random selection and probability sampling does.  At least with a probabilistic sample, we know the odds or probability that we have represented the population well."}]}, {"question": "What makes a series divergent", "positive_ctxs": [{"text": "In mathematics, a divergent series is an infinite series that is not convergent, meaning that the infinite sequence of the partial sums of the series does not have a finite limit. The divergence of the harmonic series was proven by the medieval mathematician Nicole Oresme."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Integral testIt is possible to prove that the harmonic series diverges by comparing its sum with an improper integral.  Additionally, the total area under the curve y = 1x from 1 to infinity is given by a divergent improper integral:More items"}, {"text": "Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components. Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting."}, {"text": "Time series analysis is a statistical technique that deals with time series data, or trend analysis. Time series data means that data is in a series of particular time periods or intervals.  Time series data: A set of observations on the values that a variable takes at different times."}, {"text": "Well, database normalization is the process of structuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. In simpler terms, normalization makes sure that all of your data looks and reads the same way across all records."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}]}, {"question": "Why is the Cox proportional hazards model referred to as semiparametric", "positive_ctxs": [{"text": "The Cox proportional hazards model92 is the most popular model for the analysis of survival data. It is a semiparametric model; it makes a parametric assumption concerning the effect of the predictors on the hazard function, but makes no assumption regarding the nature of the hazard function \u03bb(t) itself."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, the one in ten rule is a rule of thumb for how many predictor parameters can be estimated from data when doing regression analysis (in particular proportional hazards models in survival analysis and logistic regression) while keeping the risk of overfitting low."}, {"text": "Typically, with neural networks, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply \u201closs.\u201d"}, {"text": "The Euclidean distance corresponds to the L2-norm of a difference between vectors. The cosine similarity is proportional to the dot product of two vectors and inversely proportional to the product of their magnitudes."}, {"text": "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived."}, {"text": "At a higher level, the chief difference between the L1 and the L2 terms is that the L2 term is proportional to the square of the \u03b2 values, while the L1 norm is proportional the absolute value of the values in \u03b2."}, {"text": "At a higher level, the chief difference between the L1 and the L2 terms is that the L2 term is proportional to the square of the \u03b2 values, while the L1 norm is proportional the absolute value of the values in \u03b2."}, {"text": "Graham's law states that the rate of diffusion or of effusion of a gas is inversely proportional to the square root of its molecular weight.  In the same conditions of temperature and pressure, the molar mass is proportional to the mass density."}]}, {"question": "What does rejection sampling mean in Bayesian nets", "positive_ctxs": [{"text": "\u2013 Rejection sampling: reject samples disagreeing with evidence. \u2013 Markov chain Monte Carlo (MCMC): sample from a stochastic process. whose stationary distribution is the true posterior."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The rejection region is the interval, measured in the sampling distribution of the statistic under study, that leads to rejection of the null hypothesis H 0 in a hypothesis test."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "A test of a statistical hypothesis , where the region of rejection is on only one side of the sampling distribution , is called a one-tailed test. For example, suppose the null hypothesis states that the mean is less than or equal to 10. The alternative hypothesis would be that the mean is greater than 10."}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "The mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean \u03bc, then the mean of the sampling distribution of the mean is also \u03bc. The symbol \u03bcM is used to refer to the mean of the sampling distribution of the mean."}, {"text": "Non-probability sampling is a sampling technique where the odds of any member being selected for a sample cannot be calculated.  In addition, probability sampling involves random selection, while non-probability sampling does not\u2014it relies on the subjective judgement of the researcher."}]}, {"question": "How do you find the variance of a continuous distribution", "positive_ctxs": [{"text": "Definition: Let X be a continuous random variable with mean \u00b5. The variance of X is Var(X) = E((X \u2212 \u00b5)2)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The variance of the sampling distribution of the mean is computed as follows: That is, the variance of the sampling distribution of the mean is the population variance divided by N, the sample size (the number of scores used to compute a mean)."}, {"text": "The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable."}, {"text": "To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How to Compare Data SetsCenter. Graphically, the center of a distribution is the point where about half of the observations are on either side.Spread. The spread of a distribution refers to the variability of the data.  Shape. The shape of a distribution is described by symmetry, skewness, number of peaks, etc.Unusual features."}, {"text": "It is possible to find the correlation between a categorical variable and a continuous variable using the analysis of covariance technique."}, {"text": "The answer to that is the Erlang distribution. The Gamma distribution is a generalization of that distribution using a continuous instead of a discrete parameter for the number of events."}]}, {"question": "What can reinforcement learning do", "positive_ctxs": [{"text": "Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "Deep reinforcement learning is a promising combination between two artificial intelligence techniques: reinforcement learning, which uses sequential trial and error to learn the best action to take in every situation, and deep learning, which can evaluate complex inputs and select the best response."}, {"text": "Some of the autonomous driving tasks where reinforcement learning could be applied include trajectory optimization, motion planning, dynamic pathing, controller optimization, and scenario-based learning policies for highways. For example, parking can be achieved by learning automatic parking policies."}, {"text": "Some of the practical applications of reinforcement learning are:Manufacturing. In Fanuc, a robot uses deep reinforcement learning to pick a device from one box and putting it in a container.  Inventory Management.  Delivery Management.  Power Systems.  Finance Sector."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "Cons of Reinforcement LearningReinforcement learning as a framework is wrong in many different ways, but it is precisely this quality that makes it useful.Too much reinforcement learning can lead to an overload of states, which can diminish the results.Reinforcement learning is not preferable to use for solving simple problems.More items"}, {"text": "The example of reinforcement learning is your cat is an agent that is exposed to the environment. The biggest characteristic of this method is that there is no supervisor, only a real number or reward signal. Two types of reinforcement learning are 1) Positive 2) Negative."}]}, {"question": "How can you describe the moving average method", "positive_ctxs": [{"text": "A moving average is a technique to get an overall idea of the trends in a data set; it is an average of any subset of numbers. The moving average is extremely useful for forecasting long-term trends. You can calculate it for any period of time."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Average (or mean) filtering is a method of 'smoothing' images by reducing the amount of intensity variation between neighbouring pixels. The average filter works by moving through the image pixel by pixel, replacing each value with the average value of neighbouring pixels, including itself."}, {"text": "There are two main types of image processing: image filtering and image warping.  Two commonly implemented filters are the moving average filter and the image segmentation filter. The moving average filter replaces each pixel with the average pixel value of it and a neighborhood window of adjacent pixels."}, {"text": "The simple moving average (SMA) is the average price of a security over a specific period.  The exponential moving average (EMA) provides more weight to the most recent prices in an attempt to better reflect new market data. The difference between the two is noticeable when comparing long-term averages."}, {"text": "A moving average is a technique to get an overall idea of the trends in a data set; it is an average of any subset of numbers. The moving average is extremely useful for forecasting long-term trends. You can calculate it for any period of time.  Moving averages are usually plotted and are best visualized."}, {"text": "The exponential moving average (EMA) is a technical chart indicator that tracks the price of an investment (like a stock or commodity) over time. The EMA is a type of weighted moving average (WMA) that gives more weighting or importance to recent price data."}, {"text": "1) Your model performs better on the training data than on the unknown validation data.  It can also happen when your training loss is calculated as a moving average over 1 epoch, whereas the validation loss is calculated after the learning phase of the same epoch."}, {"text": "An ARMA model, or Autoregressive Moving Average model, is used to describe weakly stationary stochastic time series in terms of two polynomials. The first of these polynomials is for autoregression, the second for the moving average."}]}, {"question": "What is meant by Independent and Identically Distributed Random Variables", "positive_ctxs": [{"text": "In probability theory and statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d. or iid or IID."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Independent Variables An independent variable is the factor that has some influence or impact on the dependent variable."}, {"text": "The main difference between Independant and Independent is that the Independant is a misspelling of independent and Independent is a Not dependent; free; not subject to control by others; not relying on others."}, {"text": "Dependent and Independent Variables An independent variable, sometimes called an experimental or predictor variable, is a variable that is being manipulated in an experiment in order to observe the effect on a dependent variable, sometimes called an outcome variable."}, {"text": "In statistics, we usually say \u201crandom sample,\u201d but in probability it's more common to say \u201cIID.\u201d Identically Distributed means that there are no overall trends\u2013the distribution doesn't fluctuate and all items in the sample are taken from the same probability distribution."}, {"text": "In statistics, we usually say \u201crandom sample,\u201d but in probability it's more common to say \u201cIID.\u201d Identically Distributed means that there are no overall trends\u2013the distribution doesn't fluctuate and all items in the sample are taken from the same probability distribution."}, {"text": "Summary. A Random Variable is a variable whose possible values are numerical outcomes of a random experiment. Random Variables can be discrete or continuous. An important example of a continuous Random variable is the Standard Normal variable, Z."}, {"text": "Choosing the Best Algorithm for your Classification Model.\u2022Read the Data.\u2022 Create Dependent and Independent Datasets based on our Dependent and Independent features.\u2022Split the Data into Training and Testing sets.\u2022 Train our Model for different Classification Algorithms namely XGB Classifier, Decision Tree, SVM Classifier, Random Forest Classifier.\u2022Select the Best Algorithm."}]}, {"question": "What is the starting point of backward chaining for the solution", "positive_ctxs": [{"text": "Backward chaining is a type of AI program that starts with a defined end point (or goal) and works backward to figure out the best way to get there. For example, if a person wants to save $1 million for retirement, backward chaining can help them figure out how much they need to save each month to get there."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Backward chaining is the logical process of inferring unknown truths from known conclusions by moving backward from a solution to determine the initial conditions and rules. Backward chaining is often applied in artificial intelligence (AI) and may be used along with its counterpart, forward chaining."}, {"text": "Backward chaining (or backward reasoning) is an inference method described colloquially as working backward from the goal. It is used in automated theorem provers, inference engines, proof assistants, and other artificial intelligence applications.  Both rules are based on the modus ponens inference rule."}, {"text": "Despite the sample population being selected in advance, systematic sampling is still thought of as being random if the periodic interval is determined beforehand and the starting point is random."}, {"text": "The difference between forward and backward chaining is: Backward chaining starts with a goal and then searches back through inference rules to find the facts that support the goal. Forward chaining starts with facts and searches forward through the rules to find a desired goal."}, {"text": "Forward chaining starts from known facts and applies inference rule to extract more data unit it reaches to the goal. Backward chaining starts from the goal and works backward through inference rules to find the required facts that support the goal.  Backward chaining reasoning applies a depth-first search strategy."}, {"text": "Another strategy OTs typically recommend is something called \u201cbackward chaining.\" Backward chaining is working backward from the goal. For example, the goal is put on a T-shirt.  Pull shirt over head. Push right arm up through right sleeve."}, {"text": "Backward chaining is known as goal-driven technique as we start from the goal and divide into sub-goal to extract the facts.  Backward chaining is suitable for diagnostic, prescription, and debugging application. 7. Forward chaining can generate an infinite number of possible conclusions."}]}, {"question": "How can I prepare for Artificial Intelligence", "positive_ctxs": [{"text": "Focus on these key areas to lay the groundwork for successful AI implementations in your organizationExplore business opportunities.Assess your data needs.Examine your infrastructure.Determine your talent or vendor needs.Be prepared for inevitable risk."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "11 Applications of Artificial Intelligence in Business:Chatbots:  Artificial Intelligence in eCommerce:  AI to Improve Workplace Communication:  Human Resource Management:  AI in Healthcare:  Intelligent Cybersecurity:  Artificial Intelligence in Logistics and Supply Chain:  Sports betting Industry:More items\u2022"}, {"text": "The next big thing after deep learning Artificial General Intelligence (AGI) that is building machines that can surpass human intelligence. The next big thing after deep learning Artificial General Intelligence (AGI) that is building machines that can surpass human intelligence."}, {"text": "Data quality is important when applying Artificial Intelligence techniques, because the results of these solutions will be as good or bad as the quality of the data used.  The algorithms that feed systems based on Artificial Intelligence can only assume that the data to be analyzed are reliable."}, {"text": "1.1 The Role of Logic in Artificial Intelligence Logic, for instance, can provide a specification for a programming language by characterizing a mapping from programs to the computations that they license."}, {"text": "16 Best Resources to Learn AI & Machine Learning in 2019Introduction to Machine Learning Problem Framing from Google.  Artificial Intelligence: Principles and Techniques from Stanford University.  Daily email list of AI and ML coding tasks from GeekForge.  CS405: Artificial Intelligence from Saylor Academy.  Intro to Artificial Intelligence at Udacity.More items\u2022"}, {"text": "Artificial Intelligence ExamplesManufacturing robots.Smart assistants.Proactive healthcare management.Disease mapping.Automated financial investing.Virtual travel booking agent.Social media monitoring.Inter-team chat tool.More items"}, {"text": "The four popular approaches to Artificial Intelligence are self-awareness, the theory of mind, limited memory, and reactive machines."}]}, {"question": "What is Histogram of Oriented Gradients and how does it work", "positive_ctxs": [{"text": "The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts occurrences of gradient orientation in localized portions of an image."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Process of Calculating the Histogram of Oriented Gradients (HOG)Step 1: Preprocess the Data (64 x 128) This is a step most of you will be pretty familiar with.  Step 2: Calculating Gradients (direction x and y)  Step 3: Calculate the Magnitude and Orientation."}, {"text": "What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability)."}, {"text": "It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person."}, {"text": "The Shape of a Histogram A histogram is unimodal if there is one hump, bimodal if there are two humps and multimodal if there are many humps. A nonsymmetric histogram is called skewed if it is not symmetric. If the upper tail is longer than the lower tail then it is positively skewed."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is the difference between Data Analytics Data Analysis Data Mining Data Science Machine Learning and Big Data 1", "positive_ctxs": [{"text": "While data science focuses on the science of data, data mining is concerned with the process. It deals with the process of discovering newer patterns in big data sets.  In machine learning algorithms are used for gaining knowledge from data sets."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Data mining has several types, including pictorial data mining, text mining, social media mining, web mining, and audio and video mining amongst others.Read: Data Mining vs Machine Learning.Learn more: Association Rule Mining.Check out: Difference between Data Science and Data Mining.Read: Data Mining Project Ideas."}, {"text": "Data Analytics is a Bigger picture of the same thing which is referred as Machine learning. Like Data Analytics has various categories based on the Data used, similarly, Machine Learning, expresses the way one machine learns a code or work in supervised,unsupervised,semi supervised and reinforcement manner."}, {"text": "Definition: Hadoop is a kind of framework that can handle the huge volume of Big Data and process it, whereas Big Data is just a large volume of the Data which can be in unstructured and structured data."}, {"text": "Scalable Machine Learning occurs when Statistics, Systems, Machine Learning and Data Mining are combined into flexible, often nonparametric, and scalable techniques for analyzing large amounts of data at internet scale."}, {"text": "Process: In the process of Artificial Intelligence (AI), Future events are forecasted using the predictive model. But Data Science involves the process of prediction, visualization, analysis, and pre-processing of data.  But the primary goal of Data Science is to find the patterns that are hidden in the data."}, {"text": "Data Analysis. Data Analysis is the process of systematically applying statistical and/or logical techniques to describe and illustrate, condense and recap, and evaluate data.  An essential component of ensuring data integrity is the accurate and appropriate analysis of research findings."}, {"text": "Run regression analysisOn the Data tab, in the Analysis group, click the Data Analysis button.Select Regression and click OK.In the Regression dialog box, configure the following settings: Select the Input Y Range, which is your dependent variable.  Click OK and observe the regression analysis output created by Excel."}]}, {"question": "What is filter method in feature selection", "positive_ctxs": [{"text": "Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it. Filter methods are much faster compared to wrapper methods as they do not involve training the models."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection. Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."}, {"text": "So while L2 regularization does not perform feature selection the same way as L1 does, it is more useful for feature *interpretation*: a predictive feature will get a non-zero coefficient, which is often not the case with L1."}, {"text": "This approach involves either forward selection, adding features one at a time, or backward selection, removing features one at a time until some criterion is reached. Additionally, a bidirectional selection method is available that involves adding or removing a feature at each step."}, {"text": "A convolution is the simple application of a filter to an input that results in an activation. Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in an input, such as an image."}]}, {"question": "What is bagging technique in ML", "positive_ctxs": [{"text": "Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The process of training an ML model involves providing an ML algorithm (that is, the learning algorithm) with training data to learn from. The term ML model refers to the model artifact that is created by the training process.  You can use the ML model to get predictions on new data for which you do not know the target."}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "The difference is very slim between machine learning (ML) and optimization theory. In ML the idea is to learn a function that minimizes an error or one that maximizes reward over punishment.  The goal for ML is similarly to optimize the performance of a model given an objective and the training data."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}]}, {"question": "What are the two types of statistical inference", "positive_ctxs": [{"text": "There are two forms of statistical inference:Hypothesis testing.Confidence interval estimation."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Two main types of fuzzy inference systems can be implemented: Mamdani-type (1977) and Sugeno-type (1985). These two types of inference systems vary somewhat in the way outputs are determined. Mamdani-type inference expects the output membership functions to be fuzzy sets."}, {"text": "A statistical model is a family of probability distributions, the central problem of statistical inference being to identify which member of the family generated the data currently of interest."}, {"text": "Variational Bayesian methods are primarily used for two purposes: To provide an analytical approximation to the posterior probability of the unobserved variables, in order to do statistical inference over these variables."}, {"text": "The type of inference exhibited here is called abduction or, somewhat more commonly nowadays, Inference to the Best Explanation.1.1 Deduction, induction, abduction. Abduction is normally thought of as being one of three major types of inference, the other two being deduction and induction.  1.2 The ubiquity of abduction."}, {"text": "The decomposition of time series is a statistical task that deconstructs a time series into several components, each representing one of the underlying categories of patterns. There are two principal types of decomposition, which are outlined below."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}, {"text": "Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.  Bayesian updating is particularly important in the dynamic analysis of a sequence of data."}]}, {"question": "Deep Learning What is meant by a distributed representation", "positive_ctxs": [{"text": "Distributed representation describes the same data features across multiple scalable and interdependent layers. Each layer defines the information with the same level of accuracy, but adjusted for the level of scale. These layers are learned concurrently but in a non-linear fashion."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised."}, {"text": "A simple random sample is a subset of a statistical population in which each member of the subset has an equal probability of being chosen. A simple random sample is meant to be an unbiased representation of a group."}, {"text": "Usually, Deep Learning takes more time to train as compared to Machine Learning. The main reason is that there are so many parameters in a Deep Learning algorithm. Whereas Machine Learning takes much less time to train, ranging from a few seconds to a few hours."}, {"text": "Deep Learning is the evolution of Machine Learning and it will definitely help in making machines better than what Machine Learning does. But one thing to note is that Deep Learning models require a very large amount of data to train the model otherwise it won't work as expected."}, {"text": "The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word. In word2vec, a distributed representation of a word is used."}, {"text": "Deep learning itself does feature engineering whereas machine learning requires manual feature engineering. 2) Which of the following is a representation learning algorithm? Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning."}, {"text": "Definition: Random sampling is a part of the sampling technique in which each sample has an equal probability of being chosen. A sample chosen randomly is meant to be an unbiased representation of the total population.  An unbiased random sample is important for drawing conclusions."}]}, {"question": "What is a binned variable", "positive_ctxs": [{"text": "Definition. A Binned Variable (also Grouped Variable) in the context of Quantitative Risk Management is any variable that is generated via the discretization of Numerical Variable into a defined set of bins (intervals)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "A discrete variable is a variable whose value is obtained by counting. A continuous variable is a variable whose value is obtained by measuring.  A discrete random variable X has a countable number of possible values."}, {"text": "A clinical trial is a randomized controlled trial only when participants are randomly allocated to the group receiving the treatment and a control group. What participants are allocated among groups receiving different treatments the clinical trial is simply called a randomized trial."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "What is meant by pseudo random number", "positive_ctxs": [{"text": "A set of values or elements that is statistically random, but it is derived from a known starting point and is typically repeated over and over.  It is called \"pseudo\" random, because the algorithm can repeat the sequence, and the numbers are thus not entirely random."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference is obviously that, in a first order reaction, the order of reaction is one by nature. A pseudo first-order reaction is second order reaction by nature but has been altered to make it a first order reaction."}, {"text": "A simple random sample is a subset of a statistical population in which each member of the subset has an equal probability of being chosen. A simple random sample is meant to be an unbiased representation of a group."}, {"text": "Definition: Random sampling is a part of the sampling technique in which each sample has an equal probability of being chosen. A sample chosen randomly is meant to be an unbiased representation of the total population.  An unbiased random sample is important for drawing conclusions."}, {"text": "A point estimate is the value of a statistic that estimates the value of a parameter. For example, the sample mean is a point estimate of the population mean. The arithmetic mean is a single value meant to \"sum up\" a data set. To calculate the mean, add up all the values and divide by the number of values."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "There is a direct relationship between the coefficients produced by logit and the odds ratios produced by logistic. First, let's define what is meant by a logit: A logit is defined as the log base e (log) of the odds. : [1] logit(p) = log(odds) = log(p/q) The range is negative infinity to positive infinity."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}]}, {"question": "How do you find the gradient descent", "positive_ctxs": [{"text": "Gradient descent subtracts the step size from the current value of intercept to get the new value of intercept. This step size is calculated by multiplying the derivative which is -5.7 here to a small number called the learning rate. Usually, we take the value of the learning rate to be 0.1, 0.01 or 0.001."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Unlike the batch gradient descent which computes the gradient using the whole dataset, because the SGD, also known as incremental gradient descent, tries to find minimums or maximums by iteration from a single randomly picked training example, the error is typically noisier than in gradient descent."}, {"text": "Usually you don't want to find a global optimum. Because that usually requires overfitting the training data. An interesting alternative to gradient descent is the population-based training algorithms such as the evolutionary algorithms (EA) and the particle swarm optimisation (PSO)."}, {"text": "Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning."}, {"text": "Gradient descent is a simple optimization procedure that you can use with many machine learning algorithms.  Stochastic gradient descent refers to calculating the derivative from each training data instance and calculating the update immediately."}, {"text": "According to a senior data scientist, one of the distinct advantages of using Stochastic Gradient Descent is that it does the calculations faster than gradient descent and batch gradient descent. However, gradient descent is the best approach if one wants a speedier result."}, {"text": "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."}, {"text": "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."}]}, {"question": "What is artificial intelligence and expert system", "positive_ctxs": [{"text": "In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if-then rules rather than through conventional procedural code."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert.  The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision."}, {"text": "A knowledge-based system (KBS) is a form of artificial intelligence (AI) that aims to capture the knowledge of human experts to support decision-making. Examples of knowledge-based systems include expert systems, which are so called because of their reliance on human expertise."}, {"text": "An artificial neural network (ANN) is the piece of a computing system designed to simulate the way the human brain analyzes and processes information. It is the foundation of artificial intelligence (AI) and solves problems that would prove impossible or difficult by human or statistical standards."}, {"text": "Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing (NLP), speech recognition and machine vision."}, {"text": "Image recognition is the ability of a system or software to identify objects, people, places, and actions in images. It uses machine vision technologies with artificial intelligence and trained algorithms to recognize images through a camera system."}]}, {"question": "What is the purpose of factor analysis", "positive_ctxs": [{"text": "The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models."}, {"text": "The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models. Most often, factors are rotated after extraction."}, {"text": "There are two types of factor analyses, exploratory and confirmatory. Exploratory factor analysis (EFA) is method to explore the underlying structure of a set of observed variables, and is a crucial step in the scale development process. The first step in EFA is factor extraction."}, {"text": "In multivariate statistics, exploratory factor analysis (EFA) is a statistical method used to uncover the underlying structure of a relatively large set of variables. EFA is a technique within factor analysis whose overarching goal is to identify the underlying relationships between measured variables."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association."}, {"text": "Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.  Like univariate analysis, bivariate analysis can be descriptive or inferential."}]}, {"question": "What is loss in deep learning", "positive_ctxs": [{"text": "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Gram matrix is simply the matrix of the inner product of each vector and its corresponding vectors in same. It found use in the current machine learning is due to deep learning loss where while style transferring the loss function is computed using the gram matrix."}, {"text": "Hinge Loss - This has been used in SVMs (Soft Margin). The aim of this loss function is to penalize miss-classification. Cross-Entropy Loss - Probably one of best loss functions being used in classification. Now-a-days this is being used in many advanced machine learning models like deep neural networks etc."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}]}, {"question": "How do you pass data between view controllers", "positive_ctxs": [{"text": "You can pass data between view controllers in Swift in 6 ways:By using an instance property (A \u2192 B)By using segues (for Storyboards)By using instance properties and functions (A \u2190 B)By using the delegation pattern.By using a closure or completion handler.By using NotificationCenter and the Observer pattern."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "How Change Detection WorksDeveloper updates the data model, e.g. by updating a component binding.Angular detects the change.Change detection checks every component in the component tree from top to bottom to see if the corresponding model has changed.If there is a new value, it will update the component's view (DOM)"}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "In Gradient Descent (GD), we perform the forward pass using ALL the train data before starting the backpropagation pass to adjust the weights. This is called (one epoch). In Stochastic Gradient Descent (SGD), we perform the forward pass using a SUBSET of the train set followed by backpropagation to adjust the weights."}, {"text": "A psychometric and capability test aims to provide measurable, objective data that can give you a better versatile view of a candidate's skills and suitability for a position. Assessments offer scientific, valid reliable and objectivity to the process of recruiting."}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}]}, {"question": "How do you merge neural networks", "positive_ctxs": [{"text": "Basically, you're just pre-setting some of the weights of the new network. Be sure to initialize the new connections to have similar distributions. Make the last layer a concatenation of their results and then add another few layers. Make the last layer a concatenation of their results and the original input."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "Specifically, you learned: That a key approach is to use word embeddings and convolutional neural networks for text classification. That a single layer model can do well on moderate-sized problems, and ideas on how to configure it."}, {"text": "The reason why Convolutional Neural Networks (CNNs) do so much better than classic neural networks on images and videos is that the convolutional layers take advantage of inherent properties of images. Simple feedforward neural networks don't see any order in their inputs."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}, {"text": "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers."}]}, {"question": "How do you find the CDRE of a discrete random variable", "positive_ctxs": [{"text": "The cumulative distribution function (c.d.f.) of a discrete random variable X is the function F(t) which tells you the probability that X is less than or equal to t. So if X has p.d.f. P(X = x), we have: F(t) = P(X \u00a3 t) = SP(X = x)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "To find the expected value, E(X), or mean \u03bc of a discrete random variable X, simply multiply each value of the random variable by its probability and add the products. The formula is given as E(X)=\u03bc=\u2211xP(x)."}, {"text": "Note that the CDF gives us P(X\u2264x). To find P(X<x), for a discrete random variable, we can simply write P(X<x)=P(X\u2264x)\u2212P(X=x)=FX(x)\u2212PX(x). Let X be a discrete random variable with range RX={1,2,3,}. Suppose the PMF of X is given by PX(k)=12k for k=1,2,3,"}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers."}, {"text": "The probability of each value of the discrete random variable is between 0 and\u200b 1, inclusive, and the sum of all the probabilities is 1.  It is the expected value of a discrete random variable."}, {"text": "So by the definition of discrete and continuous random variables, a random variable cannot be both discrete and continuous. No. For a random variable to be discrete, there must a countable sequence such that ."}, {"text": "Definition: Entropy is a measure of uncertainty of a random variable. The entropy of a discrete random variable X with alphabet X is H(X) = -) p(x) log p(2) DEX When the base of the logarithm is 2, entropy is measured in bits.  (Note: you can prove this by assigning a variable pi to the probability of outcome i."}]}, {"question": "Is SVM good for image classification", "positive_ctxs": [{"text": "SVM can be used to optimize classification of images (or subimages, for segmentation). SVM does not provide image classification mechanisms."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Convolutional Neural Networks (CNNs) is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough."}, {"text": "Convolutional Neural Networks (CNNs) is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough."}, {"text": "Though SVM is a linear classifier which learns an (n \u2013 1)-dimensional classifier for classification of data into two classes. But SVM it can be used for classifying a non-linear dataset."}, {"text": "Another common model for classification is the support vector machine (SVM). An SVM works by projecting the data into a higher dimensional space and separating it into different classes by using a single (or set of) hyperplanes. A single SVM does binary classification and can differentiate between two classes."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}, {"text": "Unlikely to CNN, RNN learns to recognize image features across time. Although RNN can be used for image classification theoretically, only a few researches about RNN image classifier can be found."}, {"text": "A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text. So you're working on a text classification problem."}]}, {"question": "What is the difference between cluster analysis and multidimensional scaling", "positive_ctxs": [{"text": "Cluster analysis is a tool for classifying objects into groups and is not concerned with the geometric representation of the objects in a low-dimensional space. To explore the dimensionality of the space, one may use multidimensional scaling."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum."}, {"text": "The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or"}, {"text": "Average Linkage is a type of hierarchical clustering in which the distance between one cluster and another cluster is considered to be equal to the average distance from any member of one cluster to any member of the other cluster."}, {"text": "Latent classes divide the cases into their respective dimensions in relation to the variable. For example, cluster analysis groups similar cases and puts them into one group. The numbers of clusters in the cluster analysis are called the latent classes. In SEM, the number of constructs is called the latent classed."}, {"text": "Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable; the method estimates the relationship by minimizing the sum of the squares in the difference between the observed and predicted values of the"}]}, {"question": "What does a positive linear relationship between X and Y in a simple regression imply", "positive_ctxs": [{"text": "Linear relationships can be either positive or negative. Positive relationships have points that incline upwards to the right. As x values increase, y values increase. As x values decrease, y values decrease."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Simple linear regression relates X to Y through an equation of the form Y = a + bX. Both quantify the direction and strength of the relationship between two numeric variables.  The correlation squared (r2 or R2) has special meaning in simple linear regression."}, {"text": "Statistical researchers often use a linear relationship to predict the (average) numerical value of Y for a given value of X using a straight line (called the regression line). If you know the slope and the y-intercept of that regression line, then you can plug in a value for X and predict the average value for Y."}, {"text": "The fact that two variables are strongly correlated does not in itself imply a cause-and-effect relationship between the variables."}, {"text": "The proportion of Y variance explained by the linear relationship between X and Y = r2 = 0.64, or 64%."}, {"text": "If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables."}, {"text": "There are four assumptions associated with a linear regression model: Linearity: The relationship between X and the mean of Y is linear. Homoscedasticity: The variance of residual is the same for any value of X. Independence: Observations are independent of each other."}, {"text": "Logistic regression is quite different than linear regression in that it does not make several of the key assumptions that linear and general linear models (as well as other ordinary least squares algorithm based models) hold so close: (1) logistic regression does not require a linear relationship between the dependent"}]}, {"question": "What does the geometric mean represent", "positive_ctxs": [{"text": "In mathematics, the geometric mean is a mean or average, which indicates the central tendency or typical value of a set of numbers by using the product of their values (as opposed to the arithmetic mean which uses their sum)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "The geometric mean differs from the arithmetic average, or arithmetic mean, in how it is calculated because it takes into account the compounding that occurs from period to period. Because of this, investors usually consider the geometric mean a more accurate measure of returns than the arithmetic mean."}, {"text": "The geometric mean differs from the arithmetic average, or arithmetic mean, in how it is calculated because it takes into account the compounding that occurs from period to period. Because of this, investors usually consider the geometric mean a more accurate measure of returns than the arithmetic mean."}, {"text": "The geometric mean differs from the arithmetic average, or arithmetic mean, in how it is calculated because it takes into account the compounding that occurs from period to period. Because of this, investors usually consider the geometric mean a more accurate measure of returns than the arithmetic mean."}, {"text": "In mathematics, the inequality of arithmetic and geometric means, or more briefly the AM\u2013GM inequality, states that the arithmetic mean of a list of non-negative real numbers is greater than or equal to the geometric mean of the same list; and further, that the two means are equal if and only if every number in the"}, {"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "This is because geometric mean involves product term. However, for a data which follows log-normal distribution, geometric mean should be same as median."}]}, {"question": "How does logistic regression algorithm work", "positive_ctxs": [{"text": "Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes.  Mathematically, a logistic regression model predicts P(Y=1) as a function of X."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Logistic regression is quite different than linear regression in that it does not make several of the key assumptions that linear and general linear models (as well as other ordinary least squares algorithm based models) hold so close: (1) logistic regression does not require a linear relationship between the dependent"}, {"text": "K-NN is a lazy learner because it doesn't learn a discriminative function from the training data but \u201cmemorizes\u201d the training dataset instead. For example, the logistic regression algorithm learns its model weights (parameters) during training time.  A lazy learner does not have a training phase."}, {"text": "K-NN is a lazy learner because it doesn't learn a discriminative function from the training data but \u201cmemorizes\u201d the training dataset instead. For example, the logistic regression algorithm learns its model weights (parameters) during training time.  A lazy learner does not have a training phase."}, {"text": "This is because of the logistic distribution having heavier tails (than the normal distribution): Any outliers would not carry as much weight under the assumptions of the logistic (blue) distribution.  In a logistic regression does a very small P value for a predictor mean a good predictor or a bad predictor?"}, {"text": "No, logistic regression does not require any particular distribution for the independent variables. They can be normal, skewed, categorical or whatever. No regression method makes assumptions about the shape of the distribution of either the IVs or the DV."}, {"text": "No, logistic regression does not require any particular distribution for the independent variables. They can be normal, skewed, categorical or whatever. No regression method makes assumptions about the shape of the distribution of either the IVs or the DV."}, {"text": "Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one."}]}, {"question": "How do you find the covariance between two variables", "positive_ctxs": [{"text": "Covariance is calculated by analyzing at-return surprises (standard deviations from the expected return) or by multiplying the correlation between the two variables by the standard deviation of each variable."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The Pearson's correlation coefficient is calculated as the covariance of the two variables divided by the product of the standard deviation of each data sample. It is the normalization of the covariance between the two variables to give an interpretable score."}, {"text": "The Pearson's correlation coefficient is calculated as the covariance of the two variables divided by the product of the standard deviation of each data sample. It is the normalization of the covariance between the two variables to give an interpretable score."}, {"text": "Covariance provides insight into how two variables are related to one another. More precisely, covariance refers to the measure of how two random variables in a data set will change together. A positive covariance means that the two variables at hand are positively related, and they move in the same direction."}, {"text": "It is possible to find the correlation between a categorical variable and a continuous variable using the analysis of covariance technique."}, {"text": "The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of observed values of two of the variables and whose i, i element is the sample variance of the observed values of one of the variables."}, {"text": "Partial least squares discriminant analysis (PLS-DA) is a variant used when the Y is categorical. PLS is used to find the fundamental relations between two matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces."}, {"text": "You can use the covariance to determine the direction of a linear relationship between two variables as follows:If both variables tend to increase or decrease together, the coefficient is positive.If one variable tends to increase as the other decreases, the coefficient is negative."}]}, {"question": "What is bounding box in image processing", "positive_ctxs": [{"text": "A bounding box is an imaginary rectangle that serves as a point of reference for object detection and creates a collision box for that object. Data annotators draw these rectangles over images, outlining the object of interest within each image by defining its X and Y coordinates."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The model works by first splitting the input image into a grid of cells, where each cell is responsible for predicting a bounding box if the center of a bounding box falls within it. Each grid cell predicts a bounding box involving the x, y coordinate and the width and height and the confidence."}, {"text": "Evaluation overview IoU threshold : Intersection over Union, a value used in object detection to measure the overlap of a predicted versus actual bounding box for an object. The closer the predicted bounding box values are to the actual bounding box values the greater the intersection, and the greater the IoU value."}, {"text": "Bounding boxes is one of the most popular and recognizable image annotation method used in machine learning and deep learning. Using bounding boxes annotators are asked to outline the object in a box as per the machine learning project requirements."}, {"text": "Classification/Recognition: Given an image with an object , find out what that object is.  In other words, classify it in a class from a set of predefined categories. Localization : Find where the object is and draw a bounding box around it."}, {"text": "The ground-truth bounding boxes (i.e., the hand labeled bounding boxes from the testing set that specify where in the image our object is)."}, {"text": "To estimate the oriented bounding box, you need to train the network with objects and their oriented bounding boxes. For that, you need to modify the bounding box regression head of the network. Frustum PointNet[2] employs such regression but for the 3D bounding boxes. It can easily be extended for the 2D use cases."}, {"text": "Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.  The ground-truth bounding boxes (i.e., the hand labeled bounding boxes from the testing set that specify where in the image our object is). The predicted bounding boxes from our model."}]}, {"question": "What is the pdf of gamma distribution", "positive_ctxs": [{"text": "\ufffd(\ufffd) = x \ufffd\u22121 e\u2212xdx. then f(x \ufffd, \ufffd) will be a probability density function since it is nonnegative and it integrates | to one. Definition. The distribution with p.d.f. f(x \ufffd, \ufffd) is called Gamma distribution with | parameters \ufffd and \ufffd and it is denoted as \ufffd(\ufffd, \ufffd)."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "For values of x > 0, the gamma function is defined using an integral formula as \u0393(x) = Integral on the interval [0, \u221e ] of \u222b 0\u221et x \u22121 e\u2212t dt. The probability density function for the gamma distribution is given by. The mean of the gamma distribution is \u03b1\u03b2 and the variance (square of the standard deviation) is \u03b1\u03b22."}, {"text": "In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution."}, {"text": "In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution."}, {"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "The cumulative distribution function (cdf) of a continuous random variable X is defined in exactly the same way as the cdf of a discrete random variable. F (b) = P (X \u2264 b). F (b) = P (X \u2264 b) = f(x) dx, where f(x) is the pdf of X."}, {"text": "The lognormal distribution is a distribution skewed to the right. The pdf starts at zero, increases to its mode, and decreases thereafter. The degree of skewness increases as increases, for a given . For the same , the pdf's skewness increases as increases."}, {"text": "Log-likelihood is all your data run through the pdf of the likelihood (logistic function), the logarithm taken for each value, and then they are summed together."}]}, {"question": "What are the five popular algorithms of machine learning", "positive_ctxs": [{"text": "Without further ado and in no particular order, here are the top 5 machine learning algorithms for those just getting started:Linear regression.  Logical regression.  Classification and regression trees.  K-nearest neighbor (KNN)  Na\u00efve Bayes."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What are the five steps in the backpropagation learning algorithm?Initialize weights with random values and set other parameters.Read in the input vector and the desired output.Compute the actual output via the calculations, working forward through the layers."}, {"text": "Some of the most popular machine learning algorithms for creating text classification models include the naive bayes family of algorithms, support vector machines, and deep learning.Naive Bayes.  Support Vector Machines.  Deep Learning.  Text Classification with R."}, {"text": "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging."}, {"text": "Some of the popular types of regression algorithms are linear regression, regression trees, lasso regression and multivariate regression."}, {"text": "The most popular supervised NLP machine learning algorithms are: Support Vector Machines. Bayesian Networks. Maximum Entropy."}, {"text": "Bounding boxes is one of the most popular and recognizable image annotation method used in machine learning and deep learning. Using bounding boxes annotators are asked to outline the object in a box as per the machine learning project requirements."}, {"text": "Some popular examples of unsupervised learning algorithms are:k-means for clustering problems.Apriori algorithm for association rule learning problems."}]}, {"question": "What does it mean when you control for a variable", "positive_ctxs": [{"text": "\"Controlling\" for a variable means adding it to the model so its effect on your outcome variable(s) can be estimated and statistically isolated from the effect of the independent variable you're really interested in."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model."}, {"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "If you want to control for the effects of some variables on some dependent variable, you just include them into the model. Say, you make a regression with a dependent variable y and independent variable x. You think that z has also influence on y too and you want to control for this influence."}, {"text": "If you want to control for the effects of some variables on some dependent variable, you just include them into the model. Say, you make a regression with a dependent variable y and independent variable x. You think that z has also influence on y too and you want to control for this influence."}, {"text": "An ordinal variable is a categorical variable for which the possible values are ordered. Ordinal variables can be considered \u201cin between\u201d categorical and quantitative variables. Thus it does not make sense to take a mean of the values."}, {"text": "A clinical trial is a randomized controlled trial only when participants are randomly allocated to the group receiving the treatment and a control group. What participants are allocated among groups receiving different treatments the clinical trial is simply called a randomized trial."}, {"text": "Causation is the relationship between cause and effect. So, when a cause results in an effect, that's a causation.  When we say that correlation does not imply cause, we mean that just because you can see a connection or a mutual relationship between two variables, it doesn't necessarily mean that one causes the other."}]}, {"question": "When would you use a factorial", "positive_ctxs": [{"text": "We use factorials when we look at permutations and combinations. Permutations tell us how many different ways we can arrange things if their order matters. Combinations tells us how many ways we can choose k item from n items if their order does not matter."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation."}, {"text": "The number of different treatment groups that we have in any factorial design can easily be determined by multiplying through the number notation. For instance, in our example we have 2 x 2 = 4 groups. In our notational example, we would need 3 x 4 = 12 groups. We can also depict a factorial design in design notation."}, {"text": "When n * p and n * q are greater than 5, you can use the normal approximation to the binomial to solve a problem."}, {"text": "When you reject the null hypothesis with a t-test, you are saying that the means are statistically different. The difference is meaningful. Chi Square:  When you reject the null hypothesis with a Chi-Square, you are saying that there is a relationship between the two variables."}, {"text": "LSI Graph is a free LSI keyword tool designed to help you identify dozens of related terms to use in your copy. Visit the website and enter your target keyword to generate a long list of potential LSI keywords. When you have a long list of LSI keywords, it may be tempting to use as many as possible in your content."}, {"text": "What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis."}]}, {"question": "What is a learning curve in machine learning", "positive_ctxs": [{"text": "A learning curve is a plot of model learning performance over experience or time. Learning curves are a widely used diagnostic tool in machine learning for algorithms that learn from a training dataset incrementally.  Learning curves are plots that show changes in learning performance over time in terms of experience."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A learning curve plots the score over varying numbers of training samples, while a validation curve plots the score over a varying hyper parameter. The learning curve is a tool for finding out if an estimator would benefit from more data, or if the model is too simple (biased)."}, {"text": "Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}, {"text": "Bayesian inference is a machine learning model not as widely used as deep learning or regression models."}, {"text": "Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging."}, {"text": "Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network."}]}, {"question": "What is Poisson distribution and its characteristics", "positive_ctxs": [{"text": "The Poisson distribution has the following characteristics: It is a discrete distribution. Each occurrence is independent of the other occurrences. It describes discrete occurrences over an interval. The occurrences in each interval can range from zero to infinity."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times."}, {"text": "A Poisson process is a non-deterministic process where events occur continuously and independently of each other.  A Poisson distribution is a discrete probability distribution that represents the probability of events (having a Poisson process) occurring in a certain period of time."}, {"text": "Remember that the Poisson distribution assumes that the mean and variance are the same.  The negative binomial distribution has one parameter more than the Poisson regression that adjusts the variance independently from the mean. In fact, the Poisson distribution is a special case of the negative binomial distribution."}, {"text": "follows a negative binomial distribution with parameters r and p. The geometric distribution is a special case of discrete compound Poisson distribution."}, {"text": "A Poisson queue is a queuing model in which the number of arrivals per unit of time and the number of completions of service per unit of time, when there are customers waiting, both have the Poisson distribution. The Poisson distribution is good to use if the arrivals are all random and independent of each other."}]}, {"question": "How do I verify LDA model", "positive_ctxs": [{"text": "2 AnswersInspect the topics: Look at the highest-likelihood words in each topic. Do they sound like they form a cohesive \"topic\" or just some random group of words?Inspect the topic assignments: Hold out a few random documents from training and see what topics LDA assigns to them."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "According to this link LDA is a generative classifier. Also, the motto of LDA is to model a discriminant function to classify."}, {"text": "The parameters of LDA model have the prior distribution, and are estimated by Bayesian method. LDA model has attracted many scholars' attention since its start, but its mathematical theory is too complex to understand quickly."}, {"text": "Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?"}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}, {"text": "One should always conduct a residual analysis to verify that the conditions for drawing inferences about the coefficients in a linear model have been met. Recall that, if a linear model makes sense, the residuals will: have a constant variance."}, {"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox."}]}, {"question": "What is learning rate in machine learning", "positive_ctxs": [{"text": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.  In setting a learning rate, there is a trade-off between the rate of convergence and overshooting."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck.  The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate."}, {"text": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."}, {"text": "In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."}, {"text": "In machine learning, a hyperparameter is a parameter whose value is used to control the learning process.  An example of a model hyperparameter is the topology and size of a neural network. Examples of algorithm hyperparameters are learning rate and mini-batch size."}, {"text": "Learning rate decay (lrDecay) is a \\emph{de facto} technique for training modern neural networks.  We provide another novel explanation: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns."}, {"text": "False positive rate (FPR) is a measure of accuracy for a test: be it a medical diagnostic test, a machine learning model, or something else. In technical terms, the false positive rate is defined as the probability of falsely rejecting the null hypothesis."}, {"text": "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem."}]}, {"question": "Why does the sampling distribution of the mean follow a normal distribution", "positive_ctxs": [{"text": "When the sample size is sufficiently large, the shape of the sampling distribution approximates a normal curve (regardless of the shape of the parent population)! The distribution of sample means is a more normal distribution than a distribution of scores, even if the underlying population is not normal."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean \u03bc, then the mean of the sampling distribution of the mean is also \u03bc. The symbol \u03bcM is used to refer to the mean of the sampling distribution of the mean."}, {"text": "The parameters of the distribution are m and s2, where m is the mean (expectation) of the distribution and s2 is the variance. We write X ~ N(m, s2) to mean that the random variable X has a normal distribution with parameters m and s2. If Z ~ N(0, 1), then Z is said to follow a standard normal distribution."}, {"text": "Typically by the time the sample size is 30 the distribution of the sample mean is practically the same as a normal distribution.  \u00afX, the mean of the measurements in a sample of size n; the distribution of \u00afX is its sampling distribution, with mean \u03bc\u00afX=\u03bc and standard deviation \u03c3\u00afX=\u03c3\u221an."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "The variance of the sampling distribution of the mean is computed as follows: That is, the variance of the sampling distribution of the mean is the population variance divided by N, the sample size (the number of scores used to compute a mean)."}]}, {"question": "What is the pdf of normal distribution", "positive_ctxs": [{"text": "A continuous random variable Z is said to be a standard normal (standard Gaussian) random variable, shown as Z\u223cN(0,1), if its PDF is given by fZ(z)=1\u221a2\u03c0exp{\u2212z22},for all z\u2208R."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side.  The normal distribution is often called the bell curve because the graph of its probability density looks like a bell."}, {"text": "The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. The area under the normal distribution curve represents probability and the total area under the curve sums to one."}, {"text": "Normal distributions are symmetric around their mean. The mean, median, and mode of a normal distribution are equal. The area under the normal curve is equal to 1.0.  Approximately 95% of the area of a normal distribution is within two standard deviations of the mean."}]}, {"question": "What is considered active activity level", "positive_ctxs": [{"text": "Fewer than 1,000 steps a day is sedentary. 1,000 to 10,000 steps or about 4 miles a day is Lightly Active. 10,000 to 23,000 steps or 4 to 10 miles a day is considered Active. More than 23,000 steps or 10 miles a day is Highly active."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "DEFINITION 1. Given a set of active nodes and an ordering on active nodes, amorphous data-parallelism is the parallelism that arises from simultaneously processing active nodes, subject to neighborhood and ordering constraints."}, {"text": "A false positive state is when the IDS identifies an activity as an attack but the activity is acceptable behavior. A false positive is a false alarm.  This is when the IDS identifies an activity as acceptable when the activity is actually an attack. That is, a false negative is when the IDS fails to catch an attack."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "The significance level for a given hypothesis test is a value for which a P-value less than or equal to is considered statistically significant. Typical values for are 0.1, 0.05, and 0.01. These values correspond to the probability of observing such an extreme value by chance."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "When we refer to values as being \u201cstatistically equivalent\u201d or to a \u201cconclusion of statistical equivalence,\u201d we mean the difference between groups is smaller than what is considered meaningful and statistically falls within the interval indicated by the equivalence bounds. In any one-sided test, for an alpha level of ."}]}, {"question": "What is multiplicative error", "positive_ctxs": [{"text": "A multiplicative error model is one in which the dependent variable is a product of the independent variable and an error term, instead of a sum."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "A bivariate distribution, whose marginals are Poisson is developed as a product of Poisson marginals with a multiplicative factor. The correlation between the two variates can be either positive or negative, depending on the value chosen for the parameter in the above multiplicative factor."}, {"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1."}, {"text": "In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed.  A log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive."}, {"text": "What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population."}, {"text": "What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss."}]}, {"question": "How does selection bias occur", "positive_ctxs": [{"text": "Selection bias can result when the selection of subjects into a study or their likelihood of being retained in the study leads to a result that is different from what you would have gotten if you had enrolled the entire target population."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Types of selection bias The most common type of selection bias in research or statistical analysis is a sample selection bias.  In principle, the bias can occur through selection effects in other aspects of the research process, such as which variables to use in analysis, and which tools to use to perform measurement."}, {"text": "Another way researchers try to minimize selection bias is by conducting experimental studies, in which participants are randomly assigned to the study or control groups (i.e. randomized controlled studies or RCTs). However, selection bias can still occur in RCTs."}, {"text": "If the correlation between education and unobserved ability is positive, omitted variables bias will occur in an upward direction. Conversely, if the correlation between an explanatory variable and an unobserved relevant variable is negative, omitted variables bias will occur in a downward direction."}, {"text": "Evolution is not a random process. The genetic variation on which natural selection acts may occur randomly, but natural selection itself is not random at all. The survival and reproductive success of an individual is directly related to the ways its inherited traits function in the context of its local environment."}, {"text": "Randomization in an experiment is where you choose your experimental participants randomly.  If you use randomization in your experiments, you guard against bias. For example, selection bias (where some groups are underrepresented) is eliminated and accidental bias (where chance imbalances happen) is minimized."}, {"text": "How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!"}, {"text": "Conditions for Poisson Distribution: Events occur independently. In other words, if an event occurs, it does not affect the probability of another event occurring in the same time period. The rate of occurrence is constant; that is, the rate does not change based on time."}]}, {"question": "How is ridge regression related to Bayesian linear regression", "positive_ctxs": [{"text": "Ridge regression uses regularization with L2 norm, while Bayesian regression, is a regression model defined in probabilistic terms, with explicit priors on the parameters. The choice of priors can have the regularizing effect, e.g. using Laplace priors for coefficients is equivalent to L1 regularization."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In statistics, Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference."}, {"text": "Ridge regression is a term used to refer to a linear regression model whose coefficients are not estimated by ordinary least squares (OLS), but by an estimator, called ridge estimator, that is biased but has lower variance than the OLS estimator."}, {"text": "Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity.  By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. It is hoped that the net effect will be to give estimates that are more reliable."}, {"text": "Lasso regression stands for Least Absolute Shrinkage and Selection Operator.  The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero."}, {"text": "Lasso regression stands for Least Absolute Shrinkage and Selection Operator.  The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero."}, {"text": "Lasso regression stands for Least Absolute Shrinkage and Selection Operator.  The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero."}, {"text": "Basically, predicting a continuous variable is termed as regression. There are a no of regression algorithms like ridge and lasso regression you may want to check out.Linear Regression.Logistic Regression.Polynomial Regression.Stepwise Regression.Ridge Regression.Lasso Regression.ElasticNet Regression,"}]}, {"question": "What is the independent variable in a regression", "positive_ctxs": [{"text": "The outcome variable is also called the response or dependent variable, and the risk factors and confounders are called the predictors, or explanatory or independent variables. In regression analysis, the dependent variable is denoted \"Y\" and the independent variables are denoted by \"X\"."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In simple linear regression a single independent variable is used to predict the value of a dependent variable. In multiple linear regression two or more independent variables are used to predict the value of a dependent variable. The difference between the two is the number of independent variables."}, {"text": "In simple linear regression a single independent variable is used to predict the value of a dependent variable. In multiple linear regression two or more independent variables are used to predict the value of a dependent variable. The difference between the two is the number of independent variables."}, {"text": "It is used to determine the extent to which there is a linear relationship between a dependent variable and one or more independent variables.  In simple linear regression a single independent variable is used to predict the value of a dependent variable."}, {"text": "The R-squared of the regression is the fraction of the variation in your dependent variable that is accounted for (or predicted by) your independent variables. (In regression with a single independent variable, it is the same as the square of the correlation between your dependent and independent variable.)"}, {"text": "Linear regression can only be used when one has two continuous variables\u2014an independent variable and a dependent variable. The independent variable is the parameter that is used to calculate the dependent variable or outcome. A multiple regression model extends to several explanatory variables."}, {"text": "An independent variable is defined within the context of a dependent variable. In the context of a model the independent variables are input whereas the dependent variables are the targets (Input vs Output). An exogenous variable is a variable whose state is independent of the state of other variables in a system."}, {"text": "The sign of a regression coefficient tells you whether there is a positive or negative correlation between each independent variable the dependent variable. A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase."}]}, {"question": "What are the functions of neurons", "positive_ctxs": [{"text": "The neuron is the basic working unit of the brain, a specialized cell designed to transmit information to other nerve cells, muscle, or gland cells. Neurons are cells within the nervous system that transmit information to other nerve cells, muscle, or gland cells. Most neurons have a cell body, an axon, and dendrites."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.  Due to this reason, during the backpropogation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated."}, {"text": "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."}, {"text": "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."}, {"text": "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."}, {"text": "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."}, {"text": "The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."}, {"text": "Most recent answer The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."}]}, {"question": "What is the vanishing exploding gradient problem", "positive_ctxs": [{"text": "In a network of n hidden layers, n derivatives will be multiplied together. If the derivatives are large then the gradient will increase exponentially as we propagate down the model until they eventually explode, and this is what we call the problem of exploding gradient ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value."}, {"text": "The ReLU activation solves the problem of vanishing gradient that is due to sigmoid-like non-linearities (the gradient vanishes because of the flat regions of the sigmoid). The other kind of \"vanishing\" gradient seems to be related to the depth of the network (e.g. see this for example)."}, {"text": "Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron."}, {"text": "Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron."}, {"text": "If exploding gradients are still occurring, you can check for and limit the size of gradients during the training of your network. This is called gradient clipping. Dealing with the exploding gradients has a simple but very effective solution: clipping gradients if their norm exceeds a given threshold."}]}, {"question": "What deep learning Cannot do", "positive_ctxs": [{"text": "Deep learning techniques do not perform well when dealing with data with complex hierarchical structures. Deep learning identifies correlations between sets of features that are themselves \u201cflat\u201d or non-hierarchical, as in a simple, unstructured list, but much human and linguistic knowledge is more structured."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Bayesian deep learning is a field at the intersection between deep learning and Bayesian probability theory.  Bayesian deep learning models typically form uncertainty estimates by either placing distributions over model weights, or by learning a direct mapping to probabilistic outputs."}, {"text": "Fine-tuning, in general, means making small adjustments to a process to achieve the desired output or performance. Fine-tuning deep learning involves using weights of a previous deep learning algorithm for programming another similar deep learning process."}, {"text": "Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data."}, {"text": "However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to."}, {"text": "Fine-tuning deep learning involves using weights of a previous deep learning algorithm for programming another similar deep learning process. Weights are used to connect each neuron in one layer to every neuron in the next layer in the neural network."}, {"text": "Dictionary learning is learning a set of atoms so that a given image can be well approximated by a sparse linear combination of these learned atoms, while deep learning methods aim at extracting deep semantic feature representations via a deep network."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}]}, {"question": "What is random walk problem", "positive_ctxs": [{"text": "The problem is to find the probability of landing at a given spot after a given number of steps, and, in particular, to find how far away you are on average from where you started. Why do we care about this game? The random walk is central to statistical physics."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Statistics Definitions > A random walk is a sequence of discrete, fixed-length steps in random directions. Random walks may be 1-dimensional, 2-dimensional, or n-dimensional for any n. A random walk can also be confined to a lattice."}, {"text": "Statistics Definitions > A random walk is a sequence of discrete, fixed-length steps in random directions. Random walks may be 1-dimensional, 2-dimensional, or n-dimensional for any n. A random walk can also be confined to a lattice."}, {"text": "In mathematics, a random walk is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers."}, {"text": "The random walk is simple if Xk = \u00b11, with P(Xk = 1) = p and P(Xk = \u22121) = 1\u2212p = q. Imagine a particle performing a random walk on the integer points of the real line, where it in each step moves to one of its neighboring points; see Figure 1. Remark 1. You can also study random walks in higher dimensions."}, {"text": "It is the simplest model to study polymers. In other fields of mathematics, random walk is used to calculate solutions to Laplace's equation, to estimate the harmonic measure, and for various constructions in analysis and combinatorics. In computer science, random walks are used to estimate the size of the Web."}, {"text": "One of the simplest and yet most important models in time series forecasting is the random walk model. This model assumes that in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (\u201ci.i.d.\u201d)."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}]}, {"question": "What does a negative Kappa mean", "positive_ctxs": [{"text": "agreement worse than expected"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U."}, {"text": "Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?"}, {"text": "False negative would therefore mean that there was a object (result should be positive) but the algorithm did not detect it (and therefore returned negative). A true negative is simply the algorithm correctly stating that the area it checked does not hold an object."}, {"text": "A false negative is a test result that indicates a person does not have a disease or condition when the person actually does have it, according to the National Institute of Health (NIH)."}, {"text": "The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the \u201cerrors\u201d) and squaring them. The squaring is necessary to remove any negative signs."}, {"text": "In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time."}, {"text": "In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time."}]}, {"question": "What is the difference between precision and accuracy", "positive_ctxs": [{"text": "Accuracy refers to how close measurements are to the \"true\" value, while precision refers to how close measurements are to each other."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?"}, {"text": "In other words, accuracy describes the difference between the measurement and the part's actual value, while precision describes the variation you see when you measure the same part repeatedly with the same device."}, {"text": "Definition. Average precision is a measure that combines recall and precision for ranked retrieval results. For one information need, the average precision is the mean of the precision scores after each relevant document is retrieved."}, {"text": "Recall and True Positive Rate (TPR) are exactly the same. So the difference is in the precision and the false positive rate.  While precision measures the probability of a sample classified as positive to actually be positive, the false positive rate measures the ratio of false positives within the negative samples."}, {"text": "Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "It depends. If the message you want to carry is about the spread and variability of the data, then standard deviation is the metric to use. If you are interested in the precision of the means or in comparing and testing differences between means then standard error is your metric."}]}, {"question": "What is an eigenfunction of an operator", "positive_ctxs": [{"text": "An eigenfunction of an operator is a function such that the application of on gives. again, times a constant. (49) where k is a constant called the eigenvalue. It is easy to show that if is a linear operator with an eigenfunction , then any multiple of is also an eigenfunction of ."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Given a linear operator it can have associated eigenvectors / functions.  For example, given the differential operator the exponential function is an eigenfunction of it. This is why we can solve linear homogeneous differential equations by solving a characteristic equation."}, {"text": "The simplest example of a non-linear operator (non-linear functional) is a real-valued function of a real argument other than a linear function.  Under other restrictions on K(t,s,u) an Urysohn operator acts on other spaces, for instance, L2[a,b] or maps one Orlicz space LM1[a,b] into another LM2[a,b]."}, {"text": "The integral operator is a linear operator because it preserves two operations; the addition between functions and the multiplication of a function"}, {"text": "Prewitt operator is similar to the Sobel operator and is used for detecting vertical and horizontal edges in images. However, unlike the Sobel, this operator does not place any emphasis on the pixels that are closer to the center of the mask."}, {"text": "Sigma /\u02c8s\u026a\u0261m\u0259/ (uppercase \u03a3, lowercase \u03c3, lowercase in word-final position \u03c2; Greek: \u03c3\u03af\u03b3\u03bc\u03b1) is the eighteenth letter of the Greek alphabet. In the system of Greek numerals, it has a value of 200. In general mathematics, uppercase \u2211 is used as an operator for summation."}, {"text": "The Laplacian of an image highlights regions of rapid intensity change and is therefore often used for edge detection (see zero crossing edge detectors).  The operator normally takes a single graylevel image as input and produces another graylevel image as output."}, {"text": "Depending on the alternative hypothesis operator, greater than operator will be a right tailed test, less than operator is a left tailed test, and not equal operator is a two tailed test."}]}, {"question": "Why Accuracy is important in machine learning", "positive_ctxs": [{"text": "Companies use machine learning models to make practical business decisions, and more accurate model outcomes result in better decisions. The cost of errors can be huge, but optimizing model accuracy mitigates that cost.  The benefits of improving model accuracy help avoid considerable time, money, and undue stress."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Another most important role of training data for machine learning is classifying the data sets into various categorized which is very much important for supervised machine learning.  It helps them to recognize and classify the similar objects in future, thus training data is very important for such classification."}, {"text": "Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial. Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case."}, {"text": "Time series forecasting is an important area of machine learning that is often neglected. It is important because there are so many prediction problems that involve a time component.  Standard definitions of time series, time series analysis, and time series forecasting."}, {"text": "The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly."}, {"text": "The coefficients used in simple linear regression can be found using stochastic gradient descent.  Linear regression does provide a useful exercise for learning stochastic gradient descent which is an important algorithm used for minimizing cost functions by machine learning algorithms."}, {"text": "Data visualization is an important skill in applied statistics and machine learning. Statistics does indeed focus on quantitative descriptions and estimations of data. Data visualization provides an important suite of tools for gaining a qualitative understanding."}, {"text": "In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged)."}]}, {"question": "How is threshold value calculated in image processing", "positive_ctxs": [{"text": "Automatic thresholdingSelect initial threshold value, typically the mean 8-bit value of the original image.Divide the original image into two portions;  Find the average mean values of the two new images.Calculate the new threshold by averaging the two means.More items"}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Definition: An image processing method that creates a bitonal (aka binary) image based on setting a threshold value on the pixel intensity of the original image.  The thresholding process is sometimes described as separating an image into foreground values (black) and background values (white)."}, {"text": "Definition: An image processing method that creates a bitonal (aka binary) image based on setting a threshold value on the pixel intensity of the original image.  The thresholding process is sometimes described as separating an image into foreground values (black) and background values (white)."}, {"text": "The idea is to separate the image into two parts; the background and foreground.Select initial threshold value, typically the mean 8-bit value of the original image.Divide the original image into two portions;  Find the average mean values of the two new images.Calculate the new threshold by averaging the two means.More items"}, {"text": "Thresholding is a technique in OpenCV, which is the assignment of pixel values in relation to the threshold value provided. In thresholding, each pixel value is compared with the threshold value. If the pixel value is smaller than the threshold, it is set to 0, otherwise, it is set to a maximum value (generally 255)."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}, {"text": "Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image."}, {"text": "We can set a threshold value to classify all the values greater than threshold as 1 and lesser then that as 0. That's how the Y is predicted and we get 'Y-predicted'. The default value for threshold on which we generally get a Confusion Matrix is 0.50."}]}, {"question": "What is the relation between mean and standard deviation", "positive_ctxs": [{"text": "The standard deviation (SD) measures the amount of variability, or dispersion, from the individual data values to the mean, while the standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean.  SD is the dispersion of individual data values."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Mean, variance, and standard deviation The mean of the sampling distribution of the sample mean will always be the same as the mean of the original non-normal distribution. In other words, the sample mean is equal to the population mean. where \u03c3 is population standard deviation and n is sample size."}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation."}, {"text": "Describe the scores in such a sample. If the standard deviation is 0 then the variance is 0 and the mean of the squared deviation scores must be 0.  Thus, when the standard deviation equals 0, all the scores are identical and equal to the mean."}, {"text": "The coefficient of variation is a better risk measure than the standard deviation alone because the CV adjusts for the size of the project. The CV measures the standard deviation divided by the mean and therefore puts the standard deviation into context."}, {"text": "Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean \u2013 Mode = 3 (Mean \u2013 Median)"}, {"text": "The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean."}, {"text": "The difference between the two norms is that the standard deviation is calculating the square of the difference whereas the mean absolute deviation is only looking at the absolute difference. Hence large outliers will create a higher dispersion when using the standard deviation instead of the other method."}]}, {"question": "How do you calculate weighted mean", "positive_ctxs": [{"text": "SummaryWeighted Mean: A mean where some values contribute more than others.When the weights add to 1: just multiply each weight by the matching value and sum it all up.Otherwise, multiply each weight w by its matching value x, sum that all up, and divide by the sum of weights: Weighted Mean = \u03a3wx\u03a3w."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)"}, {"text": "The mean used here is referred to as the arithmetic mean \u2013 the sum of all values divided by the number of cases. When working with grouped data, this mean is sometimes referred to as the weighted mean or, more properly, the weighted arithmetic mean. Ungrouped and group methods."}, {"text": "How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075."}, {"text": "There are different types of mean, viz. arithmetic mean, weighted mean, geometric mean (GM) and harmonic mean (HM). If mentioned without an adjective (as mean), it generally refers to the arithmetic mean."}, {"text": "To calculate the standard error, follow these steps:Record the number of measurements (n) and calculate the sample mean (\u03bc).  Calculate how much each measurement deviates from the mean (subtract the sample mean from the measurement).Square all the deviations calculated in step 2 and add these together:More items\u2022"}, {"text": "How do you create a decision tree?Start with your overarching objective/\u201cbig decision\u201d at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward."}, {"text": "How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis."}]}, {"question": "What is the law of large numbers with respect to histograms", "positive_ctxs": [{"text": "A histogram (graph) of these values provides the sampling distribution of the statistic. The law of large numbers holds that as n increases, a statistic such as the sample mean (X) converges to its true mean (f)\u2014that is, the sampling distribution of the mean collapses on the population mean."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "The law of averages is not a mathematical principle, whereas the law of large numbers is. In probability theory, the law of large numbers is a theorem that describes the result of performing the same experiment a large number of times."}, {"text": "The law of averages is not a mathematical principle, whereas the law of large numbers is.  According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed."}, {"text": "The law of large numbers, in probability and statistics, states that as a sample size grows, its mean gets closer to the average of the whole population.  In a financial context, the law of large numbers indicates that a large entity which is growing rapidly cannot maintain that growth pace forever."}, {"text": "The law of large numbers is a theorem from probability and statistics that suggests that the average result from repeating an experiment multiple times will better approximate the true or expected underlying result. The law of large numbers explains why casinos always make money in the long run."}, {"text": "Functions of Random Variables One law is called the \u201cweak\u201d law of large numbers, and the other is called the \u201cstrong\u201d law of large numbers. The weak law describes how a sequence of probabilities converges, and the strong law describes how a sequence of random variables behaves in the limit."}, {"text": "The law of averages is a false belief, sometimes known as the 'gambler's fallacy,' that is derived from the law of large numbers.  The law of averages is a misconception that probability occurs with a small number of consecutive experiments so they will certainly have to 'average out' sooner rather than later."}, {"text": "Examples. According to the law of large numbers, if a large number of six-sided dice are rolled, the average of their values (sometimes called the sample mean) is likely to be close to 3.5, with the precision increasing as more dice are rolled."}]}, {"question": "Is Machine Learning Biased", "positive_ctxs": [{"text": "Machine learning, a subset of artificial intelligence (AI), depends on the quality, objectivity and size of training data used to teach it.  Machine learning bias generally stems from problems introduced by the individuals who design and/or train the machine learning systems."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models."}, {"text": "Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models."}, {"text": "Scalable Machine Learning occurs when Statistics, Systems, Machine Learning and Data Mining are combined into flexible, often nonparametric, and scalable techniques for analyzing large amounts of data at internet scale."}, {"text": "Deep Learning is the evolution of Machine Learning and it will definitely help in making machines better than what Machine Learning does. But one thing to note is that Deep Learning models require a very large amount of data to train the model otherwise it won't work as expected."}, {"text": "7 Top Linear Algebra Resources For Machine Learning BeginnersEssence Of Linear Algebra By 3Blue1Brown.Linear Algebra By Khan Academy.Basic Linear Algebra for Deep Learning By Niklas Donges.Computational Linear Algebra for Coders By fast.ai.Deep Learning Book By Ian Goodfellow and Yoshua Bengio and Aaron Courville.Linear Algebra for Machine Learning By AppliedAICourse.More items\u2022"}, {"text": "Usually, Deep Learning takes more time to train as compared to Machine Learning. The main reason is that there are so many parameters in a Deep Learning algorithm. Whereas Machine Learning takes much less time to train, ranging from a few seconds to a few hours."}, {"text": "A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either"}]}, {"question": "How do you find the difference between two categorical variables", "positive_ctxs": [{"text": "Chi-square Test. The Pearson's \u03c72 test (after Karl Pearson, 1900) is the most commonly used test for the difference in distribution of categorical variables between two or more independent groups."}], "negative_ctxs": [], "hard_negative_ctxs": [{"text": "Chi-square Test. The Pearson's \u03c72 test (after Karl Pearson, 1900) is the most commonly used test for the difference in distribution of categorical variables between two or more independent groups."}, {"text": "In linear regression the independent variables can be categorical and/or continuous. But, when you fit the model if you have more than two category in the categorical independent variable make sure you are creating dummy variables."}, {"text": "In linear regression the independent variables can be categorical and/or continuous. But, when you fit the model if you have more than two category in the categorical independent variable make sure you are creating dummy variables."}, {"text": "Covariance measures the total variation of two random variables from their expected values.  Obtain the data.Calculate the mean (average) prices for each asset.For each security, find the difference between each value and mean price.Multiply the results obtained in the previous step.More items"}, {"text": "Covariance measures the total variation of two random variables from their expected values.  Obtain the data.Calculate the mean (average) prices for each asset.For each security, find the difference between each value and mean price.Multiply the results obtained in the previous step.More items"}, {"text": "Association between two variables means the values of one variable relate in some way to the values of the other. Association is usually measured by correlation for two continuous variables and by cross tabulation and a Chi-square test for two categorical variables."}, {"text": "Association between two variables means the values of one variable relate in some way to the values of the other. Association is usually measured by correlation for two continuous variables and by cross tabulation and a Chi-square test for two categorical variables."}]}]