	input_text	target_text
1926	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is unsupervised learning how does it relate to supervised learning
6500	Set the significance level, , the probability of making a Type I error to be small — 0.01, 0.05, or 0.10. Compare the P-value to . If the P-value is less than (or equal to) , reject the null hypothesis in favor of the alternative hypothesis. If the P-value is greater than , do not reject the null hypothesis.	How do you reject the null hypothesis based on P value
5755	Minibatch Discrimination is a discriminative technique for generative adversarial networks where we discriminate between whole minibatches of samples rather than between individual samples. This is intended to avoid collapse of the generator.	What is minibatch discrimination
3957	Eigenvectors can be used to represent a large dimensional matrix. This means that a matrix M and a vector o can be replaced by a scalar n and a vector o. In this instance, o is the eigenvector and n is the eigenvalue and our target is to find o and n.	What do the eigenvectors indicate
5974	Machine learning (ML) systems promise disruptive capabilities in multiple industries.  Behind the hype, there are three essential risks to analyze when building an ML system: 1) poor problem solution alignment, 2) excessive time or monetary cost, and 3) unexpected behavior once deployed.	What is risk in machine learning
7329	There are two types of estimations used: point and interval. A point estimation is a type of estimation that uses a single value, a sample statistic, to infer information about the population.  Interval estimation is the range of numbers in which a population parameter lies considering margin of error.	What are the types of estimation in statistics
1138	"In terms of machine learning, ""concept learning"" can be defined as: “The problem of searching through a predefined space of potential hypotheses for the hypothesis that best fits the training examples.” — Tom Michell. Much of human learning involves acquiring general concepts from past experiences."	What is concept in machine learning
7236	The sample frame, or sample universe, is the data that our sample is drawn from. In the case of the March, 2000 CPS, the sample universe includes all people residing in the US in March, 2000, who were not living in institutional settings.  In theory, our sample is drawn from the sample universe.	What is universe and sampling
524	We reject the null hypothesis when the p-value is less than α. But 0.07 > 0.05 so we fail to reject H0.  For example if the p-value = 0.08, then we would fail to reject H0 at the significance level of α=0.05 since 0.08 > 0.05, but we would reject H0 at the significance level of α = 0.10 since 0.08 < 0.10.	Do you reject or fail to reject h0 at the 0.05 level of significance
97	Regression is primarily used to build models/equations to predict a key response, Y, from a set of predictor (X) variables. Correlation is primarily used to quickly and concisely summarize the direction and strength of the relationships between a set of 2 or more numeric variables.	Should I use correlation or regression
8319	For example, an ANOVA test assumes that the variances of different populations are equal (i.e. homogeneous). One example of a test is the Chi-Square Test for Homogeneity. This tests to see if two populations come from the same unknown distribution (if they do, then they are homogeneous).	How do you know if a population is homogeneous
7210	Least squares also has issues dealing with multicollinearity in data. Ridge regression avoids all of these problems. It works in part because it doesn't require unbiased estimators; While least squares produces unbiased estimates, variances can be so large that they may be wholly inaccurate.	Why does ridge regression improve over Least Squares
4430	Page 1. Abstract: Structural Vector Autoregressions (SVARs) are a multivariate, linear repre- sentation of a vector of observables on its own lags. SVARs are used by economists to recover economic shocks from observables by imposing a minimum of assumptions compatible with a large class of models.	What is a structural vector autoregressive model
1185	Time series regression is a statistical method for predicting a future response based on the response history (known as autoregressive dynamics) and the transfer of dynamics from relevant predictors.  Time series regression is commonly used for modeling and forecasting of economic, financial, and biological systems.	What are some methods of time series regression analysis
6093	Control variables are usually variables that you are not particularly interested in, but that are related to the dependent variable. You want to remove their effects from the equation. A control variable enters a regression in the same way as an independent variable - the method is the same.	How does one control for a variable in multivariate regression
1482	For example, you could be: 25 years, 10 months, 2 days, 5 hours, 4 seconds, 4 milliseconds, 8 nanoseconds, 99 picosends…and so on. Time is a continuous variable. You could turn age into a discrete variable and then you could count it.	What are examples of discrete and continuous variables
6463	DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like hallucinogenic appearance in the deliberately over-processed images.	What is the deep dream generator
1326	4.3 The method: evolutionary computation. EC is a computational intelligence technique inspired from natural evolution. An EC algorithm starts with creating a population consisting of individuals that represent solutions to the problem. The first population could be created randomly or fed into the algorithm.	What is evolutionary computation in AI
3726	Cluster Analysis and Factor Analysis. Latent Class Analysis is similar to cluster analysis. Observed data is analyzed, connections are found, and the data is grouped into clusters.  Another difference is that LCA includes discrete latent categorical variables that have a multinomial distribution.	What is the difference between cluster analysis and latent class analysis
4257	When comparing two groups, you need to decide whether to use a paired test. When comparing three or more groups, the term paired is not apt and the term repeated measures is used instead. Use an unpaired test to compare groups when the individual values are not paired or matched with one another.	What statistical analysis should I use to compare two groups
2979	In neural networks, Convolutional neural network (ConvNets or CNNs) is one of the main categories to do images recognition, images classifications. Objects detections, recognition faces etc., are some of the areas where CNNs are widely used.	What is convolutional neural network in image processing
1233	HMMs is the Hidden Markov Models library for Python. It is easy to use, general purpose library, implementing all the important submethods, needed for the training, examining and experimenting with the data models.	What is the best Python library for Hidden Markov Models
6818	These lessons on probability will include the following topics: Samples in probability, Probability of events, Theoretical probability, Experimental probability, Probability problems, Tree diagrams, Mutually exclusive events, Independent events, Dependent events, Factorial, Permutations, Combinations, Probability in	What are the topics in probability
7112	In short, Softmax Loss is actually just a Softmax Activation plus a Cross-Entropy Loss. Softmax is an activation function that outputs the probability for each class and these probabilities will sum up to one. Cross Entropy loss is just the sum of the negative logarithm of the probabilities.	Is the softmax loss the same as the cross entropy loss
1674	"In mathematical optimization and decision theory, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some ""cost"" associated with the event.  In optimal control, the loss is the penalty for failing to achieve a desired value."	What does loss function do
270	TensorFlow Federated (TFF) is an open-source framework for machine learning and other computations on decentralized data.  For example, FL has been used to train prediction models for mobile keyboards without uploading sensitive typing data to servers.	What is TensorFlow Federated
7347	There are two reasons why Mean Squared Error(MSE) is a bad choice for binary classification problems:  If we use maximum likelihood estimation(MLE), assuming that the data is from a normal distribution(a wrong assumption, by the way), we get the MSE as a Cost function for optimizing our model.	Why is squared loss bad for classification
867	(e.g. if P=1/256, that's 8 bits.) Entropy is just the average of that information bit length, over all the outcomes. The purpose of log(pi) appearing in Shannon's Entropy is that log(pi) is the only function satisfying the basic set of properties that the entropy function, H(p1,…,pN), is held to embody.	Why log is used in entropy
1262	One of the most intuitive explanations of eigenvectors of a covariance matrix is that they are the directions in which the data varies the most.  The eigenvectors of the covariance matrix of these data samples are the vectors u and v; u, longer arrow, is the first eigenvector and v, the shorter arrow, is the second.	What is an eigenvector of a covariance matrix
5579	Joint probability is the probability of two events occurring simultaneously. Marginal probability is the probability of an event irrespective of the outcome of another variable. Conditional probability is the probability of one event occurring in the presence of a second event.	What is joint Marginal and conditional probability
1088	In many situations, the degrees of freedom are equal to the number of observations minus one. Thus, if the sample size were 20, there would be 20 observations; and the degrees of freedom would be 20 minus 1 or 19.	How do you find the degrees of freedom for an F test
8440	In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code.	How are expert systems used
3543	In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution.	Is gamma distribution discrete or continuous
4583	Selection Sort in CExample of Selection Sort.Algorithm for Selection Sort:Step 1 − Set min to the first location.Step 2 − Search the minimum element in the array.Step 3 – swap the first location with the minimum value in the array.Step 4 – assign the second element as min.Step 5 − Repeat the process until we get a sorted array.More items•	How do you write an algorithm for a selection sort
8639	More formally, statistical power is the probability of finding a statistically significant result, given that there really is a difference (or effect) in the population.  So, larger sample sizes give more reliable results with greater precision and power, but they also cost more time and money.	How does sample size affect the accuracy of your sample
355	The method cannot be considered to derive composite relations. Examples s = ut + 1/2 at2 and 2as = v2 – u2. A formula containing trigonometric function, exponential function, and logarithmic function can not derive from it. The method cannot be used to derive the relationship between more than three quantities.	What are the disadvantages of dimensional analysis
320	In probability theory and related fields, a stochastic or random process is a mathematical object usually defined as a family of random variables.  Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner.	What is probability and random process
3273	From Wikipedia, the free encyclopedia. The control variates method is a variance reduction technique used in Monte Carlo methods. It exploits information about the errors in estimates of known quantities to reduce the error of an estimate of an unknown quantity.	What is control variates in variance reduction
5910	Tests for randomness can be used to determine whether a data set has a recognisable pattern, which would indicate that the process that generated it is significantly non-random.  These generators do not always generate sequences which are sufficiently random, but instead can produce sequences which contain patterns.	What does randomness in data signify
763	Deep learning neural networks are trained using the stochastic gradient descent optimization algorithm. The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.	Why is learning rate used in gradient descent optimization
270	"""Degrees of freedom"" is commonly abbreviated to df.  When this principle of restriction is applied to regression and analysis of variance, the general result is that you lose one degree of freedom for each parameter estimated prior to estimating the (residual) standard deviation."	Is degrees of freedom the same as standard deviation
8627	0:001:38Suggested clip · 98 secondsFind the matrix A given the eigenvalues and eigenvectors - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find eigenvectors from eigenvalues and matrices
7119	Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.  LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series.	What is Lstm in machine learning
211	"""Normal"" data are data that are drawn (come from) a population that has a normal distribution. This distribution is inarguably the most important and the most frequently used distribution in both the theory and application of statistics. If X is a normal random variable, then the probability distribution of X is."	What is a normal population distribution
364	We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture. Example Architecture: Overview.	How many layers does CNN have
390	Advantages of convenience samplingConvenience sampling is vey easy to carry out with few rules governing how the sample should be collected.The relative cost and time required to carry out a convenience sample are small in comparison to probability sampling techniques.More items	What are the advantages of convenience sampling
5132	In probability, the set of outcomes from an experiment is known as an Event. So say for example you conduct an experiment by tossing a coin. The outcome of this experiment is the coin landing 'heads' or 'tails'. These can be said to be the events connected with the experiment.	What is an event in probability example
1601	To develop or improve your inductive reasoning, focus on the following skills: Paying attention to detail: No one can draw conclusions based on details without first noticing those details; paying attention is crucial to inductive reasoning.	How does one improve his or her inductive reasoning skills
3402	Augmented Analytics This form of analytics is going to play a huge role in analysing data in 2020. Augmented analytics is going to be the future of data analytics because it can scrub raw data for valuable parts for analysis, automating certain parts of the process and making the data preparation process easier.	What is future of data analytics
8163	If your regression model contains independent variables that are statistically significant, a reasonably high R-squared value makes sense. The statistical significance indicates that changes in the independent variables correlate with shifts in the dependent variable.	How do you know if a regression variable is significant
6988	State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning.  The acronym for the quintuple (st, at, rt, st+1, at+1) is SARSA.	What is sarsa in machine learning
770	How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!	How do you identify omitted variable bias
342	Typical discriminative models include logistic regression (LR), support vector machines (SVM), conditional random fields (CRFs) (specified over an undirected graph), decision trees, neural networks, and many others.	Is neural network discriminative model
8516	To find your weighted average, simply multiply each number by its weight factor and then sum the resulting numbers up. For example: The weighted average for your quiz grades, exam, and term paper would be as follows: 82(0.2) + 90(0.35) + 76(0.45) = 16.4 + 31.5 + 34.2 = 82.1.	How do you calculate a weighted total score
3010	Moments help in finding AM, standard deviation and variance of the population directly, and they help in knowing the graphic shapes of the population. We can call moments as the constants used in finding the graphic shape, as the graphic shape of the population also help a lot in characterizing a population.	What is the use of moments in statistics
725	The year is a categorical variable. The ratio between two years is not meaningful which is why its not appropriate to classify it as a quantitative variable.	Is year a quantitative or categorical variable
5223	An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words.  An embedding can be learned and reused across models.	What does embedding mean in machine learning
296	In Neural network, some inputs are provided to an artificial neuron, and with each input a weight is associated. Weight increases the steepness of activation function. This means weight decide how fast the activation function will trigger whereas bias is used to delay the triggering of the activation function.	What is the meaning of bias and weight in neural network
1311	The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. The lasso does this by imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink toward zero.	Why do we use Lasso regression
7904	The log transformation can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics. Figure 1 shows an example of how a log transformation can make patterns more visible.	When should you log transform data
940	Probability density function (PDF) is a statistical expression that defines a probability distribution (the likelihood of an outcome) for a discrete random variable (e.g., a stock or ETF) as opposed to a continuous random variable.	What does probability density function represent
5226	Machine learning helps computers understand what they see So computer vision methods nowadays leverage intelligent algorithms and systems. Even to an extent that visual computing has become one of the main fields of this technology's successful application.	Does computer vision use machine learning
2029	The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig. 2.	What is the difference between neural network and convolutional neural network
1220	(mathematics) A symbol representing a product over a set of terms.	What does ∏ mean
695	While many people use the terms interchangeably, data science and big data analytics are unique fields, with the major difference being the scope.  Data science produces broader insights that concentrate on which questions should be asked, while big data analytics emphasizes discovering answers to questions being asked.	Is data science the same as data analytics
7513	Look at normality plots of the data. “Normal Q-Q Plot” provides a graphical way to determine the level of normality. The black line indicates the values your sample should adhere to if the distribution was normal.  If the dots fall exactly on the black line, then your data are normal.	How do you know if your data is normally distributed
8613	When two events are dependent events, one event influences the probability of another event. A dependent event is an event that relies on another event to happen first.	What does it mean if two events are dependent
1624	Prior probability shift. Prior probability shift refers to changes in the distribution of the class variable y It also appears with different names in the class variable y. It also appears with different names in the literature and the definitions have slight differences between them.	What is prior probability shift
6203	Values range from 0 to 1, where 0 is perfect disagreement and 1 is perfect agreement. Krippendorff suggests: “[I]t is customary to require α ≥ . 800. Where tentative conclusions are still acceptable, α ≥ .	What is a good krippendorff's Alpha
1135	Alpha levels and beta levels are related: An alpha level is the probability of a type I error, or rejecting the null hypothesis when it is true. A beta level, usually just called beta(β), is the opposite; the probability of of accepting the null hypothesis when it's false.	What is alpha and beta in statistics
8265	A commonly used rule says that a data point is an outlier if it is more than 1.5 ⋅ IQR 1.5\cdot \text{IQR} 1. 5⋅IQR1, point, 5, dot, start text, I, Q, R, end text above the third quartile or below the first quartile.	How do you determine if there are outliers in a data set
2642	Properties of the SVD U is a n × k matrix with orthonormal columns, UT U = Ik, where Ik is the k × k identity matrix. V is an orthonormal k × k matrix, V T = V −1 . S is a k ×k diagonal matrix, with the non-negative singular values, s1,s2,,sk, on the diagonal.	What is U and V SVD
2752	A random variable is a variable whose value is unknown or a function that assigns values to each of an experiment's outcomes.  Random variables are often used in econometric or regression analysis to determine statistical relationships among one another.	What exactly is a random variable
4891	For multi class classification using SVM; It is NOT (one vs one) and NOT (one vs REST). Instead learn a two-class classifier where the feature vector is (x, y) where x is data and y is the correct label associated with the data.	Can SVM do multiclass classification
28	One-shot learning is a classification task where one example (or a very small number of examples) is given for each class, that is used to prepare a model, that in turn must make predictions about many unknown examples in the future.	How does SHOT learning work
680	How To Develop a Machine Learning Model From ScratchDefine adequately our problem (objective, desired outputs…).Gather data.Choose a measure of success.Set an evaluation protocol and the different protocols available.Prepare the data (dealing with missing values, with categorial values…).Spilit correctly the data.More items	How do you run a machine learning model
4769	Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.	Logistic Regression Why sigmoid function
4807	The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable.	Why do we use sigmoid and not any increasing function from 0 to 1
815	Advantages and Disadvantages of Artificial Intelligence Reduction in Human Error: The phrase “human error” was born because humans make mistakes from time to time.   Takes risks instead of Humans:   Available 24x7:   Helping in Repetitive Jobs:   Digital Assistance:   Faster Decisions:   Daily Applications:   New Inventions:	What are some of the benefits of AI development
5098	Calculating the distance of various points in the scene relative to the position of the camera is one of the important tasks for a computer vision system.	What is depth computer vision
345	According to the central limit theorem, the mean of a sampling distribution of means is an unbiased estimator of the population mean. Note that the larger the sample, the less variable the sample mean. The mean of many observations is less variable than the mean of few.	Are the samples dependent in the Central Limit Theorem
8436	An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words.  An embedding can be learned and reused across models.	What is an embedding in deep learning
3918	Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients.  It is the most common implementation of gradient descent used in the field of deep learning.	What is mini batch gradient descent
2668	Minimax GAN loss refers to the minimax simultaneous optimization of the discriminator and generator models. Minimax refers to an optimization strategy in two-player turn-based games for minimizing the loss or cost for the worst case of the other player.	What is Gan loss
1440	Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences.	What can reinforcement learning do
549	While there are a number of different methods for measuring intelligence, the standard and most widely accepted method is by measuring a person's 'intelligence quotient' or IQ. Based on a series of tests which assess various types of abilities such a mathematical, spatial, verbal, logic and memory.	How do you assess your own level of intelligence
6788	general Neural Networks end up solving a non-convex optimization methods , while Logistic Regression end up with a Convex Optimization problem for which global optima can be found very efficiently. Modern neural networks rarely use Logistic regression. They use ReLU or tanH as the neural activation function.	How do neural networks differ from logistic regression
7177	The main argument against using linear regression for time series data is that we're usually interested in predicting the future, which would be extrapolation (prediction outside the range of the data) for linear regression. Extrapolating linear regression is seldom reliable.	Why cant you use linear regression for time series data
7922	Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.	What is the purpose of bootstrapping in statistics
7007	3:456:33Suggested clip · 56 secondsStatQuest - Sample Size and Effective Sample Size, Clearly ExplainedYouTubeStart of suggested clipEnd of suggested clip	How do you calculate effective sample size
192	5 ways to deal with outliers in dataSet up a filter in your testing tool. Even though this has a little cost, filtering out outliers is worth it.  Remove or change outliers during post-test analysis.  Change the value of outliers.  Consider the underlying distribution.  Consider the value of mild outliers.	How do you handle outliers in a data set
3024	The standard solution that psychologists take to measuring latent variables is to use a series of questions that are all designed to measure the latent variable. This is known as a multi-item scale, where an “item” is a question, and a “scale” is the resulting estimate of the latent variable.	How do you find the latent variable
2877	Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks.  Spreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents.	How does activation spread through a semantic network
3419	Part 1: Making the CalculationsStep 1: Find p,q, and n:Step 2: Figure out if you can use the normal approximation to the binomial.  Step 3: Find the mean, μ by multiplying n and p:  Step 4: Multiply step 3 by q :  Step 5: Take the square root of step 4 to get the standard deviation, σ:More items	How do you do normal approximation
4612	The Wilcoxon test is a nonparametric statistical test that compares two paired groups, and comes in two versions the Rank Sum test or the Signed Rank test. The goal of the test is to determine if two or more sets of pairs are different from one another in a statistically significant manner.	What does the Wilcoxon signed rank test measure
1389	DBSCAN works as such: Divides the dataset into n dimensions. For each point in the dataset, DBSCAN forms an n dimensional shape around that data point, and then counts how many data points fall within that shape. DBSCAN counts this shape as a cluster.	How does Dbscan algorithm work
2971	They have a similar structure but they apply under different conditions and guarantee different kinds of points. IVT guarantees a point where the function has a certain value between two given values.  MVT guarantees a point where the derivative has a certain value.	What is the difference between IVT and MVT
3082	Mixed effects logistic regression is used to model binary outcome variables, in which the log odds of the outcomes are modeled as a linear combination of the predictor variables when data are clustered or there are both fixed and random effects.	What is mixed effect logistic regression
2451	The Monty Hall problem has confused people for decades. In the game show, Let's Make a Deal, Monty Hall asks you to guess which closed door a prize is behind. The answer is so puzzling that people often refuse to accept it! The problem occurs because our statistical assumptions are incorrect.	Why the Monty Hall problem is wrong
5241	Random forest (RF) is a machine-learning method that generally works well with high-dimensional problems and allows for nonlinear relationships between predictors; however, the presence of correlated predictors has been shown to impact its ability to identify strong predictors.	Can random forest handle correlated variables
6797	Most data can be categorized into 4 basic types from a Machine Learning perspective: numerical data, categorical data, time-series data, and text.	What are the different types of data sets used in ML
6245	It's a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction.	What is loss function in machine learning
339	Momentum [1] or SGD with momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging. It is one of the most popular optimization algorithms and many state-of-the-art models are trained using it.	What is momentum in machine learning
7865	In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.	What is the learning rate in machine learning
64	Heterogeneity in statistics means that your populations, samples or results are different. It is the opposite of homogeneity, which means that the population/data/results are the same.	What does statistical heterogeneity mean
5092	Q-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It's considered off-policy because the q-learning function learns from actions that are outside the current policy, like taking random actions, and therefore a policy isn't needed.	How does Q learning work
1630	Validation set is different from test set. Validation set actually can be regarded as a part of training set, because it is used to build your model, neural networks or others. It is usually used for parameter selection and to avoild overfitting.  Test set is used for performance evaluation.	Why do we need a validation set and test set what is the difference between them
5424	The F-distribution, also known Fisher-Snedecor distribution is extensively used to test for equality of variances from two normal populations. F-distribution got its name after R.A. Fisher who initially developed this concept in 1920s. It is a probability distribution of an F-statistic.	What is F distribution used for in statistics
3875	The disadvantages are numerous. Cross-over studies are often of longer duration than parallel-group studies. There may be difficulty in incorporating multiple dosage arms and in dealing with drop-outs; patients who only complete the first evaluation phase contribute little to the analysis.	What is a weakness of the cross over randomized trials
2799	Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. (	What are gradient boosting models
6870	The intuition is simple projection. This picture is from wiki. is the observed response and we predict by the linear combination of the explanatory variables which in inside the vector space .	What is the intuition behind the least squared regression formula
6016	Much of the modern innovations in image recognition is reliant on Deep Learning technology, an advanced type of Machine Learning, and the modern wonder of Artificial Intelligence.  For image recognition, the kind of neural network used is called convolutional neural networks.	Is image recognition deep learning
6114	From Wikipedia, the free encyclopedia. In statistics and signal processing, a minimum mean square error (MMSE) estimator is an estimation method which minimizes the mean square error (MSE), which is a common measure of estimator quality, of the fitted values of a dependent variable.	What is minimum error
1316	Frequency distribution in statistics provides the information of the number of occurrences (frequency) of distinct values distributed within a given period of time or interval, in a list, table, or graphical representation. Grouped and Ungrouped are two types of Frequency Distribution.	What is frequency distribution and its types
1591	In order to fit the best intercept line between the points in the above scatter plots, we use a metric called “Sum of Squared Errors” (SSE) and compare the lines to find out the best fit by reducing errors.	What is the metric used by ordinary least squares OLS to determine the best fit line
1969	To find the interquartile range (IQR), ​first find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1.	How do u find the interquartile range
176	Data is a set of qualitative or quantitative variables – it can be structured or unstructured, machine readable or not, digital or analogue, personal or not.  There are “dimensions” that distinguish data from BIG DATA, summarised as the “3 Vs” of data: Volume, Variety, Velocity. Hence, BIG DATA, is not just “more” data.	What is difference between data and big data
4503	A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems — yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions.	What is decision tree explain with example
2715	The logarithm is to exponentiation as division is to multiplication: The logarithm is the inverse of the exponent: it undoes exponentiation. When studying logarithms, always remember the following fundamental equivalence: if and only if . Whenever one of these is true, so is the other.	What is the intuition behind the logarithm
157	A convolutional stage in a neural network ensures that each part of the neural network has essentially the same edge detector. So even if your data are biased to have edges only in, say, the lower left side of the image set, your connection weights will not reflect this systematic bias.	Why do we use convolution in neural networks
394	Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.	What is meant by Gaussian distribution
120	The AUC value lies between 0.5 to 1 where 0.5 denotes a bad classifer and 1 denotes an excellent classifier.	What is a good PR AUC score
864	Word2Vec takes texts as training data for a neural network. The resulting embedding captures whether words appear in similar contexts. GloVe focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together.	What is the difference between GloVe and Word2Vec
395	Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron.	What is the role of the activation function in a neural network How does this function in a human neural network system
1047	I often draw a distinction between exploratory and explanatory data analysis. Exploratory analysis is what you do to get familiar with the data.  Explanatory analysis is what happens when you have something specific you want to show an audience - probably about those 1 or 2 precious gemstones.	What is explanatory data analysis
128	1a(1) : of, relating to, resembling, or having a graph that is a line and especially a straight line : straight. (2) : involving a single dimension.	What does the term linear mean
629	It is easy to check that the MLE is an unbiased estimator (E[̂θMLE(y)] = θ). To determine the CRLB, we need to calculate the Fisher information of the model.	Is the MLE an unbiased estimator
7721	There are two groups of metrics that may be useful for imbalanced classification because they focus on one class; they are sensitivity-specificity and precision-recall.	When the data is highly imbalanced What are the metrics to be considered in order to evaluate the model
6618	"AUC stands for ""Area under the ROC Curve."" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). Figure 5. AUC (Area under the ROC Curve)."	What is the area under the ROC curve
7	A statistic T = r(X1,X2,··· ,Xn) is a sufficient statistic if for each t, the conditional distribution of X1,X2, ···,Xn given T = t and θ does not depend on θ.	How do you calculate sufficient statistics
49	1:314:30Suggested clip · 120 secondsCumulative Frequency Distribution (Less than and More than YouTubeStart of suggested clipEnd of suggested clip	How do you calculate less than or more cumulative frequency
2010	How to train a Machine Learning model in 5 minutesModel Naming — Give Your Model a Name: Let's start with giving your model a name, describe your model and attach tags to your model.  Data Type Selection — Choose data type(Images/Text/CSV): It's time to tell us about the type of data you want to train your model.More items	How do you train datasets in machine learning
2711	Manipulate data using Excel or Google Sheets. This may include plotting the data out, creating pivot tables, and so on. Analyze and interpret the data using statistical tools (i.e. finding correlations, trends, outliers, etc.). Present this data in meaningful ways: graphs, visualizations, charts, tables, etc.	How do you start a data analysis
4232	The second reason you may see validation loss lower than training loss is due to how the loss value are measured and reported: Training loss is measured during each epoch. While validation loss is measured after each epoch.	Why is my validation loss lower than training loss
5544	Data wrangling is the process of gathering, selecting, and transforming data to answer an analytical question. Also known as data cleaning or “munging”, legend has it that this wrangling costs analytics professionals as much as 80% of their time, leaving only 20% for exploration and modeling.	What is data wrangling process
5536	The chi-square statistic can never be negative. What does it mean to obtain a negative value for the chi-square statistic? the null hypothesis is rejected if the observed U is less than or equal to the critical U.	What does it mean to obtain a negative value for the chi square statistic
6782	Fisher's exact test obtains its two-tailed P value by computing the probabilities associated with all possible tables that have the same row and column totals. Then, it identifies the alternative tables with a probability that is less than that of the observed table.	What is an intuitive explanation of the Fisher exact test
8049	"International communication (also referred to as the study of global communication or transnational communication) is the communication practice that occurs across international borders.  International communication ""encompasses political, economic, social, cultural and military concerns""."	What is the meaning of international communication
7774	Artificial neural networks use backpropagation as a learning algorithm to compute a gradient descent with respect to weights.  Because backpropagation requires a known, desired output for each input value in order to calculate the loss function gradient, it is usually classified as a type of supervised machine learning.	Why backpropagation algorithm is used
7214	In most mechanical systems or models, you can determine the degrees of freedom using the following formula:DOF = 6 x (number of bodies not including ground) – constraints.DOF = (6 x 1) – (2 x 5)DOF = 6 x (number of bodies not including ground) – constraints + redundancies.1 = (6 x 1) – 10 + redundancies.More items	How do you find the degrees of freedom for a system
5817	Statistical Package for the Social Sciences	What does SPSS stand for in statistics
4553	In the “Compute Variable” dialog box that opens, enter a name for the new centered variable in the “Target Variable:” text box at the top right. In the “Numeric Expression:” box, write “math-52.65” as shown in Figure 2. Press OK to create the centered variable.	How do I center variables in SPSS
8581	Summary. The probability distribution of a discrete random variable X is a listing of each possible value x taken by X along with the probability P(x) that X takes that value in one trial of the experiment.	What is the distribution of the random variable X
5968	Grid-searching is the process of scanning the data to configure optimal parameters for a given model.  Grid-searching can be applied across machine learning to calculate the best parameters to use for any given model.	What is grid search in machine learning
1093	The definition of conditional probability can be rewritten as: P(E ∩F) = P(E|F)P(F) which we call the Chain Rule. Intuitively it states that the probability of observing events E and F is the. probability of observing F, multiplied by the probability of observing E, given that you have observed F.	Whats an intuitive explanation of the chain rule of probability
2462	According to the (Research Methods for Business Students) book, to assess the relationship between two ordinal variables is by using Spearman's rank correlation coefficient (Spearman's rho) or Kendall's rank-order correlation coefficient (Kendall's tau).	What type of correlation coefficient would you calculate if you wanted to examine the relationship between two ordinal variables
4918	SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.	Is SVM a linear classifier
277	The general regression tree building methodology allows input variables to be a mixture of continuous and categorical variables. A decision tree is generated when each decision node in the tree contains a test on some input variable's value. The terminal nodes of the tree contain the predicted output variable values.	What is regression tree in machine learning
1309	Properties of a normal distributionThe mean, mode and median are all equal.The curve is symmetric at the center (i.e. around the mean, μ).Exactly half of the values are to the left of center and exactly half the values are to the right.The total area under the curve is 1.	How do you interpret a normal distribution curve
1328	Traditional machine learning methods such as Naïve Bayes, Logistic Regression and Support Vector Machines (SVM) are widely used for large-scale sentiment analysis because they scale well.	Which model is best for sentiment analysis
1537	There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. For a robot that is learning to walk, the state is the position of its two legs. For a Go program, the state is the positions of all the pieces on the board.	What is state in reinforcement learning
8094	Cohen's kappa measures the agreement between two raters who each classify N items into C mutually exclusive categories.¹ A simple way to think this is that Cohen's Kappa is a quantitative measure of reliability for two raters that are rating the same thing, corrected for how often that the raters may agree by chance.	What does Cohen's kappa mean
4172	Similarity is the measure of how much alike two data objects are. Similarity in a data mining context is usually described as a distance with dimensions representing features of the objects. A small distance indicating a high degree of similarity and a large distance indicating a low degree of similarity.	What is data similarity
2593	From implementation point of view, Huffman coding is easier than arithmetic coding. Arithmetic algorithm yields much more compression ratio than Huffman algorithm while Huffman coding needs less execution time than the arithmetic coding.	Why is arithmetic coding better than Huffman coding
644	Conditional probability is the probability of one event occurring with some relationship to one or more other events. For example: Event A is that it is raining outside, and it has a 0.3 (30%) chance of raining today. Event B is that you will need to go outside, and that has a probability of 0.5 (50%).	How do you use conditional probability
1411	Naïve Bayes has a naive assumption of conditional independence for every feature, which means that the algorithm expects the features to be independent which not always is the case. Logistic regression is a linear classification method that learns the probability of a sample belonging to a certain class.	What is the difference between a naive Bayes and a logistic regression model
401	A false positive, also known as Type I error or alpha error, is an error that occurs when a researcher falsely concludes that an effect exists, or when a null hypothesis is rejected even though the null is true.  On the basis of these data, the researcher concludes that there is an effect.	What is a false positive in research
263	Association Rule Mining, as the name suggests, association rules are simple If/Then statements that help discover relationships between seemingly independent relational databases or other data repositories. Most machine learning algorithms work with numeric datasets and hence tend to be mathematical.	What are association rules in machine learning
591	Need for Batch Consumption From Kafka Data ingestion system are built around Kafka. They are followed by lambda architectures with separate pipelines for real-time stream processing and batch processing. Real-time stream processing pipelines are facilitated by Spark Streaming, Flink, Samza, Storm, etc.	Can we use Kafka for batch processing
981	A variable is a symbol that represents some quantity.  A random variable is a value that follows some probability distribution. In other words, it's a value that is subject to some randomness or chance.	What is the difference between a variable and a random variable
1455	To measure the relationship between numeric variable and categorical variable with > 2 levels you should use eta correlation (square root of the R2 of the multifactorial regression). If the categorical variable has 2 levels, point-biserial correlation is used (equivalent to the Pearson correlation).	How do you find the correlation between categorical variables
1714	With binary data the variance is a function of the mean, and in particular is not constant as the mean changes. This violates one of the standard linear regression assumptions that the variance of the residual errors is constant.	Why linear regression is not suitable for modeling binary responses
5623	Abstract: In the big data era, the data are generated from different sources or observed from different views. These data are referred to as multi-view data.  Multi-view Clustering (MvC) has attracted increasing attention in recent years by aiming to exploit complementary and consensus information across multiple views.	What is multi view clustering
8263	“Bayesian statistics is a mathematical procedure that applies probabilities to statistical problems. It provides people the tools to update their beliefs in the evidence of new data.”	Is Bayesian a statistic
6773	The least squares criterion is a formula used to measure the accuracy of a straight line in depicting the data that was used to generate it.  This mathematical formula is used to predict the behavior of the dependent variables. The approach is also called the least squares regression line.	What is the least squares criterion for linear regression equations
8681	In unsupervised learning, an AI system is presented with unlabeled, uncategorized data and the system's algorithms act on the data without prior training. The output is dependent upon the coded algorithms. Subjecting a system to unsupervised learning is an established way of testing the capabilities of that system.	How does unsupervised machine learning work
341	Currently there are three classes of TCP/IP networks. Each class uses the 32-bit IP address space differently, providing more or fewer bits for the network part of the address. These classes are class A, class B, and class C.	What are the different classes of IP addresses
5215	Linear Regression is a machine learning algorithm based on supervised learning. Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x).  So, this regression technique finds out a linear relationship between x (input) and y(output).	How does regression algorithm work
1680	Real numbers consist of zero (0), the positive and negative integers (-3, -1, 2, 4), and all the fractional and decimal values in between (0.4, 3.1415927, 1/2). Real numbers are divided into rational and irrational numbers.	Is 0 a real number
30	Kalman Filter works on prediction-correction model used for linear and time-variant or time-invariant systems. Prediction model involves the actual system and the process noise .  Kalman gain is calculated based on RLS algorithm in order to reach the optimal value within less amount of time.	Can Kalman filtering be used for prediction
1009	49:131:21:31Suggested clip · 118 secondsLinear Algebra for Beginners | Linear algebra for machine learning YouTubeStart of suggested clipEnd of suggested clip	What is the easiest way to learn linear algebra
127	In the simplest sense, if something is “quantized”, that means it can only take on certain specific values, rather than a continuous range of values. For example, the energy that an electron can have when it's bound to an atom is quantized.	What does it mean that something is quantized
6789	For a multiplicative decomposition, this is done by dividing the series by the trend values. Next, seasonal factors are estimated using the de-trended series. For monthly data, this entails estimating an effect for each month of the year. For quarterly data, this entails estimating an effect for each quarter.	What is multiplicative decomposition
8589	"A statistical test provides a mechanism for making quantitative decisions about a process or processes. The intent is to determine whether there is enough evidence to ""reject"" a conjecture or hypothesis about the process.  A classic use of a statistical test occurs in process control studies."	What are statistical tests in research
3254	Chernoff faces, invented by Herman Chernoff in 1973, display multivariate data in the shape of a human face. The individual parts, such as eyes, ears, mouth and nose represent values of the variables by their shape, size, placement and orientation.	What is a Chernoff face in data visualization
994	Statistical data analysis. Finding structure in data and making predictions are the most important steps in Data Science. Here, in particular, statistical methods are essential since they are able to handle many different analytical tasks.  Questions arising in data driven problems can often be translated to hypotheses.	Why is statistics important for data science
2307	The difference between factor analysis and principal component analysis.  Factor analysis explicitly assumes the existence of latent factors underlying the observed data. PCA instead seeks to identify variables that are composites of the observed variables.	Is PCA the same as factor analysis
6512	"The sampling distribution of the sample mean can be thought of as ""For a sample of size n, the sample mean will behave according to this distribution."" Any random draw from that sampling distribution would be interpreted as the mean of a sample of n observations from the original population."	What is sampling distribution of sample mean
3086	Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple's Siri and and Google's voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data.	Is RNN an algorithm
5508	The three main metrics used to evaluate a classification model are accuracy, precision, and recall. Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.	How do you evaluate model performance
2829	First, Cross-entropy (or softmax loss, but cross-entropy works better) is a better measure than MSE for classification, because the decision boundary in a classification task is large (in comparison with regression).  For regression problems, you would almost always use the MSE.	Why cross entropy loss is better than MSE
565	Lag sequential analysis is a method for analyzing the sequential dependency in a serially sequenced series of dichotomous codes representing different system states.  The analysis assumes that the events are sequenced in time (a time series) but does not assume equal time intervals between events.	What is lag sequential analysis
961	The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30).	What is the difference between Z statistic and t statistic
1146	Interpreting. If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer.	How do you interpret a positively skewed distribution
6664	Semi-structured data is information that doesn't reside in a relational database but that does have some organizational properties that make it easier to analyze.  Examples of semi-structured : CSV but XML and JSON documents are semi structured documents, NoSQL databases are considered as semi structured.	What is semi structured data explain with an example
7748	The universe is considered an isolated system because the energy of the universe is constant. This matches with the definition of an isolated system, which is that energy is not exchanged with the surroundings, thus staying constant.	Is the universe an isolated system
1933	A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0).	How do you interpret a linear regression equation
836	Definition. The term concept learning is originated in psychology, where it refers to the human ability to learn categories for object and to recognize new instances of those categories.	What do you mean by concept learning
8443	Deviance is a measure of error; lower deviance means better fit to data. The greater the deviance, the worse the model fits compared to the best case (saturated). Deviance is a quality-of-fit statistic for a model that is often used for statistical hypothesis testing.	What is deviance in GLM
5110	This means when calculating the output of a node, the inputs are multiplied by weights, and a bias value is added to the result. The bias value allows the activation function to be shifted to the left or right, to better fit the data.  You can think of the bias as a measure of how easy it is to get a node to fire.	What is bias value why it is used
6321	Troubleshoot steps for Deep Learning Start with something simple and make changes incrementally. Model optimizations like regularization can always wait after the code is debugged. Focus on verifying the model is functioning first. Set the regularization factors to zero.	How do you debug a deep learning model
142	Answer: A variable is a datatype whose value can not be fixed. It can be change based on other parameters. For example, Let X is a variable so that its value can be anything like 1,2,3 or a,p,r, or any word. It can not be fixed.	What is a variable answer
8119	On this site, we use the normal distribution when the population standard deviation is known and the sample size is large. We might use either distribution when standard deviation is unknown and the sample size is very large.	Can normal sampling distribution be used
667	Bayesian analysis is a statistical paradigm that answers research questions about unknown parameters using probability statements.	What is the purpose of Bayesian analysis
6098	The simplest example of a non-linear operator (non-linear functional) is a real-valued function of a real argument other than a linear function.  Under other restrictions on K(t,s,u) an Urysohn operator acts on other spaces, for instance, L2[a,b] or maps one Orlicz space LM1[a,b] into another LM2[a,b].	Which is not a linear operator
4168	t-test	What statistical test do you use for two continuous variables
1101	The receptive field is defined by the filter size of a layer within a convolution neural network. The receptive field is also an indication of the extent of the scope of input data a neuron or unit within a layer can be exposed to (see image below).	What is the receptive field of a convolution layer
440	The relationship between two variables is generally considered strong when their r value is larger than 0.7. The correlation r measures the strength of the linear relationship between two quantitative variables.	Is 0.62 A strong correlation
6609	marginal homogeneity	What does McNemar's test mean
6534	The logit is a transformation. Logistic regression is a regression model. The logit transformation transforms a line to a logistic curve. Logistic regression fits a logistic curve to set of data where the dependent variable can only take the values 0 and 1.	Are logit and logistic regression the same
5396	Bayesian analysis, a method of statistical inference (named for English mathematician Thomas Bayes) that allows one to combine prior information about a population parameter with evidence from information contained in a sample to guide the statistical inference process.	What is Bayesian analysis used for
4845	Then if there are an odd number of numbers in the list the median can be found by counting in from either end of the list to the (n + 1)/2nd number. This will be the median. If there are an even number on the list then average the n/2 and the (N + 2)/2 numbers. In general, the median is at position (n + 1)/2.	How do you find the median of N 2
5076	These software distributions are open source, licensed under the GNU General Public License (v3 or later for Stanford CoreNLP; v2 or later for the other releases).	Is Stanford NLP open source
187	How to Estimate an Agile/Scrum Story Backlog with PointsThe goal of agile/scrum estimation.  A few terms.  Set an estimation range.  Set some reference points.  Estimate stories with planning poker.  Estimate bugs, chores, and spikes.  Set aside a couple of days.  Use the big numbers: 20, 40, 100.More items•	How do you estimate in agile
7701	In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged).	Does deep learning come under machine learning
4988	Area in TailsConfidence LevelArea between 0 and z-scorez-score50%0.25000.67480%0.40001.28290%0.45001.64595%0.47501.9602 more rows	What is the z score for 50 confidence interval
2236	The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above . 05 indicates normality.	How do you know if a sample is normally distributed
1949	Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.	What is clustering used for
238	value of the Shapiro-Wilk Test is greater than 0.05, the data is normal. If it is below 0.05, the data significantly deviate from a normal distribution. If you need to use skewness and kurtosis values to determine normality, rather the Shapiro-Wilk test, you will find these in our enhanced testing for normality guide.	How do you test for normality
189	Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). These new reduced set of features should then be able to summarize most of the information contained in the original set of features.	How do you extract a feature from a dataset
338	Receptive fields are defined portion of space or spatial construct containing units that provide input to a set of units within a corresponding layer. The receptive field is defined by the filter size of a layer within a convolution neural network.	What is receptive field in deep learning
1022	An Expert system shell is a software development environment. It contains the basic components of expert systems. A shell is associated with a prescribed method for building applications by configuring and instantiating these components.	What is Expert System Shell in artificial intelligence
216	Bootstrapping is building a company from the ground up with nothing but personal savings, and with luck, the cash coming in from the first sales. The term is also used as a noun: A bootstrap is a business an entrepreneur with little or no outside cash or other support launches.	What do you mean by bootstrapping
1873	The standard deviation (SD) measures the amount of variability, or dispersion, from the individual data values to the mean, while the standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean. The SEM is always smaller than the SD.	What is the difference between mean and standard deviation
6998	A sampling error is a statistical error that occurs when an analyst does not select a sample that represents the entire population of data and the results found in the sample do not represent the results that would be obtained from the entire population.	What is a sampling error in research
8191	Gradient Backward propagation	What does training a neural network mean
7025	Word Embeddings or Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics. The process of converting words into numbers are called Vectorization.	What is vectorization in NLP
2571	Feature embedding is an emerging research area which intends to transform features from the original space into a new space to support effective learning. Feature embedding aims to learn a low-dimensional vector representation for each instance to preserve the information in its features.	What is embedding embedded space feature embedding in deep neural architectures
451	In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. With a shape parameter α = k and an inverse scale parameter β = 1/θ, called a rate parameter.  With a shape parameter k and a mean parameter μ = kθ = α/β.	What does gamma distribution look like
3489	Typically by the time the sample size is 30 the distribution of the sample mean is practically the same as a normal distribution.  ¯X, the mean of the measurements in a sample of size n; the distribution of ¯X is its sampling distribution, with mean μ¯X=μ and standard deviation σ¯X=σ√n.	What is the difference between the sampling distribution of the sample mean and the sample mean
1116	It is a rate per unit of time similar in meaning to reading a car speedometer at a particular instant and seeing 45 mph.  The failure rate (or hazard rate) is denoted by h(t) and is calculated from h(t) = \frac{f(t)}{1 - F(t)} = \frac{f(t)}{R(t)} = \mbox{the instantaneous (conditional) failure rate.}	How are hazard rates calculated
326	The probability that a random variable X X X takes a value in the (open or closed) interval [ a , b ] [a,b] [a,b] is given by the integral of a function called the probability density function f X ( x ) f_X(x) fX​(x): P ( a ≤ X ≤ b ) = ∫ a b f X ( x ) d x .	How do you find the probability density function of a random variable
751	In computer science, binary search, also known as half-interval search, logarithmic search, or binary chop, is a search algorithm that finds the position of a target value within a sorted array.  Binary search compares the target value to the middle element of the array.	What is the binary search theory
602	Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks.  In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly.	What is multi task learning in machine learning
1990	In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.	What does a generalized linear model do
4865	Some Disadvantages of KNNAccuracy depends on the quality of the data.With large data, the prediction stage might be slow.Sensitive to the scale of the data and irrelevant features.Require high memory – need to store all of the training data.Given that it stores all of the training, it can be computationally expensive.	What are the disadvantages of KNN algorithm
1465	Covariance is calculated by analyzing at-return surprises (standard deviations from the expected return) or by multiplying the correlation between the two variables by the standard deviation of each variable.	How do you find the covariance between two variables
4820	is that maximin is in decision theory and game theory etc, a rule to identify the worst outcome of each possible option to find one's best (maximum payoff) play while minimax is in decision theory, game theory, etc a decision rule used for minimizing the maximum possible loss, or maximizing the minimum gain.	What is the difference between Minimax and Maximin
1757	The learning algorithm of the Hopfield network is unsupervised, meaning that there is no “teacher” telling the network what is the correct output for a certain input.	Is Hopfield network supervised or unsupervised
109	The various metrics used to evaluate the results of the prediction are :Mean Squared Error(MSE)Root-Mean-Squared-Error(RMSE).Mean-Absolute-Error(MAE).R² or Coefficient of Determination.Adjusted R²	Which metrics can be used on a regression problem
6721	Cons of Reinforcement LearningReinforcement learning as a framework is wrong in many different ways, but it is precisely this quality that makes it useful.Too much reinforcement learning can lead to an overload of states, which can diminish the results.Reinforcement learning is not preferable to use for solving simple problems.More items	What are the disadvantages of reinforcement learning
813	Multinomial logistic regression is used when the dependent variable in question is nominal (equivalently categorical, meaning that it falls into any one of a set of categories that cannot be ordered in any meaningful way) and for which there are more than two categories.	When would you use a multinomial
4346	A node, also called a neuron or Perceptron, is a computational unit that has one or more weighted input connections, a transfer function that combines the inputs in some way, and an output connection. Nodes are then organized into layers to comprise a network.	What is a node in deep learning
2582	Today, machines are intelligent because of a science called the Artificial Intelligence.  A simple answer to explain what makes a machine intelligent is Artificial Intelligence. AI allows a machine to interact with the environment in an intelligent manner.	What makes computer intelligent
3844	Probability sampling allows researchers to create a sample that is accurately representative of the real-life population of interest.	Why is probability sampling preferred
1341	In technical terms, linear regression is a machine learning algorithm that finds the best linear-fit relationship on any given data, between independent and dependent variables. It is mostly done by the Sum of Squared Residuals Method.	How do you explain linear regression in interview
1053	An easy guide to choose the right Machine Learning algorithmSize of the training data. It is usually recommended to gather a good amount of data to get reliable predictions.  Accuracy and/or Interpretability of the output.  Speed or Training time.  Linearity.  Number of features.	Which algorithm is right for machine learning
768	A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers.	How does residual network work
257	The main advantage of CNN compared to its predecessors is that it automatically detects the important features without any human supervision. For example, given many pictures of cats and dogs, it can learn the key features for each class by itself.	What is the advantage of CNN
6655	Correlation Defined The closer the correlation coefficient is to +1.0, the closer the relationship is between the two variables.  If two variables have a correlation coefficient near zero, it indicates that there is no significant (linear) relationship between the variables.	What does a correlation coefficient near mean
4123	Variance is the measure of how far the data points are spread out whereas, MSE (Mean Squared Error) is the measure of how actually the predicted values are different from the actual values. Though, both are the measures of second moment but there is a significant difference.	What is the difference between mean square error and variance
4067	8:3514:50Suggested clip · 95 secondsLecture 6.3 — Logistic Regression | Decision Boundary — [ Machine YouTubeStart of suggested clipEnd of suggested clip	How do you determine the decision boundary in logistic regression
6094	Monte Carlo tree search algorithm	What algorithm does AlphaGo use
1245	The agent function is a mathematical function that maps a sequence of perceptions into action. The function is implemented as the agent program. The part of the agent taking an action is called an actuator. environment -> sensors -> agent function -> actuators -> environment.	What is Agent function in artificial intelligence
4772	A correlation coefficient that is greater than zero indicates a positive relationship between two variables. A value that is less than zero signifies a negative relationship between two variables. Finally, a value of zero indicates no relationship between the two variables that are being compared.	What should the correlation of variable when compared with itself *
877	The conditional probability can be calculated using the joint probability, although it would be intractable. Bayes Theorem provides a principled way for calculating the conditional probability. The simple form of the calculation for Bayes Theorem is as follows: P(A|B) = P(B|A) * P(A) / P(B)	How do you calculate probability in naive Bayes
7514	AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability.  By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.	What is the difference between AUC and ROC
6220	"""The difference between discrete choice models and conjoint models is that discrete choice models present experimental replications of the market with the focus on making accurate predictions regarding the market, while conjoint models do not, using product profiles to estimate underlying utilities (or partworths)"	What is the difference between conjoint and discrete choice
846	The arithmetic mean is often known simply as the mean. It is an average, a measure of the centre of a set of data. The arithmetic mean is calculated by adding up all the values and dividing the sum by the total number of values. For example, the mean of 7, 4, 5 and 8 is 7+4+5+84=6.	How do you find the arithmetic mean in statistics
7880	It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.	What is naive Bayes classifier algorithm
8048	Average or arithmetic means give us rough estimate about the common values in that set so that the calculations on all the values will be more or less the same.	What is the average point
3613	An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate.	What is ROC curve in machine learning
823	In statistics, a negatively skewed (also known as left-skewed) distribution is a type of distribution in which more values are concentrated on the right side (tail) of the distribution graph while the left tail of the distribution graph is longer.	What is negatively skewed distribution
358	Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.	What is difference between Bag of Words and TF IDF
7687	AUC and accuracy are fairly different things.  For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set. AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else.	Is AUC the same as accuracy
8427	The main difference between the t-test and f-test is, that t-test is used to test the hypothesis whether the given mean is significantly different from the sample mean or not. On the other hand, an F-test is used to compare the two standard deviations of two samples and check the variability.	What is the difference between F and T test
1877	The Mann Whitney U test, sometimes called the Mann Whitney Wilcoxon Test or the Wilcoxon Rank Sum Test, is used to test whether two samples are likely to derive from the same population (i.e., that the two populations have the same shape).	What is Mann Whitney U test used for
6836	The SVM classifier is a frontier which best segregates the two classes (hyper-plane/ line). You can look at support vector machines and a few examples of its working here.	What are the two classification methods that SVM support vector machine can handle
7342	(8) The moment generating function corresponding to the normal probability density function N(x;µ, σ2) is the function Mx(t) = exp{µt + σ2t2/2}.	What is the moment generating function of normal distribution
1293	5 Answers. The Fourier series is used to represent a periodic function by a discrete sum of complex exponentials, while the Fourier transform is then used to represent a general, nonperiodic function by a continuous superposition or integral of complex exponentials.	What is the relationship between the Fourier transform and Fourier series representation of a continuous function
596	The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups.	What is the purpose of analysis of variance
269	Why the Lognormal Distribution is used to Model Stock Prices Since the lognormal distribution is bound by zero on the lower side, it is therefore perfect for modeling asset prices which cannot take negative values. The normal distribution cannot be used for the same purpose because it has a negative side.	Why do we use lognormal distribution
2005	Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance.	What does principal component analysis do
7325	Machine learning usually has to achieve multiple targets, which are often conflicting with each other. Multi-objective model selection to improve the performance of learning models, such as neural networks, support vector machines, decision trees, and fuzzy systems.	What are some Machine Learning techniques for objective optimization
386	Deep Learning does this by utilizing neural networks with many hidden layers, big data, and powerful computational resources.  In unsupervised learning, algorithms such as k-Means, hierarchical clustering, and Gaussian mixture models attempt to learn meaningful structures in the data.	Is deep learning the same as unsupervised learning
1328	“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges.  Then, we perform classification by finding the hyper-plane that differentiates the two classes very well (look at the below snapshot).	Is support vector machine SVM a data structure or an algorithm
2445	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve.	Where in nature can you find a normal distribution
8417	t-test is used to test if two sample have the same mean. The assumptions are that they are samples from normal distribution. f-test is used to test if two sample have the same variance.	What is the relationship between T test and F test
966	How to Find a Sample Size Given a Confidence Interval and Width (unknown population standard deviation)za/2: Divide the confidence interval by two, and look that area up in the z-table: .95 / 2 = 0.475.  E (margin of error): Divide the given width by 2. 6% / 2.  : use the given percentage. 41% = 0.41.  : subtract. from 1.	How do you calculate sample size if population is unknown
1222	The true error rate is statistically defined as the error rate of the classifier on a large number of new cases that converge in the limit to the actual population distribution.  It turns out that there are a number of ways of presenting sample cases to a classifier to get better estimates of the true error rate.	What is true error rate
1173	These three elements allow you to take a process perspective on the data. Figure 3: The three minimum requirements for process mining: A Case ID, an Activity name and at least one Timestamp column.	How many data points are required for process mining
1703	There are two types of chi-square tests.  A very small chi square test statistic means that your observed data fits your expected data extremely well. In other words, there is a relationship. A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship.	What does it mean to obtain a large value for the chi square statistic
708	In a 2-by-2 table with cells a, b, c, and d (see figure), the odds ratio is odds of the event in the exposure group (a/b) divided by the odds of the event in the control or non-exposure group (c/d). Thus the odds ratio is (a/b) / (c/d) which simplifies to ad/bc.	How do you calculate odds ratio
8252	In marketing terms, a multi-armed bandit solution is a 'smarter' or more complex version of A/B testing that uses machine learning algorithms to dynamically allocate traffic to variations that are performing well, while allocating less traffic to variations that are underperforming.	What is a bandit algorithm
6922	Censoring is a form of missing data problem in which time to event is not observed for reasons such as termination of study before all recruited subjects have shown the event of interest or the subject has left the study prior to experiencing an event. Censoring is common in survival analysis.	What is censoring in survival analysis
2539	Null hypothesis are never accepted. We either reject them or fail to reject them. The distinction between “acceptance” and “failure to reject” is best understood in terms of confidence intervals. Failing to reject a hypothesis means a confidence interval contains a value of “no difference”.	What are the differences between accepting and not rejecting a null hypothesis
5577	ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. This is one of the easiest and effective machine learning algorithm to performing time series forecasting.  In simple words, it performs regression in previous time step t-1 to predict t.	Is Arima a machine learning model
2807	General reporting recommendations such as that of APA Manual apply. One should report exact p-value and an effect size along with its confidence interval. In the case of likelihood ratio test one should report the test's p-value and how much more likely the data is under model A than under model B.	How do you report the likelihood ratio test
8650	Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.	What is RMSE in regression
8128	Arrange your set of numbers from smallest to largest. Determine which measure of central tendency you wish to calculate. The three types are mean, median and mode. To calculate the mean, add all your data and divide the result by the number of data.	How do you find central tendency
6239	The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It can be used to estimate summary statistics such as the mean or standard deviation.  That when using the bootstrap you must choose the size of the sample and the number of repeats.	What is bootstrap method in statistics
3311	Statistical significance is a determination that a relationship between two or more variables is caused by something other than chance.  Statistical hypothesis testing is used to determine whether the result of a data set is statistically significant.	What is statistical significance and how does it relate to correlation
1219	In distributed training the workload to train a model is split up and shared among multiple mini processors, called worker nodes.  Distributed training can be used for traditional ML models, but is better suited for compute and time intensive tasks, like deep learning for training deep neural networks.	What is distributed training
4900	Feature Selection vs Dimensionality Reduction While both methods are used for reducing the number of features in a dataset, there is an important difference. Feature selection is simply selecting and excluding given features without changing them. Dimensionality reduction transforms features into a lower dimension.	Whats the difference between dimensionality reduction and feature selection
460	Bayes theorem provides a way to calculate the probability of a hypothesis based on its prior probability, the probabilities of observing various data given the hypothesis, and the observed data itself. — Page 156, Machine Learning, 1997.	What is the purpose of Bayes theorem in machine learning
7703	Multiply the Grand total by the Pretest probability to get the Total with disease. Compute the Total without disease by subtraction. Multiply the Total with disease by the Sensitivity to get the number of True positives. Multiply the Total without disease by the Specificity to get the number of True Negatives.	How do you calculate true negative
7004	There are 3 main ways of describing the intensity of an activity – vigorous, moderate, and gentle. Vigorous activities tend to make you “huff and puff”.	How do you describe your activity level
7172	How to calculate margin of errorGet the population standard deviation (σ) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:	How do you find the margin of error for a sample mean
7692	The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.	What is Adjusted R Squared in Statistics
3094	The bootstrap is a tool, which allows us to obtain better finite sample approximation of estimators. The bootstrap is used all over the place to estimate the variance, correct bias and construct CIs etc. There are many, many different types of bootstraps.	Why does the bootstrap work
3054	Order of training data during training a neural network matters a great deal. If you are training with a mini batch you may see large fluctuations in accuracy (and cost function) and may end up over fitting correlated portions of your mini batch.	Does the order of training examples within a minibatch matter when training a neural network
1173	In statistics, the logit (/ˈloʊdʒɪt/ LOH-jit) function or the log-odds is the logarithm of the odds where p is a probability. It is a type of function that creates a map of probability values from to. .	What logit means
383	An RNN has a looping mechanism that acts as a highway to allow information to flow from one step to the next. Passing Hidden State to next time step. This information is the hidden state, which is a representation of previous inputs. Let's run through an RNN use case to have a better understanding of how this works.	What is a hidden state in RNN
808	"In terms of machine learning, ""concept learning"" can be defined as: “The problem of searching through a predefined space of potential hypotheses for the hypothesis that best fits the training examples.” — Tom Michell. Much of human learning involves acquiring general concepts from past experiences."	What is Concept Learning in machine learning
2638	The Delta Rule employs the error function for what is known as Gradient Descent learning, which involves the 'modification of weights along the most direct path in weight-space to minimize error', so change applied to a given weight is proportional to the negative of the derivative of the error with respect to that	What is Delta error in neural network
3350	LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox.	Is Latent Dirichlet Allocation machine learning
2719	Entropy is simply a measure of disorder and affects all aspects of our daily lives. In fact, you can think of it as nature's tax. Left unchecked disorder increases over time. Energy disperses, and systems dissolve into chaos.	What is entropy and why is it important
4605	In mathematics, more specifically in the theory of Monte Carlo methods, variance reduction is a procedure used to increase the precision of the estimates that can be obtained for a given simulation or computational effort.  For simulation with black-box models subset simulation and line sampling can also be used.	What is variance reduction technique
6507	An autoencoder accepts input, compresses it, and then recreates the original input.  A variational autoencoder assumes that the source data has some sort of underlying probability distribution (such as Gaussian) and then attempts to find the parameters of the distribution.	Whats the difference between a Variational Autoencoder VAE and an Autoencoder
7744	Simply put, a random sample is a subset of individuals randomly selected by researchers to represent an entire group as a whole. The goal is to get a sample of people that is representative of the larger population.	What is the purpose of random sampling
2609	The most commonly used metric for regression tasks is RMSE (Root Mean Square Error). This is defined as the square root of the average squared distance between the actual score and the predicted score: rmse=√∑ni=1(yi−^yi)2n.	What are regression metrics
783	Regression lossMean Square Error, Quadratic loss, L2 Loss. Mean Square Error (MSE) is the most commonly used regression loss function.  Mean Absolute Error, L1 Loss. Mean Absolute Error (MAE) is another loss function used for regression models.  Huber Loss, Smooth Mean Absolute Error.  Log-Cosh Loss.  Quantile Loss.	What are some loss functions that we can use for either regression or classification models
5364	Definition. A study design that randomly assigns participants into an experimental group or a control group. As the study is conducted, the only expected difference between the control and experimental groups in a randomized controlled trial (RCT) is the outcome variable being studied.	What is the correct definition of a randomized controlled clinical trial
1105	In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression.	What are the features in machine learning
4905	The Decision Analysis Process is used in support of decision making bodies to help evaluate technical, cost, and schedule issues, alternatives, and their uncertainties. Decision models have the capacity for accepting and quantifying human subjective inputs: judgments of experts and preferences of decision makers.	What is the purpose of decision analysis
2132	Tensorflow feature columnsTensorflow feature columns.  If the tensor is a matrix, you can provide a shape expressing the dimensions.Partitioning a numerical column into a set of indicator categoricals can be done using bucketized_column :More items	What are feature columns in TensorFlow
1021	v) Matthews Correlation Coefficient (MCC) Similar to Correlation Coefficient, the range of values of MCC lie between -1 to +1. A model with a score of +1 is a perfect model and -1 is a poor model.	What is a good Matthews Correlation Coefficient
1388	"In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input."	What is activation function used in a neural network
1375	You description is confusing, but it is totally possible to have test error both lower and higher than training error. A lower training error is expected when a method easily overfits to the training data, yet, poorly generalizes.	Can test error be lower than training error
2873	Tips for LSTM Input The meaning of the 3 input dimensions are: samples, time steps, and features. The LSTM input layer is defined by the input_shape argument on the first hidden layer. The input_shape argument takes a tuple of two values that define the number of time steps and features.	What is the input of Lstm
214	Their only difference is that the conditional probability assumes that we already know something -- that B is true. The intersection doesn't assume that we know anything. So for P(A ∩ B), we will receive a probability between 0, impossible, and 1, certain.	What is the difference between conditional probability and intersection
5448	In probability theory and statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d. or iid or IID.	How can you explain briefly independent and identically distributed random variables in statistics
5398	Qualitative Differences The population standard deviation is a parameter, which is a fixed value calculated from every individual in the population. A sample standard deviation is a statistic. This means that it is calculated from only some of the individuals in a population.	Is population standard deviation the same as standard deviation
6760	The key assumption in ordinal regression is that the effects of any explanatory variables are consistent or proportional across the different thresholds, hence this is usually termed the assumption of proportional odds (SPSS calls this the assumption of parallel lines but it's the same thing).	What are the assumptions of ordinal logistic regression
7780	Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable.	Can you do regression with ordinal data
1331	The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(∗). Both functions will take any number and rescale it to fall between 0 and 1.	What is logit probit model
3196	The power of a test is the probability of rejecting the null hypothesis when it is false; in other words, it is the probability of avoiding a type II error. The power may also be thought of as the likelihood that a particular study will detect a deviation from the null hypothesis given that one exists.	What does the power of the test measure
784	A pseudo-random process is a process that appears to be random but is not. Pseudo-random sequences typically exhibit statistically randomness while being generated by an entirely deterministic casual process.  Two dimensional Faure sequence has been taken for quasi-random number.	What is the difference between a pseudo random number and a quasi random number
5211	The answer is a big NO. Data science gets solutions and results to specific business problems using AI as a tool. If data science is to insights, machine learning is to predictions and artificial intelligence is to actions.	Is machine learning necessary for AI
2220	The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers, and normalization layers. Here it simply means that instead of using the normal activation functions defined above, convolution and pooling functions are used as activation functions.	What is hidden layer in CNN
474	Using batch normalization makes the network more stable during training. This may require the use of much larger than normal learning rates, that in turn may further speed up the learning process. — Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, 2015.	Should I use batch normalization
1611	A normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution. Areas of the normal distribution are often represented by tables of the standard normal distribution.  For example, a Z of -2.5 represents a value 2.5 standard deviations below the mean.	What does standard distribution mean
1054	The purpose of causal analysis is trying to find the root cause of a problem instead of finding the symptoms. This technique helps to uncover the facts that lead to a certain situation.	What is the purpose of causal analysis
259	In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer.	What is a positive skew in statistics
6148	Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch and stream-processing methods.  The rise of lambda architecture is correlated with the growth of big data, real-time analytics, and the drive to mitigate the latencies of map-reduce.	What is Lambda architecture in big data
576	A small RMSE means good prediction and large means bad model. In classification, you have (finite and countable) class labels, which do not correspond to numbers. Therefore you can not use RMSE because it is difficult to find difference between, say, label 'a' and 'b'.	Can RMSE be used for classification
1790	Multilevel modelling is an approach that can be used to handle clustered or grouped data.  Multilevel modelling can also be used to analyse repeated measures data.	What is a multilevel modeling approach
917	Image annotation for deep learning is mainly done for object detection with more precision. 3D Cuboid Annotation, Semantic Segmentation, and polygon annotation are used to annotate the images using the right tool to make the objects well-defined in the image for neural network analysis in deep learning.	How do you annotate images for machine learning
2933	The formula for the Expected Value for a binomial random variable is: P(x) * X.	How do you find the expected value of a binomial random variable
823	"To construct a histogram, the first step is to ""bin"" (or ""bucket"") the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable."	What is a bin size in a histogram
759	The descriptive analysis uses mainly unsupervised learning approaches for summarizing, classifying, extracting rules to answer what happens was happened in the past. While Predictive analysis is about machine learning approaches for the aim forecasting future data based on past data.	What are predictive and descriptive learning tasks in machine learning
5569	Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs).  This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.	Why does batch normalization help
900	The Binomial Theorem is a quick way (okay, it's a less slow way) of expanding (or multiplying out) a binomial expression that has been raised to some (generally inconveniently large) power. For instance, the expression (3x – 2)10 would be very painful to multiply out by hand.	How do you use the binomial theorem
7425	Poisson Formula. P(x; μ) = (e-μ) (μx) / x! where x is the actual number of successes that result from the experiment, and e is approximately equal to 2.71828. The Poisson distribution has the following properties: The mean of the distribution is equal to μ . The variance is also equal to μ .	What is Poisson distribution formula
23	The scale-invariant feature transform (SIFT) is an algorithm used to detect and describe local features in digital images.  The descriptors are supposed to be invariant against various transformations which might make images look different although they represent the same object(s).	Why sift is scale invariant
9	S is known both as the standard error of the regression and as the standard error of the estimate. S represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable.	How do you interpret the standard error of the estimate
7847	Learning statistics means learning to communicate using the statistical language, solving statistical problems, drawing conclusions, and supporting conclusions by explaining the reasoning behind them. There are often different ways to solve a statistical problem.	What is the goal of learning statistics
860	LSTMs control the exposure of memory content (cell state) while GRUs expose the entire cell state to other units in the network. The LSTM unit has separate input and forget gates, while the GRU performs both of these operations together via its reset gate.	What is the difference between GRU and Lstm
1607	Hypothesis Tests of the Mean and MedianParametric tests (means)Nonparametric tests (medians)1-sample t test1-sample Sign, 1-sample Wilcoxon2-sample t testMann-Whitney testOne-Way ANOVAKruskal-Wallis, Mood's median testFactorial DOE with one factor and one blocking variableFriedman test	What are the different types of parametric tests
303	3.1 Comparison MatrixClassification AlgorithmsAccuracyF1-ScoreNaïve Bayes80.11%0.6005Stochastic Gradient Descent82.20%0.5780K-Nearest Neighbours83.56%0.5924Decision Tree84.23%0.63083 more rows•	Which algorithm is best for classification
8395	The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times.	What is the main difference between the binomial distribution and the Poisson distribution
2051	Quota sampling is defined as a non-probability sampling method in which researchers create a sample involving individuals that represent a population. Researchers choose these individuals according to specific traits or qualities.  These samples can be generalized to the entire population.	What type of sampling is quota sampling
405	Another serious limitation is that practitioners need to develop new skills in seeking and appraising evidence, which takes considerable time and effort. Without these skills practitioners are prone to confirmation bias – seeing only the evidence that supports their personal experience and judgment.	What are the disadvantages of evidence based practice
4043	Sampling bias occurs when some members of a population are systematically more likely to be selected in a sample than others. It is also called ascertainment bias in medical fields. Sampling bias limits the generalizability of findings because it is a threat to external validity, specifically population validity.AP ۱۳۹۹ غویی ۳۱	What is sampling bias
7232	This approach involves either forward selection, adding features one at a time, or backward selection, removing features one at a time until some criterion is reached. Additionally, a bidirectional selection method is available that involves adding or removing a feature at each step.	What is backward selection
2479	The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction.  Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level.	What is calibrated probability
8647	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve.	What is normal distribution curve with example
6520	Basically CV<10 is very good, 10-20 is good, 20-30 is acceptable, and CV>30 is not acceptable.	What is a good coefficient of variation
5238	Image features, such as edges and interest points, provide rich information on the image content. They correspond to local regions in the image and are fun- damental in many applications in image analysis: recognition, matching, recon- struction, etc.	What are the features of images
920	The reason that SVMs often outperform ANNs in practice is that they deal with the biggest problem with ANNs, SVMs are less prone to overfitting.	What is one reason we might choose to use support vector machine rather than an artificial neural network
1460	The initial task of image processing is to enhance the quality of digital images for further analysis.  This chapter also reviews methods that are used to quantitatively determine specific image information, such as relative composition, particle size, interparticle distance, intensity profile, etc.	What is image processing and analysis
3103	Random forest (RF) missing data algorithms are an attractive approach for imputing missing data. They have the desirable properties of being able to handle mixed types of missing data, they are adaptive to interactions and nonlinearity, and they have the potential to scale to big data settings.	Can random forest handle missing data
4428	Error correction rules were initially proposed as ad hoc rules for single unit training. These rules essentially drive the output error of a given unit to zero. We start with the classical perceptron learning rule and give a proof for its convergence.	What is correction in Perceptron learning rule
2791	In Logic, the Fallacy of Division is a fallacy of induction that occurs when someone assumes that what is true of a whole, must also be true of the parts of the parts. For example, it might be that an excellent baseball team is composed of mediocre players.	What is fallacy of division with example
751	In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.	What is GLM in logistic regression
1901	In probability theory and statistics, the hypergeometric distribution is a discrete probability distribution that describes the probability of successes (random draws for which the object drawn has a specified feature) in draws, without replacement, from a finite population of size that contains exactly objects with	What is hypergeometric distribution in statistics
7116	Imbalanced classification refers to a classification predictive modeling problem where the number of examples in the training dataset for each class label is not balanced. That is, where the class distribution is not equal or close to equal, and is instead biased or skewed.	What is class imbalance in machine learning
7999	Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.	What is meant by time series analysis
4497	Conditional probability is defined as the likelihood of an event or outcome occurring, based on the occurrence of a previous event or outcome. Conditional probability is calculated by multiplying the probability of the preceding event by the updated probability of the succeeding, or conditional, event.	What do you mean by conditional probability
1214	In mathematics, input and output are terms that relate to functions. Both the input and output of a function are variables, which means that they change. You can choose the input variables yourself, but the output variables are always determined by the rule established by the function.	How do you identify input and output variables
4801	To then oversample, take a sample from the dataset, and consider its k nearest neighbors (in feature space). To create a synthetic data point, take the vector between one of those k neighbors, and the current data point. Multiply this vector by a random number x which lies between 0, and 1.	How do you oversample data
209	Artificial intelligence (AI) is the attempt to let computers perform services for which humans need intelligence. However, this is still not possible today. AI systems are capable of recognizing patterns, learning and making decisions.	Is artificial intelligence intelligent
8195	Positive feedback loops enhance or amplify changes; this tends to move a system away from its equilibrium state and make it more unstable. Negative feedbacks tend to dampen or buffer changes; this tends to hold a system to some equilibrium state making it more stable.	What happens to positive feedback loops over time
450	Jakob Bernoulli	Who discovered the law of large numbers
791	Programming is the fundamental requirement of deep learning. You can't perform deep learning without using a programming language. Deep learning professionals use Python or R as their programming language because of their functionalities and effectiveness.	What is required for deep learning
5107	A machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data.  See Get ONNX models for Windows ML for more information.	What is ML model
4507	The matrix norm is similar to the magnitude of a vector. It is useful whenever a system/problem can be formulated into a matrix that has some physical meaning.	What is Matrix norm used for
5624	k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.	What is meant by K means clustering algorithm
2721	To conclude, the important thing to remember about the odds ratio is that an odds ratio greater than 1 is a positive association (i.e., higher number for the predictor means group 1 in the outcome), and an odds ratio less than 1 is negative association (i.e., higher number for the predictor means group 0 in the outcome	How do you interpret the odds ratio in logistic regression
9	the limiting value C is 1 + A times larger than the initial output y(0)A is the number of times that the initial population must grow to reach C.if B is positive, the logistic function will always increase,while if B is negative, the function will always decrease.	How do you tell if a logistic function is increasing or decreasing
4349	No, because the sample is not representative of the whole population.  Find the​ range, variance, and standard deviation for the sample data.	Is the standard deviation of the sample A good estimate
7357	Top N accuracy — Top N accuracy is when you measure how often your predicted class falls in the top N values of your softmax distribution.	What is top n accuracy
869	Class Boundaries. Separate one class in a grouped frequency distribution from another. The boundaries have one more decimal place than the raw data and therefore do not appear in the data. There is no gap between the upper boundary of one class and the lower boundary of the next class.	What are the class boundaries
124	Discrete random variables can only take on values from a countable set of numbers such as the integers or some subset of integers. (Usually, they can't be fractions.)	Can a discrete variable take any fractional value
5123	The central limit theorem states that the sampling distribution of the mean approaches a normal distribution, as the sample size increases.  Therefore, as a sample size increases, the sample mean and standard deviation will be closer in value to the population mean μ and standard deviation σ .	What happens if the sample size increases
4857	1:357:43Suggested clip · 113 secondsProbability of the Complement of an Event 128-1.4 - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you solve probability complements
5224	Bias can damage research, if the researcher chooses to allow his bias to distort the measurements and observations or their interpretation. When faculty are biased about individual students in their courses, they may grade some students more or less favorably than others, which is not fair to any of the students.	Why is being bias bad
781	Definition: Distribution means to spread the product throughout the marketplace such that a large number of people can buy it. Distribution involves doing the following things: Tracking the places where the product can be placed such that there is a maximum opportunity to buy it.	What is meant by a distribution
4268	Translational Invariance makes the CNN invariant to translation. Invariance to translation means that if we translate the inputs the CNN will still be able to detect the class to which the input belongs. Translational Invariance is a result of the pooling operation.	Is CNN translation invariant
304	The expected value of the difference between all possible sample means is equal to the difference between population means. Thus, E(x1 - x2) = μd = μ1 - μ2.	How do you find the sample mean difference
8059	It will be easier to learn and use. If you are in the industry where you need to deploy models in production, Tensorflow is your best choice. You can use Keras/Pytorch for prototyping if you want. But you don't need to switch as Tensorflow is here to stay.	Should I use PyTorch or TensorFlow
1812	Machine learning can be described in many ways. Perhaps the most useful is as type of optimization.  This is done via what is known as an objective function, with “objective” used in the sense of a goal. This function, taking data and model parameters as arguments, can be evaluated to return a number.	What is an objective function in machine learning
5333	The correlation coefficient is a statistical measure of the strength of the relationship between the relative movements of two variables. The values range between -1.0 and 1.0. A calculated number greater than 1.0 or less than -1.0 means that there was an error in the correlation measurement.	What is a correlation coefficient in simple words
8352	"At a bare minimum, collect around 1000 examples. For most ""average"" problems, you should have 10,000 - 100,000 examples. For “hard” problems like machine translation, high dimensional data generation, or anything requiring deep learning, you should try to get 100,000 - 1,000,000 examples."	How many data points are required for machine learning
3930	Independent Variables An independent variable is the factor that has some influence or impact on the dependent variable.	What is an independent relationship between two variables
1710	The response variable is the focus of a question in a study or experiment. An explanatory variable is one that explains changes in that variable. It can be anything that might affect the response variable.	How do you identify a response variable
3756	Advantages of Recurrent Neural Network It is useful in time series prediction only because of the feature to remember previous inputs as well. This is called Long Short Term Memory. Recurrent neural network are even used with convolutional layers to extend the effective pixel neighborhood.	Why is a neural network recurrent
1090	The uniform distribution defines equal probability over a given range for a continuous distribution. For this reason, it is important as a reference distribution. One of the most important applications of the uniform distribution is in the generation of random numbers.	What is the use of uniform distribution
4824	Stochastic vs. In general, stochastic is a synonym for random. For example, a stochastic variable is a random variable. A stochastic process is a random process. Typically, random is used to refer to a lack of dependence between observations in a sequence.	What is the difference between stochastic and random
2585	Definition. Data Partitioning is the technique of distributing data across multiple tables, disks, or sites in order to improve query processing performance or increase database manageability.	What is partitioning of data
5337	Depending on the skill being taught, backward chaining has a distinct advantage: It directly links the independent completion of a task to the immediate reward or reinforcement. Once the child can complete the last step independently, he or she can work on also completing the next-to-last step independently.	What is an advantage of backward chaining
637	If you have two independent groups, and the variances are equal, F = t^2.  The value of “t” is then calculated as the difference between the two sample means divided by the estimated pooled sample standard deviation (in the case of two independent samples, drawn from populations of equal variance).	How are the F statistic and t statistic related
8169	The multivariate normal distribution has two or more random variables — so the bivariate normal distribution is actually a special case of the multivariate normal distribution.	What is bivariate and multivariate distribution
2538	An important problem that arises when we search for similar items of any kind is that there may be far too many pairs of items to test each pair for their degree of similarity, even if computing the similarity of any one pair can be made very easy.	Why is finding similar items important in big data
8307	One assumption of Poisson Models is that the mean and the variance are equal, but this assumption is often violated. This can be dealt with by using a dispersion parameter if the difference is small or a negative binomial regression model if the difference is large.	What can we do if our modelling assumption are violated in Poisson Regression Modelling
6166	Big data might be big business, but overzealous data mining can seriously destroy your brand.  As companies become experts at slicing and dicing data to reveal details as personal as mortgage defaults and heart attack risks, the threat of egregious privacy violations grows.	Why is data mining bad
6318	One of the major advantages of neural nets is their ability to generalize. This means that a trained net could classify data from the same class as the learning data that it has never seen before.  The training set is used to train a neural net. The error of this dataset is minimized during training.	What is generalization in neural network
8291	Cross-entropy can be calculated using the probabilities of the events from P and Q, as follows: H(P, Q) = – sum x in X P(x) * log(Q(x))	How is cross entropy calculated
179	The function fX(x) gives us the probability density at point x. It is the limit of the probability of the interval (x,x+Δ] divided by the length of the interval as the length of the interval goes to 0. Remember that P(x<X≤x+Δ)=FX(x+Δ)−FX(x). =dFX(x)dx=F′X(x),if FX(x) is differentiable at x.	How do you write a probability density function
813	Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences.	What is the goal of reinforcement learning RL and how does it work
3280	A moving average is a technique to get an overall idea of the trends in a data set; it is an average of any subset of numbers. The moving average is extremely useful for forecasting long-term trends. You can calculate it for any period of time.  Moving averages are usually plotted and are best visualized.	How is moving average used in forecasting
1957	Try a series of runs with different amounts of training data: randomly sample 20% of it, say, 10 times and observe performance on the validation data, then do the same with 40%, 60%, 80%. You should see both greater performance with more data, but also lower variance across the different random samples.	How do you split your data between training and validation
165	Logistic Regression, also known as Logit Regression or Logit Model, is a mathematical model used in statistics to estimate (guess) the probability of an event occurring having been given some previous data. Logistic Regression works with binary data, where either the event happens (1) or the event does not happen (0).	What is logistic regression simple explanation
1503	To understand potential interaction effects, compare the lines from the interaction plot: If the lines are parallel, there is no interaction. If the lines are not parallel, there is an interaction.	How do you know if there is an interaction effect
6092	The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed.	Why do we use root mean square error
478	Motivation is the reason for people's actions, willingness and goals. Motivation is derived from the word motive which is defined as a need that requires satisfaction. These needs could be wants or desires that are acquired through influence of culture, society, lifestyle, etc. or generally innate.	What is motivation short note
3887	A person is faced with a stimulus that is very faint or confusing.  If the signal is present the person can decide that it is present or absent. These outcomes are called hits and misses. If the signal is absent the person can still decide that the signal is either present or absent.	What is a miss in signal detection theory
8640	Train a neural network with TensorFlowStep 1: Import the data.Step 2: Transform the data.Step 3: Construct the tensor.Step 4: Build the model.Step 5: Train and evaluate the model.Step 6: Improve the model.	How do I create a neural network in TensorFlow
444	Assuming a double-blind test is not possible, here are some techniques that can help:Standardize everything: the research protocol, the moderator script, the questions etc.  Have a second researcher monitor the first researcher.  Stay out of the participant's line of sight.  Practice.More items•	How do you remove experimenter bias
577	Standard deviation tells you how spread out the data is. It is a measure of how far each observed value is from the mean. In any distribution, about 95% of values will be within 2 standard deviations of the mean.	What does the standard deviation tell you about the data
8525	The false-positive rate is plotted on the x-axis and the true positive rate is plotted on the y-axis and the plot is referred to as the Receiver Operating Characteristic curve, or ROC curve.  This would be a threshold on the curve that is closest to the top-left of the plot.	What is threshold in ROC curve
2052	A recurrent neural network (RNN) is a type of neural network commonly used in speech recognition. RNNs are designed to recognize the sequential characteristics in data and use patterns to predict the next likely scenario.	Are designed to recognize a data's sequential characteristics
1283	A P.I Controller is a feedback control loop that calculates an error signal by taking the difference between the output of a system, which in this case is the power being drawn from the battery, and the set point.	What is the function of PI controller
8335	Some of the practical applications of reinforcement learning are:Manufacturing. In Fanuc, a robot uses deep reinforcement learning to pick a device from one box and putting it in a container.  Inventory Management.  Delivery Management.  Power Systems.  Finance Sector.	What are some practical applications of reinforcement learning
778	We can call a Logistic Regression a Linear Regression model but the Logistic Regression uses a more complex cost function, this cost function can be defined as the 'Sigmoid function' or also known as the 'logistic function' instead of a linear function.	What is the cost function used in logistic regression
4351	Discriminant function analysis (DFA) is a statistical procedure that classifies unknown individuals and the probability of their classification into a certain group (such as sex or ancestry group). Discriminant function analysis makes the assumption that the sample is normally distributed for the trait.	What is discriminant function analysis
6355	is that independence is the state or quality of being independent; freedom from dependence; exemption from reliance on, or control by others; self-subsistence or maintenance; direction of one's own affairs without interference while independent is a candidate or voter not affiliated with any political party, a free	What is the difference between independent and independence
1727	Blaise Pascal	Who created decision theory
3688	A mutually exclusive pair of events are complements to each other. For example: If the desired outcome is heads on a flipped coin, the complement is tails. The Complement Rule states that the sum of the probabilities of an event and its complement must equal 1, or for the event A, P(A) + P(A') = 1.	How is the rule of complement used to calculate probability
7102	The coefficient of variation represents the ratio of the standard deviation to the mean, and it is a useful statistic for comparing the degree of variation from one data series to another, even if the means are drastically different from one another.	Why is the coefficient of variation useful
1007	"An activation function is a function used in artificial neural networks which outputs a small value for small inputs, and a larger value if its inputs exceed a threshold. If the inputs are large enough, the activation function ""fires"", otherwise it does nothing."	What is the definition of squashing function in machine learning
2935	Given these assumptions, we know the following.The expected value of the difference between all possible sample means is equal to the difference between population means. Thus,  The standard deviation of the difference between sample means (σd) is approximately equal to: σd = sqrt( σ12 / n1 + σ22 / n2 )	How do you find the standard deviation of the sample mean difference
5459	), while other elements may be complex. Hermitian matrices have real eigenvalues whose eigenvectors form a unitary basis. For real matrices, Hermitian is the same as symmetric.	When a matrix is hermitian
1699	In information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how	What is a good precision and recall score
8495	One advantage of decision tree-based methods like random forests is their ability to natively handle categorical predictors without having to first transform them (e.g., by using feature engineering techniques).	Does random forest work with categorical variables
1383	Definition. Predictive analytics is an area of statistics that deals with extracting information from data and using it to predict trends and behavior patterns.  Predictive analytics statistical techniques include data modeling, machine learning, AI, deep learning algorithms and data mining.	How is predictive analytics done
747	"In Supervised learning, you train the machine using data which is well ""labeled.""  For example, Baby can identify other dogs based on past supervised learning. Regression and Classification are two types of supervised machine learning techniques. Clustering and Association are two types of Unsupervised learning."	What is supervised and unsupervised learning with example
861	The Bayesian approach permits the use of objective data or subjective opinion in specifying a prior distribution. With the Bayesian approach, different individuals might specify different prior distributions.  Bayesian methods have been used extensively in statistical decision theory (see statistics: Decision analysis).	How is Bayesian analysis used
2855	The p-value is calculated using the sampling distribution of the test statistic under the null hypothesis, the sample data, and the type of test being done (lower-tailed test, upper-tailed test, or two-sided test).  an upper-tailed test is specified by: p-value = P(TS ts | H 0 is true) = 1 - cdf(ts)	What is the P value formula
1057	In nonhierarchical clustering, such as the k-means algorithm, the relationship between clusters is undetermined. Hierarchical clustering repeatedly links pairs of clusters until every data object is included in the hierarchy.	What is the difference between hierarchical and nonhierarchical clustering methods
1464	First, it is a very quick estimate of the standard deviation. The standard deviation requires us to first find the mean, then subtract this mean from each data point, square the differences, add these, divide by one less than the number of data points, then (finally) take the square root.	How do you estimate standard deviation
1013	The Loss Function is one of the important components of Neural Networks. Loss is nothing but a prediction error of Neural Net. And the method to calculate the loss is called Loss Function. In simple words, the Loss is used to calculate the gradients. And gradients are used to update the weights of the Neural Net.	What is a loss function in neural network
1418	A CNN has multiple layers. Weight sharing happens across the receptive field of the neurons(filters) in a particular layer. Weights are the numbers within each filter.  These filters act on a certain receptive field/ small section of the image. When the filter moves through the image, the filter does not change.	What is weight sharing in CNN
1846	Variance	Which gives the measure of randomness of the random variable
353	Rather than trying to define a number, instead define what a field of numbers is; instead of defining what a vector is, consider instead all the vectors that make up a vector space. So to understand tensors of a particular type, instead consider all those tensors of the same type together.	What is a good way to understand tensors
789	Multicollinearity causes the following two basic types of problems: The coefficient estimates can swing wildly based on which other independent variables are in the model.  Multicollinearity reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model.	How does Multicollinearity affect the regression model
3154	From a practical standpoint, L1 tends to shrink coefficients to zero whereas L2 tends to shrink coefficients evenly. L1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that go to zero. L2, on the other hand, is useful when you have collinear/codependent features.	What is an advantage of l1 regularization over l2 regularization
6645	"Homoskedastic (also spelled ""homoscedastic"") refers to a condition in which the variance of the residual, or error term, in a regression model is constant. That is, the error term does not vary much as the value of the predictor variable changes."	What is Homoscedasticity in regression
857	Detection theory or signal detection theory is a means to measure the ability to differentiate between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the	What is noise in signal detection theory
1320	The definition of an ensemble is two or more people or things that function together as a whole. An example of an ensemble is a string quartet. An example of an ensemble is a group of actors in a play.  A small group of musicians playing or singing together.	What is an example of ensemble
6570	The log transformation can be used to make highly skewed distributions less skewed. This can be valuable both for making patterns in the data more interpretable and for helping to meet the assumptions of inferential statistics. Figure 1 shows an example of how a log transformation can make patterns more visible.	Why do we log transform data
4135	Cross-validation is usually the preferred method because it gives your model the opportunity to train on multiple train-test splits. This gives you a better indication of how well your model will perform on unseen data.  That makes the hold-out method score dependent on how the data is split into train and test sets.	Why is cross validation better than simple train test split
4348	Properties. The normal distribution is the only distribution whose cumulants beyond the first two (i.e., other than the mean and variance) are zero. It is also the continuous distribution with the maximum entropy for a specified mean and variance.	What is the variance in a normal distribution
2202	“Genetic Algorithms are good at taking large, potentially huge search spaces and navigating them, looking for optimal combinations of things, solutions you might not otherwise find in a lifetime.” The Genetic Algorithm (cont.)	Why genetic algorithm is better
522	The Gini index, or Gini coefficient, is a measure of the distribution of income across a population developed by the Italian statistician Corrado Gini in 1912.  The coefficient ranges from 0 (or 0%) to 1 (or 100%), with 0 representing perfect equality and 1 representing perfect inequality.	What should the Gini coefficient be
453	Logistic regression is a classification algorithm, don't confuse with the name regression.	Is logistic regression mainly used for regression True or false
2626	Data mining and machine learning are both rooted in data science. But there are several key distinctions between these two areas.Applications.Data miningMachine learningRecognizes patternsRecognizes patterns and adapts its analysis to the changing data sets2 more rows	What are the similarities and differences between data science machine learning and data mining
5497	An encoder is a network (FC, CNN, RNN, etc) that takes the input, and output a feature map/vector/tensor.  An encoder is a network (FC, CNN, RNN, etc) that takes the input, and output a feature map/vector/tensor. These feature vector hold the information, the features, that represents the input.	What is an encoder in machine learning
25	"The obvious difference between ANOVA and ANCOVA is the the letter ""C"", which stands for 'covariance'. Like ANOVA, ""Analysis of Covariance"" (ANCOVA) has a single continuous response variable. ANCOVA is also commonly used to describe analyses with a single response variable, continuous IVs, and no factors."	What is the difference between Anova and Ancova tests
302	The images shows that a higher VC dimension allows for a lower empirical risk (the error a model makes on the sample data), but also introduces a higher confidence interval. This interval can be seen as the confidence in the model's ability to generalize.	Why is VC dimension useful
1452	Data Wrangling: Preparation of data during the interactive data analysis and model building. Typically done by a data scientist or business analyst to change views on a dataset and for features engineering.	What is data wrangling in machine learning
242	When using the single sampling plan by attributes, one sample of size n is taken from the lot of size N and inspected. If there are c or less defective items in the sample, the lot is accepted. If there are more than c defective items in the sample, the lot is rejected.	Under what conditions is lot for lot acceptance sampling not accepted
5403	Connectionism, an approach to artificial intelligence (AI) that developed out of attempts to understand how the human brain works at the neural level and, in particular, how people learn and remember.  (For that reason, this approach is sometimes referred to as neuronlike computing.)	What is connectionist AI
3395	The logistic function is the inverse of the natural logit function and so can be used to convert the logarithm of odds into a probability. In mathematical notation the logistic function is sometimes written as expit in the same form as logit.	What is a logistic function used for
5644	Data labeling, in the context of machine learning, is the process of detecting and tagging data samples. The process can be manual but is usually performed or assisted by software.	What is Labelling in machine learning
2837	1 Answer. In order to come up with a split point, the values are sorted, and the mid-points between adjacent values are evaluated in terms of some metric, usually information gain or gini impurity. For your example, lets say we have four examples and the values of the age variable are (20,29,40,50).	How is a splitting point chosen for continuous variables in decision trees
7866	Sequential minimal optimization (SMO) is an algorithm for solving the quadratic programming (QP) problem that arises during the training of support-vector machines (SVM).  SMO is widely used for training support vector machines and is implemented by the popular LIBSVM tool.	What is SMO in machine learning
5340	The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed.  RMSD is the square root of the average of squared errors.	Is root mean squared error RMSE A approach
2974	So, For hidden layers the best option to use is ReLU, and the second option you can use as SIGMOID. For output layers the best option depends, so we use LINEAR FUNCTIONS for regression type of output layers and SOFTMAX for multi-class classification.	Can ReLu be used in output layer
410	With supervised learning, you have features and labels. The features are the descriptive attributes, and the label is what you're attempting to predict or forecast.  Thus, for training the machine learning classifier, the features are customer attributes, the label is the premium associated with those attributes.	What is feature and label in machine learning
1272	Just like the post period dummy variable controls for factors changing over time that are common to both treatment and control groups, the year fixed effects (i.e. year dummy variables) control for factors changing each year that are common to all cities for a given year.	What are year fixed effects
7327	A random variable is a variable whose value is a numerical outcome of a random phenomenon. A discrete random variable X has a countable number of possible values. Example: Let X represent the sum of two dice.  A continuous random variable X takes all values in a given interval of numbers.	What is the difference between a random variable and a discrete random variable
4665	Formally, the p-value is the probability of seeing a particular result (or greater one) from zero, assuming that the null hypothesis is true. If “null hypothesis is true” is confusing, replace it with, “assuming we had really run an A/A test.”	What is p value in AB testing
168	Consequences of Heteroscedasticity The OLS estimators and regression predictions based on them remains unbiased and consistent. The OLS estimators are no longer the BLUE (Best Linear Unbiased Estimators) because they are no longer efficient, so the regression predictions will be inefficient too.	What are the consequences of heteroscedasticity in linear regression
1012	Predictor variables in the machine learning context the the input data or the variables that is mapped to the target variable through an empirical relation ship usually determined through the data. In statistics you you refer to them as predictors. Each set of predictors may be called as an observation.	What is a predictor in machine learning
1330	The addition law of probability (sometimes referred to as the addition rule or sum rule), states that the probability that A or B will occur is the sum of the probabilities that A will happen and that B will happen, minus the probability that both A and B will happen.	What is sum rule of probability
5513	Descriptive statistics uses the data to provide descriptions of the population, either through numerical calculations or graphs or tables. Inferential statistics makes inferences and predictions about a population based on a sample of data taken from the population in question.	What are the key differences between descriptive and inferential statistics
1378	Within an artificial neural network, a neuron is a mathematical function that model the functioning of a biological neuron. Typically, a neuron compute the weighted average of its input, and this sum is passed through a nonlinear function, often called activation function, such as the sigmoid.	What are neurons in artificial neural networks
470	Difference between Z score vs T score. Z score is a conversion of raw data to a standard score, when the conversion is based on the population mean and population standard deviation.  T score is a conversion of raw data to the standard score when the conversion is based on the sample mean and sample standard deviation.	What is the difference between T score and Z score
8214	Clustering is useful for exploring data. If there are many cases and no obvious groupings, clustering algorithms can be used to find natural groupings. Clustering can also serve as a useful data-preprocessing step to identify homogeneous groups on which to build supervised models.	Why do we use clustering
4852	A t-test tests a null hypothesis about two means; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero.  A chi-square test tests a null hypothesis about the relationship between two variables.	What is the difference between chi square test and t test
5021	Predictor variable and independent variable are both similar in that they are used to observe how they affect some other variable or outcome. The main difference is that independent variables can be used to determine if one variable is the cause of changes in another, whereas predictor variables cannot.	What is the difference between an independent variable and a predictor variable
59	Residual analysis is used to assess the appropriateness of a linear regression model by defining residuals and examining the residual plot graphs.	What is residual analysis used for
4955	There are two different ways to encoding categorical variables. One-hot encoding converts it into n variables, while dummy encoding converts it into n-1 variables.  If we have k categorical variables, each of which has n values.	Is one hot encoding the same as dummy variables
3089	Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa).  The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).	Why is it called a confusion matrix
7167	P-value = the sum of the probabilities for all tables having a probability equal to or smaller than that observed. Page 11. Fisher's exact test: the example. ∑log(nij!)	What is p value in Fisher exact test
4594	Autocorrelation represents the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Autocorrelation measures the relationship between a variable's current value and its past values.	What is meant by autocorrelation
1380	"Sanderson points out in her book Social Psychology, confirmation bias also helps form and re-confirm stereotypes we have about people:3﻿ ""We also ignore information that disputes our expectations."	What role does confirmation bias play in stereotyping
593	In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.	What is generalized linear model in statistics
880	The square of the correlation coefficient, r², is a useful value in linear regression. This value represents the fraction of the variation in one variable that may be explained by the other variable.  The correlation coefficient also relates directly to the regression line Y = a + bX for any two variables, where .	How is the regression line related to the correlation coefficient
4471	The Finite Population Correction Factor (FPC) is used when you sample without replacement from more than 5% of a finite population. It's needed because under these circumstances, the Central Limit Theorem doesn't hold and the standard error of the estimate (e.g. the mean or proportion) will be too big.	What is the finite population correction factor in statistics
1299	Simple linear regression is a regression model that estimates the relationship between one independent variable and one dependent variable using a straight line. Both variables should be quantitative.	What is a simple linear regression model
3557	ABSTRACT. We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization.	What is l0 regularization
7005	Discrete control systems, as considered here, refer to the control theory of discrete‐time Lagrangian or Hamiltonian systems.  Geometric integrators are numericalintegration methods that preserve geometric properties of continuous systems, such as conservation of the symplectic form, momentum, and energy.	What is discrete control system
824	Kappa is widely used on Twitch in chats to signal you are being sarcastic or ironic, are trolling, or otherwise playing around with someone. It is usually typed at the end of a string of text, but, as can often the case on Twitch, it is also often used on its own or repeatedly (to spam someone).	What is Kappa used for
6283	It is linear if there exists a function H(x) = β0 + βT x such that h(x) = I(H(x) > 0). H(x) is also called a linear discriminant function. The decision boundary is therefore defined as the set {x ∈ Rd : H(x)=0}, which corresponds to a (d − 1)-dimensional hyperplane within the d-dimensional input space X.	What are the decision boundaries for linear discriminant analysis
4452	Streaming Data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes).	What is meant by streaming data
3861	Let me put some light on the key challenges that appear while processing the data.9- SecurityMost of the data processing systems have a single level of protection.No encryption of Either the raw data or the result/ output data.Access of the data to unethical IT professional that risks in data loss.	What are the problems of data processing
6149	This clustering method classifies the information into multiple groups based on the characteristics and similarity of the data.  There are many algorithms that come under partitioning method some of the popular ones are K-Mean, PAM(K-Mediods), CLARA algorithm (Clustering Large Applications) etc.	Which of the following is a partitioning based clustering algorithm
136	Structured data is highly specific and is stored in a predefined format, where unstructured data is a conglomeration of many varied types of data that are stored in their native formats. This means that structured data takes advantage of schema-on-write and unstructured data employs schema-on-read.	What is structured and unstructured data
457	GRU use less training parameters and therefore use less memory, execute faster and train faster than LSTM's whereas LSTM is more accurate on dataset using longer sequence. In short, if sequence is large or accuracy is very critical, please go for LSTM whereas for less memory consumption and faster operation go for GRU.	Why is Lstm better than GRU
4215	Probabilistic reasoning is a method of representation of knowledge where the concept of probability is applied to indicate the uncertainty in knowledge. Probabilistic reasoning is used in AI: When we are unsure of the predicates.  When it is known that an error occurs during an experiment.	What is Probabilistic Reasoning in AI
1816	Bias in Machine Learning is defined as the phenomena of observing results that are systematically prejudiced due to faulty assumptions.  This also results in bias which arises from the choice of training and test data and their representation of the true population.	What are bias in machine learning
7614	Artificial intelligence is impacting the future of virtually every industry and every human being. Artificial intelligence has acted as the main driver of emerging technologies like big data, robotics and IoT, and it will continue to act as a technological innovator for the foreseeable future.	What is the future of artificial intelligence
356	Definition. In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived.  Model validation is carried out after model training.	What is validation in machine learning
4450	When you get the features in lower dimensions then you will lose some information of data most of the times and you won't be able to interpret the lower dimension data.	What happens when you get features in lower dimensions using principal component analysis
2203	Fine tuning is one approach to transfer learning, and it is very popular in computer vision and NLP. The most common example given is when a model is trained on ImageNet is fine-tuned on a second task.  Transfer learning is when a model developed for one task is reused for a model on a second task.	What is fine tuning in transfer learning
6233	Unlike Monte Carlo sampling methods that are able to draw independent samples from the distribution, Markov Chain Monte Carlo methods draw samples where the next sample is dependent on the existing sample, called a Markov Chain.	What is the difference between Monte Carlo simulations and Markov Chain Monte Carlo MCMC
7010	It is the simplest model to study polymers. In other fields of mathematics, random walk is used to calculate solutions to Laplace's equation, to estimate the harmonic measure, and for various constructions in analysis and combinatorics. In computer science, random walks are used to estimate the size of the Web.	What is the purpose of studying random walks
135	Using proper validation techniques helps you understand your model, but most importantly, estimate an unbiased generalization performance.Splitting your data.  k-Fold Cross-Validation (k-Fold CV)  Leave-one-out Cross-Validation (LOOCV)  Nested Cross-Validation.  Time Series CV.  Comparing Models.	How do you validate a model performance
8118	There are limits on how smart humans can get, and any increases in thinking ability are likely to come with problems.  So most humans top out under six feet. Just as there are evolutionary tradeoffs for physical traits, Hills says, there are tradeoffs for intelligence.	Is there a limit to intelligence
4420	Both ExpressVPN and NordVPN offer multiple VPN protocols.  NordVPN has a slight edge because it offers the fast IKEv2 for use with mobile devices. The encryption standard offered by ExpressVPN is slightly better than that of NordVPN. Both companies use AES encryption with a 256-bit key.	Why is ExpressVPN better than NordVPN
489	NLP is short for natural language processing while NLU is the shorthand for natural language understanding.  They share a common goal of making sense of concepts represented in unstructured data, like language, as opposed to structured data like statistics, actions, etc.	What is the difference between NLU and NLP
4573	Rejection region: z > 1.645, which corresponds to α = 0.05.	What is the rejection region for testing at the 0.05 level of significance
969	"""A discrete variable is one that can take on finitely many, or countably infinitely many values"", whereas a continuous random variable is one that is not discrete, i.e. ""can take on uncountably infinitely many values"", such as a spectrum of real numbers."	What is the difference between discrete and continuous random variables
289	ANOVA is used to compare and contrast the means of two or more populations. ANCOVA is used to compare one variable in two or more populations while considering other variables.	Why we use Ancova instead of Anova
24	Lasso tends to do well if there are a small number of significant parameters and the others are close to zero (ergo: when only a few predictors actually influence the response). Ridge works well if there are many large parameters of about the same value (ergo: when most predictors impact the response).	When would you prefer using Lasso regression instead of ridge regression
7940	As discussed above, these two tests should be used for different data structures. Two-sample t-test is used when the data of two samples are statistically independent, while the paired t-test is used when data is in the form of matched pairs.	When should a paired t test be performed instead of a two sample t test
3565	Logistic regression assumes linearity of independent variables and log odds. Although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.	Does logistic regression assume independence
4557	The Hidden layer of the neural network is the intermediate layer between Input and Output layer. Activation function applies on hidden layer if it is available.  Hidden nodes or hidden neurons are the neurons that are neither in the input layer nor the output layer [3].	What are hidden nodes in neural network
1213	Reactive management is the polar opposite, and usually a follow-up, of proactive management. When a proactive leader gets swarmed enough with problems long enough, they turn reactive. Reactive management is an approach to management when the company leadership cannot or does not plan ahead for potential problems.	What is reactive management
3167	Markovian is an adjective that may describe: In probability theory and statistics, subjects named for Andrey Markov: A Markov chain or Markov process, a stochastic model describing a sequence of possible events. The Markov property, the memoryless property of a stochastic process.	What does Markov mean
1054	One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture.	What solves the vanishing gradient problem
1554	Ordinary least squares (OLS) is a non-iterative method that fits a model such that the sum-of-squares of differences of observed and predicted values is minimized. Gradient descent finds the linear model parameters iteratively.	Does OLS use gradient descent
6366	Bivariate analysis looks at two paired data sets, studying whether a relationship exists between them. Multivariate analysis uses two or more variables and analyzes which, if any, are correlated with a specific outcome. The goal in the latter case is to determine which variables influence or cause the outcome.	What is the difference between bivariate and multivariate analysis
6145	Transfer learning without any labeled data from the target domain is referred to as unsupervised transfer learning.	Is transfer learning unsupervised
6860	In an observational study or an experiment, the variable whose values are to be predicted from values of other values is called a response variable and variables whose values are used to predict values of another variable are called predictor variables.	What are response and predictor variables
1136	You can use KNN by converting the categorical values into numbers.Enumerate the categorical data, give numbers to the categories, like cat = 1, dog = 2 etc.Perform feature scaling. So that the loss function is not biased to some particular features.Done, now apply the K- nearnest neighbours algorithm.	How does Knn handle categorical data
2846	A psychometric and capability test aims to provide measurable, objective data that can give you a better versatile view of a candidate's skills and suitability for a position. Assessments offer scientific, valid reliable and objectivity to the process of recruiting.	What is psychometric and skills testing
3184	Connected components, in a 2D image, are clusters of pixels with the same value, which are connected to each other through either 4-pixel, or 8-pixel connectivity.  We offer several user-friendly ways to segment, and then rapidly calculate and display the connected components of 2D and 3D segmentations.	What is connected component image processing
6690	9:4220:54Suggested clip · 97 secondsPermutations Combinations Factorials & Probability - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the probability of a factorial
6933	No. Stock return is not always stationary.  Using non-stationary time series data in financial models produces unreliable and spurious results and leads to poor understanding and forecasting. The solution to the problem is to transform the time series data so that it becomes stationary.	Are financial time series stationary
205	The intercept (often labeled the constant) is the expected mean value of Y when all X=0. Start with a regression equation with one predictor, X. If X sometimes equals 0, the intercept is simply the expected mean value of Y at that value.	How do you interpret intercepts in logistic regression
225	Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant.	Why Multicollinearity is a problem
1971	In general, prediction is the process of determining the magnitude of statistical variates at some future point of time.	What is a statistical prediction
4356	follows a negative binomial distribution with parameters r and p. The geometric distribution is a special case of discrete compound Poisson distribution.	Is geometric distribution discrete or continuous
769	Divide the total by the number of members of the cluster. In the example above, 283 divided by four is 70.75, and 213 divided by four is 53.25, so the centroid of the cluster is (70.75, 53.25).	How do you find the centroid in statistics
1393	The Poisson regression model introduced above is the most natural example of such a count data regression model.  It provides a fully parametric approach and suggests MCMC techniques for fitting a model to the given data.	Is Poisson regression Parametric
5004	The cumulative distribution function (CDF) calculates the cumulative probability for a given x-value. Use the CDF to determine the probability that a random observation that is taken from the population will be less than or equal to a certain value.	What does the cumulative distribution function measure
2834	Betas are calculated by subtracting the mean from the variable and dividing by its standard deviation. This results in standardized variables having a mean of zero and a standard deviation of 1. Standardized beta coefficients are also called: Betas.	How are the coefficients beta determined in a regression
1020	The simplest form of language model simply throws away all conditioning context, and estimates each term independently. Such a model is called a unigram language model : (95) There are many more complex kinds of language models, such as bigram language models , which condition on the previous term, (96)	What is unigram language model
1733	Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.	What exactly is Artificial Intelligence
3109	In the absence of a class label, clustering analysis is also called unsupervised learning, as opposed to supervised learning that includes classification and regression. Accordingly, approaches to clustering analysis are typically quite different from supervised learning.	Is clustering supervised or unsupervised
1364	Nonstandard units provide a good rationale for using standard units. It allows for a good transition into standard units because the students can understand the need for standard units if they have measured the same object but determined differing answers.	Do you agree with using non standard units instead of standard units in instructional activities
362	The input to multidimensional scaling is a distance matrix. The output is typically a two-dimensional scatterplot, where each of the objects is represented as a point.	What is the input data for multi dimensional scaling
1215	three	How many independent and dependent variables can you have in an experiment
7093	The training data is an initial set of data used to help a program understand how to apply technologies like neural networks to learn and produce sophisticated results.  Training data is also known as a training set, training dataset or learning set.	What is a training set in data mining
6867	The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes.	What does the Kolmogorov Smirnov test mean
514	Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2).	What is the difference between K means clustering and hierarchical clustering
1142	Types of predictive modelsForecast models. A forecast model is one of the most common predictive analytics models.  Classification models.  Outliers Models.  Time series model.  Clustering Model.  The need for massive training datasets.  Properly categorising data.	What are the different types of predictive models
3965	A/B testing (also known as split testing) is the process of comparing two versions of a web page, email, or other marketing asset and measuring the difference in performance. You do this giving one version to one group and the other version to another group. Then you can see how each variation performs.	What is AB testing tools
912	Rule-based systems process data and output information, but they also process rules and make decisions.  Knowledge-based systems also process data and rules to output information and make decisions. In addition, they also process expert knowledge to output answers, recommendations, and expert advice.	What is the difference between a rule based system and a knowledge based system
5902	For a discrete random variable, the expected value, usually denoted as or , is calculated using: μ = E ( X ) = ∑ x i f ( x i )	What is the expected value of a random variable
869	The General Linear Model (GLM) is a useful framework for comparing how several variables affect different continuous variables. In it's simplest form, GLM is described as: Data = Model + Error (Rutherford, 2001, p.3) GLM is the foundation for several statistical tests, including ANOVA, ANCOVA and regression analysis.	What is the general linear model GLM Why does it matter
4753	In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success/yes/true/one (with probability p)	What are the parameters of binomial distribution
1249	Content-based recommendation systems uses their knowledge about each product to recommend new ones. Recommendations are based on attributes of the item. Content-based recommender systems work well when descriptive data on the content is provided beforehand. “Similarity” is measured against product attributes.	How does recommendation algorithm work
5429	1 Answer. 1. 8. Without math: The delta rule uses gradient descent to minimize the error from a perceptron network's weights. Gradient descent is a general algorithm that gradually changes a vector of parameters in order to minimize an objective function.	What is gradient descent and Delta Rule
5860	Preparing Text for Natural Language ProcessingFeature Extraction.  Step 1 : Collect Data , for example consider the nursery rhyme.  Step 2 : Design the vocabulary , while defining the vocabulary we take the pre-processing text steps as mentioned previously to clean the text of punctuation , converting all words to small case etc.  Step 3 : Create Document Vectors.More items•	How do you prepare text data for deep learning
327	The error sum of squares is obtained by first computing the mean lifetime of each battery type. For each battery of a specified type, the mean is subtracted from each individual battery's lifetime and then squared. The sum of these squared terms for all battery types equals the SSE. SSE is a measure of sampling error.	How is the error sum of squares calculated
2519	For most common clustering software, the default distance measure is the Euclidean distance.  Correlation-based distance considers two objects to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance.	What is distance measure in clustering
6146	Decision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves).	What is decision tree learning in machine learning
1104	To calculate probabilities involving two random variables X and Y such as P(X > 0 and Y ≤ 0), we need the joint distribution of X and Y . The way we represent the joint distribution depends on whether the random variables are discrete or continuous. p(x,y) = P(X = x and Y = y),x ∈ RX ,y ∈ RY .	How do you find the joint probability distribution
1590	Since a Naive Bayes text classifier is based on the Bayes's Theorem, which helps us compute the conditional probabilities of occurrence of two events based on the probabilities of occurrence of each individual event, encoding those probabilities is extremely useful.	How does naive Bayes work in text classification
7238	Kurtosis is the characteristic of being flat or peaked. It is a measure of whether data is heavy-tailed or light-tailed in a normal distribution.	What is kurtosis in machine learning
2400	The S-curve shows the innovation from its slow early beginnings as the technology or process is developed, to an acceleration phase (a steeper line) as it matures and, finally, to its stabilisation over time (the flattening curve), with corresponding increases in performance of the item or organisation using it.	What is the innovation S curve
8200	Since p < 0.05 is enough to reject the null hypothesis (no association), p = 0.002 reinforce that rejection only. If the significance value that is p-value associated with chi-square statistics is 0.002, there is very strong evidence of rejecting the null hypothesis of no fit. It means good fit.	What is a good chi squared value
3480	A feedforward neural network is a biologically inspired classification algorithm. It consist of a (possibly large) number of simple neuron-like processing units, organized in layers. Every unit in a layer is connected with all the units in the previous layer.  This is why they are called feedforward neural networks.	What is meant by feed forward neural network
227	The total entropy of a system either increases or remains constant in any process; it never decreases. For example, heat transfer cannot occur spontaneously from cold to hot, because entropy would decrease. Entropy is very different from energy. Entropy is not conserved but increases in all real processes.	What happens to entropy as energy is converted
1081	Specifical- ly, for periodic signals we can define the Fourier transform as an impulse train with the impulses occurring at integer multiples of the fundamental frequency and with amplitudes equal to 27r times the Fourier series coefficients.	What is the Fourier transform of a periodic signal
3436	Groupthink can lead collective rationalization, lack of personal accountability and pressure to acquiesce. Groupthink is a common factor in bad decision-making and serious ethical breaches.  They take precautions to prevent groupthink from taking hold.	How is groupthink dangerous
320	The Antardasha of Mercury with Ketu Mahadasha can be evil and good depending on the placement of both Mercury and Ketu in the birth chart.  The antardasha of Mercury with Mahadasha of Ketu brings very bad results if the planet Mercury is weak, afflicted, aspect by Rahu, Saturn and Mars.	Is Ketu Mahadasha bad
3948	Some of the algorithms used in image recognition (Object Recognition, Face Recognition) are SIFT (Scale-invariant Feature Transform), SURF (Speeded Up Robust Features), PCA (Principal Component Analysis), and LDA (Linear Discriminant Analysis).	Which algorithm is used for image recognition
2856	Thus logit regression is simply the GLM when describing it in terms of its link function, and logistic regression describes the GLM in terms of its activation function.	What is the difference between logit and logistic regression
4345	Leaky ReLU & Parametric ReLU (PReLU) Leaky ReLU has two benefits: It fixes the “dying ReLU” problem, as it doesn't have zero-slope parts. It speeds up training. There is evidence that having the “mean activation” be close to 0 makes training faster.	What are the advantages of using Leaky Rectified Linear Units Leaky ReLU over normal ReLU in deep learning
1151	If the absolute value of the t-value is greater than the critical value, you reject the null hypothesis. If the absolute value of the t-value is less than the critical value, you fail to reject the null hypothesis.	How do you reject the null hypothesis in t test
8	 Birst employs caching and aggregate awareness to send queries to the cache first, and then data to the user-ready data store.  If data is not cached, Birst generates one or more queries depending on how the data is sourced.  Birst's in-memory caching includes both exact and fuzzy matching.	What are the matching types that birst employs while searching data in cache
5783	Extended Kalman filter (EKF): While the Kalman filter is designed for linear discrete-time dynamical system, EKF works for discrete-time nonlinear systems.	What is the difference between Kalman filter and extended Kalman filter
4122	User-Based Collaborative Filtering is a technique used to predict the items that a user might like on the basis of ratings given to that item by the other users who have similar taste with that of the target user. Many websites use collaborative filtering for building their recommendation system.	What is user based collaborative filtering
1643	In this work, we present Deep Neural Decision Trees (DNDT) -- tree models realised by neural networks. A DNDT is intrinsically interpretable, as it is a tree. Yet as it is also a neural network (NN), it can be easily implemented in NN toolkits, and trained with gradient descent rather than greedy splitting.	Is decision tree a neural network
587	The probability of a specific value of a continuous random variable will be zero because the area under a point is zero.	Why is the probability of a continuous random variable 0
2694	The accuracy is a measure of the degree of closeness of a measured or calculated value to its actual value. The percent error is the ratio of the error to the actual value multiplied by 100. The precision of a measurement is a measure of the reproducibility of a set of measurements.  A systematic error is human error.	Does percent error measure accuracy or precision explain
1551	Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them. Sometimes this is incorrect.	What is the common problem with linear regression
122	You can reduce High variance, by reducing the number of features in the model. There are several methods available to check which features don't add much value to the model and which are of importance. Increasing the size of the training set can also help the model generalise.	How do you deal with variance and bias
7134	A path coefficient is interpreted: If X changes by one standard deviation Y changes by b standard deviations (with b beeing the path coefficient). Dr. Jan-Michael Becker, University of Cologne, SmartPLS Developer. Researchgate: https://www.researchgate.net/profile/Jan_Michael_Becker.	What is path coefficient in SmartPLS
894	The main use of F-distribution is to test whether two independent samples have been drawn for the normal populations with the same variance, or if two independent estimates of the population variance are homogeneous or not, since it is often desirable to compare two variances rather than two averages.	Why do we use F distribution
824	"In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input."	What is an activation function in machine learning
5019	Suggest Edits. Support Vector Machines (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis.	What is a linear SVM
359	In what might only be perceived as a win for Facebook, OpenAI today announced that it will migrate to the social network's PyTorch machine learning framework in future projects, eschewing Google's long-in-the-tooth TensorFlow platform.	Does OpenAI use TensorFlow
7008	If x(n), y(n) and z(n) are the samples of the signals, the correlation coefficient between x and y is given by Sigma x(n) * y(n) divided by the root of [Sigma x(n)^2 * y(n)^2], where ' * ' denotes simple multiplication and ^2 denotes squaring. The summation is taken over all the samples of the signals.	How do you find the correlation between two signals
3294	Again, random forest is very effective on a wide range of problems, but like bagging, performance of the standard algorithm is not great on imbalanced classification problems.	Can random forest handle imbalanced data
6663	Ideally you have some kind of pre-clustered data (supervised learning) and test the results of your clustering algorithm on that. Simply count the number of correct classifications divided by the total number of classifications performed to get an accuracy score.	How do you test a clustering algorithm
7929	Nonlinear regression can fit many more types of curves, but it can require more effort both to find the best fit and to interpret the role of the independent variables. Additionally, R-squared is not valid for nonlinear regression, and it is impossible to calculate p-values for the parameter estimates.	Can we perform regression on non linear data
1160	Classification and prediction are two forms of data analysis that can be used to extract models describing important data classes or to predict future data trends [8]. Classification is a data mining (machine learning) technique used to predict group membership for data instances.	Why classification is important in machine learning
94	The t distributions were discovered by William S.  Gosset was a statistician employed by the Guinness brewing company which had stipulated that he not publish under his own name. He therefore wrote under the pen name ``Student. '' These distributions arise in the following situation.	Why is it called Student t distribution
3349	"1. Getting Data from Twitter Streaming APICreate a twitter account if you do not already have one.Click ""Create New App""Fill out the form, agree to the terms, and click ""Create your Twitter application""In the next page, click on ""API keys"" tab, and copy your ""API key"" and ""API secret"".More items•"	How do you get a dataset from twitter
5437	In calculating a simple average, or arithmetic mean, all numbers are treated equally and assigned equal weight. But a weighted average assigns weights that determine in advance the relative importance of each data point.	What is the difference between arithmetic mean and weighted mean
3627	The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data.	Why does bootstrap work in machine learning
2573	The chief difference between MEMM and CRF is that MEMM is locally renormalized and suffers from the label bias problem, while CRFs are globally renormalized.	How do Conditional Random Fields CRF compare to Maximum Entropy Models and Hidden Markov Models
668	Calculate bias by finding the difference between an estimate and the actual value. To find the bias of a method, perform many estimates, and add up the errors in each estimate compared to the real value. Dividing by the number of estimates gives the bias of the method.	How do you calculate bias
5368	Definition: Gamma distribution is a distribution that arises naturally in processes for which the waiting times between events are relevant. It can be thought of as a waiting time between Poisson distributed events.	What does gamma distribution mean
5979	Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point.  Both types of predictive distributions have the form of a compound probability distribution (as does the marginal likelihood).	What is Bayesian predictive modeling
696	"m (the greek letter ""mu"") is used to denote the population mean. The population mean is worked out in exactly the same way as the sample mean: add all of the scores together, and divide the result by the total number of scores. In journal articles, the mean is usually represented by M, and the median by Mdn."	Is Capital M mean or median
146	Divergent Thinking. By contrast, divergent means “developing in different directions” and so divergent thinking opens your mind in all directions.	What type of thinking could be described as taking different directions
978	Get startedPrepare your TensorBoard logs. (or download a sample from here).Upload the logs. Install the latest version of TensorBoard to use the uploader. $ pip install -U tensorboard.  View your experiment on TensorBoard. dev. Follow the link provided to view your experiment, or share it with others.	How do you share a TensorBoard
2387	more  A symbol for a value we don't know yet. It is usually a letter like x or y. Example: in x + 2 = 6, x is the variable.	What is variable example
8430	The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis.	How do I interpret p value in logistic regression
293	In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the	What is probability density function in normal distribution
2615	Though originally proposed as a form of generative model for unsupervised learning, GANs have also proven useful for semi-supervised learning, fully supervised learning, and reinforcement learning.	Is GANs a reinforcement learning
4353	Factorials are symbolized by exclamation points (!). A factorial is a mathematical operation in which you multiple the given number by all of the positive whole numbers less than it. In other words. = n × ( n − 1 ) × … × 2 × 1 .	What is Statistics Factorial
7506	Linear algebra is called linear because it is the study of straight lines. A linear function is any function that graphs to a straight line, and linear algebra is the mathematics for solving systems that are modeled with multiple linear functions.  Multiple linear equations can be expressed as vectors and matrices.	Why is linear algebra called linear
228	Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on. If your training/validation loss are about equal then your model is underfitting. Increase the size of your model (either number of layers or the raw number of neurons per layer)	How does neural network reduce loss
1401	Under the batch processing model, a set of data is collected over time, then fed into an analytics system. In other words, you collect a batch of information, then send it in for processing. Under the streaming model, data is fed into analytics tools piece-by-piece. The processing is usually done in real time.	What are the differences between batch processing and stream processing systems
7631	4. An implementation of Reinforcement LearningInitialize the Values table 'Q(s, a)'.Observe the current state 's'.Choose an action 'a' for that state based on one of the action selection policies (eg.  Take the action, and observe the reward 'r' as well as the new state 's'.More items•	How do you implement reinforcement in learning
464	7 Techniques to Handle Imbalanced DataUse the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	How do you handle imbalanced datasets
618	Pooled data occur when we have a “time series of cross sections,” but the observations in each cross section do not necessarily refer to the same unit. Panel data refers to samples of the same cross-sectional units observed at multiple points in time.	How is panel data different from cross sectional data
4219	The hazard function (also called the force of mortality, instantaneous failure rate, instantaneous death rate, or age-specific failure rate) is a way to model data distribution in survival analysis.  The function is defined as the instantaneous risk that the event of interest happens, within a very narrow time frame.	How is a hazard function defined
6484	The most common hash functions used in digital forensics are Message Digest 5 (MD5), and Secure Hashing Algorithm (SHA) 1 and 2.	What are the two common hash functions
5960	From Wikipedia, the free encyclopedia. In computational linguistics and computer science, edit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other.	What is string edit distance
446	Positive Skewness means when the tail on the right side of the distribution is longer or fatter. The mean and median will be greater than the mode. Negative Skewness is when the tail of the left side of the distribution is longer or fatter than the tail on the right side. The mean and median will be less than the mode.	Why can skewness be negative
4263	It's a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction.	What are loss functions in machine learning
5094	These are three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.	What are the two types of machine learning
1117	Probabilities for the two diceTotalNumber of combinationsProbability6513.89%7616.67%8513.89%9411.11%8 more rows	What is the probability of rolling a sum of 6 with two dice
939	As regards the normality of group data, the one-way ANOVA can tolerate data that is non-normal (skewed or kurtotic distributions) with only a small effect on the Type I error rate. However, platykurtosis can have a profound effect when your group sizes are small.	Can you use Anova if data is not normally distributed
4780	Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics.	What is posterior inference
7504	Null and alternate hypothesis are different and you can't interchange them. Alternate hypothesis is just the opposite of null which means there is a statistical difference in Mean / median of both the data sets.	Can I switch around the null and alternative hypothesis in hypothesis testing
6253	The expected value for a random variable, X, from a Bernoulli distribution is: E[X] = p. For example, if p = . 04, then E[X] = 0.4.	What is the expected value of a Bernoulli random variable
1065	One of the most widely used predictive analytics models, the forecast model deals in metric value prediction, estimating numeric value for new data based on learnings from historical data. This model can be applied wherever historical numerical data is available.	Which model is used for prediction
6153	The mean, expected value, or expectation of a random variable X is writ- ten as E(X) or µX. If we observe N random values of X, then the mean of the N values will be approximately equal to E(X) for large N. The expectation is defined differently for continuous and discrete random variables.	What is the expectation of a random variable
7146	Standard deviation measures the spread of a data distribution. It measures the typical distance between each data point and the mean. The formula we use for standard deviation depends on whether the data is being considered a population of its own, or the data is a sample representing a larger population.	What is sample and population standard deviation
6351	Sampling bias occurs when some members of a population are systematically more likely to be selected in a sample than others. It is also called ascertainment bias in medical fields. Sampling bias limits the generalizability of findings because it is a threat to external validity, specifically population validity.	What happens if you have a sampling bias in a survey
50	When we have a high degree linear polynomial that is used to fit a set of points in a linear regression setup, to prevent overfitting, we use regularization, and we include a lambda parameter in the cost function. This lambda is then used to update the theta parameters in the gradient descent algorithm.	What is lambda in linear regression
3826	Definition of artificial intelligence AI is the ability of a machine to display human-like capabilities such as reasoning, learning, planning and creativity. AI enables technical systems to perceive their environment, deal with what they perceive, solve problems and act to achieve a specific goal.	What is AI and how is it used
49	The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It can be used to estimate summary statistics such as the mean or standard deviation.	What is bootstrap sampling in machine learning and why is it important 1
925	The averaged height is just one number now. Sample distribution: Just the distribution of the data from the sample. Sampling distribution: The distribution of a statistic from several samples.	What is the difference between a distribution and a sampling distribution
1755	"The homunculus argument is a fallacy whereby a concept is explained in terms of the concept itself, recursively, without first defining or explaining the original concept.  The obvious answer is that there is another homunculus inside the first homunculus's ""head"" or ""brain"" looking at this ""movie""."	What is the homunculus problem
3153	The general application of the matrix norm is the derivative form of finding proof in terms of interplay and tandem of vectorial normalized formats to whom are extended.. It can be used in tandem with Graphical processing, image processing, all kinds of algorithmics in terms of calculations and derivatives..	What is the application of matrix norm
5797	Data is the currency of applied machine learning.  Resampling is a methodology of economically using a data sample to improve the accuracy and quantify the uncertainty of a population parameter. Resampling methods, in fact, make use of a nested resampling method.	What is resampling in machine learning
1229	You do need distributional assumptions about the response variable in order to make inferences (e.g, confidence intervals), but it is not necessary that the response variable be normallhy distributed.	Does dependent variable need to be normally distributed
5314	Naive Bayes works best when you have small training data set, relatively small features(dimensions). If you have huge feature list, the model may not give you accuracy, because the likelihood would be distributed and may not follow the Gaussian or other distribution.	When should we use naive Bayes
5306	You take the sum of the squares of the terms in the distribution, and divide by the number of terms in the distribution (N). From this, you subtract the square of the mean (μ2). It's a lot less work to calculate the standard deviation this way.	How do you find the sum of squares with standard deviation
172	Some of the main drawbacks of association rule algorithms in e-learning are: the used algorithms have too many parameters for somebody non expert in data mining and the obtained rules are far too many, most of them non-interesting and with low comprehensibility.	What is the limitations behind rule generation in association rule mining
646	3:1532:58Suggested clip · 96 secondsHow To Deploy TensorFlow Models On Mobile Platforms - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you deploy TensorFlow models on mobile platforms
4206	The common wisdom is, Interpolation is likely to be more accurate than extrapolation. And the further you extrapolate from your data, the more inaccurate your predictions are likely to be.  The closer you are to a known data point, the more accurate your estimate is likely to be.	Which is better interpolation or extrapolation
4049	7 Best Models for Image Classification using Keras1 Xception. It translates to “Extreme Inception”.  2 VGG16 and VGG19: This is a keras model with 16 and 19 layer network that has an input size of 224X224.  3 ResNet50. The ResNet architecture is another pre-trained model highly useful in Residual Neural Networks.  4 InceptionV3.  5 DenseNet.  6 MobileNet.  7 NASNet.	What is the best model for image classification
936	Stepwise Selection Stepwise regression is a modification of the forward selection so that after each step in which a variable was added, all candidate variables in the model are checked to see if their significance has been reduced below the specified tolerance level.	What is stepwise variable selection
6925	"“Critical"" values of z are associated with interesting central areas under the standard normal curve.  In other words, there is an 80% probability that any normal variable will fall within 1.28 standard deviations of its mean. So we say that 1.28 is the critical value of z that corresponds to a central area of 0.80."	What is Z critical value
410	When examining the distribution of a quantitative variable, one should describe the overall pattern of the data (shape, center, spread), and any deviations from the pattern (outliers).	How do you describe the distribution
3457	At a higher level, the chief difference between the L1 and the L2 terms is that the L2 term is proportional to the square of the β values, while the L1 norm is proportional the absolute value of the values in β.	What is the difference between l1 and l2 norms
122	Multivariate Normality–Multiple regression assumes that the residuals are normally distributed. No Multicollinearity—Multiple regression assumes that the independent variables are not highly correlated with each other. This assumption is tested using Variance Inflation Factor (VIF) values.	What is normality assumption in regression
1035	Center: The center is not affected by sample size. The mean of the sample means is always approximately the same as the population mean µ = 3,500. Spread: The spread is smaller for larger samples, so the standard deviation of the sample means decreases as sample size increases.	Does the mean change with sample size
7213	Time series analysis is a statistical technique that deals with time series data, or trend analysis. Time series data means that data is in a series of particular time periods or intervals.  Time series data: A set of observations on the values that a variable takes at different times.	What is Time Series Analytics
1108	Deep learning is a class of machine learning algorithms that uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.	What is a deep learning model
2162	A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text. So you're working on a text classification problem.	What is SVM and how it works
3323	The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum.	What is the difference between a cluster sample and a stratified random sample
79	Data for two variables (usually two types of related data). Example: Ice cream sales versus the temperature on that day. The two variables are Ice Cream Sales and Temperature.	What is an example of bivariate data
477	1 Posterior is compromise of prior and likelihood.  For a prior distribution expressed as beta(θ|a,b), the prior mean of θ is a/(a + b). Suppose we observe z heads in N flips, which is a proportion of z/N heads in the data. The posterior mean is (z + a)/[(z + a) + (N ‒ z + b)] = (z + a)/(N + a + b).	What is the posterior mean
1744	Distance MatrixThe proximity between object can be measured as distance matrix.  For example, distance between object A = (1, 1) and B = (1.5, 1.5) is computed as.Another example of distance between object D = (3, 4) and F = (3, 3.5) is calculated as.More items	How do you find the distance of a clustered Matrix
1656	On a broad level, we can differentiate both AI and ML as: AI is a bigger concept to create intelligent machines that can simulate human thinking capability and behavior, whereas, machine learning is an application or subset of AI that allows machines to learn from data without being programmed explicitly.	What is the difference between artificial learning and machine learning
821	"The label ""moving average"" is technically incorrect since the MA coefficients may be negative and may not sum to unity. This label is used by convention.  The name ""moving average"" is somewhat misleading because the weights 1,−θ1,−θ2,…,−θq, which multiply the a's, need not total unity nor need that be positive."	Why is it called Moving Average
310	The central limit theorem applies to almost all types of probability distributions, but there are exceptions. For example, the population must have a finite variance. That restriction rules out the Cauchy distribution because it has infinite variance.	Does the central limit theorem apply to variance
4097	First of all, you don't need to normalise your inputs until one/more of the inputs start to dominate others - which is the fundamental reason behind normalization/standardization.	Do I have to normalize the output variables also when Im normalizing input variables for gradient descent
1536	Examples of Artificial Intelligence: Work & School1 – Google's AI-Powered Predictions.  2 – Ridesharing Apps Like Uber and Lyft.  3 — Commercial Flights Use an AI Autopilot.1 – Spam Filters.2 – Smart Email Categorization.1 –Plagiarism Checkers.  2 –Robo-readers.  1 – Mobile Check Deposits.More items•	What are some applications of AI in real life
2351	A frequency distribution is a representation, either in a graphical or tabular format, that displays the number of observations within a given interval. The interval size depends on the data being analyzed and the goals of the analyst. The intervals must be mutually exclusive and exhaustive.	What is frequency distribution and how do you construct frequency distribution
2102	Logistic regression analysis is used to examine the association of (categorical or continuous) independent variable(s) with one dichotomous dependent variable. This is in contrast to linear regression analysis in which the dependent variable is a continuous variable.	What is the main purpose of logistic regression
612	It is the process of transforming a categorical variable into a continuous variable and using them in the model. Lets start with basic and go to advanced methods. One Hot Encoding & Label Encoding.	What is feature encoding
2625	Calculating Standard Error of the MeanFirst, take the square of the difference between each data point and the sample mean, finding the sum of those values.Then, divide that sum by the sample size minus one, which is the variance.Finally, take the square root of the variance to get the SD.	How do we calculate standard error
439	Bias allows you to shift the activation function by adding a constant (i.e. the given bias) to the input. Bias in Neural Networks can be thought of as analogous to the role of a constant in a linear function, whereby the line is effectively transposed by the constant value.	What are biases in neural network
8097	Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group.	What is clustering algorithm in machine learning
1127	The finite frequency theory of probability defines the probability of an outcome as the frequency of the number of times the outcome occurs relative to the number of times that it could have occured. This is defined as the limiting frequency with which that outcome appears in a long series of similar events.	What is the frequency definition of probability
428	This is the reference consumption model where every infrastructure component (ML platform, algorithms, compute, and data) is deployed and managed by the user. The user builds, trains, and deploys ML models. The user is also responsible for installing and managing all components of the developer environment.	What is an AI stack
998	Events A and B are independent if the equation P(A∩B) = P(A) · P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together.	How do you know if events are independent
58	Start by looking at the left side of your degrees of freedom and find your variance. Then, go upward to see the p-values. Compare the p-value to the significance level or rather, the alpha. Remember that a p-value less than 0.05 is considered statistically significant.	How do you determine statistical significance
1422	Principal component analysis is a dimensionality reduction method.  Canonical correlation analysis, on the other hand, is a method for comparing draws from two different multivariate distributions.	How does canonical correlation analysis CCA compare to principal component analysis PCA
6562	Unlike R-squared, you can use the standard error of the regression to assess the precision of the predictions. Approximately 95% of the observations should fall within plus/minus 2*standard error of the regression from the regression line, which is also a quick approximation of a 95% prediction interval.	How do you interpret the standard error of estimate in regression
6072	Feature extraction describes the relevant shape information contained in a pattern so that the task of classifying the pattern is made easy by a formal procedure. In pattern recognition and in image processing, feature extraction is a special form of dimensionality reduction.	What is feature extraction in image processing
4313	In statistics, a generalized linear mixed model (GLMM) is an extension to the generalized linear model (GLM) in which the linear predictor contains random effects in addition to the usual fixed effects. They also inherit from GLMs the idea of extending linear mixed models to non-normal data.	What is the difference between GLM and GLMM
4550	The fundamental assumption of statistical mechanics is that, over time, an isolated system in a given macrostate is equally likely to be found in any of it's microstates. Thus, our system of 2 atoms is most likely to be in a microstate where energy is split up 50/50.	What is the fundamental assumption of statistical mechanics
1182	Another sign of overfitting may be seen in the classification accuracy on the training data, If the training accuracy is out performing our test accuracy, it means that our model is learning details and noises of training data and specifically working of training data. Overfitting is a major problem in neural networks.	How do you know if a neural network is Overfitting
7082	How to conduct a multivariate testIdentify a problem.  Formulate a hypothesis.  Create variations.  Determine your sample size.  Test your tools.  Start driving traffic.  Analyze your results.  Learn from your results.	How do you do a multivariate test
2963	For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms can perform inference and learning in Bayesian networks.	What is Bayesian network with example
2622	A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution.	What is the difference between a normal distribution and a standard normal distribution
5934	Quality Glossary Definition: Reliability. Reliability is defined as the probability that a product, system, or service will perform its intended function adequately for a specified period of time, or will operate in a defined environment without failure.	What do you mean by reliability
4748	Andrew Ng says that batch normalization should be applied immediately before the non-linearity of the current layer. The authors of the BN paper said that as well, but now according to François Chollet on the keras thread, the BN paper authors use BN after the activation layer.	Where is batch normalization applied
6754	Using multiple features from multiple filters improve the performance of the network. Other than that, there is another fact that makes the inception architecture better than others. All the architectures prior to inception, performed convolution on the spatial and channel wise domain together.	What is the primary advantage of using multiple filters
4414	Clustering or cluster analysis is an unsupervised learning problem. It is often used as a data analysis technique for discovering interesting patterns in data, such as groups of customers based on their behavior. There are many clustering algorithms to choose from and no single best clustering algorithm for all cases.	Why do we use clustering in machine learning
1000	Vue provides higher customizability and hence is easier to learn than Angular or React. Further, Vue has an overlap with Angular and React with respect to their functionality like the use of components. Hence, the transition to Vue from either of the two is an easy option.	Which is better react VUE or angular
6152	In mathematics, the inequality of arithmetic and geometric means, or more briefly the AM–GM inequality, states that the arithmetic mean of a list of non-negative real numbers is greater than or equal to the geometric mean of the same list; and further, that the two means are equal if and only if every number in the	What are some distinctive proofs of arithmetic mean and geometric mean inequality
7362	is that numerical is of or pertaining to numbers while nonnumerical is not numerical; containing data other than numbers.	What is the difference between numerical and non numerical data
4765	To calculate the centroid from the cluster table just get the position of all points of a single cluster, sum them up and divide by the number of points.	How do you find the centroid of a cluster
4928	Inter-Rater Reliability MethodsCount the number of ratings in agreement. In the above table, that's 3.Count the total number of ratings. For this example, that's 5.Divide the total by the number in agreement to get a fraction: 3/5.Convert to a percentage: 3/5 = 60%.	How is interobserver reliability calculated
3768	Created by the Google Brain team, TensorFlow is an open source library for numerical computation and large-scale machine learning. TensorFlow bundles together a slew of machine learning and deep learning (aka neural networking) models and algorithms and makes them useful by way of a common metaphor.	What is TensorFlow used for
3356	Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).	What is the linear model in machine learning
7642	Recurrent Neural Networks (RNNs) are a form of machine learning algorithm that are ideal for sequential data such as text, time series, financial data, speech, audio, video among others.	Is recurrent neural networks are best suited for text processing
2362	Q-learning is called off-policy because the updated policy is different from the behavior policy, so Q-Learning is off-policy. In other words, it estimates the reward for future actions and appends a value to the new state without actually following any greedy policy.	How is Q learning off policy
75	In signal processing, a nonlinear (or non-linear) filter is a filter whose output is not a linear function of its input.  Like linear filters, nonlinear filters may be shift invariant or not. Non-linear filters have many applications, especially in the removal of certain types of noise that are not additive.	What is non linear filter in image processing
2962	Variance and standard deviation (Square root of variance) is useful in any control system.  But it is used more often than variance because the unit in which it is measured is the same as that of mean, a measure of central tendency. Variance is measured in square of the units whereas standard deviation in just units.	How are variance and standard deviation practically used
454	Mean Absolute Error (MAE) The MAE is a simple way to measure error magnitude. It consists on the average of the absolute differences between the predictions and the observed values. Th measure goes from 0 to infinite, being 0 the best value you can get.	What is the error measure used in reinforcement learning
7788	"""A discrete variable is one that can take on finitely many, or countably infinitely many values"", whereas a continuous random variable is one that is not discrete, i.e. ""can take on uncountably infinitely many values"", such as a spectrum of real numbers."	What is the difference between discrete and continuous random variables
2515	Examples of semi-structured data include JSON and XML are forms of semi-structured data. The reason that this third category exists (between structured and unstructured data) is because semi-structured data is considerably easier to analyse than unstructured data.	Is JSON data is unstructured data or structured data
956	0:008:33Suggested clip · 112 secondsHow to read a log scale. - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you read a log scale
0	"A ""single-layer"" perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight line to separate the points (0,0),(1,1) from the points (0,1),(1,0). Led to invention of multi-layer networks."	Why can t Perceptron learn XOR
3735	Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.	How does batch normalization help
8415	0:5328:17Suggested clip 66 secondsCalculus 2 - Integral Test For Convergence and Divergence of SeriesYouTubeStart of suggested clipEnd of suggested clip	How do you tell if a series converges or diverges integral test
6827	0:1110:28المقطع المقترح · 110 ثانيةLambda Measure of Association for Two Nominal Variables in SPSS YouTubeبداية المقطع المقترَحنهاية المقطع المقترَح	How do you interpret lambda in SPSS
163	Word2Vec slightly customizes the process and calls it negative sampling. In Word2Vec, the words for the negative samples (used for the corrupted pairs) are drawn from a specially designed distribution, which favours less frequent words to be drawn more often.	What is negative sampling in Word2Vec
5218	Common examples of algorithms with coefficients that can be optimized using gradient descent are Linear Regression and Logistic Regression.	Which machine learning algorithms use gradient descent
2767	6.5. 9 Artificial Neural Network, Supervised Learning. A supervised learning is a type of machine learning algorithm that uses a known dataset this is known as training dataset, and it is used to make predictions of other datasets. The dataset includes two types of information: input data and response values.	What is a training set and how is it used to train neural networks
6435	It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.	What does naive Bayes classifier do
5331	Both are data reduction techniques—they allow you to capture the variance in variables in a smaller set.  Despite all these similarities, there is a fundamental difference between them: PCA is a linear combination of variables; Factor Analysis is a measurement model of a latent variable.	What is latent class analysis and how is it different from principal component analysis
8108	2:268:01Suggested clip · 91 secondsIntroduction to Univariate Analysis - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you conduct a univariate analysis
6967	4:2514:33Suggested clip · 73 secondsDiscrete Uniform Distribution: Introduction, Mean and Variance YouTubeStart of suggested clipEnd of suggested clip	How do you find the mean of a discrete uniform distribution
1303	0:004:30Suggested clip · 84 secondsExponential distribution moment generating function - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the moment generating function of an exponential distribution
622	Law of large numbers, in statistics, the theorem that, as the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean. The law of large numbers was first proved by the Swiss mathematician Jakob Bernoulli in 1713.	How was the law of large numbers proven
1387	Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.	How do you explain gradient descent
616	Data for two variables (usually two types of related data). Example: Ice cream sales versus the temperature on that day. The two variables are Ice Cream Sales and Temperature.	What are some examples of bivariate data
5600	Because deeper networks capture the natural “hierarchy” that is present everywhere in nature. See a convnet for example, it captures low level features in first layer, a little better but still low level features in the next layer and at higher layers object parts and simple structures are captured.	Why are deep neural networks better
604	Emotions can also be detected through body postures. Research has shown that body postures are more accurately recognised when an emotion is compared with a different or neutral emotion. For example, a person feeling angry would portray dominance over the other, and their posture would display approach tendencies.	How does body language show emotion
3410	ARIMA models allow both autoregressive (AR) components as well as moving average (MA) components.  The (I) in ARIMA determines the level of differencing to use, which helps make the data stationary. ARIMA models are more flexible than other statistical models such as exponential smoothing or simple linear regression.	Why do we use Arima model
1226	Chi-squared test	What statistical test is used for nominal data
3866	If a population is known to be normally distributed, then it follows that the sample mean must equal the population mean. If the sampled population distribution is skewed, then in most cases the sampling distribution of the mean can be approximated by the normal distribution if the sample size n is at least 30.	When the population has a normal distribution the sampling distribution of is normally distributed
7145	The Wilcoxon rank-sum test is commonly used for the comparison of two groups of nonparametric (interval or not normally distributed) data, such as those which are not measured exactly but rather as falling within certain limits (e.g., how many animals died during each hour of an acute study).	What is the Wilcoxon rank sum test used for
671	A knowledge representation is an encoding of this information or understanding in a particular substrate, such as a set of if-then rules, a semantic network, conditional probability tables, a Venn diagram, a mind map, or the axioms of formal logic.	What is knowledge representation in neural network
3995	The random effects assumption is that the individual-specific effects are uncorrelated with the independent variables. The fixed effect assumption is that the individual-specific effects are correlated with the independent variables.	What is the difference between fixed and random effects
8273	Spectral analysis or Spectrum analysis is analysis in terms of a spectrum of frequencies or related quantities such as energies, eigenvalues, etc. In specific areas it may refer to: Spectroscopy in chemistry and physics, a method of analyzing the properties of matter from their electromagnetic interactions.	What does spectral analysis mean
494	for a time series is one in which there is no trend or seasonal component and in which the observations are simply independent and identically distributed (iid) random variables with zero mean. We refer to such a sequence of random variables X1,X2, as iid noise.	Is IID a time series
70	"= P(A)^P(B) which is just the probability of A times the probability of B. If they are dependent, then P(A and B) = P(A)*P(B|A) which is the probability of A times the probability of ""B happening if A has occurred,"" which is different than the ""Probability of B if A has not occurred."""	What is the formula for dependent events
6184	A t-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related in certain features.  A t-test is used as a hypothesis testing tool, which allows testing of an assumption applicable to a population.	Why would you use a t test
8036	The inversion method relies on the principle that continuous cumulative distribution functions (cdfs) range uniformly over the open interval (0,1). If u is a uniform random number on (0,1), then x = F - 1 ( u ) generates a random number x from any continuous distribution with the specified cdf F .	How do you generate a random number from a uniform distribution
7270	The unit of measurement usually given when talking about statistical significance is the standard deviation, expressed with the lowercase Greek letter sigma (σ). The term refers to the amount of variability in a given set of data: whether the data points are all clustered together, or very spread out.	What is a sigma in statistics
316	Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task.  Transfer learning only works in deep learning if the model features learned from the first task are general.	Is transfer learning deep learning
4474	Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series. There are several autocorrelation coefficients, corresponding to each panel in the lag plot.	What is autocorrelation in time series analysis
2605	linear threshold unit (LTU) A linear threshold unit is a simple artificial neuron whose output is its thresholded total net input. That is, an LTU with threshold T calculates the weighted sum of its inputs, and then outputs 0 if this sum is less than T, and 1 if the sum is greater than T.	What is a linear threshold unit
1381	Labeled data is data that comes with a tag, like a name, a type, or a number. Unlabeled data is data that comes with no tag.  The set of algorithms in which we use a labeled dataset is called supervised learning. The set of algorithms in which we use an unlabeled dataset, is called unsupervised learning.	What is labeled and unlabeled data in machine learning
4738	The basic steps to build a stochastic model are:Create the sample space (Ω) — a list of all possible outcomes,Assign probabilities to sample space elements,Identify the events of interest,Calculate the probabilities for the events of interest.	How do you model a stochastic process
3652	Usually you don't want to find a global optimum. Because that usually requires overfitting the training data. An interesting alternative to gradient descent is the population-based training algorithms such as the evolutionary algorithms (EA) and the particle swarm optimisation (PSO).	What alternatives are there to gradient descent for machine learning
3615	Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.	What are the conditions in which Gradient descent is applied
82	1:085:00Suggested clip · 93 secondsInterpreting Hazard Ratios - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret hazard ratios
7579	The main difference between the two, is that a Perceptron takes that binary response (like a classification result) and computes an error used to update the weights, whereas an Adaline uses a continous response value to update the weights (so before the binarized output is produced).	Artificial Neural Networks what is the difference between perceptron and adaline in recognition + and X images
1256	In the AI-enabled future, humans will be able to converse and interact with each other in the native language of choice, not having to worry about miscommunicating intentions. Machine learning models will be able to understand context, nuance, and colloquialisms that help to fill the gaps of human communication.	How can AI help us in the future
2048	In short, it ensures each subgroup within the population receives proper representation within the sample. As a result, stratified random sampling provides better coverage of the population since the researchers have control over the subgroups to ensure all of them are represented in the sampling.	What's stratified sampling Why is it preferred
1309	To find the relative frequency, divide the frequency by the total number of data values. To find the cumulative relative frequency, add all of the previous relative frequencies to the relative frequency for the current row.	How do you do relative frequency
2343	Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data.	How do you ensemble a model
1240	A Classification tree labels, records, and assigns variables to discrete classes.  A Classification tree is built through a process known as binary recursive partitioning. This is an iterative process of splitting the data into partitions, and then splitting it up further on each of the branches.	What is classification tree in data mining
8100	Covariance: An Overview. Variance and covariance are mathematical terms frequently used in statistics and probability theory. Variance refers to the spread of a data set around its mean value, while a covariance refers to the measure of the directional relationship between two random variables.	What is the meaning of covariance in statistics
510	Definition 1. Suppose that events A and B are defined on the same probability space, and the event B is such that P(B) > 0. The conditional probability of A given that B has occurred is given by P(A|B) = P(A ∩ B)/P(B).	How do you prove conditional probability
352	Initializers define the way to set the initial random weights of Keras layers. The keyword arguments used for passing initializers to layers depends on the layer. Usually, it is simply kernel_initializer and bias_initializer : from tensorflow.keras import layers from tensorflow.keras import initializers layer = layers.	What is kernel initializer in keras
3740	"In March 2014, just two months after DeepMind was acquired, Musk warned that AI is ""potentially more dangerous than nukes,"" suggesting that his investment might have been made because he was concerned about where the technology was headed."	Why is Elon Musk against AI
2774	In the context of CNN, a filter is a set of learnable weights which are learned using the backpropagation algorithm. You can think of each filter as storing a single template/pattern.  Filter is referred to as a set of shared weights on the input.	What is a filter in a CNN
559	Scikit-learn is a Python library used for machine learning.  The framework is built on top of several popular Python packages, namely NumPy, SciPy, and matplotlib. A major benefit of this library is the BSD license it's distributed under.	Is Scikit learning framework
381	Statistical modeling is the process of applying statistical analysis to a dataset. A statistical model is a mathematical representation (or mathematical model) of observed data.  “When you analyze data, you are looking for patterns,” says Mello. “You are using a sample to make an inference about the whole.”	What is statistical data modeling
3239	A chi-square test is used when you want to see if there is a relationship between two categorical variables. In SPSS, the chisq option is used on the statistics subcommand of the crosstabs command to obtain the test statistic and its associated p-value.	How do you test the relationship between two categorical variables
1001	Feature detection is a process by which the nervous system sorts or filters complex natural stimuli in order to extract behaviorally relevant cues that have a high probability of being associated with important objects or organisms in their environment, as opposed to irrelevant background or noise.	How does feature detection work
261	The recommended reference range of serum TNF-α was from nondetectable to 8.1 pg/mL. Among 147 patients with IgAN, 98 patients were with elevated serum TNF-α and 49 patients were without elevated serum TNF-α.	What is normal range of TNF alpha
4617	Gradient boosting is a greedy algorithm and can overfit a training dataset quickly. It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting.	Why does gradient boosting work so well
989	A linear regression equation simply sums the terms. While the model must be linear in the parameters, you can raise an independent variable by an exponent to fit a curve. For instance, you can include a squared or cubed term. Nonlinear regression models are anything that doesn't follow this one form.	What is the difference between a linear and a non linear model
2811	Big Data Meets Machine Learning By feeding big data to a machine-learning algorithm, we might expect to see defined and analyzed results, like hidden patterns and analytics, that can assist in predictive modeling. For some companies, these algorithms might automate processes that were previously human-centered.	How is machine learning used in big data
4046	For a hypothesis test, a researcher collects sample data.  If the statistic falls within a specified range of values, the researcher rejects the null hypothesis . The range of values that leads the researcher to reject the null hypothesis is called the region of rejection.	What is the region of rejection
8182	A little bit of coding skills is enough, but it's better to have knowledge of data structures, algorithms, and OOPs concept. Some of the popular programming languages to learn machine learning in are Python, R, Java, and C++.	Is coding required in machine learning
4338	A facial recognition system uses biometrics to map facial features from a photograph or video. It compares the information with a database of known faces to find a match.  That's because facial recognition has all kinds of commercial applications. It can be used for everything from surveillance to marketing.	How does facial verification work
2554	Exogenous causes are factors that influence the business cycle from outside of the system, e.g. climate (drought and other natural disasters) and the political situation of a country. Endogenous causes are factors that influence the business cycle from inside the system, e.g. total expenditure.	How exogenous causes differ from the endogenous causes
2201	There are several ways to check your Linear Regression model accuracy. Usually, you may use Root mean squared error. You may train several Linear Regression models, adding or removing features to your dataset, and see which one has the lowest RMSE - the best one in your case.	How do you find the accuracy of a linear regression model
7767	Examples of Disjoint Events A football game can't be held at the same time as a rugby game on the same field. Heading East and West at the same time is impossible. Tossing a coin and getting a heads and a tails at the same time is impossible. You can't take the bus and the car to work at the same time.	What is an example of a disjoint event
265	The least squares method provides the overall rationale for the placement of the line of best fit among the data points being studied.  An analyst using the least squares method will generate a line of best fit that explains the potential relationship between independent and dependent variables.	Why least square method is best
6317	Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).  Stemming is also a part of queries and Internet search engines.	What is stemming in NLP
5571	The latent space is simply a representation of compressed data in which similar data points are closer together in space. Latent space is useful for learning data features and for finding simpler representations of data for analysis.	What are latent spaces in representation learning
1185	This learning process is independent.  During the training of ANN under unsupervised learning, the input vectors of similar type are combined to form clusters. When a new input pattern is applied, then the neural network gives an output response indicating the class to which input pattern belongs.	What is unsupervised learning in neural network
7490	The symbol epsilon in mathematics is often used as an “infinitesimal” quantity since you can definite it to be as arbitrarily close to zero as you want, and it is in this generality that the epsilon-neighborhood definition of a limit furnishes us with the properties of a limit that we desire.	What is Epsilon in real analysis
3552	Photo Credit: Pixabay. Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic.	What is LDA topic modeling
38	It's true that the unit step function is bounded. However, a system which has the unit step function as its impulse response is not stable, because the integral (of the absolute value) is infinite. Bounded and stable are not the same thing.	Is unit step function bounded
1069	Word Embedding is really all about improving the ability of networks to learn from text data. By representing that data as lower dimensional vectors.  This technique is used to reduce the dimensionality of text data but these models can also learn some interesting traits about words in a vocabulary.	Why is word embedded
490	Accuracy can be computed by comparing actual test set values and predicted values. Well, you got a classification rate of 96.49%, considered as very good accuracy. For further evaluation, you can also check precision and recall of model.	How does SVM calculate accuracy
7209	A turing machine is a theoretical machine that computes a function. It is theoretical because it has unlimited tape and time. A human brain is a limited object in space and time, hence we can ignore the unlimited tape requirement.	Is the human brain a Turing machine
129	Target Concept Term used in the machine learning literature to denote the Bayes decision rule, or the regression function, depending on the context. The target concept is a member of the concept space. Synonyms: Bayes Decision Rule in classification, Regression Function in regression.	What is the target concept in machine learning
3694	The simplest way to compare two distributions is via the Z-test. The error in the mean is calculated by dividing the dispersion by the square root of the number of data points.  This is one way you can use to determine, in fact, the likelihood that your sample means it a good indicator of the true population mean.	How do we know two distributions are from different populations
2501	Multivariate Regression is a method used to measure the degree at which more than one independent variable (predictors) and more than one dependent variable (responses), are linearly related.  A mathematical model, based on multivariate regression analysis will address this and other more complicated questions.	What is a multivariate regression analysis
499	Summary: Goodness of Fit: used to compare a single sample proportion against a publicized model. Homogeneity: used to examine whether things have changed or stayed the same or whether the proportions that exist between two populations are the same, or when comparing data from MULTIPLE samples.	What is the difference between chi square goodness of fit and homogeneity
400	Events A and B are independent if the equation P(A∩B) = P(A) · P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together.	How do you calculate the probability of independent events
6984	Computing accuracy for clustering can be done by reordering the rows (or columns) of the confusion matrix so that the sum of the diagonal values is maximal. The linear assignment problem can be solved in O(n3) instead of O(n!). Coclust library provides an implementation of the accuracy for clustering results.	How do you find the accuracy of a clustering algorithm
3208	The regions of the brain comprising the “reward system” use the neurotransmitter dopamine to communicate.  Neurons that release dopamine are activated when we expect to receive a reward. Dopamine also enhances reward-related memories.	What triggers the reward system in the brain
369	A histogram is drawn like a bar chart, but often has bars of unequal width. It is the area of the bar that tells us the frequency in a histogram, not its height. Instead of plotting frequency on the y-axis, we plot the frequency density. To calculate this, you divide the frequency of a group by the width of it.	What does density mean on a histogram
2361	In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  In supervised feature learning, features are learned using labeled input data.	What is feature learning in machine learning
4574	First step is to split predicted probability into 10 parts (decile) and then compute the cumulative % of events and non-events in each decile and check the decile where difference is maximum (as shown in the image below.) In the image below, KS is 57.8% and it is at third decile. KS curve is shown below.	How is KS value calculated
866	Neural network activation functions are a crucial component of deep learning. Activation functions determine the output of a deep learning model, its accuracy, and also the computational efficiency of training a model—which can make or break a large scale neural network.	What are activation functions in deep learning
7149	So instead of updating the weight by taking in the output of a neuron in the previous layer, multiplying it by the learning rate and delta value, then subtracting that final value from the current weight, it will multiply the delta value and learning rate by 1, then subtract that final value from the bias weight in	Which is bias updation formula of Delta network
7591	How to use them while designing a CNN: Conv2D filters are used only in the initial layers of a Convolutional Neural Network. They are put there to extract the initial high level features from an image.	What filters are used in CNN
202	The value function represent how good is a state for an agent to be in. It is equal to expected total reward for an agent starting from state s . The value function depends on the policy by which the agent picks actions to perform.	What is RL value function
3041	Bayesian decision making is the process in which a decision is made based on the probability of a successful outcome, where this probability is informed by both prior information and new evidence that the decision maker obtains.	What is the Bayesian approach to decision making
807	There are three big-picture methods to understand if a continuous and categorical are significantly correlated — point biserial correlation, logistic regression, and Kruskal Wallis H Test. The point biserial correlation coefficient is a special case of Pearson's correlation coefficient.	How can I measure the correlation between continuous and categorical variables
213	A one-tailed test is a statistical test in which the critical area of a distribution is one-sided so that it is either greater than or less than a certain value, but not both. If the sample being tested falls into the one-sided critical area, the alternative hypothesis will be accepted instead of the null hypothesis.	What is a one sided t test
3578	Markov chains are used in a broad variety of academic fields, ranging from biology to economics. When predicting the value of an asset, Markov chains can be used to model the randomness. The price is set by a random factor which can be determined by a Markov chain.	What are Markov chains used for
4945	Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable).	When would you use multiple linear regression
1284	In probability theory and statistics, a categorical distribution (also called a generalized Bernoulli distribution, multinoulli distribution) is a discrete probability distribution that describes the possible results of a random variable that can take on one of K possible categories, with the probability of each	How do you describe a categorical distribution
5362	The difference between the two norms is that the standard deviation is calculating the square of the difference whereas the mean absolute deviation is only looking at the absolute difference. Hence large outliers will create a higher dispersion when using the standard deviation instead of the other method.	What is the logical difference between mean deviation and standard deviation
766	Noun. optimizer (plural optimizers) A person in a large business whose task is to maximize profits and make the business more efficient. (computing) A program that uses linear programming to optimize a process. (computing) A compiler or assembler that produces optimized code.	What does Optimizer mean
6687	In regression analysis, those factors are called variables. You have your dependent variable — the main factor that you're trying to understand or predict. In Redman's example above, the dependent variable is monthly sales.	What is the dependent variable in regression analysis
8591	Bandish Bandits tells the story of the love between Tamanna, a popstar, and Radhe, a classical music prodigy in Jodhpur. Radhe's grandfather, Pandit Radhemohan Rathod, is a celebrated singer, a strict disciplinarian, and believes the purity of music should not be tainted by filthy lucre or light music.	What is the story of Bandish bandits
540	If k is given, the K-means algorithm can be executed in the following steps: Partition of objects into k non-empty subsets. Identifying the cluster centroids (mean point) of the current partition.  Compute the distances from each point and allot points to the cluster where the distance from the centroid is minimum.	What is K means algorithm with example
2564	In order to label some more of the data my idea is to do the following:Build a classifier on the whole data set separating the class 'A from the unlabelled data.Run the classifier on the unlabelled data.Add the unlabelled items classified as being in class 'A' to class 'A'.Repeat.	How do you label unlabeled data
335	As a human can easily recognize the image by seeing its color, shape, texture or some other feature, the same way machine first extracts the features of the object and then it applies the classification algorithm to label a particular class of the recognized object according to the extracted features.	What properties make an image easy to recognize
4709	Detection theory or signal detection theory is a means to measure the ability to differentiate between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the	What is noise in signal detection theory
1469	"""Controlling"" for a variable means adding it to the model so its effect on your outcome variable(s) can be estimated and statistically isolated from the effect of the independent variable you're really interested in."	What does it mean when you control for a variable
2903	Geometrically, an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction in which it is stretched by the transformation and the eigenvalue is the factor by which it is stretched.	What is meant by eigenvalues and eigenvectors
25	A regression model will have unit changes between the x and y variables, where a single unit change in x will coincide with a constant change in y. Taking the log of one or both variables will effectively change the case from a unit change to a percent change.  A logarithm is the base of a positive number.	Why do we use logarithms in regression
4937	The Pearson product-moment correlation coefficient, also known as r, R, or Pearson's r, is a measure of the strength and direction of the linear relationship between two variables that is defined as the covariance of the variables divided by the product of their standard deviations.	Is correlation coefficient the same as R
466	It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.  The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better.	Why is rectified linear unit a good activation function
2396	A tensor is a vector or matrix of n-dimensions that represents all types of data. All values in a tensor hold identical data type with a known (or partially known) shape. The shape of the data is the dimensionality of the matrix or array. A tensor can be originated from the input data or the result of a computation.	What is the shape of a tensor
762	FFTs are great at analyzing vibration when there are a finite number of dominant frequency components; but power spectral densities (PSD) are used to characterize random vibration signals.	What is the difference between FFT and PSD
615	In a census, data about all individual units (e.g. people or households) are collected in the population. In a survey, data are only collected for a sub-part of the population; this part is called a sample. These data are then used to estimate the characteristics of the whole population.	What is the difference between census survey and sample survey
5017	RELU activation solves this by having a gradient slope of 1, so during backpropagation, there isn't gradients passed back that are progressively getting smaller and smaller. but instead they are staying the same, which is how RELU solves the vanishing gradient problem.	How does ReLU help in vanishing gradient
5421	Data Collection & Analysis Tools Related TopicsBox & Whisker Plot.Check Sheet.Control Chart.Design of Experiments (DOE)Histogram.Scatter Diagram.Stratification.Survey.	What are the tools for analysis
554	"A Markov model is a system that produces a Markov chain, and a hidden Markov model is one where the rules for producing the chain are unknown or ""hidden."" The rules include two probabilities: (i) that there will be a certain observation and (ii) that there will be a certain state transition, given the state of the"	What is hidden Markov model in bioinformatics
5408	Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable.	What does multiple logistic regression mean
140	Moments are are very useful in statistics because they tell you much about your data. There are four commonly used moments in statistics: the mean, variance, skewness, and kurtosis. The mean gives you a measure of center of the data.	What are the uses of moments
645	Kalman filters are used to optimally estimate the variables of interests when they can't be measured directly, but an indirect measurement is available. They are also used to find the best estimate of states by combining measurements from various sensors in the presence of noise.	What is a Kalman filter used for
237	"The relative efficiency of two procedures is the ratio of their efficiencies, although often this concept is used where the comparison is made between a given procedure and a notional ""best possible"" procedure."	What is meant by relative efficiency
1188	The emergence of artificial intelligence (AI) and its progressively wider impact on many sectors requires an assessment of its effect on the achievement of the Sustainable Development Goals.  Failure to do so could result in gaps in transparency, safety, and ethical standards.	Can artificial intelligence contribute in sustainable development
616	In exploratory studies, p-values enable the recognition of any statistically noteworthy findings. Confidence intervals provide information about a range in which the true value lies with a certain degree of probability, as well as about the direction and strength of the demonstrated effect.	What is the difference between P value and confidence interval
616	Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive.	What is the difference between a Type I and Type II error
1327	Facial recognition is a category of biometric software that maps an individual's facial features mathematically and stores the data as a faceprint. The software uses deep learning algorithms to compare a live capture or digital image to the stored faceprint in order to verify an individual's identity.	How is facial recognition data stored
5629	Action words, or action verbs, simply express an action. The action is something the subject of the sentence or clause is doing and includes sleeping, sitting, and napping-so even though there is no movement, there is still an action.	What are the action words
6888	Overfitting in Machine Learning Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model.	What causes Overfitting in machine learning
4064	How to Choose a Machine Learning Model – Some GuidelinesCollect data.Check for anomalies, missing data and clean the data.Perform statistical analysis and initial visualization.Build models.Check the accuracy.Present the results.	How do I choose a machine learning model
6187	The difference between the hypergeometric and the binomial distributions.  For the binomial distribution, the probability is the same for every trial. For the hypergeometric distribution, each trial changes the probability for each subsequent trial because there is no replacement.	What is the difference between binomial and hypergeometric distribution
5151	Observer bias can be reduced or eliminated by: Screening observers for potential biases. Having clear rules and procedures in place for the experiment. Making sure behaviors are clearly defined. Setting a time frame for: collecting data, for the duration of the experiment, and for experimental parts.	What is experimenter bias How can it be reduced or eliminated
8561	In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  Feature learning can be either supervised or unsupervised.	What does feature learning mean in Machine Learning
7056	We can construct a single HMM for all words. Hidden states = all characters in the alphabet. Transition probabilities and initial probabilities are calculated from language model. Observations and observation probabilities are as before.	What are hidden states in hmm
6777	Ridge regression does not really select variables in the many predictors situation.  Both ridge regression and the LASSO can outperform OLS regression in some predictive situations – exploiting the tradeoff between variance and bias in the mean square error.	Can ridge regression be used for variable selection
949	Intuitively, this selects the parameter values that make the observed data most probable. The specific value that maximizes the likelihood function is called the maximum likelihood estimate. Further, if the function so defined is measurable, then it is called the maximum likelihood estimator.	What is are the outcome of optimizing the likelihood function
11	Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space.	What is cluster algorithm
2286	Linear regression is one of the most common techniques of regression analysis. Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables.	What is the difference between linear regression and multiple regression
613	noun. (in an experiment or clinical trial) a group of subjects who are exposed to the variable under study: a lower infection rate in the experimental group that received the vaccine.	What is experimental subject
1384	A false positive is an outcome where the model incorrectly predicts the positive class. And a false negative is an outcome where the model incorrectly predicts the negative class. In the following sections, we'll look at how to evaluate classification models using metrics derived from these four outcomes.	What is false negative in a classification table
4563	We see right away that if two matrices have different eigenvalues then they are not similar. Also, if two matrices have the same distinct eigen values then they are similar. Suppose A and B have the same distinct eigenvalues. Then they are both diagonalizable with the same diagonal 2 Page 3 matrix A.	How do you know if two matrices are similar
8092	This chapter presents several ways to summarize quantitative data by a typical value (a measure of location, such as the mean, median, or mode) and a measure of how well the typical value represents the list (a measure of spread, such as the range, inter-quartile range, or standard deviation).	Is range a measure of location
4152	Abstract: The k-means algorithm is known to have a time complexity of O(n 2 ), where n is the input data size. This quadratic complexity debars the algorithm from being effectively used in large applications.	What is the time complexity of K means clustering
1132	From Wikipedia, the free encyclopedia. Quantization is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set (such as the integers).	What is the meaning of quantization
497	Linear models describe a continuous response variable as a function of one or more predictor variables. They can help you understand and predict the behavior of complex systems or analyze experimental, financial, and biological data.	What do you mean by linear model
1241	The k-means clustering algorithm attempts to split a given anonymous data set (a set containing no information as to class identity) into a fixed number (k) of clusters.  The resulting classifier is used to classify (using k = 1) the data and thereby produce an initial randomized set of clusters.	How K means algorithm works
1168	There are two main types of decision trees that are based on the target variable, i.e., categorical variable decision trees and continuous variable decision trees.Categorical variable decision tree.  Continuous variable decision tree.  Assessing prospective growth opportunities.More items	What are the types of decision tree
8091	In the large-sample case, a 95% confidence interval estimate for the population mean is given by x̄ ± 1.96σ/ √n. When the population standard deviation, σ, is unknown, the sample standard deviation is used to estimate σ in the confidence interval formula.	How do you calculate population mean from sample data
3142	Why the Lognormal Distribution is used to Model Stock Prices Since the lognormal distribution is bound by zero on the lower side, it is therefore perfect for modeling asset prices which cannot take negative values. The normal distribution cannot be used for the same purpose because it has a negative side.	Why do we use log normal distribution
2434	Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results.	Why is Multicollinearity a problem in linear regression select the correct option
281	0.05	At what P value do you reject your hypothesis
3022	In statistics, importance sampling is a general technique for estimating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest. It is related to umbrella sampling in computational physics.	What is the importance of sampling in statistics
2019	In order to calculate the sample size needed for your survey or experiment, you will need to follow these steps: Determine the total population size.Complete the calculation.Determine the total population size.  Decide on a margin of error.  Choose a confidence level.  Pick a standard of deviation.  Complete the calculation.	How do you calculate sample size needed
814	6:3017:57Suggested clip · 93 secondsSAS - Logistic Regression - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you run a logistic regression in SAS
6774	R-squared should accurately reflect the percentage of the dependent variable variation that the linear model explains. Your R2 should not be any higher or lower than this value.  However, if you analyze a physical process and have very good measurements, you might expect R-squared values over 90%.	What is an acceptable R squared value
399	Prior probability, in Bayesian statistical inference, is the probability of an event before new data is collected. This is the best rational assessment of the probability of an outcome based on the current knowledge before an experiment is performed.	What is prior probability in statistics
7809	Big data analytics and data mining are not the same. Both of them involve the use of large data sets, handling the collection of the data or reporting of the data which is mostly used by businesses. However, both big data analytics and data mining are both used for two different operations.	Is data mining a part of big data
3025	The SVM in particular defines the criterion to be looking for a decision surface that is maximally far away from any data point. This distance from the decision surface to the closest data point determines the margin of the classifier.  Figure 15.1 shows the margin and support vectors for a sample problem.	What is a margin in SVM
4418	Theory of mind refers to the ability to attribute mental states such as beliefs, desires, goals, and intentions to others, and to understand that these states are different from one's own.  A theory of mind makes it possible to understand emotions, infer intentions, and predict behavior.	What is theory of mind AI
3292	The F Distribution The distribution of all possible values of the f statistic is called an F distribution, with v1 = n1 - 1 and v2 = n2 - 1 degrees of freedom. The curve of the F distribution depends on the degrees of freedom, v1 and v2.	What is an F distribution in statistics
4782	An example of a false positive is when a particular test designed to detect melanoma, a type of skin cancer , tests positive for the disease, even though the person does not have cancer.	What is an assessment example of a false positive
7187	"Random forest is a supervised learning algorithm. The ""forest"" it builds, is an ensemble of decision trees, usually trained with the “bagging” method. The general idea of the bagging method is that a combination of learning models increases the overall result."	What is random forest with example
980	Some of the more common ways to normalize data include:Transforming data using a z-score or t-score.  Rescaling data to have values between 0 and 1.  Standardizing residuals: Ratios used in regression analysis can force residuals into the shape of a normal distribution.Normalizing Moments using the formula μ/σ.More items	How do you normalize a distribution
5358	The most frequently used are the Naive Bayes (NB) family of algorithms, Support Vector Machines (SVM), and deep learning algorithms.	What are some of the most popular text analytics techniques algorithms
374	The expected value of the sum of several random variables is equal to the sum of their expectations, e.g., E[X+Y] = E[X]+ E[Y] . On the other hand, the expected value of the product of two random variables is not necessarily the product of the expected values.	How do you find the expected value of the sum
2956	In ideal conditions, facial recognition systems can have near-perfect accuracy. Verification algorithms used to match subjects to clear reference images (like a passport photo or mugshot) can achieve accuracy scores as high as 99.97% on standard assessments like NIST's Facial Recognition Vendor Test (FRVT).	What is the accuracy of facial recognition
7439	Intra-rater reliability refers to the consistency a single scorer has with himself when looking at the same data on different occasions. Finally, inter-rater reliability is how often different scorers agree with each other on the same cases.	What is the difference between inter and intra rater reliability
236	How to Find a Sample Size Given a Confidence Interval and Width (unknown population standard deviation)za/2: Divide the confidence interval by two, and look that area up in the z-table: .95 / 2 = 0.475.  E (margin of error): Divide the given width by 2. 6% / 2.  : use the given percentage. 41% = 0.41.  : subtract. from 1.	How do you find the sample size when given the confidence interval and standard deviation
4899	K-means clustering algorithm computes the centroids and iterates until we it finds optimal centroid.  In this algorithm, the data points are assigned to a cluster in such a manner that the sum of the squared distance between the data points and centroid would be minimum.	What is K means clustering algorithm explain with an example
2740	(There are two red fours in a deck of 52, the 4 of hearts and the 4 of diamonds). Conditional probability: p(A|B) is the probability of event A occurring, given that event B occurs.  Joint probability is the probability of two events occurring simultaneously. The probability of event A and event B occurring together.	Can you point out the difference between joint probability and conditional probability
6989	The standard error is a statistical term that measures the accuracy with which a sample distribution represents a population by using standard deviation. In statistics, a sample mean deviates from the actual mean of a population—this deviation is the standard error of the mean.	What does standard error mean
1984	A Kalman Filter is an algorithm that can predict future positions based on current position. It can also estimate current position better than what the sensor is telling us. It will be used to have better association.	How can a Kalman filter be used in computer vision
2839	A statistical hypothesis is a formal claim about a state of nature structured within the framework of a statistical model. For example, one could claim that the median time to failure from (acce]erated) electromigration of the chip population described in Section 6.1.	What is hypothesis in statistics with example
4962	Feature Selection. Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.	How feature selection is different from feature extraction
279	So, the prediction of stock Prices using machine learning is 100% correct and not 99%. This is theoritically true, and one can prove this mathematically. BUT THE MACHINE LEARNING TECHNIQUES FOR PREDICTION, DOES NOT ABLE TO PREDECT THE PSYCHOLOGICAL FACTORS OF HUMEN , ON THE PRICES OF THE STOCKS and others.	Can machine learning predict stock market
1182	Currently AI is Used is Following Things/Fields:Virtual Assistant or Chatbots.Agriculture and Farming.Autonomous Flying.Retail, Shopping and Fashion.Security and Surveillance.Sports Analytics and Activities.Manufacturing and Production.Live Stock and Inventory Management.More items•	Where are AI used
79	Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.	What is the difference between feature selection and feature extraction
7029	Facebook uses a powerful AI technology to identify people based on their interests, demographics and online activity.	Does Facebook use artificial intelligence
6898	It depends. If the message you want to carry is about the spread and variability of the data, then standard deviation is the metric to use. If you are interested in the precision of the means or in comparing and testing differences between means then standard error is your metric.	Can you use standard error instead of standard deviation
1688	"A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for ""unsharp masking"" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."	What is the use of Gaussian filter in image processing
858	A t-test is used to compare the mean of two given samples. Like a z-test, a t-test also assumes a normal distribution of the sample. A t-test is used when the population parameters (mean and standard deviation) are not known.	Why do we use t test and z test
236	1950s	When did artificial intelligence start
6558	Agents can be grouped into four classes based on their degree of perceived intelligence and capability :Simple Reflex Agents.Model-Based Reflex Agents.Goal-Based Agents.Utility-Based Agents.Learning Agent.	What are the types of agents in artificial intelligence
487	Misleading graphs are sometimes deliberately misleading and sometimes it's just a case of people not understanding the data behind the graph they create. The “classic” types of misleading graphs include cases where: The Vertical scale is too big or too small, or skips numbers, or doesn't start at zero.	How can data be misleading
2796	The goal of a company should be to achieve the target performance with minimal variation. That will minimize the customer dissatisfaction. A real life example of the Taguchi Loss Function would be the quality of food compared to expiration dates.  That is when the orange will taste the best (customer satisfaction).	What is the Taguchis loss function What is an example
5565	For example, a two-way ANOVA allows a company to compare worker productivity based on two independent variables, such as salary and skill set. It is utilized to observe the interaction between the two factors and tests the effect of two factors at the same time.	What is analysis of variance example
2164	Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices.	Is matrix factorization collaborative filtering
5475	Robust statistics are resistant to outliers. In other words, if your data set contains very high or very low values, then some statistics will be good estimators for population parameters, and some statistics will be poor estimators.	What does robustness mean in statistics
2490	Though SVM is a linear classifier which learns an (n – 1)-dimensional classifier for classification of data into two classes. But SVM it can be used for classifying a non-linear dataset.	Which algorithm is used for the classification of both linear and non linear data
2527	A correlation matrix is a table showing correlation coefficients between sets of variables. Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj).  The diagonal of the table is always a set of ones, because the correlation between a variable and itself is always 1.	How correlation matrix is calculated
384	In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.	What is Perceptron in machine learning
3420	XFL teams will have two timeouts per half, one fewer than in the NFL. Halftime is 10 minutes, two minutes less than the NFL. Another attempt to shorten the game is not allowing coaches to challenge an official's ruling. All plays are subject to review by the replay official.	What rules are different in the XFL
8269	"In information theory, the entropy of a random variable is the average level of ""information"", ""surprise"", or ""uncertainty"" inherent in the variable's possible outcomes. The concept of information entropy was introduced by Claude Shannon in his 1948 paper ""A Mathematical Theory of Communication""."	Is information an entropy
5947	"In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input."	What is activation in neural network
6595	Population variance (σ2) tells us how data points in a specific population are spread out. It is the average of the distances from each data point in the population to the mean, squared.	What is population variance
1037	The crucial difference between FIR and IIR filter is that the FIR filter provides an impulse response of finite period. As against IIR is a type of filter that generates impulse response of infinite duration for a dynamic system.	What is difference between FIR and IIR filters
112	A value of zero indicates that there is no relationship between the two variables. Correlation among variables does not (necessarily) imply causation.  If the correlation coefficient of two variables is zero, it signifies that there is no linear relationship between the variables.	What does it mean if the correlation coefficient r 0
872	Anomaly detection (or outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.	What is outlier detection in machine learning
6483	Analysis of variance (ANOVA) is an analysis tool used in statistics that splits an observed aggregate variability found inside a data set into two parts: systematic factors and random factors.  1﻿﻿2﻿ ANOVA is also called the Fisher analysis of variance, and it is the extension of the t- and z-tests.	What is the concept of analysis of variance ANOVA in statistics
2485	R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.	What is the significance of R Squared in Regression
5514	Mini-batch training is a combination of batch and stochastic training. Instead of using all training data items to compute gradients (as in batch training) or using a single training item to compute gradients (as in stochastic training), mini-batch training uses a user-specified number of training items.	What is mini batch in neural network
6264	Poisson Formula. Suppose we conduct a Poisson experiment, in which the average number of successes within a given region is μ. Then, the Poisson probability is: P(x; μ) = (e-μ) (μx) / x! where x is the actual number of successes that result from the experiment, and e is approximately equal to 2.71828.	How do you find the probability of a Poisson distribution
366	The simplest solution is to use other activation functions, such as ReLU, which doesn't cause a small derivative. Residual networks are another solution, as they provide residual connections straight to earlier layers.	How do you fix a vanishing gradient problem
7216	In the development of the probability function for a discrete random variable, two conditions must be satisfied: (1) f(x) must be nonnegative for each value of the random variable, and (2) the sum of the probabilities for each value of the random variable must equal one.	What are the conditions of a probability distribution
512	Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting.	What is bagging in machine learning
4755	GAN Training Step 1 — Select a number of real images from the training set. Step 2 — Generate a number of fake images. This is done by sampling random noise vectors and creating images from them using the generator. Step 3 — Train the discriminator for one or more epochs using both fake and real images.	How do you create a generative adversarial network
481	Markov chains and random walks are examples of random processes i.e. an indexed collection of random variables.  Markov chains and random walks are examples of random processes i.e. an indexed collection of random variables. A random walk is a specific kind of random process made up of a sum of iid random variables.	Whats the difference between a Markov chain and a random walk
6188	Normalization: Similarly, the goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values.  So we normalize the data to bring all the variables to the same range.	Why do we normalize data
7992	The odds ratio is the measure of association for a case-control study. It tells us how much higher the odds of exposure is among cases of a disease compared with controls. The odds ratio compares the odds of exposure to the factor of interest among cases to the odds of exposure to the factor among controls.	What is odds ratio in case control study
184	The law of averages is the commonly held belief that a particular outcome or event will over certain periods of time occur at a frequency that is similar to its probability. Depending on context or application it can be considered a valid common-sense observation or a misunderstanding of probability.	Is there such a thing as the law of averages
8376	Word2Vec takes texts as training data for a neural network. The resulting embedding captures whether words appear in similar contexts. GloVe focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together.	What is the difference between GloVe and word2vec
6055	Active Learning StrategiesGroup Activities. Case-based learning. Case-based learning requires students to apply their knowledge to reach a conclusion about an open-ended, real-world situation.  Individual Activities. Application cards.  Partner Activities. Role playing.  Visual Organizing Activities. Categorizing grids.	What are the active learning strategies
725	KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.  KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point.	Why KNN algorithm is used
2025	Three of the more widely used experimental designs are the completely randomized design, the randomized block design, and the factorial design. In a completely randomized experimental design, the treatments are randomly assigned to the experimental units.	What are the different types of statistical model for experimental design
7554	Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection. It can also be thought of as a form of text mining – a way to obtain recurring patterns of words in textual material.	How does a topic model work
6740	We write the likelihood function as L(\theta;x)=\prod^n_{i=1}f(X_i;\theta) or sometimes just L(θ). Algebraically, the likelihood L(θ ; x) is just the same as the distribution f(x ; θ), but its meaning is quite different because it is regarded as a function of θ rather than a function of x.	How do you write a likelihood function
2732	Split learning is a new technique developed at the MIT Media Lab's Camera Culture group that allows for participating entities to train machine learning models without sharing any raw data.	What is split learning
8519	Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections.	What is a federated learning model
5491	"The outcome variable is also called the response or dependent variable, and the risk factors and confounders are called the predictors, or explanatory or independent variables. In regression analysis, the dependent variable is denoted ""Y"" and the independent variables are denoted by ""X""."	What is another term for the independent variable in a regression analysis
3618	distribution free test	What does non parametric test mean
2737	The metric system uses units such as meter, liter, and gram to measure length, liquid volume, and mass, just as the U.S. customary system uses feet, quarts, and ounces to measure these.	How is the metric system different than the standard United States system
993	The nominator is the joint probability and the denominator is the probability of the given outcome.  This is the conditional probability: P(A∣B)=P(A∩B)P(B) This is the Bayes' rule: P(A∣B)=P(B|A)∗P(A)P(B).	What is the difference between Bayes rule and conditional probability
1092	We analyze the expected run time because it represents the more typical time cost.	Why do we analyze the expected running time of a randomized algorithm and not its worst case running time
5998	Stanley F. Schmidt is generally credited with developing the first implementation of a Kalman filter. He realized that the filter could be divided into two distinct parts, with one part for time periods between sensor outputs and another part for incorporating measurements.	Who invented the extended Kalman filter
5822	Real-time processing is the process in which a system can input rapidly changing data and then provide output instantaneously so that the change over time can be seen very quickly. Real-time data processing is a method that is used when data input requests need to be dealt with quickly.	What is real time processing in information technology
1367	The first thing you need to do is learn a programming language. Though there are a lot of languages that you can start with, Python is what many prefer to start with because its libraries are better suited to Machine Learning. Here are some good resources for Python: CodeAcademy.	Where do I start with artificial intelligence
5622	How to Annotate an Image in WordIn the “Illustrations” section, click “Pictures”.  The cursor changes to a big “+” symbol.  Right-click on the callout and select “Fill” from the popup box above the popup menu.  Once you've moved the callout, you may need to reposition the callout arrow to point where you want.	How do you annotate a picture
5454	Now the centripetal acceleration is given by the second expression in. ac=v2r;ac=rω2 a c = v 2 r ; a c = r ω 2 as ac = rω2.	What is centripetal acceleration given the expression for it
4443	Parametric tests involve specific probability distributions (e.g., the normal distribution) and the tests involve estimation of the key parameters of that distribution (e.g., the mean or difference in means) from the sample data.	Is parametric data normally distributed
1104	Covariate shift refers to the change in the distribution of the input variables present in the training and the test data. It is the most common type of shift and it is now gaining more attention as nearly every real-world dataset suffers from this problem.	What is covariate shift
5816	Advertisements. Multi-Layer perceptron defines the most complicated architecture of artificial neural networks. It is substantially formed from multiple layers of perceptron.	What is TensorFlow multilayer perceptron learning
1631	Explanation: Entropy (S) by the modern definition is the amount of energy dispersal in a system. Therefore, the system entropy will increase when the amount of motion within the system increases. For example, the entropy increases when ice (solid) melts to give water (liquid).	What does it mean when it says increase or decrease in entropy
773	Because a researcher rarely has direct access to the entire population of interest in social science research, a researcher must rely upon a sampling frame to represent all of the elements of the population of interest. Generally, sampling frames can be divided into two types, list and nonlist.	What is the importance of a sample frame
1615	A model represents what was learned by a machine learning algorithm. The model is the “thing” that is saved after running a machine learning algorithm on training data and represents the rules, numbers, and any other algorithm-specific data structures required to make predictions.	What is a model in deep learning
5818	In the visual system, visual receptive fields are volumes in visual space.  The receptive field is often identified as the region of the retina where the action of light alters the firing of the neuron.	What is a receptive field in the visual system
8543	The linear, polynomial and RBF or Gaussian kernel are simply different in case of making the hyperplane decision boundary between the classes.  Usually linear and polynomial kernels are less time consuming and provides less accuracy than the rbf or Gaussian kernels.	What is the difference between a Gaussian kernel a polynomial kernel a linear kernel and an RBF based kernel
7908	A stratified random sampling involves dividing the entire population into homogeneous groups called strata (plural for stratum). Random samples are then selected from each stratum.  A random sample from each stratum is taken in a number proportional to the stratum's size when compared to the population.	What is strata in stratified sampling
4379	Two sets A and B are called disjoint if A and B have no elements in common. Another equivalent definition of disjoint sets would be that the intersection of the two sets actually equals the empty set.	What does it mean for two sets to be disjoint
7324	Poisson regression – Poisson regression is often used for modeling count data. Poisson regression has a number of extensions useful for count models. Negative binomial regression – Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean.	Why do we use Poisson regression
4425	Hadoop Examples: 5 Real-World Use CasesFinancial services companies use analytics to assess risk, build investment models, and create trading algorithms; Hadoop has been used to help build and run those applications.Retailers use it to help analyze structured and unstructured data to better understand and serve their customers.More items•	What are the real time applications of Hadoop
8588	For an upper-tailed test, the p-value is equal to one minus this probability; p-value = 1 - cdf(ts). For a two-sided test, the p-value is equal to two times the p-value for the lower-tailed p-value if the value of the test statistic from your sample is negative.	What is the formula for P value
4328	The model works by first splitting the input image into a grid of cells, where each cell is responsible for predicting a bounding box if the center of a bounding box falls within it. Each grid cell predicts a bounding box involving the x, y coordinate and the width and height and the confidence.	How does a bounding box work
1408	Ratio scales are like interval scales except they have true zero points. A good example is the Kelvin scale of temperature. This scale has an absolute zero.	Is Kelvin an interval or ratio
6378	Descriptive analytics is a statistical method that is used to search and summarize historical data in order to identify patterns or meaning.	What are descriptive analytics
199	"For this, you aim to maximize the Youden's index, which is Maximum=Sensitivity + Specificity - 1. So you choose those value of the ROC-curve as a cut-off, where the term ""Sensitivity + Specificity - 1"" (parameters taken from the output in the same line as the observed value, see attachments) is maximal."	How cut off value is calculated from ROC curve
4	Your classifier would have learned an equal an opposite rule, with the same performance and same AUC / ROC curve.	What will happen to AUC if I switch the positive and negative classes in the test data
8596	When you reject the null hypothesis with a t-test, you are saying that the means are statistically different. The difference is meaningful. Chi Square:  When you reject the null hypothesis with a Chi-Square, you are saying that there is a relationship between the two variables.	What is the difference between at test and a chi square
1232	Interpreting the Range The range is interpreted as the overall dispersion of values in a dataset or, more literally, as the difference between the largest and the smallest value in a dataset. The range is measured in the same units as the variable of reference and, thus, has a direct interpretation as such.	How do you interpret the range
7062	LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate's activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.	How does Lstm solve vanishing gradient
7827	A Logit function, also known as the log-odds function, is a function that represents probability values from 0 to 1, and negative infinity to infinity. The function is an inverse to the sigmoid function that limits values between 0 and 1 across the Y-axis, rather than the X-axis.	What are Logits in machine learning
3039	ANCOVA and multiple linear regression are similar, but regression is more appropriate when the emphasis is on the dependent outcome variable, while ANCOVA is more appropriate when the emphasis is on comparing the groups from one of the independent variables.	Is Ancova the same as multiple regression
819	Jakob Bernoulli	Who came up with the law of averages
1006	Correspondence analysis reveals the relative relationships between and within two groups of variables, based on data given in a contingency table. For brand perceptions, these two groups are brands and the attributes that apply to these brands.	How does correspondence analysis work
5718	A relative frequency distribution shows the proportion of the total number of observations associated with each value or class of values and is related to a probability distribution, which is extensively used in statistics.	How do you describe the relative frequency distribution
7336	Histograms are generally used to show the results of a continuous data set such as height, weight, time, etc. A bar graph has spaces between the bars, while a histogram does not. A histogram often shows the frequency that an event occurs within the defined range. It shows you how many times that event happens.	How do you read histograms
3078	Structured data is clearly defined and searchable types of data, while unstructured data is usually stored in its native format. Structured data is quantitative, while unstructured data is qualitative. Structured data is often stored in data warehouses, while unstructured data is stored in data lakes.	What is structured data vs unstructured data
1109	Machine learning is perhaps the principal technology behind two emerging domains: data science and artificial intelligence. The rise of machine learning is coming about through the availability of data and computation, but machine learning methdologies are fundamentally dependent on models.	What are the domains of machine learning
409	Gradient Descent with Momentum considers the past gradients to smooth out the update.  It computes an exponentially weighted average of your gradients, and then use that gradient to update your weights instead.	What is gradient descent with momentum
5729	Neural nets are a means of doing machine learning, in which a computer learns to perform some task by analyzing training examples.  Most of today's neural nets are organized into layers of nodes, and they're “feed-forward,” meaning that data moves through them in only one direction.	How does a neural net work
7229	"Discretion traces back to the Latin verb discernere, ""to separate, to discern,"" from the prefix dis-, ""off, away,"" plus cernere, ""separate, sift."" If you use discretion, you sift away what is not desirable, keeping only the good."	Where does discretions choice meaning come from
2273	A probability sampling method is any method of sampling that utilizes some form of random selection. In order to have a random selection method, you must set up some process or procedure that assures that the different units in your population have equal probabilities of being chosen.	What is probability sampling technique
510	Marginal effect is a measure of the instantaneous effect that a change in a particular explanatory variable has on the predicted probability of , when the other covariates are kept fixed.	What is a marginal effect in statistics
7755	word2vec itself is a simple bi-layered neural network architecture, it turns text into meaningful vectors form that deeper networks can understand. In other words the out put of simple neural word2vec model is used as input for Deep Networks.	Is word2vec a neural network
250	A decision making threshold is the value of the decision making variable at which the decision is made, such that an action is selected or a commitment to one alternative is made, marking the end of accumulation of information.	What is meant by threshold value and how is it used to make decision
4548	Partitioning methods Horizontal partitioning involves putting different rows into different tables.  Vertical partitioning involves creating tables with fewer columns and using additional tables to store the remaining columns.	How is horizontal partitioning different from vertical partitioning
5602	Top Applications of Deep Learning Across IndustriesSelf Driving Cars.News Aggregation and Fraud News Detection.Natural Language Processing.Virtual Assistants.Entertainment.Visual Recognition.Fraud Detection.Healthcare.More items•	What can I do with deep learning
987	Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value.	What is the use of ridge regression
7011	Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation.	What is reinforcement learning examples
403	The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). Classifiers that give curves closer to the top-left corner indicate a better performance.  The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.	How does a ROC curve work
502	Neo is contacted by Trinity (Carrie-Anne Moss), a beautiful stranger who leads him into an underworld where he meets Morpheus. They fight a brutal battle for their lives against a cadre of viciously intelligent secret agents. It is a truth that could cost Neo something more precious than his life.	What is the story of the Matrix
7664	Unsupervised learning is very useful in exploratory analysis because it can automatically identify structure in data.  Dimensionality reduction, which refers to the methods used to represent data using less columns or features, can be accomplished through unsupervised methods.	Why unsupervised learning is important
99	The standard error of the sample mean depends on both the standard deviation and the sample size, by the simple relation SE = SD/√(sample size).	What is the relation between sample size and standard error
4071	K-Nearest Neighbors Underfitting and Overfitting The value of k in the KNN algorithm is related to the error rate of the model.  Overfitting imply that the model is well on the training data but has poor performance when new data is coming.	What is Overfitting in Knn
2849	A stratified random sampling involves dividing the entire population into homogeneous groups called strata (plural for stratum).  A random sample from each stratum is taken in a number proportional to the stratum's size when compared to the population. These subsets of the strata are then pooled to form a random sample.	How does stratified sampling work
1617	15 Most Used Machine Learning Tools By ExpertsKnime. Knime is again an open-source machine learning tool that is based on GUI.  Accord.net. Accord.net is a computational machine learning framework.  Scikit-Learn. Scikit-Learn is an open-source machine learning package.  TensorFlow.  Weka.  Pytorch.  RapidMiner.  Google Cloud AutoML.More items•	What are the tools for machine learning
4522	Spearman Rank Correlation: Worked Example (No Tied Ranks)The formula for the Spearman rank correlation coefficient when there are no tied ranks is:  Step 1: Find the ranks for each individual subject.  Step 2: Add a third column, d, to your data.  Step 5: Insert the values into the formula.More items•	How do you calculate rank correlation coefficient
1194	Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake; they're like optical illusions for machines.	What are adversarial examples
3784	You can see SVM as an instance-based learning algorithm because you need to memorize the support vectors if you cannot represent the feature space and hence the discriminating hyperplane in this space explicitly.	Is SVM instance based learning
4407	A random variable, usually written X, is a variable whose possible values are numerical outcomes of a random phenomenon. There are two types of random variables, discrete and continuous.	What is a random variable What are the various types of random variables
3143	Well labeled dataset can be used to train a custom model.In the Data Labeling Service UI, you create a dataset and import items into it from the same page.Open the Data Labeling Service UI.  Click the Create button in the title bar.On the Add a dataset page, enter a name and description for the dataset.More items	How do I create a labeled dataset
74	Extrapolation is an estimation of a value based on extending a known sequence of values or facts beyond the area that is certainly known.  Interpolation is an estimation of a value within two known values in a sequence of values. Polynomial interpolation is a method of estimating values between known data points.	What is extrapolation and interpolation with examples
4121	To “converge” in machine learning is to have an error so close to local/global minimum, or you can see it aa having a performance so clise to local/global minimum. When the model “converges” there is usually no significant error decrease / performance increase anymore. ( Unless a more modern optimizer is applied)	What is convergence in deep learning
148	"An Iterator is an object that can be used to loop through collections, like ArrayList and HashSet. It is called an ""iterator"" because ""iterating"" is the technical term for looping. To use an Iterator, you must import it from the java."	What is the use of iterator
1602	In statistics, an outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analyses.	What is an outlier in data mining
4001	Random utility theory is based on the hypothesis that every individual is a rational decision-maker, maximizing utility relative to his or her choices. Specifically, the theory is based on the following assumptions.	What is random utility model
6990	Definition: Hadoop is a kind of framework that can handle the huge volume of Big Data and process it, whereas Big Data is just a large volume of the Data which can be in unstructured and structured data.	What is difference between Hadoop and Bigdata
328	One-vs-rest (OvR for short, also referred to as One-vs-All or OvA) is a heuristic method for using binary classification algorithms for multi-class classification. It involves splitting the multi-class dataset into multiple binary classification problems.	What is one vs all classification in machine learning
137	Statistical researchers often use a linear relationship to predict the (average) numerical value of Y for a given value of X using a straight line (called the regression line). If you know the slope and the y-intercept of that regression line, then you can plug in a value for X and predict the average value for Y.	How do you use linear regression to predict future values
5621	In the nervous system, dendrites, branches of neurons that transmit signals between synapses and soma, play a critical role in processing functions, such as nonlinear integration of postsynaptic signals.	What are dendrites in neural network
379	Overfitting refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.	What is Overfitting in deep learning
5984	The receptive field in Convolutional Neural Networks (CNN) is the region of the input space that affects a particular unit of the network.  The numbers inside the pixels on the left image represent how many times this pixel was part of a convolution step (each sliding step of the filter).	What is receptive field in CNN
6168	An autocorrelation plot is designed to show whether the elements of a time series are positively correlated, negatively correlated, or independent of each other. (The prefix auto means “self”— autocorrelation specifically refers to correlation among the elements of a time series.)	What does autocorrelation plot tell us
6278	The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance.	Why is the formula of sample variance different from population variance
2300	The Structural Topic Model allows researchers to flexibly estimate a topic model that includes document-level metadata.  The stm package provides many useful features, including rich ways to explore topics, estimate uncertainty, and visualize quantities of interest.	What is structural topic modeling
687	Z = Given Z value. p = Percentage of population. C = Confidence level. Pop = Population.Sample Size Formula for Infinite and Finite Population.Formulas for Sample Size (SS)For Infinite Sample SizeSS = [Z2p (1 − p)]/ C2For Finite Sample SizeSS/ [1 + {(SS − 1)/Pop}]	How do you determine sample size from a finite population
442	Consider a binomial distribution with parameters (n, p). When n is large and p is small , approximate the probability using Poisson distribution. When n is large and p is close to 0.5, use normal approximation.	When do I approximate Binomial Distribution with Normal vs Poisson
6089	Each sample contains different elements so the value of the sample statistic differs for each sample selected. These statistics provide different estimates of the parameter. The sampling distribution describes how these different values are distributed.	What is the difference between sample and sampling distribution
6498	Value (V): Vπ(s) is defined as the expected value of the cumulative reward (discounted) that an agent will receive if he starts in state s at t = 0 and follows policy π. Vπ(s) is also called state value function or value function. The value function estimates value of a state.	What is state value function
1291	If the points on the scatter plot seem to form a line that slants down from left to right, there is a negative relationship or negative correlation between the variables. If the points on the scatter plot seem to be scattered randomly, there is no relationship or no correlation between the variables.	How do you describe a scatter plot with no correlation
8215	In this context, a neural network is one of several machine learning algorithms that can help solve classification problems. Its unique strength is its ability to dynamically create complex prediction functions, and emulate human thinking, in a way that no other algorithm can.	Can neural networks be used for classification
1374	Decision Trees bisect the space into smaller and smaller regions, whereas Logistic Regression fits a single line to divide the space exactly into two.  A single linear boundary can sometimes be limiting for Logistic Regression.	What is the difference between decision tree and logistic regression
8387	Google built the underlying TensorFlow software with the C++ programming language. But in developing applications for this AI engine, coders can use either C++ or Python, the most popular language among deep learning researchers.	What language does TensorFlow use
430	The joint behavior of two random variables X and Y is determined by the. joint cumulative distribution function (cdf):(1.1) FXY (x, y) = P(X ≤ x, Y ≤ y),where X and Y are continuous or discrete. For example, the probability.  P(x1 ≤ X ≤ x2,y1 ≤ Y ≤ y2) = F(x2,y2) − F(x2,y1) − F(x1,y2) + F(x1,y1).	How do you find the joint distribution of X and Y
764	"Asymptotic analysis of an algorithm refers to defining the mathematical boundation/framing of its run-time performance.  Asymptotic analysis is input bound i.e., if there's no input to the algorithm, it is concluded to work in a constant time. Other than the ""input"" all other factors are considered constant."	What is asymptotic analysis of an algorithm explain
336	Statistics, when used in a misleading fashion, can trick the casual observer into believing something other than what the data shows. That is, a misuse of statistics occurs when a statistical argument asserts a falsehood. In some cases, the misuse may be accidental.	How statistics can be misused
1148	Data structure and algorithms help in understanding the nature of the problem at a deeper level and thereby a better understanding of the world. If you want to know more about Why Data Structures and Algorithms then you must watch this video of Mr.	Do I need to learn data structures and algorithms
953	In hierarchical k-means we pick some k to be the branching factor.  at each level of the clustering hierarchy. We then clus- ter the set of points into k clusters using a standard k- means algorithm. Finally, we recursively cluster each sub-cluster until we hit some small fixed number of points.	How can K means be used for hierarchical clustering
344	Data Science Interview Questions based on AUC. AUC stands for Area Under the Curve.  The way it is done is to see how much area has been covered by the ROC curve. If we obtain a perfect classifier, then the AUC score is 1.0. If the classifier is random in its guesses, then the AUC score is 0.5.	What is AUC in data science
517	Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed.	How are decision trees used in classification
713	The finite population correction (fpc) factor is used to adjust a variance estimate for an estimated mean or total, so that this variance only applies to the portion of the population that is not in the sample.	What is a finite population correction factor
6174	Some argue that such scores are at the ordinal level, providing only an ordering of performance. But, given the large number of potential values (95% of the population falls between 70 and 130 on an IQ scale), the scores function well as interval-scaled values.	Is “IQ” a ratio scale or an interval scale
347	Generally, standardized scores refer to raw data being converted to standard or normalized scores in order to maintain uniformity in interpretation of statistical data.  Z scores are one of the most commonly used scores for data in statistics. They are also known as normal scores and standardized variables.	What is a standardized score in statistics
8670	For a discrete random variable, the expected value, usually denoted as or , is calculated using: μ = E ( X ) = ∑ x i f ( x i )	What is the expected value of a discrete distribution
3646	Generally speaking, gradient boosted trees are more robust in multicollinearity situations than OLS regression.  When two independent variables are highly correlated, applying OLS regression could create problems. For example, p-values may not be reliable or even worse the OLS solution can't even be calculated.	Is multicollinearity a problem with gradient boosted trees
1513	In the literal meaning of the terms, a parametric statistical test is one that makes assumptions about the parameters (defining properties) of the population distribution(s) from which one's data are drawn, while a non-parametric test is one that makes no such assumptions.	What is a nonparametric test what is a parametric test
7469	"This unit will calculate and/or estimate binomial probabilities for situations of the general ""k out of n"" type, where k is the number of times a binomial outcome is observed or stipulated to occur, p is the probability that the outcome will occur on any particular occasion, q is the complementary probability (1-p)"	What does Q stand for in probability
8641	The simplest approach to identifying irregularities in data is to flag the data points that deviate from common statistical properties of a distribution, including mean, median, mode, and quantiles. Let's say the definition of an anomalous data point is one that deviates by a certain standard deviation from the mean.	How do you identify data anomalies
1921	Recurrent neural network works best for sequential data.	Which of the following model is best suited for sequential data
4664	Singularity enables users to have full control of their environment. Singularity containers can be used to package entire scientific workflows, software and libraries, and even data.  The Singularity software can import your Docker images without having Docker installed or being a superuser.	What is singularity container
243	"In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract ""topics"" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body."	What is the use of topic Modelling
1008	Just multiply the probability of the first event by the second. For example, if the probability of event A is 2/9 and the probability of event B is 3/9 then the probability of both events happening at the same time is (2/9)*(3/9) = 6/81 = 2/27.	How do you find the probability of multiple events
7462	0:082:33Suggested clip · 117 secondsHistogram Finding Frequency - Corbettmaths - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you read a relative frequency density histogram
7235	The ability to detect and adapt to changes in the distribution of examples is paramount for data stream mining algorithms. The shift in the underlying distribution of examples arriving from a data stream is referred to as concept drift. Concept drift occurs over time and the rate at which the drifts occurs varies.	What is Concept drift in data stream mining
5721	2:4411:47Suggested clip · 92 secondsHow to Design a Convolutional Neural Network | Lecture 8 - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you create a convolutional neural network
5319	Agricultural scientists often use linear regression to measure the effect of fertilizer and water on crop yields. The coefficient β0 would represent the expected crop yield with no fertilizer or water.	How is linear regression used in real life
7150	Parametric tests are those that make assumptions about the parameters of the population distribution from which the sample is drawn. This is often the assumption that the population data are normally distributed. Non-parametric tests are “distribution-free” and, as such, can be used for non-Normal variables.	What is the difference between parametric and non parametric test
965	Quantiles are points in a distribution that relate to the rank order of values in that distribution.  Centiles/percentiles are descriptions of quantiles relative to 100; so the 75th percentile (upper quartile) is 75% or three quarters of the way up an ascending list of sorted values of a sample.	What is difference between quantile and percentile
2103	Dimensional AnalysisIdentify the given (see previous concept for additional information).Identify conversion factors that will help you get from your original units to your desired unit.Set up your equation so that your undesired units cancel out to give you your desired units.  Multiply through to get your final answer.	What are the steps of dimensional analysis
3528	An unbiased estimator is an accurate statistic that's used to approximate a population parameter. “Accurate” in this sense means that it's neither an overestimate nor an underestimate. If an overestimate or underestimate does happen, the mean of the difference is called a “bias.”	Why is it good for an estimator to be unbiased
141	The scatter diagram graphs pairs of numerical data, with one variable on each axis, to look for a relationship between them. If the variables are correlated, the points will fall along a line or curve. The better the correlation, the tighter the points will hug the line.	How do you determine if a scatter plot is a line or a curve
4888	2. What is the area under a conditional Cumulative density function? Explanation: Area under any conditional CDF is 1.	What is the area under the conditional C * * * * * * * * * density function
46	Statistics is used to process complex problems in the real world so that Data Scientists and Analysts can look for meaningful trends and changes in Data. In simple words, Statistics can be used to derive meaningful insights from data by performing mathematical computations on it.	How is statistics used in data science
5063	Computer vision, however, is more than machine learning applied. It involves tasks as 3D scene modeling, multi-view camera geometry, structure-from-motion, stereo correspondence, point cloud processing, motion estimation and more, where machine learning is not a key element.	Is computer vision machine learning
1576	It's a cost function that is used as loss for machine learning models, telling us how bad it's performing, the lower the better. Also it's much easier to reason about the loss this way, to be consistent with the rule of loss functions approaching 0 as the model gets better.	Why do we use negative log likelihood
6902	Convergence in distribution is in some sense the weakest type of convergence. All it says is that the CDF of Xn's converges to the CDF of X as n goes to infinity. It does not require any dependence between the Xn's and X. We saw this type of convergence before when we discussed the central limit theorem.	What does convergence in distribution mean
440	In statistics, the Kolmogorov–Smirnov test (K–S test or KS test) is a nonparametric test of the equality of continuous (or discontinuous, see Section 2.2), one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution (one-sample K–S test), or to compare two	What is a Kolmogorov Smirnov test for discrete distributions
812	"The core idea is that we cannot know exactly how well an algorithm will work in practice (the true ""risk"") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the ""empirical"" risk)."	What does empirical risk mean
4648	A type I error (false-positive) occurs if an investigator rejects a null hypothesis that is actually true in the population; a type II error (false-negative) occurs if the investigator fails to reject a null hypothesis that is actually false in the population.	What is the relationship between type I and type II errors
85	Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we're not sure what we're looking for. Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model.	What is topic Modelling in R
1195	Linear regression is used for predicting the continuous dependent variable using a given set of independent features whereas Logistic Regression is used to predict the categorical. Linear regression is used to solve regression problems whereas logistic regression is used to solve classification problems.26‏/04‏/2020	What is difference between linear and logistic regression
743	Its observed value changes randomly from one random sample to a different sample. A test statistic contains information about the data that is relevant for deciding whether to reject the null hypothesis. The sampling distribution of the test statistic under the null hypothesis is called the null distribution.	What is the distribution of a test statistic
3762	A left-skewed distribution has a long left tail.  The normal distribution is the most common distribution you'll come across. Next, you'll see a fair amount of negatively skewed distributions. For example, household income in the U.S. is negatively skewed with a very long left tail.	What is an example of a common negatively skewed distribution
8656	A vital aspect of the model construction process is the calibration phase.  In fact, a model's predictive uncertainty will only be reduced by calibration if the information content of the calibration data set is able to constrain those parameters that have a significant bearing on that prediction.	Why is model calibration important
1446	While data science focuses on the science of data, data mining is concerned with the process. It deals with the process of discovering newer patterns in big data sets.  In machine learning algorithms are used for gaining knowledge from data sets.	What is the difference between Data Analytics Data Analysis Data Mining Data Science Machine Learning and Big Data 1
4189	A kind of average sometimes used in statistics and engineering, often abbreviated as RMS. To find the root mean square of a set of numbers, square all the numbers in the set and then find the arithmetic mean of the squares. Take the square root of the result. This is the root mean square.	How is root mean square calculated
138	"While machine learning is based on the idea that machines should be able to learn and adapt through experience, AI refers to a broader idea where machines can execute tasks ""smartly."" Artificial Intelligence applies machine learning, deep learning and other techniques to solve actual problems."	What is the relationship between artificial intelligence and machine learning
161	You always have to give a three-dimensional array as an input to your LSTM network (refer to the above image).  And the third dimension represents the number of units in one input sequence. For example, input shape looks like (batch_size, time_steps, seq_len) . Let's look at an example in Keras.	What is input shape in Lstm
5939	The following are key advantages of parallel programming that motivate its use for developing computing solutions: The main reason for parallel programming is to execute code efficiently, since parallel programming saves time, allowing the execution of applications in a shorter wall-clock time.	What are the advantages of using sequential algorithm
8184	Direct link to this answer A feedforward backpropagation net is a net that just happened to be trained with a backpropagation training algorithm. The backpropagation training algorithm subtracts the training output from the target (desired answer) to obtain the error signal.	What is feed forward backpropagation neural network
7568	Let's explore 5 common techniques used for extracting information from the above text.Named Entity Recognition. The most basic and useful technique in NLP is extracting the entities in the text.  Sentiment Analysis.  Text Summarization.  Aspect Mining.  Topic Modeling.	How do I extract information from a text
686	In the literal meaning of the terms, a parametric statistical test is one that makes assumptions about the parameters (defining properties) of the population distribution(s) from which one's data are drawn, while a non-parametric test is one that makes no such assumptions.	What does a parametric test mean
2655	Two types of Regression in machine learning: coefficient indicates the direction of the relationship between a predictor variable and the response variable. A positive sign indicates that as the predictor variable(y )increases, the response variable(X) also increases.	What is coefficient in machine learning
6308	Quota sampling means to take a very tailored sample that's in proportion to some characteristic or trait of a population. For example, you could divide a population by the state they live in, income or education level, or sex.  Care is taken to maintain the correct proportions representative of the population.	What is quota sampling example
5239	Median filtering A median filter is a nonlinear filter in which each output sample is computed as the median value of the input samples under the window – that is, the result is the middle value after the input values have been sorted. Ordinarily, an odd number of taps is used.	Why is median filter nonlinear
431	Matrix decomposition methods, also called matrix factorization methods, are a foundation of linear algebra in computers, even for basic operations such as solving systems of linear equations, calculating the inverse, and calculating the determinant of a matrix.	Why is matrix decomposition important
3352	In mathematics, a tensor is an algebraic object that describes a (multilinear) relationship between sets of algebraic objects related to a vector space. Objects that tensors may map between include vectors and scalars, and even other tensors.	How do you define a tensor
167	There are two sorts of reasons for taking the log of a variable in a regression, one statistical, one substantive.  When they are positively skewed (long right tail) taking logs can sometimes help. Sometimes logs are taken of the dependent variable, sometimes of one or more independent variables.	What does it mean to take the log of a variable
8079	How To Develop a Machine Learning Model From ScratchDefine adequately our problem (objective, desired outputs…).Gather data.Choose a measure of success.Set an evaluation protocol and the different protocols available.Prepare the data (dealing with missing values, with categorial values…).Spilit correctly the data.More items	How do you make a deep learning model from scratch
5647	A lurking variable can falsely identify a strong relationship between variables or it can hide the true relationship. For example, a research scientist studies the effect of diet and exercise on a person's blood pressure. Lurking variables that also affect blood pressure are whether a person smokes and stress levels.	What is an example of a lurking variable
5609	Makes sense	What does inference mean in AI
3921	Equal width binning is probably the most popular way of doing discretization. This means that after the binning, all bins have equal width, or represent an equal range of the original variable values, no matter how many cases are in each bin.	What is equi width binning
4250	Feedfoward neural networks are primarily used for supervised learning in cases where the data to be learned is neither sequential nor time-dependent. That is, feedforward neural networks compute a function f on fixed size input x such that f ( x ) ≈ y f(x) \approx y f(x)≈y for training pairs ( x , y ) (x, y) (x,y).	What are feedforward neural networks used for
2770	4.1 Input Layer Input layer in CNN should contain image data. Image data is represented by three dimensional matrix as we saw earlier. You need to reshape it into a single column.  If you have “m” training examples then dimension of input will be (784, m).	What is input layer in CNN
1338	Eigenface	Which algorithm is used for face detection
3005	three types	How many types of bivariate correlation are there
4093	In statistics, a Poisson distribution is a statistical distribution that shows how many times an event is likely to occur within a specified period of time. It is used for independent events which occur at a constant rate within a given interval of time.	Why do we use Poisson distribution
4584	Activation functions cannot be linear because neural networks with a linear activation function are effective only one layer deep, regardless of how complex their architecture is.  Therefore, nonlinear functions must be continuous and differentiable between this range.	Why activation functions are nonlinear in deep learning
2862	Binomial Approximation The normal distribution can be used as an approximation to the binomial distribution, under certain circumstances, namely: If X ~ B(n, p) and if n is large and/or p is close to ½, then X is approximately N(np, npq)	When can you use normal distribution to approximate binomial distribution
883	Since most natural phenomena are complex and have many factors, the same logic as above applies and distribution of measures of such phenomena tend to have most values near the mean (normal distibution has a desirable property of mean and mode being the same - i.e. the mean is the same as the most frequent value).	Why does the normal distribution show up so often in nature
5145	The biggest advantage of linear regression models is linearity: It makes the estimation procedure simple and, most importantly, these linear equations have an easy to understand interpretation on a modular level (i.e. the weights).	What are the advantages of regression
4725	Every time you conduct a t-test there is a chance that you will make a Type I error.  An ANOVA controls for these errors so that the Type I error remains at 5% and you can be more confident that any statistically significant result you find is not just running lots of tests.	Why do we use Anova instead of t test
5967	In Reinforcement Learning (RL), the problem to resolve is described as a Markov Decision Process (MDP). Theoretical results in RL rely on the MDP description being a correct match to the problem.  Conversely, if you cannot map your problem onto a MDP, then the theory behind RL makes no guarantees of any useful result.	What's the difference between Markov decision processes MDPs and reinforcement learning RL )
519	The probability distribution of a continuous random variable X is an assignment of probabilities to intervals of decimal numbers using a function f(x), called a density function, in the following way: the probability that X assumes a value in the interval [a,b] is equal to the area of the region that is bounded above	What is the probability distribution of a continuous random variable
4691	In a somewhat similar fashion you can estimate the standard deviation based on the box plot:the standard deviation is approximately equal to the range / 4.the standard deviation is approximately equal to 3/4 * IQR.	How do you interpret standard deviation from a box plot
7625	The MNIST database, an extension of the NIST database, is a low-complexity data collection of handwritten digits used to train and test various supervised machine learning algorithms. The database contains 70,000 28x28 black and white images representing the digits zero through nine.	What is the format of Mnist data
1192	You can reduce High variance, by reducing the number of features in the model. There are several methods available to check which features don't add much value to the model and which are of importance. Increasing the size of the training set can also help the model generalise.	How do you handle high variance data
1711	Latent Class Analysis (LCA) is a statistical method for identifying unmeasured class membership among subjects using categorical and/or continuous observed variables. For example, you may wish to categorize people based on their drinking behaviors (observations) into different types of drinkers (latent classes).	What is Latent class analysis used for
1778	Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning.	Which of the following is a representation learning algorithm *
8055	Root Mean Squared Error or RMSE RMSE is the standard deviation of the errors which occur when a prediction is made on a dataset. This is the same as MSE (Mean Squared Error) but the root of the value is considered while determining the accuracy of the model. from sklearn.	What is root mean square error in machine learning
1227	The variance (symbolized by S2) and standard deviation (the square root of the variance, symbolized by S) are the most commonly used measures of spread. We know that variance is a measure of how spread out a data set is. It is calculated as the average squared deviation of each number from the mean of a data set.	What is variance used for
5723	Conversion ruleTake glm output coefficient (logit)compute e-function on the logit using exp() “de-logarithimize” (you'll get odds then)convert odds to probability using this formula prob = odds / (1 + odds) . For example, say odds = 2/1 , then probability is 2 / (1+2)= 2 / 3 (~.	How do you calculate logit
4032	Specificity is the proportion of truly negative cases that were classified as negative; thus, it is a measure of how well your classifier identifies negative cases. It is also known as the true negative rate.	What is specificity in machine learning
7916	Here are some important considerations while choosing an algorithm.Size of the training data. It is usually recommended to gather a good amount of data to get reliable predictions.  Accuracy and/or Interpretability of the output.  Speed or Training time.  Linearity.  Number of features.	How do you choose an algorithm for a classification problem
3657	Generalized Linear Models let you express the relation between covariates X and response y in a linear, additive manner.	What is the practical purpose of generalized linear models
1076	Machine learning models are designed to make the most accurate predictions possible.  A statistical model is a model for the data that is used either to infer something about the relationships within the data or to create a model that is able to predict future values. Often, these two go hand-in-hand.	How are machine learning data science and statistics related
375	The general definition of a vector space allows scalars to be elements of any fixed field F. The notion is then known as an F-vector space or a vector space over F. A field is, essentially, a set of numbers possessing addition, subtraction, multiplication and division operations.	What is an F vector space
8317	Probability sampling means that every member of the population has a chance of being selected. It is mainly used in quantitative research. If you want to produce results that are representative of the whole population, you need to use a probability sampling technique. There are four main types of probability sample.	What is probability sampling in research
1608	There are different types of mean, viz. arithmetic mean, weighted mean, geometric mean (GM) and harmonic mean (HM). If mentioned without an adjective (as mean), it generally refers to the arithmetic mean.	How many types of mean in statistics
7892	This can happen in high dimensional data with feature crosses, when there's a huge mass of rare crosses that happen only on one example each. Fortunately, using L2 or early stopping will prevent this problem. Logistic regression models generate probabilities. Log Loss is the loss function for logistic regression.	What is the loss function for logistic regression
559	In the statistical analysis of time series, autoregressive–moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the autoregression (AR) and the second for the moving average (MA).	What is ARMA model used for
7546	Augmented reality uses existing reality and physical objects to trigger computer-generated enhancements over the top of reality, in real time. Essentially, AR is a technology that lays computer-generated images over a user's view of the real world. These images typically take shape as 3D models, videos and information.	How Augmented Reality is created
986	A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous). ID3 algorithm uses entropy to calculate the homogeneity of a sample.	How are decision trees constructed
2418	Linear models describe a continuous response variable as a function of one or more predictor variables. They can help you understand and predict the behavior of complex systems or analyze experimental, financial, and biological data.	What does a linear model show
5763	In short, they are independent because the bivariate normal density, in case they are uncorrelated, i.e. ρ=0, reduces to a product of two normal densities the support of each one ranges from (−∞,∞). If the joint distribution can be written as a product of nonnegative functions, we know that the RVs are independent.	Why is uncorrelated Gaussian data independent
8148	Comparison of bootstrap and jackknife Although there are huge theoretical differences in their mathematical insights, the main practical difference for statistics users is that the bootstrap gives different results when repeated on the same data, whereas the jackknife gives exactly the same result each time.	What is the difference between bootstrap and jackknife
619	To create a stratified random sample, there are seven steps: (a) defining the population; (b) choosing the relevant stratification; (c) listing the population; (d) listing the population according to the chosen stratification; (e) choosing your sample size; (f) calculating a proportionate stratification; and (g) using	How do you conduct a stratified random sample
1378	If the data is symmetrical - normally distributed - then the mean tell you where the line of symmetry falls. The standard deviation tells you more. It tells you if the data is closely distributed to the mean (small standard deviation) or is the data widely distributed (big standard deviation).	Why do we use standard deviation instead of mean deviation
1234	A regression equation is used in stats to find out what relationship, if any, exists between sets of data. For example, if you measure a child's height every year you might find that they grow about 3 inches a year. That trend (growing three inches a year) can be modeled with a regression equation.	What does the regression equation tell us
282	Discriminant function analysis (DFA) is a statistical procedure that classifies unknown individuals and the probability of their classification into a certain group (such as sex or ancestry group). Discriminant function analysis makes the assumption that the sample is normally distributed for the trait.	What are discriminant functions
2376	Advantages of Recurrent Neural Network An RNN remembers each and every information through time. It is useful in time series prediction only because of the feature to remember previous inputs as well. This is called Long Short Term Memory.	What is the main advantage of recurrent neural networks
6879	Like I said before, the AUC-ROC curve is only for binary classification problems. But we can extend it to multiclass classification problems by using the One vs All technique. So, if we have three classes 0, 1, and 2, the ROC for class 0 will be generated as classifying 0 against not 0, i.e. 1 and 2.	Is ROC curve only for binary classification
1288	Mamdani Fuzzy Inference Systems Mamdani fuzzy inference was first introduced as a method to create a control system by synthesizing a set of linguistic control rules obtained from experienced human operators [1].  These output fuzzy sets are combined into a single fuzzy set using the aggregation method of the FIS.	What is Mamdani fuzzy inference system
3616	Computational Logic is the process of designing and analyzing logic in computer applications. In this lesson, we'll discuss creating logic based on the statements and constraints provided. Logic in relation to computers is mainly of two types: Propositional Logic and First Order Logic(FOL).	What is computational logic in the context of artificial intelligence
7697	Multimodal data Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities.	What is multimodal data
523	Mixed models add at least one random variable to a linear or generalized linear model. The random variables of a mixed model add the assumption that observations within a level, the random variable groups, are correlated.	What are the assumptions of a generalized linear mixed model
325	We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture.	What are the layers of CNN
556	In computer science, a rule-based system is used to store and manipulate knowledge to interpret information in a useful way. It is often used in artificial intelligence applications and research. Normally, the term rule-based system is applied to systems involving human-crafted or curated rule sets.	What is rule based system in artificial intelligence
1544	The 5 main steps to create word clouds in RStep 1: Create a text file.  Step 2 : Install and load the required packages.  Step 3 : Text mining.  Step 4 : Build a term-document matrix.  Step 5 : Generate the Word cloud.	How do I use text mining in R
3221	The second reason you may see validation loss lower than training loss is due to how the loss value are measured and reported: Training loss is measured during each epoch. While validation loss is measured after each epoch.	Why is validation loss lower than training loss
660	Random field theory (RFT) is a recent body of mathematics defining theo- retical results for smooth statistical maps.  The way that RFT solves this problem is by using results that give the expected Euler characteristic (EC) for a smooth statistical map that has been thresholded.	What is random field theory
7570	Taking the square root of the variance gives us the units used in the original scale and this is the standard deviation. Standard deviation is the measure of spread most commonly used in statistical practice when the mean is used to calculate central tendency. Thus, it measures spread around the mean.	What is the use of variance and standard deviation
918	When there are two or more independent variables, it is called multiple regression.	How many variables can be used in multiple regression
4800	10:1614:33Suggested clip · 106 secondsPermutation Hypothesis Test in R with Examples | R Tutorial 4.6 YouTubeStart of suggested clipEnd of suggested clip	How do you do permutation test in R
1388	When most dependent variables are numeric, logistic regression and SVM should be the first try for classification. These models are easy to implement, their parameters easy to tune, and the performances are also pretty good. So these models are appropriate for beginners.	Which algorithm is used for classification
4457	In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm.	What is Delta learning rule in neural network
344	One of the common methods for organizing data is to construct frequency distribution. Frequency distribution is an organized tabulation/graphical representation of the number of individuals in each category on the scale of measurement.	What is a frequency distribution and how is it constructed
47	The k-modes algorithm tries to minimize the sum of within-cluster Hamming distance from the mode of that cluster, summed over all clusters.  The procedure is similar to k-means: a number of clusters (k) is chosen, and k cluster-mode vectors are chosen at random (or according to accepted heuristics).	How does K modes work
6564	First, to find the conditional distribution of X given a value of Y, we can think of fixing a row in Table 1 and dividing the values of the joint pmf in that row by the marginal pmf of Y for the corresponding value. For example, to find pX|Y(x|1), we divide each entry in the Y=1 row by pY(1)=1/2.	How do you find the conditional probability distribution
6999	A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical.	What is the difference between K means and hierarchical clustering
498	The mean means average. To find it, add together all of your values and divide by the number of addends. The median is the middle number of your data set when in order from least to greatest. The mode is the number that occurred the most often.	How do you calculate the mean mode and median
3705	STEPS IN DESIGNING AND CONDUCTING AN RCTGathering the Research Team.  Determining the Research Question.  Defining Inclusion and Exclusion Criteria.  Randomization.  Determining and Delivering the Intervention.  Selecting the Control.  Determining and Measuring Outcomes.  Blinding Participants and Investigators.More items	How do you do a randomized controlled trial
4247	A scale of 1 : 100 000 means that the real distance is 100 000 times the length of 1 unit on the map or drawing.Example 14. Write the scale 1 cm to 1 m in ratio form.  Example 15. Simplify the scale 5 mm : 1 m.  Example 16. Simplify the scale 5 cm : 2 km.  Example 17. A particular map shows a scale of 1 : 5000.  Example 18.	How do you express a scale as a ratio
743	Classification accuracy is the ratio of correct predictions to total predictions made. classification accuracy = correct predictions / total predictions. 1. classification accuracy = correct predictions / total predictions. It is often presented as a percentage by multiplying the result by 100.	What is accuracy in confusion matrix
512	BFS stands for Breadth First Search. DFS stands for Depth First Search. 2. BFS(Breadth First Search) uses Queue data structure for finding the shortest path. DFS(Depth First Search) uses Stack data structure.	What is BFS and DFS
4684	A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution.	What is the difference between normal distribution and standard normal distribution
33	A convolution is the simple application of a filter to an input that results in an activation. Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in an input, such as an image.	What is a convolution in deep learning
6266	A curve that represents the cumulative frequency distribution of grouped data on a graph is called a Cumulative Frequency Curve or an Ogive.	Which plot is required for cumulative frequency distribution
5422	In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error.	Is RMSE and standard error same
3261	The outcomes of a random experiment are called events connected with the experiment. For example; 'head' and 'tail' are the outcomes of the random experiment of throwing a coin and hence are events connected with it. Now we can distinguish between two types of events.	How many types of events are there in probability
3976	In order to conduct a one-sample proportion z-test, the following conditions should be met: The data are a simple random sample from the population of interest. The population is at least 10 times as large as the sample. n⋅p≥10 and n⋅(1−p)≥10 , where n is the sample size and p is the true population proportion.	What conditions must be met to use Z procedures in a significance test about a population proportion
1270	An experimental group is a test sample or the group that receives an experimental procedure. This group is exposed to changes in the independent variable being tested.  A control group is a group separated from the rest of the experiment such that the independent variable being tested cannot influence the results.	Performance Testing What is the difference between a control group and an experimental group
5236	"A common pattern is the bell-shaped curve known as the ""normal distribution."" In a normal or ""typical"" distribution, points are as likely to occur on one side of the average as on the other. Note that other distributions look similar to the normal distribution."	What is a normal distribution in a histogram
4690	The law of large numbers, in probability and statistics, states that as a sample size grows, its mean gets closer to the average of the whole population. In the 16th century, mathematician Gerolama Cardano recognized the Law of Large Numbers but never proved it.	What does the law of large numbers say
810	Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial. Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case.	What is the difference between f1 score and accuracy
6478	The computational complexity of most metric MDS methods is over O(N2), so that it is difficult to process a data set of a large number of genes N, such as in the case of whole genome microarray data.	What is the computational complexity of classical multidimensional scaling
8385	Reinforcement learning (RL) is a significant area of machine learning, with the potential to solve a lot of real world problems in various fields, like game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics.	What problems can reinforcement learning solve
8251	The SMD is preferable when the studies in a meta-analysis measure a given outcome using different scales or instruments.	What is the main advantage of the standardized mean difference SMD over the mean difference MD )
6246	Some common types of problems built on top of classification and regression include recommendation and time series prediction respectively. Some popular examples of supervised machine learning algorithms are: Linear regression for regression problems. Random forest for classification and regression problems.	What problems are suitable for supervised machine learning
7386	Logarithms are a way of showing how big a number is in terms of how many times you have to multiply a certain number (called the base) to get it.  The most common numbers to use are 2, 10, and 2.71828). Logarithms are useful because they are the way our brain naturally understands most things.	Why do we use logarithms
7477	A contingency table, sometimes called a two-way frequency table, is a tabular mechanism with at least two rows and two columns used in statistics to present categorical data in terms of frequency counts.	What is a contingency table in statistics
1232	The null hypothesis is a general statement that states that there is no relationship between two phenomenons under consideration or that there is no association between two groups. An alternative hypothesis is a statement that describes that there is a relationship between two selected variables in a study.	What is the difference between null and alternative hypothesis
8663	The main use of F-distribution is to test whether two independent samples have been drawn for the normal populations with the same variance, or if two independent estimates of the population variance are homogeneous or not, since it is often desirable to compare two variances rather than two averages.	What are the application of f distribution in statistical analysis
900	It tells the algorithm how much you care about misclassified points. SVMs, in general, seek to find the maximum-margin hyperplane. That is, the line that has as much room on both sides as possible.	What is the role of C in SVM
1355	Robust statistics are statistics with good performance for data drawn from a wide range of probability distributions, especially for distributions that are not normal. Robust statistical methods have been developed for many common problems, such as estimating location, scale, and regression parameters.	What does it mean robust in statistics
814	all sampling units have a logical and have numerical identifier.  the sampling frame has some additional information about the units that allow the use of more advanced sampling frames. every element of the population of interest is present in the frame. every element of the population is present only once in the frame.	What is the difference between sampling frame and sampling unit
4879	On a far grander scale, AI is poised to have a major effect on sustainability, climate change and environmental issues. Ideally and partly through the use of sophisticated sensors, cities will become less congested, less polluted and generally more livable. Inroads are already being made.	How did AI change the world
100	Standard interpretation of the ordered logit coefficient is that for a one unit increase in the predictor, the response variable level is expected to change by its respective regression coefficient in the ordered log-odds scale while the other variables in the model are held constant.	How do you interpret an ordered logit
5613	A partial correlation is basically the correlation between two variables when a third variable is held constant.  If we look at the relationship between exercise and weight loss, we see a negative correlation, which sounds bad but isn't. It means that the more I exercise, the more weight I lose.	What does a negative partial correlation mean
1514	Non-linearity is needed in activation functions because its aim in a neural network is to produce a nonlinear decision boundary via non-linear combinations of the weight and inputs.	Why the hidden layers in a neural network must have a non linear activation function
4470	Autocorrelation measures the relationship between a variable's current value and its past values. An autocorrelation of +1 represents a perfect positive correlation, while an autocorrelation of negative 1 represents a perfect negative correlation.	What does autocorrelation mean in statistics
334	Similarly, an absolute minimum occurs at the x value where the function is the smallest, while a local minimum occurs at an x value if the function is smaller there than points around it (i.e. an open interval around it).	What is the difference between an absolute minimum and a local minimum
4599	The proportion of Y variance explained by the linear relationship between X and Y = r2 = 0.64, or 64%.	What percent of the variation in Y is explained by X
4694	Now we'll check out the proven way to improve the performance(Speed and Accuracy both) of neural network models:Increase hidden Layers.  Change Activation function.  Change Activation function in Output layer.  Increase number of neurons.  Weight initialization.  More data.  Normalizing/Scaling data.More items•	How can neural networks improve performance
5682	Linear Shift-Invariant systems, called LSI systems for short, form a very important class of practical systems, and hence are of interest to us. They are also referred to as Linear Time-Invariant systems, in case the independent variable for the input and output signals is time.	What is linear shift invariant system
6038	The fuzzy K-nearest neighbor algorithm finds memberships of data instances into classes rather than assigning the whole class label. It is beneficial for unlabeled query instance as it is known prior that how much its neighbors belong to a class to improve the accuracy.	What is the fuzzy k nearest neighbor algorithm
5455	low-dimensional linear mapping of the original high-dimensional data that preserves some feature of interest in the data. Accordingly, linear dimensionality reduction can be used for visualizing or exploring structure in data, denoising or compressing data, extracting meaningful feature spaces, and more.	What is linear dimensionality reduction
7618	The intuition for entropy is that it is the average number of bits required to represent or transmit an event drawn from the probability distribution for the random variable. … the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution.	What is an intuitive explanation of the concept of entropy in information theory
6634	1 Introduction. The partial least squares (PLS) algorithm was first introduced for regression tasks and then evolved into a classification method that is well known as PLS-discriminant analysis (PLS-DA).	Whats the abbreviation for orthogonal partial least squares discriminant analysis
1335	A tensor field has a tensor corresponding to each point space. An example is the stress on a material, such as a construction beam in a bridge. Other examples of tensors include the strain tensor, the conductivity tensor, and the inertia tensor.	What is tensor example
1174	Two pixels, p and q, are connected if there is a path from p to q of pixels with property V. A path is an ordered sequence of pixels such that any two adjacent pixels in the sequence are neighbors. An example of an image with a connected component is shown at the right.	How do you find connected components in an image
682	General reporting recommendations such as that of APA Manual apply. One should report exact p-value and an effect size along with its confidence interval. In the case of likelihood ratio test one should report the test's p-value and how much more likely the data is under model A than under model B.	How do you report likelihood ratio tests
1374	A quantum bit, more commonly called a qubit, is the basic unit of quantum computing. It can have a value of one or zero or anything in between—at the same time. There are other things qubits can do, but multiple, simultaneous values makes quantum computers faster.	What is Quantum Al
4698	Softmax regression (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes. In logistic regression we assumed that the labels were binary: y(i)∈{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits.	Is Softmax the same as logistic regression
11	Deep learning networks can be successfully applied to big data for knowledge discovery, knowledge application, and knowledge-based prediction. In other words, deep learning can be a powerful engine for producing actionable results.	What is deep learning Good For
943	We can say that, when we move from RNN to LSTM, we are introducing more & more controlling knobs, which control the flow and mixing of Inputs as per trained Weights. And thus, bringing in more flexibility in controlling the outputs. So, LSTM gives us the most Control-ability and thus, Better Results.	Why is Lstm better than RNN
3483	"In biostatistics, logistic regression is often used when the outcome variable is dichotomous. You also list ""SPSS"" as a topic. A Google search on <SPSS logistic regression example> will no doubt yield many hits, including the UCLA ""textbook examples""."	What type of regression analysis would be used if the dependent variable was dichotomous
3003	Density is a measure of mass per volume. The average density of an object equals its total mass divided by its total volume. An object made from a comparatively dense material (such as iron) will have less volume than an object of equal mass made from some less dense substance (such as water).	What is density in simple words
555	Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data.	What is statistical learning in machine learning
4305	When you multiply a matrix by a number, you multiply every element in the matrix by the same number. This operation produces a new matrix, which is called a scalar multiple. For example, if x is 5, and the matrix A is: A =	What is meant by scalar multiple
1115	As the sample sizes increase, the variability of each sampling distribution decreases so that they become increasingly more leptokurtic. The range of the sampling distribution is smaller than the range of the original population.	What effect does sample size have on the shape of a sampling distribution
3812	Delta learning does this using the difference between a target activation and an actual obtained activation. Using a linear activation function, network connections are adjusted. Another way to explain the Delta rule is that it uses an error function to perform gradient descent learning.	What is Delta learning rule how it works
4014	The three different types of outliersType 1: Global Outliers (also called “Point Anomalies”)Type 2: Contextual (Conditional) Outliers.Type 3: Collective Outliers.Think of it this way.	What are the different types of outliers
7826	Answer: Autoecncoders work best for image data.	Which works best for image data
7254	Therefore, we can reduce the complexity of a neural network to reduce overfitting in one of two ways:Change network complexity by changing the network structure (number of weights).Change network complexity by changing the network parameters (values of weights).	What's the best way to avoid overfitting in NLP datasets
6792	8 Methods to Boost the Accuracy of a ModelAdd more data. Having more data is always a good idea.  Treat missing and Outlier values.  Feature Engineering.  Feature Selection.  Multiple algorithms.  Algorithm Tuning.  Ensemble methods.	How do you improve classification model accuracy
3431	These feature maps obtained from convolutional layers are used for classification , but using all the extracted features is computationally expensive.  So a Pooling layer is not necessary but advisable to reduce the number of extracted features and to avoid overfitting.	Is a pooling layer necessary in CNN Can it be replaced by convolution
1047	To measure the relationship between numeric variable and categorical variable with > 2 levels you should use eta correlation (square root of the R2 of the multifactorial regression). If the categorical variable has 2 levels, point-biserial correlation is used (equivalent to the Pearson correlation).	How do you find the correlation between two categorical variables
2172	AdvantagesCost Effective. As the task of assignment ogf random number to different items of population is over, the process is half done.  Involves lesser degree of judgment.  Comparatively easier way of sampling.  Less time consuming.  Can be done even by non- technical persons.  Sample representative of population.	What are the advantages and disadvantages of probability sampling
7609	It's a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction.	What is a loss function machine learning
381	One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.	What does hot encoding mean
6737	LSTM stands for long short term memory. It is a model or architecture that extends the memory of recurrent neural networks. Typically, recurrent neural networks have 'short term memory' in that they use persistent previous information to be used in the current neural network.	What is Lstm and how it works
1592	"Etymologically speaking, it's my understanding that kernel is a modernization of cyrnel (Old English, meaning seed ; it's also the word that corn ""stems"" from, if you'll forgive the pun). A kernel in that context is something from which the rest grows."	Why is it called a kernel
2645	Learning how to use machine learning isn't any harder than learning any other set of libraries for a programmer. The key is to focus on USING it, not designing the algorithm.  If you're a programmer and it's incredibly hard to learn ML, you're probably trying to learn the wrong things about it.	How hard is it to learn machine learning
1205	In data science, association rules are used to find correlations and co-occurrences between data sets. They are ideally used to explain patterns in data from seemingly independent information repositories, such as relational databases and transactional databases.	What is the applicability of association rules
332	Business Uses The K-means clustering algorithm is used to find groups which have not been explicitly labeled in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets.	What is K means used for
3913	Demographic parity or statistical parity suggests that a predictor is unbiased if the prediction ^y is independent of the protected attribute p so that. Pr(^y|p)=Pr(^y). (2.1) Here, the same proportion of each population are classified as positive.	What is statistical parity
3445	A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic , whereas Markov networks are undirected and may be cyclic.  The underlying graph of a Markov random field may be finite or infinite.	What is the difference between Bayesian and Markov
401	q is the probability of failure 1 - p.	What does Q represent in the context of binomial distributions
2107	PF expresses the ratio of true power used in a circuit to the apparent power delivered to the circuit. A 96% power factor demonstrates more efficiency than a 75% power factor. PF below 95% is considered inefficient in many regions.	How is power factor related to efficiency
488	Replaces an image by the norm of its gradient, as estimated by discrete filters. The Raw filter of the detail panel designates two filters that correspond to the two components of the gradient in the principal directions.	What is gradient norm
3496	Gamma cdf. The gamma distribution is a two-parameter family of curves. The parameters a and b are shape and scale, respectively. p = F ( x | a , b ) = 1 b a Γ ( a ) ∫ 0 x t a − 1 e − t b d t .	What is the CDF of gamma distribution
1360	Visualping is the newest, easiest and most convenient tool to monitor websites changes. Our Chrome app allows to monitor pages with only 1 click directly from the page you wish to monitor. Users receive an email when changes are detected but can also set up a Slack integration for team notifications.	What is Visualping
7059	The group of functions that are minimized are called “loss functions”. A loss function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. A most commonly used method of finding the minimum point of function is “gradient descent”.	What is regression loss
707	How to Calculate a Confusion MatrixYou need a test dataset or a validation dataset with expected outcome values.Make a prediction for each row in your test dataset.From the expected outcomes and predictions count: The number of correct predictions for each class.	How do you calculate confusion matrix
8565	Let's get right into the steps to use Twitter data for sentiment analysis of events:Get Twitter API Credentials:  Setup the API Credentials in Python:  Getting Tweet Data via Streaming API:  Get Sentiment Information:  Plot Sentiment Information:  Set this up on AWS or Google Cloud Platform:	How do I get twitter data for sentiment analysis
1472	The Poisson distribution has the following characteristics: It is a discrete distribution. Each occurrence is independent of the other occurrences. It describes discrete occurrences over an interval. The occurrences in each interval can range from zero to infinity.	What is Poisson distribution and its characteristics
6744	Whereas most machine learning based object categorization algorithms require training on hundreds or thousands of samples/images and very large datasets, one-shot learning aims to learn information about object categories from one, or only a few, training samples/images.	What is one shot learning in neural networks
5067	The primary use of interpolation is to help users, be they scientists, photographers, engineers or mathematicians, determine what data might exist outside of their collected data. Outside the domain of mathematics, interpolation is frequently used to scale images and to convert the sampling rate of digital signals.	What are the uses of interpolation
3121	The Cramér-Rao Inequality provides a lower bound for the variance of an unbiased estimator of a parameter. It allows us to conclude that an unbiased estimator is a minimum variance unbiased estimator for a parameter.	What is the meaning of Cramér Rao inequality in statistics
397	The Cox proportional-hazards model (Cox, 1972) is essentially a regression model commonly used statistical in medical research for investigating the association between the survival time of patients and one or more predictor variables.	What is Cox regression survival analysis
4288	An autoregressive integrated moving average, or ARIMA, is a statistical analysis model that uses time series data to either better understand the data set or to predict future trends.	What is Arima model used for
5428	Quantile regression forests (QRF) is an extension of random forests developed by Nicolai Meinshausen that provides non-parametric estimates of the median predicted value as well as prediction quantiles. It therefore allows spatially explicit non-parametric estimates of model uncertainty.	What is quantile random forest
5512	In natural language processing, perplexity is a way of evaluating language models. A language model is a probability distribution over entire sentences or texts.	What does perplexity mean in NLP
505	KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.  KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point.	What is KNN algorithm
1177	Standard Deviation: The Difference. The standard deviation (SD) measures the amount of variability, or dispersion, from the individual data values to the mean, while the standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean.	Is the standard deviation the error
4040	The next big thing after deep learning Artificial General Intelligence (AGI) that is building machines that can surpass human intelligence. The next big thing after deep learning Artificial General Intelligence (AGI) that is building machines that can surpass human intelligence.	What's next after deep learning
5742	In SWedge, the Gamma distribution can be useful for any variable which is always positive, such as cohesion or shear strength for example. The Gamma distribution has the following probability density function: where G(a) is the Gamma function, and the parameters a and b are both positive, i.e. a > 0 and b > 0.	When do we use gamma distribution
5658	Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.  The mutual information between two random variables X and Y can be stated formally as follows: I(X ; Y) = H(X) – H(X | Y)	How is mutual information calculated
350	A good example of the advantages of Bayesian statistics is the comparison of two data sets. Classical statistical procedures are F-test for testing the equality of variances and t test for testing the equality of means of two groups of outcomes.	Why is Bayesian statistics better
6248	Analytics helps you form hypotheses, while statistics lets you test them. Statisticians help you test whether it's sensible to behave as though the phenomenon an analyst found in the current dataset also applies beyond it.	What is the difference between analytics and statistics
4890	Multiclass classification: classification task with more than two classes. Each sample can only be labelled as one class. For example, classification using features extracted from a set of images of fruit, where each image may either be of an orange, an apple, or a pear.	What is multi class classification in machine learning
711	A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.	What does correlation matrix tell us
8046	Class limits specify the span of data values that fall within a class. Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next.	What are class boundaries used for
905	This type of index is called an inverted index, namely because it is an inversion of the forward index.  In some search engines the index includes additional information such as frequency of the terms, e.g. how often a term occurs in each document, or the position of the term in each document.	Why is it called inverted index
5912	False positive rate (FPR) is a measure of accuracy for a test: be it a medical diagnostic test, a machine learning model, or something else. In technical terms, the false positive rate is defined as the probability of falsely rejecting the null hypothesis.	What does a false positive rate mean
7375	"Because when you are constructing a linear regression model you are assuming that your dependent variable ""Y"" is normally distributed. But when you have a binary dependent variable, this assumption is heavily violated. Thus, it doesn't makes sense to use linear regression when your dependent variable is binary."	Can you use linear regression for a binary dependent variable
1332	Anchor boxes are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect and are typically chosen based on object sizes in your training datasets.	What is anchor box in object detection
4640	Statistics Definitions > A random walk is a sequence of discrete, fixed-length steps in random directions. Random walks may be 1-dimensional, 2-dimensional, or n-dimensional for any n. A random walk can also be confined to a lattice.	What is random walk in statistics
677	a description of the effect of two or more predictor variables on an outcome variable that allows for interaction effects among the predictors. This is in contrast to an additive model, which sums the individual effects of several predictors on an outcome.	What is multiplicative model
1685	Naive Bayes is a Supervised Machine Learning algorithm based on the Bayes Theorem that is used to solve classification problems by following a probabilistic approach. It is based on the idea that the predictor variables in a Machine Learning model are independent of each other.	What is naive Bayes in R
653	Each node in the decision tree works on a random subset of features to calculate the output. The random forest then combines the output of individual decision trees to generate the final output.  The Random Forest Algorithm combines the output of multiple (randomly created) Decision Trees to generate the final output.	What is decision tree and random forest
7834	Terms in this set (10)classification provides information in a shorthand form, and shorthand form leads to:  when you simplify through classification, you inevitably lose:  Although things are improving, there can still be a stigma (disgrace) associated with having a psychiatric diagnosis (T/F)More items	What are the disadvantages of classification
7383	A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array.  It is a term and set of techniques known in machine learning in the training and operation of deep learning models can be described in terms of tensors.	What is a tensor ML
1180	Decision trees: Are popular among non-statisticians as they produce a model that is very easy to interpret. Each leaf node is presented as an if/then rule.	Is a decision tree a model
2031	The probability formula is used to compute the probability of an event to occur. To recall, the likelihood of an event happening is called probability.Basic Probability Formulas.All Probability Formulas List in MathsConditional ProbabilityP(A | B) = P(A∩B) / P(B)Bayes FormulaP(A | B) = P(B | A) ⋅ P(A) / P(B)5 more rows	What is the formula of probability
4303	The hazard rate refers to the rate of death for an item of a given age (x). It is part of a larger equation called the hazard function, which analyzes the likelihood that an item will survive to a certain point in time based on its survival to an earlier time (t).	What is the hazard rate function
2246	There are 4 types of random sampling techniques:Simple Random Sampling. Simple random sampling requires using randomly generated numbers to choose a sample.  Stratified Random Sampling.  Cluster Random Sampling.  Systematic Random Sampling.	What are the four types of random sampling
314	An efficient portfolio is either a portfolio that offers the highest expected return for a given level of risk, or one with the lowest level of risk for a given expected return.  The efficient frontier represents that set of portfolios that has the maximum rate of return for every given level of risk.	What do you understand by efficient portfolio and efficient frontier
6552	NLP is short for natural language processing while NLU is the shorthand for natural language understanding.  They share a common goal of making sense of concepts represented in unstructured data, like language, as opposed to structured data like statistics, actions, etc.	What is the difference between NLP and NLU
1256	Logistic regression is a powerful machine learning algorithm that utilizes a sigmoid function and works best on binary classification problems, although it can be used on multi-class classification problems through the “one vs. all” method. Logistic regression (despite its name) is not fit for regression tasks.	Is logistic regression only for binary classification
2794	The Skills You Need to Work in Artificial IntelligenceMath: statistics, probability, predictions, calculus, algebra, Bayesian algorithms and logic.Science: physics, mechanics, cognitive learning theory, language processing.Computer science: data structures, programming, logic and efficiency.	What skills will be required for artificial intelligence
3229	A Simultaneous Equation Model (SEM) is a model in the form of a set of linear simultaneous equations.  The system is jointly determined by the equations in the system; In other words, the system exhibits some type of simultaneity or “back and forth” causation between the X and Y variables.	What is simultaneous equation model in econometrics
951	The correlation, denoted by r, measures the amount of linear association between two variables.  The R-squared value, denoted by R 2, is the square of the correlation. It measures the proportion of variation in the dependent variable that can be attributed to the independent variable.	Is r squared the same as correlation
3408	The percentage tells you what percentage of data to remove. For example, with a 5% trimmed mean, the lowest 5% and highest 5% of the data are excluded. The mean is calculated from the remaining 90% of data points.	What is trimming percentage
7433	1:136:50Suggested clip · 113 secondsZ Scores and Normal Distributions (Example Problems) - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the z score of a problem
166	Here are four common types of a learning curve and what they mean:Diminishing-Returns Learning Curve. The rate of progression increases rapidly at the beginning and then decreases over time.  Increasing-Returns Learning Curve.  Increasing-Decreasing Return Learning Curve (the S-curve)  Complex Learning Curve.	What are the types of learning curves
1118	A hierarchical clustering is a set of nested clusters that are arranged as a tree. K Means clustering is found to work well when the structure of the clusters is hyper spherical (like circle in 2D, sphere in 3D). Hierarchical clustering don't work as well as, k means when the shape of the clusters is hyper spherical.	When to use hierarchical clustering vs K means
7326	The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(∗). Both functions will take any number and rescale it to fall between 0 and 1.	What is difference between logit and probit model
7726	Leaky ReLU. Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when x < 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so).	What is leaky ReLU activation and why is it used
711	Origin of the Term The term “Receiver Operating Characteristic” has its roots in World War II. ROC curves were originally developed by the British as part of the “Chain Home” radar system. ROC analysis was used to analyze radar data to differentiate between enemy aircraft and signal noise (e.g. flocks of geese).	What is the origin of the receiver operating characteristic ROC terminology logistic roc history statistics
8001	Probabilistic data structures are a group of data structures that are extremely useful for big data and streaming applications. Generally speaking, these data structures use hash functions to randomize and compactly represent a set of items.	What is a probabilistic data structure
3172	10 Ways to Improve Your Machine Learning ModelsStudying learning curves. As a first step to improving your results, you need to determine the problems with your model.  Using cross-validation correctly.  Choosing the right error or score metric.  Searching for the best hyper-parameters.  Testing multiple models.  Averaging models.  Stacking models.  Applying feature engineering.More items	How do you optimize a machine learning model
3183	Digital image processing is the use of computer algorithms to perform image processing on digital images . Image Acquisition Image Restoration Morphological Processing Segmentation Representation & Description Image Enhancement Object Recognition Problem Domain Colour Image Processing Image Compression.	What is image processing explain all the steps
8537	The three main methods to perform linear regression analysis in Excel are: Regression tool included with Analysis ToolPak. Scatter chart with a trendline.	What method does Excel use for linear regression
7580	The marks for a group of students before (pre) and after (post) a teaching intervention are recorded below: Marks are continuous (scale) data. Continuous data are often summarised by giving their average and standard deviation (SD), and the paired t-test is used to compare the means of the two samples of related data.	What statistical test to use to compare pre and post tests
7101	Parameters are key to machine learning algorithms.  In this case, a parameter is a function argument that could have one of a range of values. In machine learning, the specific model you are using is the function and requires parameters in order to make a prediction on new data.	What are parameters in machine learning
1167	Active Learning StrategiesGroup Activities. Case-based learning. Case-based learning requires students to apply their knowledge to reach a conclusion about an open-ended, real-world situation.  Individual Activities. Application cards.  Partner Activities. Role playing.  Visual Organizing Activities. Categorizing grids.	What are some strategies that support active learning
1133	The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data.	What is bootstrap in machine learning
2325	Simple linear regression relates X to Y through an equation of the form Y = a + bX. Both quantify the direction and strength of the relationship between two numeric variables.  The correlation squared (r2 or R2) has special meaning in simple linear regression.	Does linear regression show correlation
1079	hamming distance	Which distance metric do we use in Knn for categorical variables
1114	No, it does not establish the divergence of an alternating series unless it fails the test by violating the condition limn→∞bn=0 , which is essentially the Divergence Test; therefore, it established the divergence in this case.	Can you prove divergence with the alternating series test
6907	A statistics is ancillary if its distribution does not depend on θ. More precisely, a statistic S(X) is ancillary for Θ it its distribution is the same for all θ ∈ Θ. That is, Pθ(S(X) ∈ A) is constant for θ ∈ Θ for any set A. (Xi − ¯X)2.	How do you show ancillary statistics
5503	Overview. Algorithmic probability deals with the following questions: Given a body of data about some phenomenon that we want to understand, how can we select the most probable hypothesis of how it was caused from among all possible hypotheses and how can we evaluate the different hypotheses?	What are the main concepts in algorithmic probability
6462	For quick and visual identification of a normal distribution, use a QQ plot if you have only one variable to look at and a Box Plot if you have many. Use a histogram if you need to present your results to a non-statistical public. As a statistical test to confirm your hypothesis, use the Shapiro Wilk test.	How do you check if the data is normally distributed
5301	Recurrent Neural Networks(RNN) are a type of Neural Network where the output from the previous step is fed as input to the current step. RNN's are mainly used for, Sequence Classification — Sentiment Classification & Video Classification.	Can RNN be used for classification
6082	The generalized Kronecker delta or multi-index Kronecker delta of order 2p is a type (p,p) tensor that is a completely antisymmetric in its p upper indices, and also in its p lower indices.	Is Kronecker delta a tensor
4810	To calculate the variance follow these steps:Work out the Mean (the simple average of the numbers)Then for each number: subtract the Mean and square the result (the squared difference).Then work out the average of those squared differences. (Why Square?)	How do you calculate sample variance
193	Neural network structures/arranges algorithms in layers of fashion, that can learn and make intelligent decisions on its own. Whereas in Machine learning the decisions are made based on what it has learned only. Machine learning models/methods or learnings can be two types supervised and unsupervised learnings.	What is the difference between neural networks and machine learning
6262	The most popular supervised NLP machine learning algorithms are: Support Vector Machines. Bayesian Networks. Maximum Entropy.	Which algorithm is used in NLP
5443	In short, when a dependent variable is not distributed normally, linear regression remains a statistically sound technique in studies of large sample sizes. Figure 2 provides appropriate sample sizes (i.e., >3000) where linear regression techniques still can be used even if normality assumption is violated.	Are linear regression techniques appropriate for analysis when the dependent outcome variable is not normally distributed
2391	Classical planning concentrates on problems where most actions leave most things unchanged. Think of a world consisting of a bunch of objects on a flat surface. The action of nudging an object causes that object to change its lo- cation by a vector ∆.	What is classical planning in AI
2775	The low R-squared graph shows that even noisy, high-variability data can have a significant trend. The trend indicates that the predictor variable still provides information about the response even though data points fall further from the regression line.  To assess the precision, we'll look at prediction intervals.	What does a low R Squared mean in regression
5896	Gradient boosting classifiers are a group of machine learning algorithms that combine many weak learning models together to create a strong predictive model. Decision trees are usually used when doing gradient boosting.	What is gradient boosting classifier
2898	A consecutive-k-out-of-n system is a system with n components arranged either linearly or circularly, which fails if and only if at least k consecutive components fail. An (n, f, k) system further requires that the total number of failed components is less than f for the system to be working.	What is K out of N system
606	3 Answers. Adjusted R2 is the better model when you compare models that have a different amount of variables. The logic behind it is, that R2 always increases when the number of variables increases. Meaning that even if you add a useless variable to you model, your R2 will still increase.	Should I use R Squared or adjusted R squared
1339	Three things influence the margin of error in a confidence interval estimate of a population mean: sample size, variability in the population, and confidence level.  Answer: As sample size increases, the margin of error decreases. As the variability in the population increases, the margin of error increases.	Does sample mean affect margin of error
598	A (real-valued) random variable, often denoted by X (or some other capital letter), is a function mapping a probability space (S, P) into the real line R. This is shown in Figure 1. Associated with each point s in the domain S the function X assigns one and only one value X(s) in the range R.	What is a function of a random variable
4846	Definition of a Non-Randomized Trial. • A study where participants have been assigned to the. treatment, procedure, or intervention alternatives by a. method that is not random.	What is non randomized test
6154	As such, sparse coding is closely related to compressed sensing, but compressed sensing specifically deals with finding the sparsest solution to an under-determined set of linear equations which, as the theory shows, is the correct solution in this case with high probability.	What relationships are there between compressed sensing and sparse coding
6858	For example, the amount of time (beginning now) until an earthquake occurs has an exponential distribution. Other examples include the length of time, in minutes, of long distance business telephone calls, and the amount of time, in months, a car battery lasts.	What is exponential distribution example
6079	A method for solving such problems by using matrix iterations is presented. In this method a related linear eigenvalue problem describing the perturbation of the solution from a nominal approximation is solved and updated successively until convergence.	What is Matrix iteration method
2833	Essentially, a control variable is what is kept the same throughout the experiment, and it is not of primary concern in the experimental outcome. Any change in a control variable in an experiment would invalidate the correlation of dependent variables (DV) to the independent variable (IV), thus skewing the results.	How do control variables affect both dependent and independent variables
71	Use the formula (zy)i = (yi – ȳ) / s y and calculate a standardized value for each yi. Add the products from the last step together. Divide the sum from the previous step by n – 1, where n is the total number of points in our set of paired data. The result of all of this is the correlation coefficient r.	How do you find the correlation coefficient of data
3377	Data Preprocessing With R: Hands-On TutorialDealing with missing data.Dealing with categorical data.Splitting the dataset into training and testing sets.Scaling the features.	How do I preprocess a dataset in R
6679	Random initialization refers to the practice of using random numbers to initialize the weights of a machine learning model. Random initialization is one way of performing symmetry breaking, which is the act of preventing all of the weights in the machine learning model from being the same.	What is random initialization
1081	Probit regression, also called a probit model, is used to model dichotomous or binary outcome variables. In the probit model, the inverse standard normal distribution of the probability is modeled as a linear combination of the predictors.	What is probit regression used for
704	"However, experts expect that it won't be until 2060 until AGI has gotten good enough to pass a ""consciousness test"". In other words, we're probably looking at 40 years from now before we see an AI that could pass for a human."	How far away is artificial general intelligence
185	0:0010:19Suggested clip · 120 secondsThe Power Spectral Density - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret power spectral density
923	The name 'variational' comes most likely from the fact that it searches for distribution q that optimizes ELBO, and this setup is kind of like in calculus of variations, a field that studies optimization over functions (for example, problems like: given a family of curves in 2D between two points, find one with	What does variational mean
6599	The coefficient of determination (denoted by R2) is a key output of regression analysis. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.  An R2 of 0 means that the dependent variable cannot be predicted from the independent variable.	What is the meaning of the coefficient of determination r2
1419	Suppose we conduct a Poisson experiment, in which the average number of successes within a given region is μ. Then, the Poisson probability is: P(x; μ) = (e-μ) (μx) / x! where x is the actual number of successes that result from the experiment, and e is approximately equal to 2.71828.	How do you do Poisson distribution
922	A validation dataset is a sample of data held back from training your model that is used to give an estimate of model skill while tuning model's hyperparameters.  Procedures that you can use to make the best use of validation and test datasets when evaluating your models.	What is validation set in ML
2617	The probability of a specific value of a continuous random variable will be zero because the area under a point is zero.	Why is the probability that a continuous random variable is equal to a single number zero
8061	The cumulative frequency is calculated by adding each frequency from a frequency distribution table to the sum of its predecessors. The last value will always be equal to the total for all observations, since all frequencies will already have been added to the previous total.	How do you find the cumulative frequency distribution
2438	""" The value(s) assigned to a population parameter based on the value of a sample statistic is called an estimate. The sample statistic used to estimate a population param-eter is called an estimator."""	What is the difference between estimate and estimator in statistics
36	PythonC++CUDA	What language is TensorFlow in
2838	A random variable is a numerical description of the outcome of a statistical experiment.  For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x). This function provides the probability for each value of the random variable.	What is random variables in probability
1324	Task parallelism is the simultaneous execution on multiple cores of many different functions across the same or different datasets. Data parallelism (aka SIMD) is the simultaneous execution on multiple cores of the same function across the elements of a dataset.	What is the difference between data parallelism and task parallelism
1582	In short, the beta distribution can be understood as representing a probability distribution of probabilities- that is, it represents all the possible values of a probability when we don't know what that probability is.	What is the beta distribution used for
1085	In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning.	What is PAC in machine learning
2830	PDF. Typically, machine learning algorithms accept parameters that can be used to control certain properties of the training process and of the resulting ML model. In Amazon Machine Learning, these are called training parameters.	What are training parameters
6539	Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.	What is ordinal logistic regression used for
688	The harmonic mean is a type of numerical average. It is calculated by dividing the number of observations by the reciprocal of each number in the series. Thus, the harmonic mean is the reciprocal of the arithmetic mean of the reciprocals.	What is the meaning of harmonic mean
6882	To teach an algorithm how to recognise objects in images, we use a specific type of Artificial Neural Network: a Convolutional Neural Network (CNN). Their name stems from one of the most important operations in the network: convolution. Convolutional Neural Networks are inspired by the brain.	Why is it called convolutional neural network
2543	Models that are pre-trained on ImageNet are good at detecting high-level features like edges, patterns, etc. These models understand certain feature representations, which can be reused.	Why it is beneficial to use pre trained models
7069	Techniques to reduce underfitting :Increase model complexity.Increase number of features, performing feature engineering.Remove noise from the data.Increase the number of epochs or increase the duration of training to get better results.	How do you prevent Underfitting in machine learning
7635	2.3. Random forest (RF) is an ensemble classifier that uses multiple models of several DTs to obtain a better prediction performance. It creates many classification trees and a bootstrap sample technique is used to train each tree from the set of training data.	What is Random Forest model used for
4333	Softmax is a function :) It is mainly used to normalize neural networks output to fit between zero and one. It is used to represent the certainty “probability” in the network output.	What is Softmax function in CNN
1144	Definition of outliers. An outlier is an observation that lies an abnormal distance from other values in a random sample from a population.	What defines an outlier
4460	Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.	What is a logistic regression used for
8626	The area under the ROC curve (AUC) results were considered excellent for AUC values between 0.9-1, good for AUC values between 0.8-0.9, fair for AUC values between 0.7-0.8, poor for AUC values between 0.6-0.7 and failed for AUC values between 0.5-0.6.	What is considered a good AUC score
8578	Cosine Proximity / Cosine Similarity Cosine similarity is a measure of similarity between two vectors. The mathematical representation is — — given two vectors A and B, where A represents the prediction vector and B represents the target vector. A higher cosine proximity/similarity indicates a higher accuracy.	What is cosine proximity
462	Train the model using a suitable machine learning algorithm such as SVM (Support Vector Machines), decision trees, random forest, etc. Training is the process through which the model learns or recognizes the patterns in the given data for making suitable predictions. The test set contains already predicted values.	Which machine learning technique is used for pattern recognition
8642	Imbalanced data typically refers to a classification problem where the number of observations per class is not equally distributed; often you'll have a large amount of data/observations for one class (referred to as the majority class), and much fewer observations for one or more other classes (referred to as the	What is the problem with imbalanced data
542	Predictive analytics are used to determine customer responses or purchases, as well as promote cross-sell opportunities. Predictive models help businesses attract, retain and grow their most profitable customers. Improving operations. Many companies use predictive models to forecast inventory and manage resources.	What is predictive analytics used for
493	The first thing you do is use the z-score formula to figure out what the z-score is. In this case, it is the difference between 30 and 21, which is 9, divided by the standard deviation of 5, which gives you a z-score of 1.8. If you look at the z-table below, that gives you a probability value of 0.9641.	How do you convert probability to Z score
5921	In mathematics, the binary logarithm (log2 n) is the power to which the number 2 must be raised to obtain the value n. That is, for any real number x, For example, the binary logarithm of 1 is 0, the binary logarithm of 2 is 1, the binary logarithm of 4 is 2, and the binary logarithm of 32 is 5.	What is a logarithm 2
6718	7 Techniques to Handle Imbalanced DataUse the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	How do you balance a dataset
496	We shall look at 5 popular clustering algorithms that every data scientist should be aware of.K-means Clustering Algorithm.  Mean-Shift Clustering Algorithm.  DBSCAN – Density-Based Spatial Clustering of Applications with Noise.  EM using GMM – Expectation-Maximization (EM) Clustering using Gaussian Mixture Models (GMM)More items•	What are the most popular clustering algorithms
6336	Difference between K means and Hierarchical Clusteringk-means ClusteringHierarchical ClusteringK Means clustering needed advance knowledge of K i.e. no. of clusters one want to divide your data.In hierarchical clustering one can stop at any number of clusters, one find appropriate by interpreting the dendrogram.8 more rows•	What are the similarities and differences between average link clustering and K means
891	The Kappa Architecture was first described by Jay Kreps. It focuses on only processing data as a stream. It is not a replacement for the Lambda Architecture, except for where your use case fits.  The idea is to handle both real-time data processing and continuous reprocessing in a single stream processing engine.	What is Lambda and Kappa architecture
588	A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  Restricted Boltzmann machines can also be used in deep learning networks.	What is RBM in deep learning
2927	The input layer has its own weights that multiply the incoming data. The input layer then passes the data through the activation function before passing it on. The data is then multiplied by the first hidden layer's weights.	Does the input layer have weights
7508	Principal Component Analysis (PCA) is a popular dimensionality reduction technique used in Machine Learning applications. PCA condenses information from a large set of variables into fewer variables by applying some sort of transformation onto them.	What is principal component analysis in image processing
3999	The 95% confidence interval (CI) is a range of values calculated from our data, that most likely, includes the true value of what we're estimating about the population.	What is confidence interval in machine learning
3040	Parallel analysis is a method for determining the number of components or factors to retain from pca or factor analysis. Essentially, the program works by creating a random dataset with the same numbers of observations and variables as the original data.	What is parallel analysis in factor analysis
238	"The modern mathematical theory of probability has its roots in attempts to analyze games of chance by Gerolamo Cardano in the sixteenth century, and by Pierre de Fermat and Blaise Pascal in the seventeenth century (for example the ""problem of points"")."	Who developed the theory of probability
7478	So we need 2 things in order to apply reinforcement learning. Agent: An AI algorithm.Thus following are the steps to create an environment.Create a Simulation.Add a State vector which represents the internal state of the Simulation.Add a Reward system into the Simulation.	How can we create environment for reinforcement learning
485	Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.	How does batch normalization help optimization
4	Hashing is the transformation of a string of characters into a usually shorter fixed-length value or key that represents the original string. Hashing is used to index and retrieve items in a database because it is faster to find the item using the shorter hashed key than to find it using the original value.	What is the point of hashing
5538	Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.	What is Hyperplane in machine learning
334	The effect size is the main finding of a quantitative study. While a P value can inform the reader whether an effect exists, the P value will not reveal the size of the effect.	Is effect size the same as P value
24	An affine function is the composition of a linear function with a translation, so while the linear part fixes the origin, the translation can map it somewhere else.  While affine functions don't preserve the origin, they do preserve some of the other geometry of the space, such as the collection of straight lines.	What is the difference between affine and linear
4309	Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.  Interrupted time series analysis is the analysis of interventions on a single time series. Time series data have a natural temporal ordering.	What is an example of time series data
509	In Gradient Descent or Batch Gradient Descent, we use the whole training data per epoch whereas, in Stochastic Gradient Descent, we use only single training example per epoch and Mini-batch Gradient Descent lies in between of these two extremes, in which we can use a mini-batch(small portion) of training data per epoch	What is the difference between gradient descent and stochastic gradient descent
2555	3 layers	How many layers are there in deep learning
1071	Discriminant function analysis (DFA) is a statistical procedure that classifies unknown individuals and the probability of their classification into a certain group (such as sex or ancestry group). Discriminant function analysis makes the assumption that the sample is normally distributed for the trait.	What are discriminant functions
397	Well, if you break down the words, forward implies moving ahead and propagation is a term for saying spreading of anything. forward propagation means we are moving in only one direction, from input to the output, in a neural network.	What is forward propagation in machine learning
1653	"Linear least squares regression is by far the most widely used modeling method. It is what most people mean when they say they have used ""regression"", ""linear regression"" or ""least squares"" to fit a model to their data."	Is linear regression A least squares
191	In mathematics, a system of equations is considered overdetermined if there are more equations than unknowns. An overdetermined system is almost always inconsistent (it has no solution) when constructed with random coefficients.  Such systems usually have an infinite number of solutions.	Does an inconsistent linear system with more unknowns than equations exist
6647	approximation of the expected error called the empirical error which is the average error on the. training set. Given a function f, a loss function V , and a training set S consisting of n data points, the empirical error of f is: IS[f] =	What is an empirical error in machine learning
985	In general, an LSTM can be used for classification or regression; it is essentially just a standard neural network that takes as input, in addition to input from that time step, a hidden state from the previous time step. So, just as a NN can be used for classification or regression, so can an LSTM.	Can Lstm be used for classification
265	Probability distributions are a fundamental concept in statistics. They are used both on a theoretical level and a practical level. Some practical uses of probability distributions are: To calculate confidence intervals for parameters and to calculate critical regions for hypothesis tests.	Why do we use probability distribution
123	Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset.  Large numbers of input features can cause poor performance for machine learning algorithms. Dimensionality reduction is a general field of study concerned with reducing the number of input features.	What is Dimension reduction in machine learning
1428	The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word. In word2vec, a distributed representation of a word is used.	What is the output of Word2Vec
683	Unbiasedness implies consistency, whereas a consistent estimator can be biased.	Which of the following describes the difference if any between an unbiased and a consistent estimator
6446	Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.	Where do we use decision tree
2748	Hashing is the practice of using an algorithm to map data of any size to a fixed length. This is called a hash value (or sometimes hash code or hash sums or even a hash digest if you're feeling fancy). Whereas encryption is a two-way function, hashing is a one-way function.  Every hash value is unique.	What is the difference between hash and hashing
4921	For a dichotomous categorical variable and a continuous variable you can calculate a Pearson correlation if the categorical variable has a 0/1-coding for the categories.  But when you have more than two categories for the categorical variable the Pearson correlation is not appropriate anymore.	Can you use categorical variables in correlation
8325	Observer bias and other “experimenter effects” occur when researchers' expectations influence study outcome.  To minimize bias, it is good practice to work “blind,” meaning that experimenters are unaware of the identity or treatment group of their subjects while conducting research.	What should have been done to minimize the effects of experimenter bias on the outcome of the study
1381	What factors inhibit collective intelligence?In-group bias.  Out-group homogeneity bias.  Groupthink, bandwagon effect, herd behavior.  Facilitation and loafing .  Group polarization.  Biased use of information and the common knowledge effect.  Risky shift.  Distortions in multi-level group decisions.	What factors inhibit collective intelligence
746	Linear mixed models (sometimes called “multilevel models” or “hierarchical models”, depending on the context) are a type of regression model that take into account both (1) variation that is explained by the independent variables of interest (like lm() ) – fixed effects, and (2) variation that is not explained by the	What does a linear mixed model tell you
7828	Uncertainty is a popular phenomenon in machine learning and a variety of methods to model uncertainty at different levels has been developed.  Different types of uncertainty can be observed: (i) Input data are subject to noise, outliers, and errors.	What is uncertainty in machine learning
870	ROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests.  In addition, the area under the ROC curve gives an idea about the benefit of using the test(s) in question.	Why do we use ROC curves
7784	In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.	What is GLM in statistics
7054	EXC functions both find a requested quartile of a supplied data set. The difference between these two functions is that QUARTILE. INC bases its calculation on a percentile range of 0 to 1 inclusive, whereas QUARTILE. EXC bases its calculation on a percentile range of 0 to 1 exclusive.	What is the difference between quartile exc and quartile inc
1224	Origin of the term. The earliest reference to the concept of a confusion matrix appears to have been made by Karl Pearson in 1904 “on the Theory of Contingency and Its Relation to Association and Normal Correlation” [3].	What is the origin of the term confusion matrix
432	Model specification refers to the determination of which independent variables should be included in or excluded from a regression equation.  A multiple regression model is, in fact, a theoretical statement about the causal relationship between one or more independent variables and a dependent variable.	What is a model in regression analysis
1997	If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical.  If skewness is less than −1 or greater than +1, the distribution is highly skewed.	What does it mean when the skewness is negative
318	A recurrent neural network (RNN) is a type of artificial neural network commonly used in speech recognition and natural language processing (NLP). RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario.	What are the purposes of using recurrent neural networks
5175	Example 1: Draw a box-and-whisker plot for the data set {3, 7, 8, 5, 12, 14, 21, 13, 18}.  The box part represents the interquartile range and represents approximately the middle 50% of all the data. The data is divided into four regions, which each represent approximately 25% of the data.	What is an example of a box plot
7162	Order the values of a data set of size n from smallest to largest. If n is odd, the sample median is the value in position (n + 1)/2; if n is even, it is the average of the values in positions n/2 and n/2 + 1.	How do you find the sample median in statistics
1148	BFS vs DFS BFS stands for Breadth First Search. DFS stands for Depth First Search. 2. BFS(Breadth First Search) uses Queue data structure for finding the shortest path. DFS(Depth First Search) uses Stack data structure.	What is BFS and DFS algorithm
180	Note the difference between parameters and arguments: Function parameters are the names listed in the function's definition. Function arguments are the real values passed to the function. Parameters are initialized to the values of the arguments supplied.	What is the difference between an argument and a parameter
8454	The expression double standard originally referred to 18th- and 19th-century economic policies of bimetallism. Bimetallism was a monetary system that was based on two metals—a double standard, in its financial “prescribed value” sense, of gold and silver.	Where do double standards come from
5332	The standardized mean difference (SMD) measure of effect is used when studies report efficacy in terms of a continuous measurement, such as a score on a pain-intensity rating scale. The SMD is also known as Cohen's d.  The SMD is a point estimate of the effect of a treatment.	What is standardized difference mean
7318	Mentor: Well, if the line is a good fit for the data then the residual plot will be random. However, if the line is a bad fit for the data then the plot of the residuals will have a pattern.	How can you tell if a residual plot is a good fit for the data
6307	Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association.	What is the use of bivariate analysis
6871	2 Answers. Normalization would be required if you are doing some form a similarity measurement. Dummy variables by its nature acts as a binary switch.  Usually, normalization is used when the variables are measured on different scales such that a proper comparison is not possible.	Should I normalize dummy variables
3540	In statistics, a Poisson distribution is a statistical distribution that shows how many times an event is likely to occur within a specified period of time. It is used for independent events which occur at a constant rate within a given interval of time.	What is Poisson distribution in statistics
1202	Getting Familiar with ML Pipelines A machine learning pipeline is used to help automate machine learning workflows. They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative.	What do you think is important in a machine learning Pipeline
5653	Nevertheless, the same has been delineated briefly below:Step 1: Visualize the Time Series. It is essential to analyze the trends prior to building any kind of time series model.  Step 2: Stationarize the Series.  Step 3: Find Optimal Parameters.  Step 4: Build ARIMA Model.  Step 5: Make Predictions.	How do you analyze time series data
3675	Top 10 Free Resources To Learn Reinforcement Learning1| Reinforcement Learning Explained. Source: edX.  2| Reinforcement Learning.  3| Advanced Deep Learning & Reinforcement Learning.  4| Deep Reinforcement Learning.  5| An Introduction to Reinforcement Learning.  6| An Introduction to Reinforcement Learning.  8| Reinforcement Learning Specialisation.  9| Reinforcement Learning.More items•	Where can I learn reinforcement
4554	Credible intervals capture our current uncertainty in the location of the parameter values and thus can be interpreted as probabilistic statement about the parameter. In contrast, confidence intervals capture the uncertainty about the interval we have obtained (i.e., whether it contains the true value or not).	What is the difference between a confidence interval and a credible interval
2762	Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.	What is Adam Optimizer in neural network
2395	An almost essential property is that the estimator should be consistent: T is a consistent estimator of θ if T converges to θ in probability as n → ∞. Consistency implies that, as the sample size increases, any bias in T tends to 0 and the variance of T also tends to 0.	What are the properties of estimators
5363	SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs.	How does SVM predict
2384	A recurrent neural network is shown one input each timestep and predicts one output. Conceptually, BPTT works by unrolling all input timesteps. Each timestep has one input timestep, one copy of the network, and one output. Errors are then calculated and accumulated for each timestep.	How does backpropagation work in RNN
1010	First, let's review how to calculate the population standard deviation:Calculate the mean (simple average of the numbers).For each number: Subtract the mean. Square the result.Calculate the mean of those squared differences.  Take the square root of that to obtain the population standard deviation.	How do you find the population standard deviation
6548	In statistics and control theory, Kalman filtering, also known as linear quadratic estimation (LQE), is an algorithm that uses a series of measurements observed over time, containing statistical noise and other inaccuracies, and produces estimates of unknown variables that tend to be more accurate than those based on a	What does the Kalman filter do
4764	Markov Chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its stationary distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution.	How does MCMC sampling work
3358	Alternate-form reliability is the consistency of test results between two different – but equivalent – forms of a test. Alternate-form reliability is used when it is necessary to have two forms of the same tests.  – Alternative-form reliability is needed whenever two test forms are being used to measure the same thing.	What is alternate form of reliability
5524	The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. The expression was coined by Richard E.	What is curse of dimensionality in machine learning
2065	❖ The variable that is used to explain or predict the response variable is called the explanatory variable. It is also sometimes called the independent variable because it is independent of the other variable. ▪ In regression, the order of the variables is very important.	What is an explanatory variable in regression
1011	Gaussian elimination for solving an n × n linear system of equations Ax = b is the archetypal direct method of numerical linear algebra. In this note we point out that GE has an iterative side too.  It is now one of the mainstays of computational science—the archetypal iterative method.	Is Gauss elimination an iterative method
4991	Numeric Outlier is the simplest, nonparametric outlier detection technique in a one-dimensional feature space. The outliers are calculated by means of the IQR (InterQuartile Range).  Using the interquartile multiplier value k=1.5, the range limits are the typical upper and lower whiskers of a box plot.	How are outliers detected using data mining
3697	The input() function accepts an optional string argument called prompt and returns a string. Note that the input() function always returns a string even if you entered a number. To convert it to an integer you can use int() or eval() functions.	What is the datatype of the output for the function input ()
3859	A probability sampling method is any method of sampling that utilizes some form of random selection. In order to have a random selection method, you must set up some process or procedure that assures that the different units in your population have equal probabilities of being chosen.	What is meant by probability sampling
8355	A continuous variable is one which can take on a value between any other two values, such as: indoor temperature, time spent waiting, water consumed, color wavelength, and direction of travel. A discrete variable corresponds to a digital quantity, while a continuous variable corresponds to an analog quantity.	Is time a discrete variable
3130	FAQ Explanation: Volumetric efficiency is the ratio of the volume of charge admitted at N.T.P. to the swept volume of the piston while mechanical efficiency is the ratio of the brake power to the indicated power and relative efficiency is the ratio of the indicated thermal efficiency to the air standard efficiency	What is engine relative efficiency
4195	@shuvayan - Theoretically, 25 to 30% is the maximum missing values are allowed, beyond which we might want to drop the variable from analysis. Practically this varies.At times we get variables with ~50% of missing values but still the customer insist to have it for analyzing.	What percentage of missing data is acceptable
3390	"Factor analysis is as much of a ""test"" as multiple regression (or statistical tests in general) in that it is used to reveal hidden or latent relationships/groupings in one's dataset.  Multiple regression takes data points in some n-dimensional space and finds the best fit line."	How is factor analysis different from multiple regression
7291	Reinforcement learning enables the learning of optimal behavior in tasks that require the selection of sequential actions.  Through repeated interactions with the environment, and the receipt of rewards, the agent learns which actions are associated with the greatest cumulative reward.	What is reinforcement learning in neural network
5578	5:1310:53Suggested clip · 105 secondsStochastic Gradient Descent, Clearly Explained!!! - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do stochastic gradient descent
776	Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).	How do you explain regression
2738	OLS does not require that the error term follows a normal distribution to produce unbiased estimates with the minimum variance. However, satisfying this assumption allows you to perform statistical hypothesis testing and generate reliable confidence intervals and prediction intervals.	Does OLS require normal distribution
2728	Using the Interquartile Rule to Find Outliers Multiply the interquartile range (IQR) by 1.5 (a constant used to discern outliers). Add 1.5 x (IQR) to the third quartile. Any number greater than this is a suspected outlier. Subtract 1.5 x (IQR) from the first quartile.	How do you use interquartile range to find outliers
7796	Predictive analytics is the use of data, statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data. The goal is to go beyond knowing what has happened to providing a best assessment of what will happen in the future.	What do you mean by predictive analytics
1361	How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together.	How do you solve probability distributions
658	Nonparametric statistics refers to a statistical method in which the data are not assumed to come from prescribed models that are determined by a small number of parameters; examples of such models include the normal distribution model and the linear regression model.	What does nonparametric mean in statistics
1346	Conditional probability is probability of a second event given a first event has already occurred.  This is conditional probability with two dependent events. A dependent event is when one event influences the outcome of another event in a probability scenario.	Is conditional probability dependent
5490	Key Takeaways. Standard deviation looks at how spread out a group of numbers is from the mean, by looking at the square root of the variance. The variance measures the average degree to which each point differs from the mean—the average of all data points.	What is the difference between variance and standard deviation
4699	The probability of the intersection of Events A and B is denoted by P(A ∩ B). If Events A and B are mutually exclusive, P(A ∩ B) = 0. The probability that Events A or B occur is the probability of the union of A and B.	How do you find the probability of intersection of two events
5795	Correlation Coefficient Equation The correlation coefficient is determined by dividing the covariance by the product of the two variables' standard deviations. Standard deviation is a measure of the dispersion of data from its average.	How do you calculate a correlation coefficient
6065	Discriminant analysis is a technique that is used by the researcher to analyze the research data when the criterion or the dependent variable is categorical and the predictor or the independent variable is interval in nature.	What type of variables are used in discriminant analysis
5944	NLP is short for natural language processing while NLU is the shorthand for natural language understanding. Similarly named, the concepts both deal with the relationship between natural language (as in, what we as humans speak, not what computers understand) and artificial intelligence.	What is NLP and NLU
8223	Big data analytics as the name suggest is the analysis of big data by discovering hidden patterns or extracting information from it.  Big data has got more to do with High-Performance Computing, while Machine Learning is a part of Data Science. Machine learning performs tasks where human interaction doesn't matter.	Is Big Data Machine Learning
3211	A vector is a quantity or phenomenon that has two independent properties: magnitude and direction. The term also denotes the mathematical or geometrical representation of such a quantity.  A quantity or phenomenon that exhibits magnitude only, with no specific direction, is called a scalar .	What is the term vector
4985	Natural Language Processing (NLP) is the part of AI that studies how machines interact with human language.  Combined with machine learning algorithms, NLP creates systems that learn to perform tasks on their own and get better through experience.	Is NLP a part of machine learning
8595	you can use this formula [(W−K+2P)/S]+1 .W is the input volume - in your case 128.K is the Kernel size - in your case 5.P is the padding - in your case 0 i believe.S is the stride - which you have not provided.	How do you calculate the output of a convolutional layer
3488	Sentiment Analysis is a procedure used to determine if a chunk of text is positive, negative or neutral. In text analytics, natural language processing (NLP) and machine learning (ML) techniques are combined to assign sentiment scores to the topics, categories or entities within a phrase.	How does sentiment analysis work
2401	"For skewed distributions, it is quite common to have one tail of the distribution considerably longer or drawn out relative to the other tail. A ""skewed right"" distribution is one in which the tail is on the right side. A ""skewed left"" distribution is one in which the tail is on the left side."	What does a left skewed distribution mean
1093	Data bias in machine learning is a type of error in which certain elements of a dataset are more heavily weighted and/or represented than others. A biased dataset does not accurately represent a model's use case, resulting in skewed outcomes, low accuracy levels, and analytical errors.	What is the meaning of bias in machine learning
8267	Probability is the study of random events. It is used in analyzing games of chance, genetics, weather prediction, and a myriad of other everyday events. Statistics is the mathematics we use to collect, organize, and interpret numerical data.	What are statistics and probability
8261	8 Methods to Boost the Accuracy of a ModelAdd more data. Having more data is always a good idea.  Treat missing and Outlier values.  Feature Engineering.  Feature Selection.  Multiple algorithms.  Algorithm Tuning.  Ensemble methods.	How can machine learning models improve performance
78	Active learning: Reinforces important material, concepts, and skills. Provides more frequent and immediate feedback to students. Provides students with an opportunity to think about, talk about, and process course material.	What is active learning and why is it important
392	An endogenous variable is a variable in a statistical model that's changed or determined by its relationship with other variables within the model. In other words, an endogenous variable is synonymous with a dependent variable, meaning it correlates with other factors within the system being studied.	What is endogenous variable
820	Like a standard normal distribution (or z-distribution), the t-distribution has a mean of zero. The normal distribution assumes that the population standard deviation is known. The t-distribution does not make this assumption.	How is the t distribution similar to the standard Z
1341	Poisson Formula. P(x; μ) = (e-μ) (μx) / x! where x is the actual number of successes that result from the experiment, and e is approximately equal to 2.71828. The Poisson distribution has the following properties: The mean of the distribution is equal to μ . The variance is also equal to μ .	What is mean in Poisson distribution
177	Multi-label classification is a type of classification in which an object can be categorized into more than one class. For example, In the above dataset, we will classify a picture as the image of a dog or cat and also classify the same image based on the breed of the dog or cat.	What is multi label image classification
3937	Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate.	What is Gru in NLP
2601	The Poisson distribution is used to model the number of events occurring within a given time interval. λ is the shape parameter which indicates the average number of events in the given time interval. The following is the plot of the Poisson probability density function for four values of λ.	What is the parameter of the Poisson distribution
871	In an economic model, an exogenous variable is one whose value is determined outside the model and is imposed on the model, and an exogenous change is a change in an exogenous variable.  In contrast, an endogenous variable is a variable whose value is determined by the model.	What is the difference between endogenous and exogenous variables
6997	The difference between interval and ratio scales comes from their ability to dip below zero. Interval scales hold no true zero and can represent values below zero. For example, you can measure temperature below 0 degrees Celsius, such as -10 degrees. Ratio variables, on the other hand, never fall below zero.	What is difference between interval and ratio scale
567	Inverted dropout is a variant of the original dropout technique developed by Hinton et al.  The one difference is that, during the training of a neural network, inverted dropout scales the activations by the inverse of the keep probability q=1−p q = 1 − p .	What is inverted dropout technique
309	The difference is a matter of emphasis. The joint distribution depends on some unknown parameters. So the model you are using is a joint density function where is the total number of measurements and is the total number of parameters.  That's the likelihood function.	What is the difference between joint distribution and likelihood function
304	Among the learning algorithms, one of the most popular and easiest to understand is the decision tree induction. The popularity of this method is related to three nice characteristics: interpretability, efficiency, and flexibility. Decision tree can be used for both classification and regression kind of problem.	What are the important characteristics of decision tree induction algorithm
3031	Under simple random sampling, a sample of items is chosen randomly from a population, and each item has an equal probability of being chosen. Meanwhile, systematic sampling involves selecting items from an ordered population using a skip or sampling interval.	What is the difference between random and systematic sampling
1028	A beta weight is a standardized regression coefficient (the slope of a line in a regression equation).  A beta weight will equal the correlation coefficient when there is a single predictor variable. β can be larger than +1 or smaller than -1 if there are multiple predictor variables and multicollinearity is present.	What is beta in multiple linear regression
8327	A multivariate normal distribution is a vector in multiple normally distributed variables, such that any linear combination of the variables is also normally distributed.	What does multivariate normality mean
6261	Deep Neural Networks (DNN) have greater capabilities for image pattern recognition and are widely used in Computer Vision algorithms. And, Convolutional Neural Network (CNN, or ConvNet) is a class of DNN which is most commonly applied to analyzing visual imagery.	Why convolutional neural networks are preferred for computer vision applications
5648	Spatial mining is the extraction of knowledge/spatial relationship and interesting measures that are not explicitly stored in spatial database. Temporal mining is the extraction of knowledge about occurrence of an event whether they follow Cyclic , Random ,Seasonal variations etc.	What is temporal and spatial data mining
5500	where 'In' denotes the n-by-n identity matrix. The matrix B is called the inverse matrix of A. A square matrix is Invertible if and only if its determinant is non-zero.	How do you determine if a matrix is invertible
890	Statistically significant means a result is unlikely due to chance. The p-value is the probability of obtaining the difference we saw from a sample (or a larger one) if there really isn't a difference for all users.	What does it mean when sample results are statistically significant
6493	"In information theory, the information content, self-information, surprisal, or Shannon information is a basic quantity derived from the probability of a particular event occurring from a random variable.  The Shannon information can be interpreted as quantifying the level of ""surprise"" of a particular outcome."	What does information content mean
2458	5 years	How long until AI is smarter than humans
3393	Affecting Entropy If you increase temperature, you increase entropy. (1) More energy put into a system excites the molecules and the amount of random activity. (2) As a gas expands in a system, entropy increases.	What energy increases entropy
6384	There is a wide rangeof statistical tests.  There are many different types of tests in statistics like t-test,Z-test,chi-square test, anova test ,binomial test, one sample median test etc. Choosing a Statistical test- Parametric tests are used if the data is normally distributed .	What are the types of test statistics
536	LSTM ( Long Short Term Memory ) Networks are called fancy recurrent neural networks with some additional features. Rolled Network. Just like RNN, we have time steps in LSTM but we have extra piece of information which is called “MEMORY” in LSTM cell for every time step.	What is Lstm in NLP
7032	Sparse Approximation (also known as Sparse Representation) theory deals with sparse solutions for systems of linear equations. Techniques for finding these solutions and exploiting them in applications have found wide use in image processing, signal processing, machine learning, medical imaging, and more.	What is sparse representation in image processing
598	Unsupervised learning is the Holy Grail of Deep Learning. The goal of unsupervised learning is to create general systems that can be trained with little data.  Today Deep Learning models are trained on large supervised datasets. Meaning that for each data, there is a corresponding label.	Can deep learning be unsupervised
2106	Reinforcement learning (RL) is a machine learning technique that focuses on training an algorithm following the cut-and-try approach. The algorithm (agent) evaluates a current situation (state), takes an action, and receives feedback (reward) from the environment after each act.	How do you explain reinforcement learning
7551	In econometrics, the seemingly unrelated regressions (SUR) or seemingly unrelated regression equations (SURE) model, proposed by Arnold Zellner in (1962), is a generalization of a linear regression model that consists of several regression equations, each having its own dependent variable and potentially different sets	What is a seemingly unrelated regression model
4863	Matrix Inventory allows you to add and manage product lists that consist of similar items that are available in a variety of attributes, such as size or color.  Each product is defined by a combination of attributes is a unique product with its own price, inventory and/or recipe.	What is Matrix inventory
4613	While in Gradient Descent (GD) the whole Training Set is considered before taking one Model Parameters Update Step, in Stochastic Gradient Descent (SGD) only one Data Point is considered for each Model Parameters Update Step, cycling over the Training Set.	Whats the difference between gradient descent and stochastic gradient descent
8006	"Analysis of covariance is used to test the main and interaction effects of categorical variables on a continuous dependent variable, controlling for the effects of selected other continuous variables, which co-vary with the dependent. The control variables are called the ""covariates."""	What is the purpose of a covariate
8410	Disadvantage:A small change in the data can cause a large change in the structure of the decision tree causing instability.For a Decision tree sometimes calculation can go far more complex compared to other algorithms.Decision tree often involves higher time to train the model.More items	What are the advantages and disadvantages of decision trees over other classification methods
6577	Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms.	What is the difference between data science and statistics 1
269	Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.	What is transfer learning in machine learning
6413	7 Techniques to Handle Imbalanced DataUse the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	How does machine learning deal with imbalanced data
1016	Dimensional Analysis (also called Factor-Label Method or the Unit Factor Method) is a problem-solving method that uses the fact that any number or expression can be multiplied by one without changing its value. It is a useful technique.	What is dimensional analysis method
243	f(x) = Pr[X = x] The following is the plot of the normal probability density function. Cumulative Distribution Function. The cumulative distribution function (cdf) is the probability that the variable takes a value less than or equal to x.	What is probability density function and cumulative distribution
946	Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.	What is a word2vec model
8359	Feature Selection: Select a subset of input features from the dataset.Unsupervised: Do not use the target variable (e.g. remove redundant variables). Correlation.Supervised: Use the target variable (e.g. remove irrelevant variables). Wrapper: Search for well-performing subsets of features. RFE.	What are the different attributes that can be selected by model selection methods
1125	Two events are independent, statistically independent, or stochastically independent if the occurrence of one does not affect the probability of occurrence of the other (equivalently, does not affect the odds).	What are statistically independent events in probability
8171	The framework, which stands for Setting, People, Alternatives, Decide and Explain, has been used to make important calls, without depending on the slow crawl of consensus decision-making.	What is your framework in making decisions
170	In statistics, the one in ten rule is a rule of thumb for how many predictor parameters can be estimated from data when doing regression analysis (in particular proportional hazards models in survival analysis and logistic regression) while keeping the risk of overfitting low.	How many predictors can be used in logistic regression
1211	Ensemble model combines multiple 'individual' (diverse) models together and delivers superior prediction power. If you want to relate this to real life, a group of people are likely to make better decisions compared to individuals, especially when group members come from diverse background.	Why are ensemble methods superior to individual models
375	A curve or pattern in the residual plot indicates a nonlinear relationship in the original data set. A random scatter of points in the residual plot indicates a linear relationship in the original data set.	What does it mean when there is a curved pattern in the residual plot
8563	Marginal probability: the probability of an event occurring (p(A)), it may be thought of as an unconditional probability. It is not conditioned on another event. Example: the probability that a card drawn is red (p(red) = 0.5).	What is marginal probability in statistics
545	US, informal. 1 or hash over : to talk about (something) : discuss (something) The detectives hashed out their theories about who committed the murder. They've spent quite a bit of time hashing over the problem.	What does it mean to hash something
2381	The input nodes take in information, in the form which can be numerically expressed. The information is presented as activation values, where each node is given a number, the higher the number, the greater the activation.  The output nodes then reflect the input in a meaningful way to the outside world.	What is an activation value *
1112	Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation.  Its goal is to maximize the total reward.	What is reinforcement learning in AI
6313	Linear Discriminant Analysis or Normal Discriminant Analysis or Discriminant Function Analysis is a dimensionality reduction technique which is commonly used for the supervised classification problems. It is used for modeling differences in groups i.e. separating two or more classes.	What is linear discriminant analysis used for
6236	Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay.  It is often used in signal processing for analyzing functions or series of values, such as time domain signals.	What is the use of autocorrelation
1633	Feature weighting is a technique used to approximate the optimal degree of influence of individual features using a training set. When successfully applied relevant features are attributed a high weight value, whereas irrelevant features are given a weight value close to zero.	What is feature weighting
5208	A distribution is skewed if one of its tails is longer than the other. The first distribution shown has a positive skew. This means that it has a long tail in the positive direction. The distribution below it has a negative skew since it has a long tail in the negative direction.	What does SKEWED DISTRIBUTION mean
5469	A sampling error is a statistical error that occurs when an analyst does not select a sample that represents the entire population of data and the results found in the sample do not represent the results that would be obtained from the entire population.	What do you mean by sampling error
4939	The median is the number in the middle {2, 3, 11, 13, 26, 34, 47}, which in this instance is 13 since there are three numbers on either side. To find the median value in a list with an even amount of numbers, one must determine the middle pair, add them, and divide by two.	How do you find the median value
835	ReLU stands for rectified linear unit, and is a type of activation function. Mathematically, it is defined as y = max(0, x). Visually, it looks like the following: ReLU is the most commonly used activation function in neural networks, especially in CNNs.	What is ReLU in machine learning
8074	A One-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On One Side Of The Mean. A Two-tailed Test Is Used When The Null Hypothesis Should Be Rejected If The Test Value Is In The Critical Region On Either	When should a one tailed test be used a two tailed test chegg
3787	The previous module introduced the idea of dividing your data set into two subsets: training set—a subset to train a model. test set—a subset to test the trained model.	What is training set and test set in machine learning
5977	Word embeddings are distributed representations of text in an n-dimensional space. These are essential for solving most NLP problems.  Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.”	What are Embeddings in NLP
1151	ANSWER. A false positive means that the results say you have the condition you were tested for, but you really don't. With a false negative, the results say you don't have a condition, but you really do.	What do a false positive and a false negative mean
6795	A/B tests are easy and seem harmless, but many consumers become disturbed when they find out they're being tested without knowing it. Some argue that A/B testing tracks along the same ethical lines as a product launch; others believe organizations​ must be transparent about their testing even if it seems harmless.	Is a B testing ethical
6783	The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid.	What is a kernel in SVM Why do we use kernels in SVM
3269	Establish face validity.Conduct a pilot test.Enter the pilot test in a spreadsheet.Use principal component analysis (PCA)Check the internal consistency of questions loading onto the same factors.Revise the questionnaire based on information from your PCA and CA.	How do you establish reliability and validity of a questionnaire
1124	The Fourier transform of a function of time is a complex-valued function of frequency, whose magnitude (absolute value) represents the amount of that frequency present in the original function, and whose argument is the phase offset of the basic sinusoid in that frequency.	What does the Fourier transform represent
4388	According to a senior data scientist, one of the distinct advantages of using Stochastic Gradient Descent is that it does the calculations faster than gradient descent and batch gradient descent. However, gradient descent is the best approach if one wants a speedier result.	Why is stochastic gradient descent better
2905	The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.	What data is used to train vector space models of words such as word2vec
1034	A histogram is drawn like a bar chart, but often has bars of unequal width. It is the area of the bar that tells us the frequency in a histogram, not its height. Instead of plotting frequency on the y-axis, we plot the frequency density. To calculate this, you divide the frequency of a group by the width of it.	What does density mean in histogram
640	A discrete variable is a variable whose value is obtained by counting. A continuous variable is a variable whose value is obtained by measuring.  A discrete random variable X has a countable number of possible values. Example: Let X represent the sum of two dice.	What is the difference between continuous and discrete variables
2237	An activation function is defined by and defines the output of a neuron in terms of its input (aka induced local field) . There are three types of activation functions. Threshhold function an example of which is. This function is also termed the Heaviside function. Piecewise Linear.	What is activation function and its types
4931	In addition, scales can be constructed from categorical variables. This is covered in a later section. The Count property returns the number of levels in the scale. The IsOrdered property indicates whether the scale is ordered or unordered.	Can Scaling be applied to categorical variables
1103	Ordered probit, like ordered logit, is a particular method of ordinal regression.  The ordered probit model provides an appropriate fit to these data, preserving the ordering of response options while making no assumptions of the interval distances between options.	What is ordered probit regression
1096	Five main Component of Natural Language processing are:Morphological and Lexical Analysis.Syntactic Analysis.Semantic Analysis.Discourse Integration.Pragmatic Analysis.	What are the components of NLP
8025	Some of the most popular machine learning algorithms for creating text classification models include the naive bayes family of algorithms, support vector machines, and deep learning.Naive Bayes.  Support Vector Machines.  Deep Learning.  Text Classification with R.	Which machine learning algorithm is best for text classification
1115	How to train your Deep Neural NetworkTraining data.  Choose appropriate activation functions.  Number of Hidden Units and Layers.  Weight Initialization.  Learning Rates.  Hyperparameter Tuning: Shun Grid Search - Embrace Random Search.  Learning Methods.  Keep dimensions of weights in the exponential power of 2.More items•	How do I train a deep neural network
327	The hazard ratio is a clinical trial statistic that allows the physician to say with confidence that healing is faster with the new drug. The hazard ratio must be >1 and the lower limit of the 95% confidence interval of the hazard ratio must be >1, which was the case in this example.	What is a hazard ratio with confidence interval
1121	The Real World is a term by the redpills to refer to reality, the true physical world and life outside the Matrix.	What is the real world in the Matrix
2921	The use of sigmoidal nonlinear functions was inspired by the ouputs of biological neurons.  However, this function is not smooth (it fails to be differential at the threshold value). Therefore, the sigmoid class of functions is a differentiable alternative that still captures much of the behavior of biological neurons.	Why is sigmoid nonlinear
4638	The Chi-Square Test for Normality allows us to check whether or not a model or theory follows an approximately normal distribution. The Chi-Square Test for Normality is not as powerful as other more specific tests (like Lilliefors).	Does chi square test require normal distribution
158	Explanation: There are total three types of questions that can be put to a regression analysis, that are, causal analysis, forecasting and affect and trend forecasting.	What is an example of a question that can be put to a regression analysis
8459	Values must be positive as log(x) exists only for positive values of x. The shape of the lognormal distribution is defined by three parameters: σ, the shape parameter. Also the standard deviation for the lognormal, this affects the general shape of the distribution.	What are the parameters of a lognormal distribution
6615	The Monty Hall problem is one of those rare curiosities – a mathematical problem that has made the front pages of national news. Everyone now knows, or thinks they know, the answer but a realistic look at the problem demonstrates that the standard mathematician's answer is wrong.	Is the Monty Hall problem true
7355	Find all of your absolute errors, xi – x. Add them all up. Divide by the number of errors. For example, if you had 10 measurements, divide by 10.Mean Absolute Errorn = the number of errors,Σ = summation symbol (which means “add them all up”),|xi – x| = the absolute errors.	How do you find the mean absolute error
1372	Variational autoencoders (VAEs) are a deep learning technique for learning latent representations.  They have also been used to draw images, achieve state-of-the-art results in semi-supervised learning, as well as interpolate between sentences. There are many online tutorials on VAEs.	What is variational auto encoder
4754	For example for a t-test, we assume that a random variable follows a normal distribution. For discrete data key distributions are: Bernoulli, Binomial, Poisson and Multinomial.	Is Bernoulli distribution discrete or continuous
479	3 Answers. Attempts to find an average value of AC would directly provide you the answer zero Hence, RMS values are used. They help to find the effective value of AC (voltage or current). This RMS is a mathematical quantity (used in many math fields) used to compare both alternating and direct currents (or voltage).	Why use root mean square instead of average
1086	For example, polynomial regression consists of performing multiple regression with variables. in order to find the polynomial coefficients (parameters). These types of regression are known as parametric regression since they are based on models that require the estimation of a finite number of parameters.	What is parametric regression
3629	Model fitting is a measure of how well a machine learning model generalizes to similar data to that on which it was trained.  During the fitting process, you run an algorithm on data for which you know the target variable, known as “labeled” data, and produce a machine learning model.	What is fitting in machine learning
5720	Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.	What is association rule in machine learning
7630	Template matching is a technique for finding areas of an image that match (or are similar) to a template image which requires two images. Source image (I): The image in which we expect to find a match to the template image. Template image (T): The patch image which will be compared to the template image.	What is template matching in image processing
1784	Neural networks: A mathematical model used to predict and classify results from the given data set is referred to as neural networks.  They contain a set of algorithms and functions similar to that of a neuron of the brain. A neural network classifies the inputs by the process of learning.	Is a neural nets way of classifying inputs in deep learning
1274	The component form of simple exponential smoothing is given by: Forecast equation^yt+h|t=ℓtSmoothing equationℓt=αyt+(1−α)ℓt−1, Forecast equation y ^ t + h | t = ℓ t Smoothing equation ℓ t = α y t + ( 1 − α ) ℓ t − 1 , where ℓt is the level (or the smoothed value) of the series at time t .	What is the exponential smoothing formula
5160	In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated.	What is a biased estimator in statistics
5415	The general formula for calculating a harmonic mean is:Harmonic mean = n / (∑1/x_i)Weighted Harmonic Mean = (∑w_i ) / (∑w_i/x_i)P/E (Index) = (0.4+0.6) / (0.4/50 + 0.6/4) = 6.33.P/E (Index) = 0.4×50 + 0.6×4 = 22.4.	How do you calculate weighted harmonic mean
22	Parameters are like exogenous variables in that their values are taken as given. They are distinct, however, from exogenous variables in that they tend to represent things that are given by nature such as consumer preferences or production technologies.	Is there a difference between a parameter and an exogenous variable
3102	The main difference between the t-test and f-test is, that t-test is used to test the hypothesis whether the given mean is significantly different from the sample mean or not. On the other hand, an F-test is used to compare the two standard deviations of two samples and check the variability.	What is the difference between F test and t test
551	A continuous random variable takes a range of values, which may be finite or infinite in extent. Here are a few examples of ranges: [0, 1], [0, ∞), (−∞, ∞), [a, b]. The function f(x) is called the probability density function (pdf).	What is range where X is a continuous random variable
638	A graphical representation of a single dataset, tallied into classes. The graph consists of a series of rectangles whose widths are defined by the limits of the classes, and whose heights are calculated by dividing relative frequency by class width.	What is a density histogram
403	N-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.  This post describes several different ways to generate n-grams quickly from input sentences in Python.	What is N grams Python
7883	Alternative procedures include: Different linear model: fitting a linear model with additional X variable(s) Nonlinear model: fitting a nonlinear model when the linear model is inappropriate.  Weighted least squares linear regression: dealing with unequal variances in Y by performing a weighted least squares fit.	What is linear regression What are some better alternatives
7118	The main advantage of quantile regression methodology is that the method allows for understanding relationships between variables outside of the mean of the data,making it useful in understanding outcomes that are non-normally distributed and that have nonlinear relationships with predictor variables.	What does quantile regression do
6403	To increase the stability of a neural network, batch normalization normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation.	How does batch normalization work
7906	Basically CV<10 is very good, 10-20 is good, 20-30 is acceptable, and CV>30 is not acceptable.	What is the acceptable coefficient of variation
4334	Gans can not be directly applied for natural language as the space in which sentence are present is not continuous and thereby not differentiable.	Can one use generative adversarial networks in NLP related problems
602	We calibrate our model when the probability estimate of a data point belonging to a class is very important. Calibration is comparison of the actual output and the expected output given by a system.	What is calibration in machine learning
4597	KNN algorithm is one of the simplest classification algorithm and it is one of the most used learning algorithms.  KNN is a non-parametric, lazy learning algorithm. Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point.	Why do we use KNN algorithm
1217	An estimator of a given parameter is said to be unbiased if its expected value is equal to the true value of the parameter. In other words, an estimator is unbiased if it produces parameter estimates that are on average correct.	How is an estimator unbiased
5171	The standard deviation of this set of mean values is the standard error. In lieu of taking many samples one can estimate the standard error from a single sample. This estimate is derived by dividing the standard deviation by the square root of the sample size.	What is the standard error of the mean difference
1221	There are four assumptions associated with a linear regression model: Linearity: The relationship between X and the mean of Y is linear. Homoscedasticity: The variance of residual is the same for any value of X. Independence: Observations are independent of each other.	What are the assumptions for regression analysis
400	“Expert systems” basically set a number of “if this, then do that” statements. It does not learn by itself (so it is not machine learning), and it still can be very useful for use cases like medical diagnosis and treatment.	What part of AI is not machine learning
2100	Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process.	Does batch size affect accuracy
5760	Collinearity is a linear association between two predictors. Multicollinearity is a situation where two or more predictors are highly linearly related.  But, correlation 'among the predictors' is a problem to be rectified to be able to come up with a reliable model.	What is the difference between correlation and multicollinearity
7593	Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.	Why is Word2Vec used
1641	(definition) Definition: A computational problem in which the object is to find the best of all possible solutions. More formally, find a solution in the feasible region which has the minimum (or maximum) value of the objective function.	What is optimization problem in algorithm
7244	While many people use the terms interchangeably, data science and big data analytics are unique fields, with the major difference being the scope.  Data science produces broader insights that concentrate on which questions should be asked, while big data analytics emphasizes discovering answers to questions being asked.	What is the difference between data science and data analytics
6305	1:146:07Suggested clip · 120 secondsUnivariate analysis SPSS - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you calculate univariate analysis in SPSS
1845	In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process.  MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning.	What is Markov decision process in machine learning
906	Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension.	What is dimension reduction in data science
3070	Results: Within a given review, a change in prevalence from the lowest to highest value resulted in a corresponding change in sensitivity or specificity from 0 to 40 percentage points.  Overall, specificity tended to be lower with higher disease prevalence; there was no such systematic effect for sensitivity.	How does prevalence affect sensitivity and specificity
4816	Shift-invariance: this means that if we shift the input in time (or shift the entries in a vector) then the output is shifted by the same amount.	What is shift invariance in a convolutional neural network CNN
3223	Stepwise regression is an appropriate analysis when you have many variables and you're interested in identifying a useful subset of the predictors. In Minitab, the standard stepwise regression procedure both adds and removes predictors one at a time.	Why do we use stepwise regression
1247	With supervised learning, you have features and labels. The features are the descriptive attributes, and the label is what you're attempting to predict or forecast.	What is features and labels in machine learning
309	Key Differences between AI, ML, and NLP ML is an application of AI. Machine Learning is basically the ability of a system to learn by itself without being explicitly programmed. Deep Learning is a part of Machine Learning which is applied to larger data-sets and based on ANN (Artificial Neural Networks).	What is the difference between AI machine learning NLP and deep learning
528	Cluster sampling refers to a type of sampling method . With cluster sampling, the researcher divides the population into separate groups, called clusters.  For example, given equal sample sizes, cluster sampling usually provides less precision than either simple random sampling or stratified sampling.	What is cluster sampling and its example
4140	A sample may be selected from a population through a number of ways, one of which is the stratified random sampling method. A stratified random sampling involves dividing the entire population into homogeneous groups called strata (plural for stratum). Random samples are then selected from each stratum.	How might you collect a stratified random sample
4240	Maximum likelihood estimation involves defining a likelihood function for calculating the conditional probability of observing the data sample given a probability distribution and distribution parameters. This approach can be used to search a space of possible distributions and parameters.	What is the maximum likelihood estimation in machine learning
8474	In statistics, an F-test of equality of variances is a test for the null hypothesis that two normal populations have the same variance.	What is the null hypothesis in F test for equality of variances
5651	A problem is an issue you can resolve while a constraint is an issue you cannot resolve. That is the simplest definition of these two terms. You can also define it in terms of your control over the situation. A problem is an issue where you have control over while a constraint is one where you do not have control over.	What are the constraints for the problem
1922	Rejecting the null hypothesis when it is in fact true is called a Type I error.  When a hypothesis test results in a p-value that is less than the significance level, the result of the hypothesis test is called statistically significant. Common mistake: Confusing statistical significance and practical significance.	When performing hypothesis testing is the probability of Type I error equals to the p value or level of significance
6258	Essentially, multivariate analysis is a tool to find patterns and relationships between several variables simultaneously. It lets us predict the effect a change in one variable will have on other variables.	What is multivariate analysis used for
926	Benefits of Usability TestingUsability testing provides an unbiased, accurate, and direct examination of your product or website's user experience.  Usability testing is convenient.  Usability testing can tell you what your users do on your site or product and why they take these actions.More items•	Why should you test your instructions for usability
3481	Odds ratios are one of those concepts in statistics that are just really hard to wrap your head around.  For example, in logistic regression the odds ratio represents the constant effect of a predictor X, on the likelihood that one outcome will occur. The key phrase here is constant effect.	What is odds ratio in logistic regression
4484	Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.  Often the weights of a neural network are contained within the hidden layers of the network.	What is weight in neural network
5495	The Monty Hall problem is one of those rare curiosities – a mathematical problem that has made the front pages of national news. Everyone now knows, or thinks they know, the answer but a realistic look at the problem demonstrates that the standard mathematician's answer is wrong.	Is the Monty Hall problem correct
703	We say that X and Y are independent if P(X=x,Y=y)=P(X=x)P(Y=y), for all x,y.  Intuitively, two random variables X and Y are independent if knowing the value of one of them does not change the probabilities for the other one. In other words, if X and Y are independent, we can write P(Y=y|X=x)=P(Y=y), for all x,y.	How do you know if X and Y are independent
1313	Describe the scores in such a sample. If the standard deviation is 0 then the variance is 0 and the mean of the squared deviation scores must be 0.  Thus, when the standard deviation equals 0, all the scores are identical and equal to the mean.	What does it mean for a sample to have a standard deviation of 0
3433	The negative binomial distribution is a probability distribution that is used with discrete random variables. This type of distribution concerns the number of trials that must occur in order to have a predetermined number of successes.	What is negative binomial distribution used for
7089	In General, A Discriminative model ‌models the decision boundary between the classes. A Generative Model ‌explicitly models the actual distribution of each class.  A Discriminative model ‌learns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems.	How does the principle of empirical risk minimization relate to the distinction between generative and discriminative statistical models
4615	Optimization falls in this category — given an optimization problem, you can, in principle, find a solution to the problem, without any ambiguity whatsoever. Machine learning, on the other hand, falls in the domain of engineering. Problems in engineering are often not mathematically well-defined.	What is the difference between an optimization problem and a machine learning problem
15	Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.	What is feature engineering and why is it especially important when working with machine learning methods
4543	·2 min read Here is a comparison of three basic pooling methods that are widely used. The three types of pooling operations are: Max pooling: The maximum pixel value of the batch is selected.  Average pooling: The average value of all the pixels in the batch is selected.	What is Max pooling and average pooling
4669	Hinge loss simplifies the mathematics needed for SVM thus leading to computational effective results while maximazing the error. If you need real time decisions with a lesser accuracy depend on it. Cross entropy is one of ancestor probabilistic decision making that minimizes the error but computationally ineffective.	What is the advantage disadvantage of Hinge loss compared to cross entropy
772	Backward chaining is known as goal-driven technique as we start from the goal and divide into sub-goal to extract the facts.  Backward chaining is suitable for diagnostic, prescription, and debugging application. 7. Forward chaining can generate an infinite number of possible conclusions.	Why is backward chaining effective for diagnostic problems
3574	"The short answer is ""no""--there is no unbiased estimator of the population standard deviation (even though the sample variance is unbiased). However, for certain distributions there are correction factors that, when multiplied by the sample standard deviation, give you an unbiased estimator."	Is sample standard deviation unbiased
4161	BioInformatics – This is one of the most well-known applications of Supervised Learning because most of us use it in our day-to-day lives. BioInformatics is the storage of Biological Information of us humans such as fingerprints, iris texture, earlobe and so on.	Where is supervised learning used
1426	A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function.	What is the difference between probability distribution and probability density
193	Independence two jointly continuous random variables X and Y are said to be independent if fX,Y (x,y) = fX(x)fY (y) for all x,y. It is easy to show that X and Y are independent iff any event for X and any event for Y are independent, i.e. for any measurable sets A and B P( X ∈ A ∩ Y ∈ B ) = P(X ∈ A)P(Y ∈ B).	How do you test the independence of two continuous variables
1404	A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic.  In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.	What do you mean by randomized algorithms
1258	The standard normal (or Z-distribution), is the most common normal distribution, with a mean of 0 and standard deviation of 1.  The t-distribution is typically used to study the mean of a population, rather than to study the individuals within a population.	What is T distribution and Z distribution
4399	Knuth Morris Pratt (KMP) is an algorithm, which checks the characters from left to right. When a pattern has a sub-pattern appears more than one in the sub-pattern, it uses that property to improve the time complexity, also for in the worst case. The time complexity of KMP is O(n).	What is the time complexity of KMP algorithm
1300	Hyperparameters are the variables which determines the network structure(Eg: Number of Hidden Units) and the variables which determine how the network is trained(Eg: Learning Rate). Hyperparameters are set before training(before optimizing the weights and bias).	What is Hyperparameter in deep learning
2204	Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.	Why AI algorithms are biased
2943	Weighted regression The idea is to give small weights to observations associated with higher variances to shrink their squared residuals. Weighted regression minimizes the sum of the weighted squared residuals. When you use the correct weights, heteroscedasticity is replaced by homoscedasticity.	How do you deal with Heteroskedasticity in regression
4102	AIC and BIC are Information criteria methods used to assess model fit while penalizing the number of estimated parameters.	What are AIC and BIC values
1419	collaborative filtering: user-based for example, CF calculates users' similarities in the item space.  matrix factorization amounts to mapping features of user and item via linear combination to latent factor space respectively.	Whats the difference between matrix factorization and collaborative filtering
684	Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map. The results are down sampled or pooled feature maps that highlight the most present feature in the patch, not the average presence of the feature in the case of average pooling.	What is Max pooling
3757	The “trick” is that kernel methods represent the data only through a set of pairwise similarity comparisons between the original data observations x (with the original coordinates in the lower dimensional space), instead of explicitly applying the transformations ϕ(x) and representing the data by these transformed	What is the kernel trick SVM
3527	How to Find the Mean. The mean is the average of the numbers. It is easy to calculate: add up all the numbers, then divide by how many numbers there are. In other words it is the sum divided by the count.	How do you find the mean in statistics
599	Artificial Intelligence (AI) is the branch of computer sciences that emphasizes the development of intelligence machines, thinking and working like humans. For example, speech recognition, problem-solving, learning and planning.	What is artificial intelligence with examples
8408	Class limits specify the span of data values that fall within a class. Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next.  Class limits are not possible data values. Class boundaries specify the span of data values that fall within a class.	What is the difference between a class boundary in a class limit
8569	Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning 'far' and high values meaning 'close'.  The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.	What is Gamma in RBF kernel
8089	Implicit or unconscious bias operates outside of the person's awareness and can be in direct contradiction to a person's espoused beliefs and values. What is so dangerous about implicit bias is that it automatically seeps into a person's affect or behavior and is outside of the full awareness of that person.	What is the difference between implicit bias and unconscious bias
2660	Here are 5 common machine learning problems and how you can overcome them.1) Understanding Which Processes Need Automation.  2) Lack of Quality Data.  3) Inadequate Infrastructure.  4) Implementation.  5) Lack of Skilled Resources.	What are the problems of machine learning
3841	Automated machine learning benefits This reduces the quality time that they spend in solving critical problems. Automated machine learning changes the making and use of machine learning models with ease and with the predeveloped systems so that the data scientists in the organization can focus more on complex problems.	What are the benefits of automated machine learning
1428	A sampling technique where a group of subjects (a sample) for study is selected from a larger group (a population).  A non-stratified sample does not take separate samples from strata or sub-groups of a population.	What is non stratified sampling
7985	An input is data that a computer receives. An output is data that a computer sends. Computers only work with digital information.  An input device is something you connect to a computer that sends information into the computer. An output device is something you connect to a computer that has information sent to it.	What is input and output unit
2389	The Utility of Signal Detection Theory Initially developed by radar researchers in the early 1950s (Peterson et al., 1954), the value of SDT was quickly recognized by cognitive scientists and adapted for application in human decision-making (Tanner and Swets, 1954; Green and Swets, 1966).	Who gave signal detection
5001	"6 Answers. Machine learning algorithms use optimization all the time.  Nonetheless, as mentioned in other answers, convex optimization is faster, simpler and less computationally intensive, so it is often easier to ""convexify"" a problem (make it convex optimization friendly), then use non-convex optimization."	Is convex optimization important for machine learning
96	Assumptions for the Kruskal Wallis Test One independent variable with two or more levels (independent groups). The test is more commonly used when you have three or more levels. For two levels, consider using the Mann Whitney U Test instead. Ordinal scale, Ratio Scale or Interval scale dependent variables.	What are the conditions for the Kruskal Wallis test
2413	This is the basis of the Breusch–Pagan test. It is a chi-squared test: the test statistic is distributed nχ2 with k degrees of freedom. If the test statistic has a p-value below an appropriate threshold (e.g. p < 0.05) then the null hypothesis of homoskedasticity is rejected and heteroskedasticity assumed.	What is the null hypothesis for Heteroskedasticity
970	The moving average is calculated by adding a stock's prices over a certain period and dividing the sum by the total number of periods. For example, a trader wants to calculate the SMA for stock ABC by looking at the high of day over five periods.	How do you calculate a moving average
2460	The adjusted coefficient of determination (also known as adjusted R2 or. pronounced “R bar squared”) is a statistical measure that shows the proportion of variation explained by the estimated regression line.	What is adjusted coefficient of determination
647	In pattern recognition, information retrieval and classification (machine learning), precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of the total amount of relevant instances that were	What is the relationship between sensitivity specificity and recall precision
1749	Preparing Your Dataset for Machine Learning: 8 Basic Techniques That Make Your Data BetterArticulate the problem early.Establish data collection mechanisms.Format data to make it consistent.Reduce data.Complete data cleaning.Decompose data.Rescale data.Discretize data.	How would you prepare a dataset for deep learning
2533	For example, a perfect precision and recall score would result in a perfect F-Measure score:F-Measure = (2 * Precision * Recall) / (Precision + Recall)F-Measure = (2 * 1.0 * 1.0) / (1.0 + 1.0)F-Measure = (2 * 1.0) / 2.0.F-Measure = 1.0.	How is Precision Recall calculated
2897	z = (x – μ) / σ For example, let's say you have a test score of 190. The test has a mean (μ) of 150 and a standard deviation (σ) of 25. Assuming a normal distribution, your z score would be: z = (x – μ) / σ	How do you find the standardized score in a normal distribution
4990	Volume is continuous, so the amount of water would be represented by a continuous random variable. The number of minutes is countable, so it would be a discrete variable.	Is volume a continuous or discrete variable
96	A matrix that has only real entries is Hermitian if and only if it is symmetric. A real and symmetric matrix is simply a special case of a Hermitian matrix.	What makes a matrix Hermitian
5392	Maximum likelihood, also called the maximum likelihood method, is the procedure of finding the value of one or more parameters for a given statistic which makes the known likelihood distribution a maximum. The maximum likelihood estimate for a parameter is denoted . For a Bernoulli distribution, (1)	What does maximum likelihood mean
881	In short, it ensures each subgroup within the population receives proper representation within the sample. As a result, stratified random sampling provides better coverage of the population since the researchers have control over the subgroups to ensure all of them are represented in the sampling.	What are the advantages of stratified random sampling
1840	Deep learning (sometimes known as deep structured learning) is a subset of machine learning, where machines employ artificial neural networks to process information. Inspired by biological nodes in the human body, deep learning helps computers to quickly recognize and process images and speech.	What is deep learning and how is it useful
5031	To calculate the standard deviation of those numbers:Work out the Mean (the simple average of the numbers)Then for each number: subtract the Mean and square the result.Then work out the mean of those squared differences.Take the square root of that and we are done!	How do you calculate the standard deviation
7410	Approximate non-negative matrix factorization Usually the number of columns of W and the number of rows of H in NMF are selected so the product WH will become an approximation to V. The full decomposition of V then amounts to the two non-negative matrices W and H as well as a residual U, such that: V = WH + U.	How does non negative matrix factorization work
6607	definition 1: ability to learn quickly.	What is a word for the ability to learn quickly
8274	1. a) Higher level of entropy refers to higher state of disorder in the system and it can be reduced by input of energy to lower the entropy.	What does it mean for a system to be in a higher level of entropy
6476	The central limit theorem has been extended to the case of dependent random variables by several authors (Bruns, Markoff, S.  The conditions under which these theorems are stated either are very restrictive or involve conditional distributions, which makes them difficult to apply.	Is the central limit theorem applicable for a dependent random variable How
5249	Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.	What is gradient descent in machine learning
5106	RPN Loss Function The first term is the classification loss over 2 classes (There is object or not). The second term is the regression loss of bounding boxes only when there is object (i.e. p_i* =1). Thus, RPN network is to pre-check which location contains object.	What is RPN loss
915	Discriminant analysis is a versatile statistical method often used by market researchers to classify observations into two or more groups or categories. In other words, discriminant analysis is used to assign objects to one group among a number of known groups.	Why is discriminant analysis used
5654	Results/Conclusions In exploratory studies, p-values enable the recognition of any statistically noteworthy findings. Confidence intervals provide information about a range in which the true value lies with a certain degree of probability, as well as about the direction and strength of the demonstrated effect.	What are the advantages of using p values in statistics
6670	They have too few levels of structure: Neurons, Layers, and Whole Nets. We need to group neurons in each layer in 'capsules' that do a lot of internal computation and then output a compact result.”	What is wrong with convolutional neural nets
20	The main difference between DevOps and SRE is that SRE is more operationally driven from the top-down, and it's governed by the developer or development team, instead of the operations team.	What is the difference between DevOps and SRE
8213	In statistics, the theoretical curve that shows how often an experiment will produce a particular result. The curve is symmetrical and bell shaped, showing that trials will usually give a result near the average, but will occasionally deviate by large amounts.	What does a normal distribution curve show
781	The metric system is based upon powers of ten, which is convenient because: A measurement in the metric system that is represented by a rational number remains a rational number after metric unit conversion. (For example, 250 mm = 25 cm = .SI Units.Physical QuantityName of UnitAbbreviationLuminous IntensityLumenIv6 more rows	What is the difference between SI units and metric
952	A single pixel camera uses only one light sensor to measure the entire image.  This allows the use of one really good light sensor as opposed to 10 million very cheap ones. Compressed Sensing is used to measure the entire image using only a single sensor.	What is a single pixel camera
7832	Test-retest reliability is the degree to which test scores remain unchanged when measuring a stable individual characteristic on different occasions.	What does test retest reliability mean
8547	1. A Multi-Agent System (MAS) is a loosely coupled network of software agents that interact to solve problems that are beyond the individual capacities or knowledge of each software agent. Learn more in: Using Multi-Agent Systems to Support e-Health Services. A system composed of multiple interacting intelligent agents	What are multi agent systems 1
714	Causation is the relationship between cause and effect. So, when a cause results in an effect, that's a causation.  When we say that correlation does not imply cause, we mean that just because you can see a connection or a mutual relationship between two variables, it doesn't necessarily mean that one causes the other.	Does correlation imply causation Why or why not
274	In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.	What is machine learning error
681	In the context of conventional artificial neural networks convergence describes a progression towards a network state where the network has learned to properly respond to a set of training patterns within some margin of error.	What is convergence in neural network
27	A learning algorithm is a method used to process data to extract patterns appropriate for application in a new situation. In particular, the goal is to adapt a system to a specific input-output transformation task.	What is a learning algorithm
5190	Interaction effects occur when the effect of one variable depends on the value of another variable.  In this manner, analysts use models to assess the relationship between each independent variable and the dependent variable. This kind of an effect is called a main effect.	How do you describe the interaction effect
3562	Analytics is the systematic computational analysis of data or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data. It also entails applying data patterns towards effective decision making.	What do u mean by Analytics
7154	Elon Musk says he's terrified of AI taking over the world and most scared of Google's DeepMind AI project. Tesla and SpaceX CEO Elon Musk has repeatedly said that he thinks artificial intelligence poses a threat to humanity.	How does Elon Musk feel about AI
8599	The receptive field size of a unit can be increased in a number of ways. One option is to stack more layers to make the network deeper, which increases the receptive field size linearly by theory, as each extra layer increases the receptive field size by the kernel size.	How do you increase receptive fields
528	RODGERS APPROACH TO CONCEPT ANALYSIS identify and name the concept of interest; identify the surrogate terms and relevant uses of the concept; select an appropriate realm (sample) for data collection; recognize attributes of the concept;More items	How do you write a concept analysis
8287	Unlike the standard boxplot, a modified boxplot does not include the outliers. Instead, the outliers are represented as points beyond the 'whiskers', in order to represent more accurately the dispersion of the data.	What is the difference between a box plot and a modified Boxplot
4562	"Trainable weights are the weights that will be learnt during the training process.  You might see some ""strange numbers"" because either you are using a pre-trained network that has its weights already learnt or you might be using random initialization when defining the model."	What does trainable weights mean in neural network
4473	"If the statistical software renders a p value of 0.000 it means that the value is very low, with many ""0"" before any other digit.  So the interpretation would be that the results are significant, same as in the case of other values below the selected threshold for significance."	What does a significance level of 0 mean
586	Neural networks generally perform supervised learning tasks, building knowledge from data sets where the right answer is provided in advance. The networks then learn by tuning themselves to find the right answer on their own, increasing the accuracy of their predictions.	How do artificial neural networks learn
1785	The distribution function , also called the cumulative distribution function (CDF) or cumulative frequency function, describes the probability that a variate takes on a value less than or equal to a number . The distribution function is sometimes also denoted. (Evans et al. 2000, p.	What does distribution function mean
3993	A sampling method is called biased if it systematically favors some outcomes over others.	How do you know if a sample is biased
891	Any finite sequence of independent and identically distributed random variables is exchangeable, but the converse is not true. The classic example of a sequence of random variables that's exchangeable but not iid is the sequence of draws you get when sampling without replacement from a finite population.	What are exchangeable random variables How are they different from independent random variables
4476	Statistical binary classification It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. When there are only two categories the problem is known as statistical binary classification.	What is a binary classification in machine learning
8646	three	How many forms of normalization are there
3579	Understanding the differences Detection refers to mining insights or information in a data pool when it is being processed.  Prediction or predictive analysis employs probability based on the data analyses and processing.	What is the difference between detection and prediction
190	The trace is sometimes called the spur, from the German word Spur, which means track or trace. For example, the trace of the n by n identity matrix is equal to n. A matrix in which all the elements below the diagonal elements vanish is called an upper triangular matrix.	What is the trace of a matrix example
7147	An iterative algorithm is said to converge when as the iterations proceed the output gets closer and closer to a specific value. In some circumstances, an algorithm will diverge; its output will undergo larger and larger oscillations, never approaching a useful result.	What is convergence in machine learning
3699	Two random variables X and Y are said to be bivariate normal, or jointly normal, if aX+bY has a normal distribution for all a,b∈R. In the above definition, if we let a=b=0, then aX+bY=0. We agree that the constant zero is a normal random variable with mean and variance 0.	How do you know if a bivariate is normal distribution
1720	A major difference is in its shape: the normal distribution is symmetrical, whereas the lognormal distribution is not. Because the values in a lognormal distribution are positive, they create a right-skewed curve.  A further distinction is that the values used to derive a lognormal distribution are normally distributed.	What is the difference between lognormal and normal distribution
8451	Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network.	What is meant by the term deep learning
7612	The “reinforcement” in reinforcement learning refers to how certain behaviors are encouraged, and others discouraged. Behaviors are reinforced through rewards which are gained through experiences with the environment.  Reinforcement learning borrowed his name from the first thread of studies.	What is reinforcement learning & Why is it called so
6932	So far we've looked at GBMs that use two different direction vectors, the residual vector (Gradient boosting: Distance to target) and the sign vector (Gradient boosting: Heading in the right direction).	Does gradient boosting use gradient descent
646	AI can signal posts of people who might be in need and/or perhaps driven by suicidal tendencies. The AI uses machine learning to flag key phrases in posts and concerned comments from friends or family members to help identify users who may be at risk.	How does Facebook use artificial intelligence
2238	The methods of signal processing include time domain, frequency domain, and complex frequency domain.	What are signal processing techniques
7444	Feature extraction involves reducing the number of resources required to describe a large set of data.GeneralIndependent component analysis.Isomap.Kernel PCA.Latent semantic analysis.Partial least squares.Principal component analysis.Multifactor dimensionality reduction.Nonlinear dimensionality reduction.More items	What are the feature extraction methods
4440	Stratified sampling offers several advantages over simple random sampling. A stratified sample can provide greater precision than a simple random sample of the same size. Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money.	Is stratified sampling better than simple random sampling
2465	In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.	Why do we use logit model
1361	To get α subtract your confidence level from 1. For example, if you want to be 95 percent confident that your analysis is correct, the alpha level would be 1 – . 95 = 5 percent, assuming you had a one tailed test. For two-tailed tests, divide the alpha level by 2.	How do you find the alpha level
3083	MinMax scaling will not affect the values of dummy variables but Standardised scaling will.	Does Standardised scaling will affect the values of dummy variables
335	A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words.	What are generative models used for
7822	The normalisation ensures that the inputs have a mean of 0 and a standard deviation of 1, meaning that the input distribution to every neuron will be the same, thereby fixing the problem of internal covariate shift and providing regularisation.	How does Batch normalization address the problem of Internal Covariate Shift
1668	A data stream is a set of extracted information from a data provider.  It contains raw data that was gathered out of users' browser behavior from websites, where a dedicated pixel is placed.	What is data stream in GIS
886	A Convolutional Neural Networks Introduction so to speak.Step 1: Convolution Operation.  Step 1(b): ReLU Layer.  Step 2: Pooling.  Step 3: Flattening.  Step 4: Full Connection.  Step 1 - Convolution Operation.  Step 1(b): The Rectified Linear Unit (ReLU)  Step 2 - Max Pooling.More items•	What are the steps in convolution neural network
4187	1 Natural Language Processing. Computational linguistics (CL), natural language processing (NLP) and machine translation (MT) are domains whose perspective on natural language is different from that of linguistic fields such as semantics, pragmatics and syntax.	Is machine translation An application of NLP
3525	From Example 20.2, the posterior distribution of P is Beta(s+α, n−s+α). The posterior mean is then (s+α)/(n+2α), and the posterior mode is (s+α−1)/(n+2α−2). Both of these may be taken as a point estimate p for p.	How do you calculate posterior distribution
567	The trace of a matrix is the sum of its (complex) eigenvalues, and it is invariant with respect to a change of basis. This characterization can be used to define the trace of a linear operator in general. The trace is only defined for a square matrix (n × n).	What is the trace of a linear operator
458	If each member actively seeks out knowledge and information, and feels empowered to share it, they can open up a collaborative discussion. This cognitive mechanism enables individuals to share views, ideas and attitudes when focusing on issues together, something that cannot be replicated by individual attention.	Why is collective intelligence important
6099	Technically, all interpreters do the same thing and follow the same basic principles. But since sign languages are visual-manual while spoken languages are based on speaking, hearing and writing/reading, the difference entails several special requirements for interpreting.	In what ways are spoken languages and signed languages the same different
812	The P-value is the probability that a chi-square statistic having 2 degrees of freedom is more extreme than 19.58. We use the Chi-Square Distribution Calculator to find P(Χ2 > 19.58) = 0.0001.	What is the relationship between p value and chi square
1952	Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit.	Is RMSE the same as R Squared
2030	The data used in calculating a chi-square statistic must be random, raw, mutually exclusive, drawn from independent variables, and drawn from a large enough sample.  Chi-square tests are often used in hypothesis testing.	What types of data are suitable for chi square analysis
7242	A lazy learner delays abstracting from the data until it is asked to make a prediction while an eager learner abstracts away from the data during training and uses this abstraction to make predictions rather than directly compare queries with instances in the dataset.	What is the main difference between lazy and eager learning methods in nearest neighbor clustering
578	Explanation of Collaborative Filtering vs Content Based Filtering. Recommender systems help users select similar items when something is being chosen online. The method is based on content and collaborative filtering approach that captures correlation between user preferences and item features.	Is collaborative filtering machine learning
5619	Abstract: The Local Binary Pattern Histogram(LBPH) algorithm is a simple solution on face recognition problem, which can recognize both front face and side face.  To solve this problem, a modified LBPH algorithm based on pixel neighborhood gray median(MLBPH) is proposed.	What is LBPH face recognizer
2578	Box plots divide the data into sections that each contain approximately 25% of the data in that set. Box plots are useful as they provide a visual summary of the data enabling researchers to quickly identify mean values, the dispersion of the data set, and signs of skewness.	What is the point of a box plot
3591	Ordinary least squares assumes things like equal variance of the noise at every x location. Generalized least squares does not assume a diagonal co-variance matrix.	What is the difference between ordinary least squares and generalized least squares
5432	In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.	Why is the lasso regression analysis method performed in statistics
7013	Linear regression is the next step up after correlation. It is used when we want to predict the value of a variable based on the value of another variable. The variable we want to predict is called the dependent variable (or sometimes, the outcome variable).	What is linear regression used for
7079	Hebbian learning is one of the oldest learning algorithms, and is based in large part on the dynamics of biological systems. A synapse between two neurons is strengthened when the neurons on either side of the synapse (input and output) have highly correlated outputs.	How does Hebbian learning work
2063	Random error can be caused by numerous things, such as inconsistencies or imprecision in equipment used to measure data, in experimenter measurements, in individual differences between participants who are being measured, or in experimental procedures.	What are the causes of random error
681	Random error can be reduced by: Using an average measurement from a set of measurements, or. Increasing sample size.	What is random error and how can it be reduced
2865	Yes, it is possible but not in the near future. We are nowhere close to building an AI like JARVIS. It would take decades of research.	Is it possible to create an AI like Jarvis
2121	Random Forests / Ensemble Trees. One approach to dimensionality reduction is to generate a large and carefully constructed set of trees against a target attribute and then use each attribute's usage statistics to find the most informative subset of features.	Can random forest be used as a dimensionality reduction algorithm
3257	The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time.  The P value is determined from the F ratio and the two values for degrees of freedom shown in the ANOVA table.	What is the F ratio in Anova
6344	Forecasting is the process of making predictions of the future based on past and present data and most commonly by analysis of trends. A commonplace example might be estimation of some variable of interest at some specified future date.  In some cases the data used to predict the variable of interest is itself forecast.	What are forecasting variables
1807	Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable; the method estimates the relationship by minimizing the sum of the squares in the difference between the observed and predicted values of the	Why is ordinary least squares regression called ordinary least squares
3932	If you want to ingest DynamoDB data into Redshift you have a few options.The Redshift Copy command.Build a Data Pipeline that copies the data using an EMR job to S3.Export the DynamoDB data to a file using the AWS CLI and load the flat file into Redshift.More items	How do I transfer data from DynamoDB to redshift
686	A sample standard deviation is a statistic. This means that it is calculated from only some of the individuals in a population. Since the sample standard deviation depends upon the sample, it has greater variability. Thus the standard deviation of the sample is greater than that of the population.	Why is standard deviation different for population and sample
7658	Artificial neural networks are forecasting methods that are based on simple mathematical models of the brain. They allow complex nonlinear relationships between the response variable and its predictors.	What are neural network models
2426	Key Terms. normal approximation: The process of using the normal curve to estimate the shape of the distribution of a data set. central limit theorem: The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed.	What does normal approximation mean
7626	Batch normalisation is a technique for improving the performance and stability of neural networks, and also makes more sophisticated deep learning architectures work in practice (like DCGANs).  That means we can think of any layer in a neural network as the first layer of a smaller subsequent network.	What is batch normalization and why does it work
1278	1 Answer. In word2vec, you train to find word vectors and then run similarity queries between words. In doc2vec, you tag your text and you also get tag vectors.  If two authors generally use the same words then their vector will be closer.	What is the difference between Word2Vec and Doc2Vec
3185	Others tried to use deep learning to solve problems that were beyond its scope.  But according to famous data scientist and deep learning researcher Jeremy Howard, the “deep learning is overhyped” argument is a bit— well—overhyped.	Is deep learning Overhyped
6231	An F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled.	What is the F statistic used for
5165	Bayesian Model Averaging (BMA) is an application of Bayesian inference to the problems of model selection, combined estimation and prediction that produces a straightforward model choice criteria and less risky predictions.	What is the Bayesian Model Averaging and its applications
4513	Time series analysis can be useful to see how a given asset, security, or economic variable changes over time. It can also be used to examine how the changes associated with the chosen data point compare to shifts in other variables over the same time period.	What is the purpose of time series analysis
2843	"Like random forests, gradient boosting is a set of decision trees. The two main differences are:  Combining results: random forests combine results at the end of the process (by averaging or ""majority rules"") while gradient boosting combines results along the way."	What is the difference between gradient boosting and Random Forest
902	Inverse reinforcement learning is the problem of inferring the reward function of an observed agent, given its policy or behavior. Researchers perceive IRL both as a problem and as a class of methods.	What is inverse reinforcement learning
7935	As the area of a bar represents the frequency of its interval, the height of the bar represents the density. If you label the scare it is either frequency per unit or, if you divide by the total frequency, relative frequency per unit.	What is the density scale
428	It is common to allocate 50 percent or more of the data to the training set, 25 percent to the test set, and the remainder to the validation set. Some training sets may contain only a few hundred observations; others may include millions.	How much data will you allocate for your training validation and test sets
1125	The Absolute min, is the smallest function value of the domain of the function, whereas, the Local min at point c, is the smallest function value where x is near c. A function is a local minimum at x=c, if f(c) > or = to f(x), for all x values near c ) some interval containing c).	What is the difference between local minimum and absolute minimum
7959	How k-means cluster analysis worksStep 1: Specify the number of clusters (k).  Step 2: Allocate objects to clusters.  Step 3: Compute cluster means.  Step 4: Allocate each observation to the closest cluster center.  Step 5: Repeat steps 3 and 4 until the solution converges.	How do you analyze K means clustering
5975	One-shot learning is a classification task where one, or a few, examples are used to classify many new examples in the future.  One-shot learning are classification tasks where many predictions are required given one (or a few) examples of each class, and face recognition is an example of one-shot learning.	What is known as one shot learning
5850	PVQ is an acronym for Pressure Vessel Quality. That means any steel plate using this designation is designed for use in pressure vessels. These pressure vessels are normally some type of closed container meant to hold any gas or liquid that is held at a pressure much different than its surrounding ambient pressure.	What is Pvq
944	Linear algebra is usually taken by sophomore math majors after they finish their calculus classes, but you don't need a lot of calculus in order to do it.	When should you learn linear algebra
7376	18:2725:32Suggested clip · 115 secondsStructural Equation Modeling: what is it and what can we use it for YouTubeStart of suggested clipEnd of suggested clip	How do you read a structural equation model
3790	The major difference between a traditional Artificial Neural Network (ANN) and CNN is that only the last layer of a CNN is fully connected whereas in ANN, each neuron is connected to every other neurons as shown in Fig.	What is the difference between Ann and CNN
382	Translational Invariance makes the CNN invariant to translation. Invariance to translation means that if we translate the inputs the CNN will still be able to detect the class to which the input belongs. Translational Invariance is a result of the pooling operation.	What makes CNNs translation invariant
854	Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.	What is transfer learning in ML
2970	Now we'll check out the proven way to improve the performance(Speed and Accuracy both) of neural network models:Increase hidden Layers.  Change Activation function.  Change Activation function in Output layer.  Increase number of neurons.  Weight initialization.  More data.  Normalizing/Scaling data.More items•	How do you increase the accuracy of a neural network
3523	This term is used in statistics in its ordinary sense, but most frequently occurs in connection with samples from different populations which may or may not be identical. If the populations are identical they are said to be homogeneous, and by extension, the sample data are also said to be homogeneous.	What does homogeneity mean in statistics
4790	Coding theory is one of the most important and direct applications of information theory.  Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source.	Why is information theory important
1230	The basic steps to build an image classification model using a neural network are:Flatten the input image dimensions to 1D (width pixels x height pixels)Normalize the image pixel values (divide by 255)One-Hot Encode the categorical column.Build a model architecture (Sequential) with Dense layers.More items•	How do you implement CNN image classification
763	In general, in deep learning NNs are quite large and so the number of (local) minima is much larger than in simple cases of NNs. TL;DR: In general they are non convex.	Is deep learning convex optimization
1525	They are different types of clustering methods, including:Partitioning methods.Hierarchical clustering.Fuzzy clustering.Density-based clustering.Model-based clustering.	What are the different types of clustering algorithms
3952	If we use a generalized linear model (GLM) to model the relationship, deviance is a measure of goodness of fit: the smaller the deviance, the better the fit. The exact definition of deviance is as follows: for a particular GLM (denoted ), let denote the maximum achievable likelihood under this model.	What is deviance in GLM generalized linear models
8466	"Association rules are ""if-then"" statements, that help to show the probability of relationships between data items, within large data sets in various types of databases."	What do you mean by association rules
4008	Synapses are the couplings between neurons, allowing signals to pass from one neuron to another. However, synapses are much more than mere relays: they play an important role in neural computation.	What is synapse in neural network
102	A recurrent neural network, however, is able to remember those characters because of its internal memory. It produces output, copies that output and loops it back into the network. Simply put: recurrent neural networks add the immediate past to the present.	How does recurrent neural network work
7557	"A number used to multiply a variable. Example: 6z means 6 times z, and ""z"" is a variable, so 6 is a coefficient. Variables with no number have a coefficient of 1. Example: x is really 1x. Sometimes a letter stands in for the number."	What is the meaning of coefficient
2211	Definition: To give a helpful lift up to someone, either physically or emotionally. This phrasal verb means to lift someone up to reach a higher point. This can be physically, if someone cannot reach something, or emotionally, if someone needs a boost, or increase, in confidence or morale.	What does it mean when someone is boosting
920	From Wikipedia, the free encyclopedia. In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.	What is elastic net regularization in machine learning
1153	Probability of Two Events Occurring Together: Independent Just multiply the probability of the first event by the second. For example, if the probability of event A is 2/9 and the probability of event B is 3/9 then the probability of both events happening at the same time is (2/9)*(3/9) = 6/81 = 2/27.	What is the probability of two independent events occurring
471	A non-stationary process with a deterministic trend becomes stationary after removing the trend, or detrending. For example, Yt = α + βt + εt is transformed into a stationary process by subtracting the trend βt: Yt - βt = α + εt, as shown in the figure below.	How do you convert a stationary to a non stationary series
4104	Prerequisite for Machine LearningStatistics, Calculus, Linear Algebra and Probability. A) Statistics contain tools that are used to get an outcome from data.  Programming Knowledge. Being able to write code is one of the most important things when it comes to Machine Learning.  Data Modeling.	What are the basics required for machine learning
8064	The AUC value lies between 0.5 to 1 where 0.5 denotes a bad classifer and 1 denotes an excellent classifier.	What is a good ROC curve score
5471	It can be seen that the function of the loss of quality is a U-shaped curve, which is determined by the following simple quadratic function: L(x)= Quality loss function. x = Value of the quality characteristic (observed). N = Nominal value of the quality characteristic (Target value – target).	How is the target value of a Taguchi loss function identified
4719	Negative Log-Likelihood (NLL) Recall that when training a model, we aspire to find the minima of a loss function given a set of parameters (in a neural network, these are the weights and biases). We can interpret the loss as the “unhappiness” of the network with respect to its parameters.	What is the negative log likelihood
766	Backpropagation (backward propagation) is an important mathematical tool for improving the accuracy of predictions in data mining and machine learning.  Artificial neural networks use backpropagation as a learning algorithm to compute a gradient descent with respect to weights.	What is Backpropagation in machine learning
239	1. Evaluate ARIMA ModelSplit the dataset into training and test sets.Walk the time steps in the test dataset. Train an ARIMA model. Make a one-step prediction. Store prediction; get and store actual observation.Calculate error score for predictions compared to expected values.	How do you evaluate an Arima model
7237	ReLU provides just enough non-linearity so that it is nearly as simple as a linear activation, but this non-linearity opens the door for extremely complex representations. Because unlike in the linear case, the more you stack non-linear ReLUs, the more it becomes non-linear.	Why is ReLU better than linear
5289	Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting.	What is bootstrap aggregating in machine learning
6129	parameter-list is the list of parameters that the function takes separated by commas. If no parameters are given, then the function does not take any and should be defined with an empty set of parenthesis or with the keyword void. If no variable type is in front of a variable in the paramater list, then int is assumed.	What is the difference between a function with parameters and a functions without parameters
1909	Weights are the co-efficients of the equation which you are trying to resolve. Negative weights reduce the value of an output. When a neural network is trained on the training set, it is initialised with a set of weights.  A neuron first computes the weighted sum of the inputs.	How weights are calculated in neural networks
6729	Random variables are denoted by capital letters If you see a lowercase x or y, that's the kind of variable you're used to in algebra. It refers to an unknown quantity or quantities. If you see an uppercase X or Y, that's a random variable and it usually refers to the probability of getting a certain outcome.	How do you know if a variable is random
6622	Error -- subtract the theoretical value (usually the number the professor has as the target value) from your experimental data point. Percent error -- take the absolute value of the error divided by the theoretical value, then multiply by 100.	How do you determine data error
2560	How to Find a Sample Size Given a Confidence Interval and Width (unknown population standard deviation)za/2: Divide the confidence interval by two, and look that area up in the z-table: .95 / 2 = 0.475.  E (margin of error): Divide the given width by 2. 6% / 2.  : use the given percentage. 41% = 0.41.  : subtract. from 1.	How do you find the sample size when given the margin of error and standard deviation
6252	Unsupervised feature learning is learning features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that captures some structure underlying the high-dimensional input data.	What is unsupervised feature learning
253	As the area of a bar represents the frequency of its interval, the height of the bar represents the density. If you label the scare it is either frequency per unit or, if you divide by the total frequency, relative frequency per unit.	What is the density scale in histograms
6347	Definition. In probability theory, a normalizing constant is a constant by which an everywhere non-negative function must be multiplied so the area under its graph is 1, e.g., to make it a probability density function or a probability mass function.	What is a normalizing function
1284	Statistical analysis is used in order to gain an understanding of a larger population by analysing the information of a sample.  Data analysis is the process of inspecting, presenting and reporting data in a way that is useful to non-technical people.	Is statistical analysis the same as data analysis
8072	LDA is a probabilistic generative model that extracts the thematic structure in a big document collection. The model assumes that every topic is a distribution of words in the vocabulary, and every document (described over the same vocabulary) is a distribution of a small subset of these topics.	What is LDA clustering
536	Additional terms will always improve the model whether the new term adds significant value to the model or not. As a matter of fact, adding new variables can actually make the model worse. Adding more and more variables makes it more and more likely that you will overfit your model to the training data.	What is the effect of adding more independent variables to a regression model
242	Covariance indicates the relationship of two variables whenever one variable changes. If an increase in one variable results in an increase in the other variable, both variables are said to have a positive covariance.  Both variables move together in the same direction when they change.	What does Covariance indicate
6322	You can use regression equations to make predictions. Regression equations are a crucial part of the statistical output after you fit a model.  However, you can also enter values for the independent variables into the equation to predict the mean value of the dependent variable.	Is it appropriate to use the linear regression equation to make predictions
860	The Mean Squared Error (MSE) is a measure of how close a fitted line is to data points.  The MSE has the units squared of whatever is plotted on the vertical axis. Another quantity that we calculate is the Root Mean Squared Error (RMSE). It is just the square root of the mean square error.	What is the difference between MSE and RMSE
3372	The dependent variable is the variable being tested and measured in an experiment, and is 'dependent' on the independent variable. An example of a dependent variable is depression symptoms, which depends on the independent variable (type of therapy).	What is the Dependant variable
7858	In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method proposed by Thomas Cover used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space.	What is K nearest neighbor used for
229	In edge detection, we find the boundaries or edges of objects in an image, by determining where the brightness of the image changes dramatically. Edge detection can be used to extract the structure of objects in an image.	What is the use of edge detection
7559	12 Common Biases That Affect How We Make Everyday DecisionsThe Dunning-Kruger Effect.  Confirmation Bias.  Self-Serving Bias.  The Curse of Knowledge and Hindsight Bias.  Optimism/Pessimism Bias.  The Sunk Cost Fallacy.  Negativity Bias.  The Decline Bias (a.k.a. Declinism)More items•	What are the 12 cognitive biases
1517	Attention-based models belong to a class of models commonly called sequence-to-sequence models. The aim of these models, as name suggests, it to produce an output sequence given an input sequence which are, in general, of different lengths.	What is Attention based model
5501	Semi-Markov decision processes (SMDPs), generalize MDPs by allowing the state transitions to occur in continuous irregular times. In this framework, after the agent takes action a in state s, the environment will remain in state s for time d and then transits to the next state and the agent receives the reward r.	What is semi Markov decision process
849	Downsampling an image When data is removed the image also degrades to some extent, although not nearly as much as when you upsample. By removing this extra data ( downsampling) this results in a much smaller file size. For example, you can see below that our original image was 17.2 MB at 3000 by 2000 pixels.	Does downsampling reduce image quality
7643	"In statistical hypothesis testing, a type I error is the rejection of a true null hypothesis (also known as a ""false positive"" finding or conclusion; example: ""an innocent person is convicted""), while a type II error is the non-rejection of a false null hypothesis (also known as a ""false negative"" finding or conclusion"	How do you determine Type 1 and Type 2 errors
3744	In cluster sampling, researchers divide a population into smaller groups known as clusters.You thus decide to use the cluster sampling method.Step 1: Define your population.  Step 2: Divide your sample into clusters.  Step 3: Randomly select clusters to use as your sample.  Step 4: Collect data from the sample.	How do you select clusters in cluster sampling
1149	The shape of the t distribution changes with sample size.  As the sample size increases the t distribution becomes more and more like a standard normal distribution. In fact, when the sample size is infinite, the two distributions (t and z) are identical.	What happens to the shape of the T distribution as the sample size increases
1620	Standard deviation tells you how spread out the data is. It is a measure of how far each observed value is from the mean. In any distribution, about 95% of values will be within 2 standard deviations of the mean.	What does the standard deviation tell you
8407	The mean for the standard normal distribution is zero, and the standard deviation is one. The transformation z=x−μσ z = x − μ σ produces the distribution Z ~ N(0, 1).	What is the mean μ of the standard normal distribution
1490	Automatic thresholdingSelect initial threshold value, typically the mean 8-bit value of the original image.Divide the original image into two portions;  Find the average mean values of the two new images.Calculate the new threshold by averaging the two means.More items	How is threshold value calculated in image processing
2078	categorization have not been convincingly shown. In this work we demonstrated that image segmentation can in fact improve object recognition and categorization and it also adds object localization and multi-class categorization ca- pabilities to an off-the-shelf categorization system.	Does image segmentation improve object categorization
5485	Coreference resolution is the task of finding all expressions that refer to the same entity in a text. It is an important step for a lot of higher level NLP tasks that involve natural language understanding such as document summarization, question answering, and information extraction.	What is Coreference resolution in NLP
6969	Digital image processing, as a computer-based technology, carries out automatic processing, manipulation and interpretation of such visual information, and it plays an increasingly important role in many aspects of our daily life, as well as in a wide variety of disciplines and fields in science and technology, with	What are the application of image processing
1063	The three main metrics used to evaluate a classification model are accuracy, precision, and recall. Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.	How do you check machine learning accuracy
906	Estimation, in statistics, any of numerous procedures used to calculate the value of some property of a population from observations of a sample drawn from the population.  A point estimate, for example, is the single number most likely to express the value of the property.	What is an estimate statistics
7515	An HMM topology is defined as the statistical behavior of an observable symbol sequence in terms of a network of states, which represents the overall process behavior with regard to movement between states of the process, and describes the inherent variations in the behavior of the observable symbols within a state.	What is Hidden Markov Model Topology
1672	In statistical classification, Bayes error rate is the lowest possible error rate for any classifier of a random outcome (into, for example, one of two categories) and is analogous to the irreducible error. A number of approaches to the estimation of the Bayes error rate exist.	What is the Bayesian probability of an error
3727	The Poisson parameter Lambda (λ) is the total number of events (k) divided by the number of units (n) in the data (λ = k/n).	How do you find the lambda in a Poisson distribution
6436	Dense layer is the regular deeply connected neural network layer. It is most common and frequently used layer. Dense layer does the below operation on the input and return the output.  dot represent numpy dot product of all input and its corresponding weights.	What is dense layer in sequential model
999	The eigenvalues and eigenvectors of a matrix are often used in the analysis of financial data and are integral in extracting useful information from the raw data. They can be used for predicting stock prices and analyzing correlations between various stocks, corresponding to different companies.	Where do we use eigen values
989	Random forest does handle missing data and there are two distinct ways it does so: 1) Without imputation of missing data, but providing inference. 2) Imputing the data.  Prior to splitting a node, missing data for a variable is imputed by randomly drawing values from non-missing in-bag data.	How does random forest deal with missing values
4287	In supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.	What is sample error in machine learning
7760	The coefficient of determination is the square of the correlation (r) between predicted y scores and actual y scores; thus, it ranges from 0 to 1. With linear regression, the coefficient of determination is also equal to the square of the correlation between x and y scores.	How is coefficient of determination calculated
6439	The independent variable is called the Explanatory variable (or better known as the predictor) - the variable which influences or predicts the values. i.e. if the explanatory variable changes then it affects the response variable. Here Y is the Dependent variable or response variable.	Which variable is the explanatory variable
2754	TL;DR: It is possible to learn Data Science with Low-Code experience.  There are some basic principles of data science that you need to learn before learning Python, and you can start solving many real world problems without any coding at all!	Can you be a data scientist without coding
7552	Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.	Can logistic regression be applied to multi class classification problem
6059	"The technological singularity—also, simply, the singularity—is a hypothetical point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.  The first use of the concept of a ""singularity"" in the technological context was John von Neumann."	What is it called when AI takes over
5166	In cryptography, padding is any of a number of distinct practices which all include adding data to the beginning, middle, or end of a message prior to encryption.	What is padding in encryption
3891	A Latent Class regression model: Is used to predict a dependent variable as a function of predictor variables (Regression model). Includes a K-category latent variable X to cluster cases (LC model)  Each case may contain multiple records (Regression with repeated measurements).	What is latent class regression
5033	A CNN has multiple layers. Weight sharing happens across the receptive field of the neurons(filters) in a particular layer. Weights are the numbers within each filter.  These filters act on a certain receptive field/ small section of the image. When the filter moves through the image, the filter does not change.	What is weight sharing in CNN
783	The main difference between the two, is that a Perceptron takes that binary response (like a classification result) and computes an error used to update the weights, whereas an Adaline uses a continous response value to update the weights (so before the binarized output is produced).	What is the difference between a Perceptron Adaline and neural network model
7829	Bennett University. White noise is used in context of linear regression. It refers to a case when residuals (errors) are random and come from a single N(0, sigma^2) distribution. Clearly, the residuals are iid with a condition that their expectation is zero.	What is white noise in Regression
3275	Feature extraction is a general term for methods of constructing combinations of the variables to get around these problems while still describing the data with sufficient accuracy. Many machine learning practitioners believe that properly optimized feature extraction is the key to effective model construction.	What are feature extraction algorithms
8330	Hidden Markov model (HMM) has been successfully used for sequential data modeling problems.  In the proposed GenHMM, each HMM hidden state is associated with a neural network based generative model that has tractability of exact likelihood and provides efficient likelihood computation.	Is a hidden Markov model a neural network
1491	The standard deviation (SD) measures the amount of variability, or dispersion, from the individual data values to the mean, while the standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean.  SD is the dispersion of individual data values.	What is the relation between mean and standard deviation
7380	How to Choose a Machine Learning Model – Some GuidelinesCollect data.Check for anomalies, missing data and clean the data.Perform statistical analysis and initial visualization.Build models.Check the accuracy.Present the results.	How do I decide which model to use
1089	Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.	Is machine learning important for Artificial Intelligence
468	How to Run Your First Classifier in WekaDownload Weka and Install. Visit the Weka Download page and locate a version of Weka suitable for your computer (Windows, Mac, or Linux).  Start Weka. Start Weka.  Open the data/iris. arff Dataset.  Select and Run an Algorithm.  Review Results.	How is weka used in data mining
330	In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm. #	What is Delta rule in machine learning
8401	How to optimize your meta tags: A checklistCheck whether all your pages and your content have title tags and meta descriptions.Start paying more attention to your headings and how you structure your content.Don't forget to mark up your images with alt text.More items•	How do you optimize meta tags
421	Principal Component Analysis PCA's approach to data reduction is to create one or more index variables from a larger set of measured variables. It does this using a linear combination (basically a weighted average) of a set of variables. The created index variables are called components.	What is a component in factor analysis
8658	In the modern context, computational intelligence tends to use bio-inspired computing, like evolutionary and genetic algorithms. AI tends to prefer techniques with stronger theoretical guarantees, and still has a significant community focused on purely deductive reasoning.	What is the difference between computational intelligence and artificial intelligence
3736	Inter-Rater or Inter-Observer Reliability: Used to assess the degree to which different raters/observers give consistent estimates of the same phenomenon. Test-Retest Reliability: Used to assess the consistency of a measure from one time to another.	For which measure would inter rater reliability be applicable
1185	We call vectorization the general process of turning a collection of text documents into numerical feature vectors.  Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.	What is feature vectorization
7983	2:0210:15Suggested clip · 117 secondsConducting a Multiple Regression using Microsoft Excel Data YouTubeStart of suggested clipEnd of suggested clip	How do you do regression with multiple variables
5256	KNN represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points, while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters.	What is KNN clustering
5280	Classification accuracy is our starting point. It is the number of correct predictions made divided by the total number of predictions made, multiplied by 100 to turn it into a percentage.	How do you calculate classification accuracy
1208	Model calibration is the process of adjustment of the model parameters and forcing within the margins of the uncertainties (in model parameters and / or model forcing) to obtain a model representation of the processes of interest that satisfies pre-agreed criteria (Goodness-of-Fit or Cost Function).	What is model calibration
176	Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model.	Is boosting an ensemble method
537	The midrange is a type of average, or mean. Electronic gadgets are sometimes classified as “midrange”, meaning they're in the middle-price bracket. The formula to find the midrange = (high + low) / 2.	What is the formula for a mid range in statistics
5438	There are six broad steps to data wrangling, which are:Discovering. In this step, the data is to be understood more deeply.  Structuring. Raw data is given to you in a haphazard manner, in most cases – there will not be any structure to it.  Cleaning.  Enriching.  Validating.  Publishing.	How do you do data wrangling
8480	A hidden unit corresponds to the output of a single filter at a single particular x/y offset in the input volume.	What is a hidden unit
226	Effect size is a simple way of quantifying the difference between two groups that has many advantages over the use of tests of statistical significance alone. Effect size emphasises the size of the difference rather than confounding this with sample size.  A number of alternative measures of effect size are described.	What does effect size mean in statistics
1475	When the sample size is sufficiently large, the shape of the sampling distribution approximates a normal curve (regardless of the shape of the parent population)! The distribution of sample means is a more normal distribution than a distribution of scores, even if the underlying population is not normal.	Why does the sampling distribution of the mean follow a normal distribution
3823	Using Logarithmic Functions Much of the power of logarithms is their usefulness in solving exponential equations. Some examples of this include sound (decibel measures), earthquakes (Richter scale), the brightness of stars, and chemistry (pH balance, a measure of acidity and alkalinity).	What is the application of logarithm
514	Machine learning and Data Science are intricately linked To take your career as high as you can't even imagine, you can become competent in both these fields, which will enable you to analyse a frightening amount of data, and then proceed to extract value and provide insight on the data.	Why did you learn Machine Learning
2922	Definition. The cumulative distribution function (CDF) of random variable X is defined as FX(x)=P(X≤x), for all x∈R. Note that the subscript X indicates that this is the CDF of the random variable X. Also, note that the CDF is defined for all x∈R. Let us look at an example.	What is cumulative distribution function with example
7727	Accuracy (Figure 1) is a measure of how close an achieved position is to a desired target position. Repeatability (Figure 2) is a measure of a system's consistency to achieve identical results across multiple tests. The ultimate goal is to have both a highly accurate and highly repeatable system.	What is the difference between repeatability and accuracy
8053	Since this impulse response in infinitely long, recursive filters are often called infinite impulse response (IIR) filters. In effect, recursive filters convolve the input signal with a very long filter kernel, although only a few coefficients are involved.	Is IIR filter recursive
5832	Definition and Notation In handwriting, a tilde, arrow or underline is used to denote a vector. The convention for handwritten notation varies with geography and subject area. Vectors can be described using Cartesian coordinates, giving the components of the vector along each of the axes. Example: a=(a1,a2,a3).	How do you denote a vector
152	Sensitivity is the proportion of patients with disease who test positive. In probability notation: P(T+|D+) = TP / (TP+FN). Specificity is the proportion of patients without disease who test negative. In probability notation: P(T-|D-) = TN / (TN + FP).  It is the proportion of total patients who have the disease.	What are TP FP TN FN
4485	One of the simplest and yet most important models in time series forecasting is the random walk model. This model assumes that in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (“i.i.d.”).	What is a random walk model
532	When the order doesn't matter, it is a Combination. When the order does matter it is a Permutation.	What is the difference between permutation and combination in statistics
3363	Univariate analysis is the simplest form of analyzing data. “Uni” means “one”, so in other words your data has only one variable. It doesn't deal with causes or relationships (unlike regression ) and it's major purpose is to describe; It takes data, summarizes that data and finds patterns in the data.	How do you analyze univariate data
4229	A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.	What is a posterior mean
1943	Hierarchical clustering is a powerful technique that allows you to build tree structures from data similarities. You can now see how different sub-clusters relate to each other, and how far apart data points are.	What is hierarchical clustering used for
7178	Statement: A continuous time signal can be represented in its samples and can be recovered back when sampling frequency fs is greater than or equal to the twice the highest frequency component of message signal.	What are the ideal conditions under which a signal can be reconstructed back in process of sampling
2101	It is often used as a gauge of economic inequality, measuring income distribution or, less commonly, wealth distribution among a population. The coefficient ranges from 0 (or 0%) to 1 (or 100%), with 0 representing perfect equality and 1 representing perfect inequality.	What does a lower Gini coefficient mean
4213	A Markov process is a random process in which the future is independent of the past, given the present. Thus, Markov processes are the natural stochastic analogs of the deterministic processes described by differential and difference equations. They form one of the most important classes of random processes.	What is meant by Markov process
1035	To write a null hypothesis, first start by asking a question. Rephrase that question in a form that assumes no relationship between the variables. In other words, assume a treatment has no effect. Write your hypothesis in a way that reflects this.	How do you write a hypothesis and null hypothesis
4529	A non-parametric test is a hypothesis test that does not make any assumptions about the distribution of the samples.  It does not rely on any properties of the distributions. The null hypothesis is that the samples were drawn from the same distribution.	What is a non parametric hypothesis test
5509	Enter (Regression) . A procedure for variable selection in which all variables in a block are entered in a single step. Stepwise . At each step, the independent variable not in the equation that has the smallest probability of F is entered, if that probability is sufficiently small.	What is the Enter method in multiple regression
538	This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction.	What is a one tailed vs a two tailed test
4342	By Jim Frost 45 Comments. Heteroscedasticity means unequal scatter. In regression analysis, we talk about heteroscedasticity in the context of the residuals or error term. Specifically, heteroscedasticity is a systematic change in the spread of the residuals over the range of measured values.	What is homoscedasticity in linear regression
1509	As much as I understand, in value iteration, you use the Bellman equation to solve for the optimal policy, whereas, in policy iteration, you randomly select a policy π, and find the reward of that policy.	How is policy iteration different from value iteration
5876	There are three primary assumptions in ANOVA: The responses for each factor level have a normal population distribution. These distributions have the same variance. The data are independent.	What are the assumptions of analysis of variance
8229	An unbiased estimator is an accurate statistic that's used to approximate a population parameter.  That's just saying if the estimator (i.e. the sample mean) equals the parameter (i.e. the population mean), then it's an unbiased estimator.	What does it mean to be an unbiased estimator
8084	Grid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.  In grid searching, you first define the range of values for each of the hyperparameters a1, a2 and a3.	Machine Learning How does grid search work
4297	In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.  Given these hyperparameters, the training algorithm learns the parameters from the data.	What are Hyperparameters in machine learning
6627	Advantages and Disadvantages of Machine Learning LanguageEasily identifies trends and patterns. Machine Learning can review large volumes of data and discover specific trends and patterns that would not be apparent to humans.  No human intervention needed (automation)  Continuous Improvement.  Handling multi-dimensional and multi-variety data.  Wide Applications.	What are the advantages and disadvantages of machine learning
2595	The least squares principle states that the SRF should be constructed (with the constant and slope values) so that the sum of the squared distance between the observed values of your dependent variable and the values estimated from your SRF is minimized (the smallest possible value).	What is the principle of least square
2135	A cantilever beam is given an initial deflection and then released. Its vibration is an eigenvalue problem and the eigenvalues are the natural frequencies of vibration and the eigenvectors are the mode shapes of the vibration.	What is eigenvalue and eigenvector in vibration
3619	The cosine similarity is the cosine of the angle between two vectors. Figure 1 shows three 3-dimensional vectors and the angles between each pair. In text analysis, each vector can represent a document. The greater the value of θ, the less the value of cos θ, thus the less the similarity between two documents.	How do you find the cosine similarity between two documents
6875	The stress state is a second order tensor since it is a quantity associated with two directions. As a result, stress components have 2 subscripts. A surface traction is a first order tensor (i.e. vector) since it a quantity associated with only one direction. Vector components therefore require only 1 subscript.	Why Stress is a second order tensor
7617	Traditional programming is a manual process—meaning a person (programmer) creates the program. But without anyone programming the logic, one has to manually formulate or code rules. In machine learning, on the other hand, the algorithm automatically formulates the rules from the data.	How is machine learning programmed
5052	An autoregressive model is when a value from a time series is regressed on previous values from that same time series.  In this regression model, the response variable in the previous time period has become the predictor and the errors have our usual assumptions about errors in a simple linear regression model.	What is autoregressive model in time series
4188	Number of discriminant functions. There is one discriminant function for 2- group discriminant analysis, but for higher order DA, the number of functions is the lesser of (g - 1), where g is the number of groups, or p,the number of discriminating (independent) variables.	How many functions does the discriminant have
596	Since the theory is about eigenvalues of linear operators, and Heisenberg and other physicists related the spectral lines seen with prisms or gratings to eigenvalues of certain linear operators in quantum mechanics, it seems logical to explain the name as inspired by relevance of the theory in atomic physics.	Why is it called spectral theorem
1227	The area percentage (proportion, probability) calculated using a z-score will be a decimal value between 0 and 1, and will appear in a Z-Score Table. The total area under any normal curve is 1 (or 100%).	What is the area under the normal curve between z
6702	An F-test is any statistical test in which the test statistic has an F-distribution under the null hypothesis. It is most often used when comparing statistical models that have been fitted to a data set, in order to identify the model that best fits the population from which the data were sampled.	What is the purpose of an F test
2230	A distribution with a single mode is said to be unimodal. A distribution with more than one mode is said to be bimodal, trimodal, etc., or in general, multimodal.	Is the distribution unimodal or multimodal
546	"In statistics, a unimodal probability distribution or unimodal distribution is a probability distribution which has a single peak. The term ""mode"" in this context refers to any peak of the distribution, not just to the strict definition of mode which is usual in statistics."	What does unimodal mean in statistics
7465	Test statistic. The test statistic is a z-score (z) defined by the following equation. where P is the hypothesized value of population proportion in the null hypothesis, p is the sample proportion, and σ is the standard deviation of the sampling distribution.	Which statistic is used in hypothesis testing for population proportions
2900	It's greedy because you always mark the closest vertex. It's dynamic because distances are updated using previously calculated values. I would say it's definitely closer to dynamic programming than to a greedy algorithm. To find the shortest distance from A to B, it does not decide which way to go step by step.	Why is Dijkstra A greedy algorithm
419	Strongly Connected Components1) Create an empty stack 'S' and do DFS traversal of a graph. In DFS traversal, after calling recursive DFS for adjacent vertices of a vertex, push the vertex to stack.  2) Reverse directions of all arcs to obtain the transpose graph.3) One by one pop a vertex from S while S is not empty. Let the popped vertex be 'v'.	How do you find strongly connected components
7287	To calculate the Sharpe ratio on a portfolio or individual investment, you first calculate the expected return for the investment. You then subtract the risk free rate from the expected return, then divide this sum by the standard deviation of the of the portfolio or individual investment. This gives you the ratio.	How do I calculate the average deviation for a standard deviation in the Sharpe ratio
1123	In statistics and probability analysis, the expected value is calculated by multiplying each of the possible outcomes by the likelihood each outcome will occur and then summing all of those values. By calculating expected values, investors can choose the scenario most likely to give the desired outcome.	What is the expected value of an estimator
1337	K-means algorithm can be summarized as follow:Specify the number of clusters (K) to be created (by the analyst)Select randomly k objects from the dataset as the initial cluster centers or means.Assigns each observation to their closest centroid, based on the Euclidean distance between the object and the centroid.More items	How do I run K means clustering in R
6267	Skewness refers to distortion or asymmetry in a symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed. Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution.	What is the meaning of skewness in statistics
1262	remove outliers data.Do feature selection, some of features may not be as informative.May be the linear regression under fitting or over fitting the data you can check ROC curve and try to use more complex model like polynomial regression or regularization respectively.	How do you reduce RMSE in linear regression
8122	The top 5 AI developments as chosen by our team are as follows:The increased speed of AI-enabled medical research.  Computer vision, image, and video analysis technology is evolving.  Powerful AI-based tools become mainstream.  AI learns increasingly higher-level human functions.More items•	What are the recent developments in AI
6438	In fact, we show that they coincide precisely when the confusion matrix is perfectly symmetric. In other situations, however, their behaviour can diverge to the point that Kappa should be avoided as a measure of behaviour to compare classifiers in favor of more robust measures as MCC.	Why Cohen's kappa should be avoided as performance measure in classification
2028	"Low-rank approximation is thus a way to recover the ""original"" (the ""ideal"" matrix before it was messed up by noise etc.) low-rank matrix i.e., find the matrix that is most consistent (in terms of observed entries) with the current matrix and is low-rank so that it can be used as an approximation to the ideal matrix."	Why do we need low rank approximation
2812	A histogram looks like a bar chart , except the area of the bar, and not the height, shows the frequency of the data . Histograms are typically used when the data is in groups of unequal width.  This is called frequency density.	Do histograms use frequency density
758	Even when multicollinearity is great, the least-squares regression equation can be highly predictive. So, if you are only interested in prediction, multicollinearity is not a problem.	Is Multicollinearity a problem for prediction
5768	The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models. Most often, factors are rotated after extraction.	What is the point of factor analysis
965	Features: The characteristics that define your problem. These are also called attributes. Parameters: The variables your algorithm is trying to tune to build an accurate model.	What are the features and parameters in machine learning
781	The “Linear-by-Linear Association” statistic is used when the variables are ordinal, but many simply use the Pearson for those as well. Column 2 shows the Chi Square values for each alternative test. The main one of interest is the Pearson Chi-Square value of .	When one should use a linear by linear association chi square test
8526	Machine learning algorithms are almost always optimized for raw, detailed source data. Thus, the data environment must provision large quantities of raw data for discovery-oriented analytics practices such as data exploration, data mining, statistics, and machine learning.	What type of data does machine learning use
6742	There are two types of methods used for image processing namely, analogue and digital image processing.  Image analysts use various fundamentals of interpretation while using these visual techniques.	What are the two major types of image analysis
7912	If your regression model contains independent variables that are statistically significant, a reasonably high R-squared value makes sense. The statistical significance indicates that changes in the independent variables correlate with shifts in the dependent variable.	How do you determine statistical significance in regression
6469	Not usually. SGD tends to perform better than using line search.	Is line search used commonly with SGD while learning the parameters for a deep neural networks
1935	In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right. In a negatively skewed distribution, the mean is usually less than the median because the few low scores tend to shift the mean to the left.	Which is an implication of a positively skewed distribution
429	Random effect models assist in controlling for unobserved heterogeneity when the heterogeneity is constant over time and not correlated with independent variables.  Two common assumptions can be made about the individual specific effect: the random effects assumption and the fixed effects assumption.	Why do we use random effects
563	Abstract. This work centers on a novel data mining technique we term supervised clustering. Unlike traditional clustering, supervised clustering assumes that the examples are classified. The goal of supervised clustering is to identify class-uniform clusters that have high probability densities.	What is supervised clustering
1034	The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.  The rectified linear activation is the default activation when developing multilayer Perceptron and convolutional neural networks.	What is ReLU function in neural network
7230	Time series analysis, on the other hand, is a field of statistics/econometrics where we try to understand trends in time series data , draw graphs : and make predictions out of it (via linear regression, ARIMA, etc..). In that way, we could say that it's more of a supervised approach.	Is Arima supervised or unsupervised
7648	Word embeddings are created using a neural network with one input layer, one hidden layer and one output layer. The computer does not understand that the words king, prince and man are closer together in a semantic sense than the words queen, princess, and daughter. All it sees are encoded characters to binary.	How are word Embeddings generated
4700	Normal Distribution is a probability distribution where probability of x is highest at centre and lowest in the ends whereas in Uniform Distribution probability of x is constant. Uniform Distribution is a probability distribution where probability of x is constant.	What is the difference between normal and uniform distribution
1450	Distributed representation describes the same data features across multiple scalable and interdependent layers. Each layer defines the information with the same level of accuracy, but adjusted for the level of scale. These layers are learned concurrently but in a non-linear fashion.	Deep Learning What is meant by a distributed representation
4560	Fixed effects models remove omitted variable bias by measuring changes within groups across time, usually by including dummy variables for the missing or unknown characteristics.	When would you use a fixed effects model
5176	According to gradient descent rule, we should update the weight according to w = w - df/dw.	Which formula is used to update weights while performing gradient descent
6173	If an overestimate or underestimate does happen, the mean of the difference is called a “bias.” That's just saying if the estimator (i.e. the sample mean) equals the parameter (i.e. the population mean), then it's an unbiased estimator.	How do you prove an estimator is biased
155	K-fold cross-validationRandomly split the data set into k-subsets (or k-fold) (for example 5 subsets)Reserve one subset and train the model on all other subsets.Test the model on the reserved subset and record the prediction error.Repeat this process until each of the k subsets has served as the test set.More items•	How do you do k fold cross validation in R
7706	Parametric tests are those that make assumptions about the parameters of the population distribution from which the sample is drawn. This is often the assumption that the population data are normally distributed. Non-parametric tests are “distribution-free” and, as such, can be used for non-Normal variables.	What is parametric and non parametric tests
561	An example of numerical data would be the number of people that attended the movie theater over the course of a month.  You can also put data in ascending (least to greatest) and descending (greatest to least) order. Data can only be numerical if the answers can be represented in fraction and/or decimal form.	Which is an example of numeric data
4897	Max pooling is a sample-based discretization process. The objective is to down-sample an input representation (image, hidden-layer output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.	What is the purpose of Max pooling
3915	Digital Signal Processing is important because it significantly increases the overall value of hearing protection. Unlike passive protection, DSP suppresses noise without blocking the speech signal.	Why signal processing is needed
1356	Statistical inference can be divided into two areas: estimation and hypothesis testing. In estimation, the goal is to describe an unknown aspect of a population, for example, the average scholastic aptitude test (SAT) writing score of all examinees in the State of California in the USA.	What two ways can we do statistical inference
1325	As the name suggests, GLM models are the generalization of the linear regression model.  we mean that rather than forcing a linear relationship between the dependent and independent variables, it allows the dependent variable to be related with the independent variables through a link function.	What is a generalized linear model in laymans terms
2565	Logarithmic Loss, or simply Log Loss, is a classification loss function often used as an evaluation metric in Kaggle competitions.  Log Loss quantifies the accuracy of a classifier by penalising false classifications.	What is logarithmic loss
7565	Kinesthetic learners are the most hands-on learning type. They learn best by doing and may get fidgety if forced to sit for long periods of time. Kinesthetic learners do best when they can participate in activities or solve problems in a hands-on manner.	What learning style is best
6770	Any dataset with an unequal class distribution is technically imbalanced. However, a dataset is said to be imbalanced when there is a significant, or in some cases extreme, disproportion among the number of examples of each class of the problem.	What is an imbalanced dataset
1111	Descriptive statistics help us to simplify large amounts of data in a sensible way. Each descriptive statistic reduces lots of data into a simpler summary. For instance, consider a simple number used to summarize how well a batter is performing in baseball, the batting average.	What are the advantages of descriptive statistics
2345	The false discovery rate is the ratio of the number of false positive results to the number of total positive test results. Out of 10,000 people given the test, there are 450 true positive results (box at top right) and 190 false positive results (box at bottom right) for a total of 640 positive results.	How do you interpret a false discovery rate
3262	The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image).	What is median filter in image processing
2214	Cross-sectional data are the result of a data collection, carried out at a single point in time on a statistical unit. With cross-sectional data, we are not interested in the change of data over time, but in the current, valid opinion of the respondents about a question in a survey.	What does cross sectional data mean
8143	A paired t-test is used when we are interested in the difference between two variables for the same subject. Often the two variables are separated by time. For example, in the Dixon and Massey data set we have cholesterol levels in 1952 and cholesterol levels in 1962 for each subject.	Why would you use a paired t test
6828	POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition. This task is not straightforward, as a particular word may have a different part of speech based on the context in which the word is used.	How is POS tagging done
8254	There are multiple ways to select a good starting point for the learning rate. A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. We might start with a large value like 0.1, then try exponentially lower values: 0.01, 0.001, etc.	How do you determine learning rate
3272	fastText is another word embedding method that is an extension of the word2vec model. Instead of learning vectors for words directly, fastText represents each word as an n-gram of characters.  This helps capture the meaning of shorter words and allows the embeddings to understand suffixes and prefixes.	What is fastText embedding
8014	The number of units is a parameter in the LSTM, referring to the dimensionality of the hidden state and dimensionality of the output state (they must be equal). a LSTM comprises an entire layer.	What is a unit in Lstm
1770	"AUC stands for ""Area under the ROC Curve."" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). Figure 5. AUC (Area under the ROC Curve). AUC provides an aggregate measure of performance across all possible classification thresholds."	What does AUC measure
248	People always think crime is increasing” even if it's not. He addresses the logical fallacy of confirmation bias, explaining that people's tendency, when testing a hypothesis they're inclined to believe, is to seek examples confirming it.  “Most people think they're not like other people.	Is confirmation bias a fallacy
3837	Ensemble learning methods are widely used nowadays for its predictive performance improvement. Ensemble learning combines multiple predictions (forecasts) from one or multiple methods to overcome accuracy of simple prediction and to avoid possible overfit.	Is it possible to use ensemble learning for time series forecast
3817	We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search.	What are the current challenges in neural machine translation
779	When we know an input value and want to determine the corresponding output value for a function, we evaluate the function.  When we know an output value and want to determine the input values that would produce that output value, we set the output equal to the function's formula and solve for the input.	What is output value
7368	The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered. In machine learning, one aims to construct algorithms that are able to learn to predict a certain target output.	What is learning bias
737	We use factorials when we look at permutations and combinations. Permutations tell us how many different ways we can arrange things if their order matters. Combinations tells us how many ways we can choose k item from n items if their order does not matter.	Why do we use Factorials
8329	You simply measure the number of correct decisions your classifier makes, divide by the total number of test examples, and the result is the accuracy of your classifier. It's that simple. The vast majority of research results report accuracy, and many practical projects do too.	How do you evaluate a classifier
2925	The output layer is responsible for producing the final result. There must always be one output layer in a neural network. The output layer takes in the inputs which are passed in from the layers before it, performs the calculations via its neurons and then the output is computed.	What is output layer in neural network
1049	From the menus of SPSS choose: Analyze Scale Multidimensional Scaling… In Distances, select either Data are distances or Create distances from data. If your data are distances, you must select at least four numeric variables for analysis, and you can click Shape to indicate the shape of the distance matrix.	How do you do multidimensional scaling in SPSS
8399	Overfitting occurs when a statistical model or machine learning algorithm captures the noise of the data.  Specifically, underfitting occurs if the model or algorithm shows low variance but high bias. Underfitting is often a result of an excessively simple model.	What is Overfitting and Underfitting in learning
8257	Supervised learning allows collecting data and produce data output from the previous experiences. Helps to optimize performance criteria with the help of experience. Supervised machine learning helps to solve various types of real-world computation problems.	Why do we use supervised learning
1170	Definition. A convenience sample is a type of non-probability sampling method where the sample is taken from a group of people easy to contact or to reach. For example, standing at a mall or a grocery store and asking people to answer questions would be an example of a convenience sample.	What is convenience sampling with example
473	The weighted kappa is calculated using a predefined table of weights which measure the degree of disagreement between the two raters, the higher the disagreement the higher the weight.	When should I use weighted kappa
7876	Go to 'Filter > Blur > Gaussian Blur…' and the 'Gaussian Blur' window will appear. You can drag the image in the 'Gaussian Blur' window to look for the object you're going to blur. If you find it too small, tick the 'Preview' box and the result of the 'Gaussian Filter' blur will be visible in the image.	How do you use Gaussian blur
207	Nonetheless, they are not the same. Standard deviation is used to measure the spread of data around the mean, while RMSE is used to measure distance between some values and prediction for those values.  If you use mean as your prediction for all the cases, then RMSE and SD will be exactly the same.	Is RMSE the same as standard deviation
2952	Discrete variables are countable in a finite amount of time. For example, you can count the change in your pocket. You can count the money in your bank account. You could also count the amount of money in everyone's bank accounts.	What are examples of discrete variables
3582	The “regular” normal distribution has one random variable; A bivariate normal distribution is made up of two independent random variables. The two variables in a bivariate normal are both are normally distributed, and they have a normal distribution when both are added together.	What is the probability from this bivariate normal distribution
3929	For classification: As a general rule, the more the hidden layers, the better the network. But, as the hidden layers increase, your network becomes data hungry. So, your dataset should have sufficient number of samples to feed the hungry network. Otherwise your network will overfit the training set.	How does the number of hidden units affect the final accuracy on the test data
3314	Orange is an open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative rapid qualitative data analysis and interactive data visualization.	What is orange Data Mining Tool
2896	In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.	What is an LDA model
530	Here are some important considerations while choosing an algorithm.Size of the training data. It is usually recommended to gather a good amount of data to get reliable predictions.  Accuracy and/or Interpretability of the output.  Speed or Training time.  Linearity.  Number of features.	How do I find the best machine learning algorithm
5981	Variance (σ2) in statistics is a measurement of the spread between numbers in a data set. That is, it measures how far each number in the set is from the mean and therefore from every other number in the set.	What does variance mean in at test
1030	The ACF stands for Autocorrelation function, and the PACF for Partial Autocorrelation function. Looking at these two plots together can help us form an idea of what models to fit. Autocorrelation computes and plots the autocorrelations of a time series.	What is ACF and PACF in Arima
1044	Lets find out the model of the binomial distribution who PMF is:f(x) = C(n, x)p^x(1-p)^(n-x) where 0<= p <= 1 and x = 0(1)n.Note that when p=0, f(0)=1 and f(x)=0 when x> 0.  So, binomial distribution is unimodal if p = 0 or p=1.But what happens when 0<p<1?In such cases,f(x+1)/f(x) = (n-x)p / [(x+1)(1-p)]More items	How can I prove that binomial distribution is unimodal
552	fits that relationship. That line is called a Regression Line and has the equation ŷ= a + b x. The Least Squares Regression Line is the line that makes the vertical distance from the data points to the regression line as small as possible.	What is the least squares regression equation
1112	The binomial distribution is a probability distribution that summarizes the likelihood that a value will take one of two independent values under a given set of parameters or assumptions.	What is a binomial distribution in statistics
33	Principle Component Analysis (PCA) is a common feature extraction method in data science. Technically, PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues and then uses those to project the data into a new subspace of equal or less dimensions.	Is PCA feature extraction
3164	Qualitative Variables. Also known as categorical variables, qualitative variables are variables with no natural sense of ordering. They are therefore measured on a nominal scale. For instance, hair color (Black, Brown, Gray, Red, Yellow) is a qualitative variable, as is name (Adam, Becky, Christina, Dave . . .).	What is an example of a qualitative variable
6589	The main differences therefore are that Gradient Boosting is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function. Hence, gradient boosting is much more flexible.	What is the difference between AdaBoost and gradient boosting
448	"A weighted average (weighted mean or scaled average) is used when we consider some data values to be more important than other values and so we want them to contribute more to the final ""average"". This often occurs in the way some professors or teachers choose to assign grades in their courses."	Why weighted mean is used
1269	The converse of Theorem 1 is the following: Given vector field F = Pi + Qj on D with C1 coefficients, if Py = Qx, then F is the gradient of some function.	How do you tell if a vector field is a gradient field
1374	In statistics, the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary. The number of independent ways by which a dynamic system can move, without violating any constraint imposed on it, is called number of degrees of freedom.	What is degree of freedom in standard deviation
7252	The formula for conditional probability is derived from the probability multiplication rule, P(A and B) = P(A)*P(B|A). You may also see this rule as P(A∪B). The Union symbol (∪) means “and”, as in event A happening and event B happening.	What is the formula for conditional probability
185	According to Cohen's original article, values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.	What is acceptable inter rater reliability
2352	Correlation coefficient values below 0.3 are considered to be weak; 0.3-0.7 are moderate; >0.7 are strong. You also have to compute the statistical significance of the correlation.	Is 0.3 A strong correlation
82	"Mean Symbol With Alt Codes Type the letter ""x,"" hold the Alt key and type ""0772"" into the number pad. This adds the bar symbol to the x."	How do you insert x bar population mean into a Word document
2440	A continuous distribution has a range of values that are infinite, and therefore uncountable. For example, time is infinite: you could count from 0 seconds to a billion seconds…a trillion seconds…and so on, forever.	What are some examples of continuous distribution probability
1419	Gradient Descent is the most basic but most used optimization algorithm. It's used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm.	Which optimization technique is the most commonly used for neural network training
7097	(1) False-positive results may occur in patients with prior infection with M marinum, M szulgai, or M kansasii. Negative: No IFN-gamma response to M tuberculosis antigens was detected. Infection with M tuberculosis is unlikely.	What causes false positive QuantiFERON gold
4736	Introduction. This method determines the chloride ion concentration of a solution by titration with silver nitrate. As the silver nitrate solution is slowly added, a precipitate of silver chloride forms. Ag+(aq) + Cl–(aq) → AgCl(s) The end point of the titration occurs when all the chloride ions are precipitated.	What is Mohr method of titration
543	The sensitivity and specificity of a test often vary with disease prevalence; this effect is likely to be the result of mechanisms, such as patient spectrum, that affect prevalence, sensitivity and specificity.	Can sensitivity and specificity depend on prevalence
140	Minimum description length (MDL) refers to various formalizations of Occam's razor based on formal languages used to parsimoniously describe data. In its most basic form, MDL is a model selection principle: the shortest description of the data as the best model.	What is minimum description length principle explain with example
4841	Despite having similar aims and processes, there are two main differences between them: Machine learning works out predictions and recalibrates models in real-time automatically after design. Meanwhile, predictive analytics works strictly on “cause” data and must be refreshed with “change” data.	What is the difference between analytics and machine learning
56	Autoregression is a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step. It is a very simple idea that can result in accurate forecasts on a range of time series problems.	What are autoregressive models in machine learning
83	In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated.	Which is a biased estimator
1440	(regression, Anova, location problems?) Typically when an assumption is made that the error is normally distributed the reason lies in history: assuming the error structure was normal made the work required to develop test statistics, estimates, and other calculations, relatively easy.	Why we assume in linear regression the errors are normally distributed
1365	Given sufficient training data (often hundreds or thousands of images per label), an image classification model can learn to predict whether new images belong to any of the classes it has been trained on. This process of prediction is called inference.	What do image classification models predict
8158	Variables are the factors in a experiment that change or potentially change. There are two types of variables independent and dependent, these variables can also be viewed as the cause and effect of an experiment.	What identify variables
1321	Ordinary least-square regression has no normality requirement.	What is the normality requirement for ordinary least squares regression
2180	Negative binomial regression – Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean.	When would you use a negative binomial distribution
1526	"The difference between quota sampling and stratified sampling is: although both ""group"" participants by an important characteristic, stratified sampling relies on random selection within each group, while quota sampling relies on convenience sampling within each group."	What is the difference between quota sampling and stratified sampling
7857	Density values can be greater than 1. In the frequency histogram the y-axis was percentage, but in the density curve the y-axis is density and the area gives the percentage. When creating the density curve the values on the y-axis are calculated (scaled) so that the total area under the curve is 1.	What is the relation between a histogram and a density curve
892	Reinforcement learning will be the next big thing in data science in 2019.  The potential value in using RL in proactive analytics and AI is enormous, but it also demands a greater skillset to master.	Will reinforcement learning become big
926	Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data.	What are the unsupervised machine learning algorithms
491	In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed.  A random variable which is log-normally distributed takes only positive real values.	Is log normally distributed
369	It provides an optimal move for the player assuming that opponent is also playing optimally. Mini-Max algorithm uses recursion to search through the game-tree.  This Algorithm computes the minimax decision for the current state. In this algorithm two players play the game, one is called MAX and other is called MIN.	Why does the Minimax algorithm is termed as Minimax
6241	Any point directly on the y-axis has an X value of 0. Multiple Choice: In a simple Linear regression problem, r and b1. Explanation: r= correlation coefficient and b1= slope. If we have a downward sloping trend-line then that means we have a negative (or inverse) correlation coefficient.	What is the relationship between the linear correlation coefficient r and the slope b 1 of a regression line
282	Applications of Dimensional AnalysisTo check the consistency of a dimensional equation.To derive the relation between physical quantities in physical phenomena.To change units from one system to another.	What are the uses of dimensional analysis
720	μˆP and a standard deviation. σˆP.  Thus the population proportion p is the same as the mean μ of the corresponding population of zeros and ones. In the same way the sample proportion ˆp is the same as the sample mean ˉx.	Is sample proportion the same as sample mean
884	There are four main types of probability sample.Simple random sampling. In a simple random sample, every member of the population has an equal chance of being selected.  Systematic sampling.  Stratified sampling.  Cluster sampling.	What are the types of probability sampling
8248	8 Common Data Structures every Programmer must know. A quick introduction to 8 commonly used data structures.  Arrays. An array is a structure of fixed-size, which can hold items of the same data type.  Linked Lists.  Stacks.  Queues.  Hash Tables.  Trees.  Heaps.More items	What are the topics in data structures
1292	Learning Rate and Gradient Descent Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem.	What is the learning rate in the context of deep learning
1676	The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.	What is ROC AUC in multi label classification in Machine Learning
4374	It is also called flat clustering algorithm. The number of clusters identified from data by algorithm is represented by 'K' in K-means. In this algorithm, the data points are assigned to a cluster in such a manner that the sum of the squared distance between the data points and centroid would be minimum.	What is K in K means clustering
5342	Supervised learning is simply a process of learning algorithm from the training dataset.  Unsupervised learning is modeling the underlying or hidden structure or distribution in the data in order to learn more about the data. Unsupervised learning is where you only have input data and no corresponding output variables.	What is the difference between supervised learning and unsupervised learning
5739	Answer. P(A ∩ B) and P(A|B) are very closely related. Their only difference is that the conditional probability assumes that we already know something -- that B is true.  For P(A|B), however, we will receive a probability between 0, if A cannot happen when B is true, and P(B), if A is always true when B is true.	What is the difference between probability and conditional probability
5584	From Wikipedia, the free encyclopedia. In machine learning, the radial basis function kernel, or RBF kernel, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in support vector machine classification.	What is RBF in SVM
1463	The estimation of distribution algorithm (EDA) aims to explicitly model the probability distribution of the quality solutions to the underlying problem. By iterative filtering for quality solution from competing ones, the probability model eventually approximates the distribution of global optimum solutions.	Evolutionary Computation Estimation of Distribution Algorithm EDA
2773	2:266:36Suggested clip · 120 secondsAn Easy Rule to Setting Up the Null & Alternate Hypotheses YouTubeStart of suggested clipEnd of suggested clip	How do you create a null and alternative hypothesis
2179	You can use a scatter plot to analyze trends in your data and to help you to determine whether or not there is a relationship between two variables.  If the points on the scatter plot seem to form a line that slants down from left to right, there is a negative relationship or negative correlation between the variables.	How are scatter plots used to display analyze and make predictions about real world data
1486	agreement worse than expected	What does a negative Kappa mean
3065	The difference is a matter of design. In the test of independence, observational units are collected at random from a population and two categorical variables are observed for each unit.  In the goodness-of-fit test there is only one observed variable.	What is the difference between the chi square goodness of fit and independence tests
2157	"In probability, we say two events are independent if knowing one event occurred doesn't change the probability of the other event.  So the result of a coin flip and the day being Tuesday are independent events; knowing it was a Tuesday didn't change the probability of getting ""heads."""	What is independence in probability
947	Convolutional Neural Networks (CNNs) is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough.	Which algorithm is best for image classification
515	Linear regression is commonly used for predictive analysis and modeling. For example, it can be used to quantify the relative impacts of age, gender, and diet (the predictor variables) on height (the outcome variable).	Where do we use linear regression explain linear regression
7660	Multi-class Classification using Decision Tree, Random Forest and Extra Trees Algorithm in Python: An End-To-End Data Science Recipe — 016. a) Different types of Machine Learning problems.  i) How to implement Decision Tree, Random Forest and Extra Tree Algorithms for Multiclass Classification in Python.	Can random forest be used for multiclass classification
4778	Bayesian networks encode the dependencies and independencies between variables. Under the causal Markov assumption, each variable in a Bayesian network is independent of its ancestors given the values of its parents.	What is the Markov assumption for a dynamic Bayesian network
1343	How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together.	How do you find the distribution in statistics
620	In class limit, the upper extreme value of the first class interval and the lower extreme value of the next class interval will not be equal. In class boundary, the upper extreme value of the first class interval and the lower extreme value of the next class interval will be equal.	What is the difference between class interval and class boundary
703	Because it is a cost function, a lower Brier score indicates more accurate predictions while a higher Brier score indicates less accurate predictions. In its most common formulation, the best and worst possible Brier scores are 0 and 1 respectively.	What is considered a good Brier score
413	The hazard function (also called the force of mortality, instantaneous failure rate, instantaneous death rate, or age-specific failure rate) is a way to model data distribution in survival analysis.  The function is defined as the instantaneous risk that the event of interest happens, within a very narrow time frame.	What is hazard function in survival analysis
684	A one-sided argument (also known as card stacking, stacking the deck, ignoring the counterevidence, slanting, and suppressed evidence) is an informal fallacy that occurs when only the reasons supporting a proposition are supplied, while all reasons opposing it are omitted.	What fallacy involves a deliberate selection of data to support only one side of an issue
2583	Linear regression can only be used when one has two continuous variables—an independent variable and a dependent variable. The independent variable is the parameter that is used to calculate the dependent variable or outcome. A multiple regression model extends to several explanatory variables.	What data is used for multiple linear regression
7094	Deep reinforcement learning is a promising combination between two artificial intelligence techniques: reinforcement learning, which uses sequential trial and error to learn the best action to take in every situation, and deep learning, which can evaluate complex inputs and select the best response.	How does deep reinforcement learning work
8653	"The ""mean"" is the ""average"" you're used to, where you add up all the numbers and then divide by the number of numbers. The ""median"" is the ""middle"" value in the list of numbers.  If no number in the list is repeated, then there is no mode for the list."	What is the difference between mean and median
3970	The parameters of a logistic regression model can be estimated by the probabilistic framework called maximum likelihood estimation.This tutorial is divided into four parts; they are:Logistic Regression.Logistic Regression and Log-Odds.Maximum Likelihood Estimation.Logistic Regression as Maximum Likelihood.	What are the parameters of logistic regression
8490	Summary: Chaos theory is a mathematical theory that can be used to explain complex systems such as weather, astronomy, politics, and economics. Although many complex systems appear to behave in a random manner, chaos theory shows that, in reality, there is an underlying order that is difficult to see.	What is the chaos theory used for
1998	Hello every one, We know that Pearson linear correlation coefficient gives the strength of linear relationship, while Spearman rank correlation coefficient gives the strength of monotonic relationship between two variables.	Is rank correlation coefficient different from Pearson correlation coefficient explain with reason
5692	Theoretical probability is a method to express the likelihood that something will occur. It is calculated by dividing the number of favorable outcomes by the total possible outcomes. The result is a ratio that can be expressed as a fraction (like 2/5), or a decimal (like .	How do you calculate the theoretical probability
5873	In statistics, an efficient estimator is an estimator that estimates the quantity of interest in some “best possible” manner.	What is efficient estimator
1083	Unsupervised: All data is unlabeled and the algorithms learn to inherent structure from the input data. Semi-supervised: Some data is labeled but most of it is unlabeled and a mixture of supervised and unsupervised techniques can be used.	Which machine learning algorithms use both labeled and unlabeled data for training
5470	In statistics universe (population) refers to an aggregate of all items about which we want to obtain information.  Sample is only the part of the population or the universe. This part must represent the characteristics of universe.	What is the difference between universe and sampling
1409	A t-test tests a null hypothesis about two means; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero.  A chi-square test tests a null hypothesis about the relationship between two variables.	What is the difference between a chi square test and t test
2145	Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics.	Why is Bayesian inference
1942	The sign of a regression coefficient tells you whether there is a positive or negative correlation between each independent variable the dependent variable. A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase.	How do you interpret regression results
666	7 steps to improve your data structure and algorithm skillsStep 1: Understand Depth vs. Breadth.Step 2: Start the Depth-First Approach—make a list of core questions.Step 3: Master each data structure.Step 4: Spaced Repetition.Step 5: Isolate techniques that are reused. Isolate actual code blocks.Step 6: Now, it's time for Breadth.Step 7: Practice on paper.	How do you get really good at algorithms
3508	An odds ratio (OR) is a measure of association between an exposure and an outcome. The OR represents the odds that an outcome will occur given a particular exposure, compared to the odds of the outcome occurring in the absence of that exposure.	What is an odds ratio in statistics
4775	The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution.	Is at distribution the same as a normal distribution
5582	This is referred to as the joint probability of X = x and Y = y. If X and Y are discrete random variables, the function given by f (x, y) = P(X = x, Y = y) for each pair of values (x, y) within the range of X is called the joint probability distribution of X and Y .	What is the joint distribution of X and Y
8270	Standard deviation is the deviation from the mean, and a standard deviation is nothing but the square root of the variance. Mean is an average of all set of data available with an investor or company. Standard deviation used for measuring the volatility of a stock.  Standard deviation is easier to picture and apply.	What is difference between standard deviation and mean deviation
73	Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. ConvNets have been successful in identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars.	What is convolutional neural network
1212	Guidelines for comparing boxplotsCompare the respective medians, to compare location.Compare the interquartile ranges (that is, the box lengths), to compare dispersion.Look at the overall spread as shown by the adjacent values.  Look for signs of skewness.  Look for potential outliers.	How do I compare two box plots
1145	ASUS EZ Flash 3 allows you to download and update to the latest BIOS through the Internet without having to use a bootable disk or an OS-based utility.	What is EZ flash
3044	Skewed data often occur due to lower or upper bounds on the data. That is, data that have a lower bound are often skewed right while data that have an upper bound are often skewed left. Skewness can also result from start-up effects.	What causes skewness in data
6128	0:005:54Suggested clip · 111 secondsInterpreting correlation coefficients in a correlation matrix - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret Pearson correlation matrix
4609	Now to draw calibration plot the following steps are followed.Create a data set with two columns that are actual label and its predicted probability given by the model.Sort this data set in ascending order of the probability predicted by the model.Now divide the data set in bins of some fixed size .More items•	How do you calibrate a machine learning model
6611	Non-response bias can be tested by comparing characteristics of respondents who returned completed surveys and non-respondents who failed to return a completed survey.	How do you find non response bias
6668	The following are the primary advantages of AI:AI drives down the time taken to perform a task.  AI enables the execution of hitherto complex tasks without significant cost outlays.AI operates 24x7 without interruption or breaks and has no downtime.AI augments the capabilities of differently abled individuals.More items	What are the advantages of artificial intelligence
7961	If you know nothing about the data other than the mean, one way to interpret the relative magnitude of the standard deviation is to divide it by the mean. This is called the coefficient of variation. For example, if the mean is 80 and standard deviation is 12, the cv = 12/80 = . 15 or 15%.	How do you interpret standard deviation and coefficient of variation
948	In mathematics, a generating function is a way of encoding an infinite sequence of numbers (an) by treating them as the coefficients of a formal power series.  Generating functions are often expressed in closed form (rather than as a series), by some expression involving operations defined for formal series.	What is meant by generating function
896	1:325:14Suggested clip · 104 secondsConditional probability density function - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the conditional density function
1058	▶ The endogeneity problem occurs when. ► there is an omitted variable that is correlated with some. regressors. ► the dependent variable and at least one of the independent. variables are determined simultaneously in a system.	What is Endogeneity problem in panel data
6944	The definition of a dummy dependent variable model is quite simple: If the dependent, response, left-hand side, or Y variable is a dummy variable, you have a dummy dependent variable model. The reason dummy dependent variable models are important is that they are everywhere.	Can a dummy variable be a dependent variable
7058	A marginal distribution is the percentages out of totals, and conditional distribution is the percentages out of some column.  Conditional distribution, on the other hand, is the probability distribution of certain values in the table expressed as percentages out of sums (or local totals) of certain rows or columns.	What is marginal and conditional distribution
3304	The purpose of statistical inference is to estimate this sample to sample variation or uncertainty.	What is the main goal of statistical inference
8605	Lasso regression stands for Least Absolute Shrinkage and Selection Operator.  The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero.	What is the difference between ridge regression and Lasso
509	SummaryWeighted Mean: A mean where some values contribute more than others.When the weights add to 1: just multiply each weight by the matching value and sum it all up.Otherwise, multiply each weight w by its matching value x, sum that all up, and divide by the sum of weights: Weighted Mean = ΣwxΣw.	How do you calculate a weighted mean
1145	Back-propagation is the process of calculating the derivatives and gradient descent is the process of descending through the gradient, i.e. adjusting the parameters of the model to go down through the loss function.	What is the difference between Backpropagation and gradient descent
7003	Non-linearity is not a concept specifically in Machine Learning, it is a notion broadly used in mathematics. Linearity means homogeneity of degree 1 and additiveness. This means, given a function , it should be both: homogeneous of degree 1, which means, Additive, which means.	What is nonlinearity in machine learning
7475	At Google, we call it Wide & Deep Learning. It's useful for generic large-scale regression and classification problems with sparse inputs (categorical features with a large number of possible feature values), such as recommender systems, search, and ranking problems.	What is wide and deep learning
8367	Pandas have been around for 2 million years Giant pandas have been around a long time. In fact, the first pandas were around over 2 million years ago. That makes them an older species than other bears, like grizzlies, polar bears, and black bears. Giant pandas are only found in the wild in China.	What is an interesting fact about pandas
7978	Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling.	What is the disadvantage of cluster sampling
2712	Artificial intelligence is impacting the future of virtually every industry and every human being. Artificial intelligence has acted as the main driver of emerging technologies like big data, robotics and IoT, and it will continue to act as a technological innovator for the foreseeable future.	How AI is useful in future
5775	Anything central is in the middle of something — or essential to it. Central things are fundamental and important. Think about the center of a circle: it's right in the middle, equidistant from all sides. Similarly, anything central is in the middle of something.	What does the word central mean
1064	To calculate the variance follow these steps:Work out the Mean (the simple average of the numbers)Then for each number: subtract the Mean and square the result (the squared difference).Then work out the average of those squared differences. (Why Square?)	How do you find the sample variance
6876	Research bias, also called experimenter bias, is a process where the scientists performing the research influence the results, in order to portray a certain outcome.	What is research bias
7407	Classification is the process of classifying the data with the help of class labels. On the other hand, Clustering is similar to classification but there are no predefined class labels. Classification is geared with supervised learning. As against, clustering is also known as unsupervised learning.	What is the main difference between classification and clustering explain using examples of both
896	The test statistic used in ANOVA is Student's t. One characteristic of the F distribution is that F cannot be negative. One characteristic of the F distribution is that the computed F can only range between -1 and +1.	What is a characteristic of the F distribution that is used in Anova
4175	The probability of P(a < Z < b) is calculated as follows. Then express these as their respective probabilities under the standard normal distribution curve: P(Z < b) – P(Z < a) = Φ(b) – Φ(a). Therefore, P(a < Z < b) = Φ(b) – Φ(a), where a and b are positive.	How do you find the probability of a normal distribution curve
8070	Confidence intervals measure the degree of uncertainty or certainty in a sampling method. They can take any number of probability limits, with the most common being a 95% or 99% confidence level. Confidence intervals are conducted using statistical methods, such as a t-test.	What are confidence intervals used for
2424	The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs. In other words, model with high bias pays very little attention to the training data and oversimplifies the model.	What is bias in Knn
7505	A certain continuous random variable has a probability density function (PDF) given by: f ( x ) = C x ( 1 − x ) 2 , f(x) = C x (1-x)^2, f(x)=Cx(1−x)2, where x x x can be any number in the real interval [ 0 , 1 ] [0,1] [0,1]. Compute C C C using the normalization condition on PDFs.	How do you find the probability density function of a continuous random variable
5492	The random walk is simple if Xk = ±1, with P(Xk = 1) = p and P(Xk = −1) = 1−p = q. Imagine a particle performing a random walk on the integer points of the real line, where it in each step moves to one of its neighboring points; see Figure 1. Remark 1. You can also study random walks in higher dimensions.	What is the random walk equation
1118	This is because of the Fuinjutsu used to seal Kurama. It converts Kurama's chakra into Naruto's passively.  This chakra had been changed to adept to Naruto. Similarly when activating the Kurama avatar, the chakra changes to adept to Naruto and takes the form of kyuubi but with the markings of Naruto's seal all over it.	Why is Naruto's jinchuriki form different
7795	Data Clustering Basics. The classification of observations into groups requires some methods for computing the distance or the (dis)similarity between each pair of observations. The result of this computation is known as a dissimilarity or distance matrix.	What is dissimilarity in clustering
5081	"String interpolation is a process substituting values of variables into placeholders in a string. For instance, if you have a template for saying hello to a person like ""Hello {Name of person}, nice to meet you!"", you would like to replace the placeholder for name of person with an actual name."	How is string interpolation performed
1831	This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. A simple relation for linear regression looks like this.	What is the use of regularization in machine learning
6211	The importance of data in decision lies in consistency and continual growth. It enables companies to create new business opportunities, generate more revenue, predict future trends, optimize current operational efforts, and produce actionable insights.	What is the role of data in decision making
7645	Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true .	What is the log loss function
2004	Fallacies of Relevance These fallacies attempt to persuade people with irrelevant information, appealing to emotions rather than logic. Examples of these fallacies include: Appeal to Authority - also referred to as Argumentum ad Verecundia (argument from modesty).	What is a logical fallacy give an example of two types
1989	Quantization, in mathematics and digital signal processing, is the process of mapping input values from a large set (often a continuous set) to output values in a (countable) smaller set, often with a finite number of elements. Rounding and truncation are typical examples of quantization processes.	What is Quantisation process
7503	Standard deviation is never negative. Standard deviation is sensitive to outliers. A single outlier can raise the standard deviation and in turn, distort the picture of spread. For data with approximately the same mean, the greater the spread, the greater the standard deviation.	How does an outlier affect the standard deviation
2297	HYPERPLANE. Now that we understand the SVM logic lets formally define the hyperplane . A hyperplane in an n-dimensional Euclidean space is a flat, n-1 dimensional subset of that space that divides the space into two disconnected parts.	What is a hyperplane in SVM
5715	1:3711:38Suggested clip · 115 secondsFinding the equation of the regression line y on x - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the regression line of Y on X
1390	Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.  The ground-truth bounding boxes (i.e., the hand labeled bounding boxes from the testing set that specify where in the image our object is). The predicted bounding boxes from our model.	What is ground truth bounding box
4242	A Bayesian network (also known as a Bayes network, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG).  Efficient algorithms can perform inference and learning in Bayesian networks.	What does Bayesian networks mean in Machine Learning
614	– Validation set: A set of examples used to tune the parameters of a classifier, for example to choose the number of hidden units in a neural network. – Test set: A set of examples used only to assess the performance of a fully-specified classifier.	What is the difference between test set and validation set
629	1. It refers to the probability distribution of the robot pose estimate conditioned upon information such as control and sensor measurement data. The extended Kalman filter and particle filter are two different methods for computing the posterior belief.	What is posterior belief
677	If r is not between the positive and negative critical values, then the correlation coefficient is significant. If r is significant, then you may want to use the line for prediction. Suppose you computed r=0.801 using n=10 data points. df=n−2=10−2=8.	Is it possible to determine the statistical significance of a correlation coefficient
422	Parametric tests assume a normal distribution of values, or a “bell-shaped curve.” For example, height is roughly a normal distribution in that if you were to graph height from a group of people, one would see a typical bell-shaped curve.	What is a parametric test example
2043	The XGBoost algorithm is effective for a wide range of regression and classification predictive modeling problems.  This modified version of XGBoost is referred to as Class Weighted XGBoost or Cost-Sensitive XGBoost and can offer better performance on binary classification problems with a severe class imbalance.	Does XGBoost handle class imbalance
6200	Classification/Recognition: Given an image with an object , find out what that object is.  In other words, classify it in a class from a set of predefined categories. Localization : Find where the object is and draw a bounding box around it.	What is localization in deep learning
896	In many US airports, Customs and Border Protection now uses facial recognition to screen passengers on international flights. And in cities such as Baltimore, police have used facial recognition software to identify and arrest individuals at protests.	How is facial recognition being used today
889	So regression performance is measured by how close it fits an expected line/curve, while machine learning is measured by how good it can solve a certain problem, with whatever means necessary. I'll argue that the distinction between machine learning and statistical inference is clear.	What is the difference between machine learning and regression
677	According to Bezdek (1994), Computational Intelligence is a subset of Artificial Intelligence. There are two types of machine intelligence: the artificial one based on hard computing techniques and the computational one based on soft computing methods, which enable adaptation to many situations.	What is the difference between computational intelligence and artificial intelligence
5930	Now we'll check out the proven way to improve the accuracy of a model:Add more data. Having more data is always a good idea.  Treat missing and Outlier values.  Feature Engineering.  Feature Selection.  Multiple algorithms.  Algorithm Tuning.  Ensemble methods.	How can you improve the accuracy of a predictive model
664	It is an open source artificial intelligence library, using data flow graphs to build models. It allows developers to create large-scale neural networks with many layers. TensorFlow is mainly used for: Classification, Perception, Understanding, Discovering, Prediction and Creation.	Where is TensorFlow used
750	Bias is calculated as the product of two components: non-response rate and the difference between the observed and non-respondent answers. Increasing either of the two components will lead to an increase in bias.	How do you measure non response bias
1094	Bayesian deep learning is a field at the intersection between deep learning and Bayesian probability theory.  Bayesian deep learning models typically form uncertainty estimates by either placing distributions over model weights, or by learning a direct mapping to probabilistic outputs.	What is Bayesian deep learning
816	"In mathematics, the operator norm is a means to measure the ""size"" of certain linear operators. Formally, it is a norm defined on the space of bounded linear operators between two given normed vector spaces."	What is the operator norm of a matrix
2958	It means having a strong sense of self-worth and self-belief. You can take immediate steps to project greater self-confidence in the way you behave, and how you approach other people.	What does increased confidence mean
94	A statistical hypothesis is an explanation about the relationship between data populations that is interpreted probabilistically.  A machine learning hypothesis is a candidate model that approximates a target function for mapping inputs to outputs.	What is specific hypothesis in machine learning
6826	A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence.  Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.	Is a type of recurrent neural network
3293	In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value.	Why does a vanishing gradient occur
1867	The main challenge of NLP is the understanding and modeling of elements within a variable context. In a natural language, words are unique but can have different meanings depending on the context resulting in ambiguity on the lexical, syntactic, and semantic levels.	What are NLP problems
1496	Many everyday data sets typically follow a normal distribution: for example, the heights of adult humans, the scores on a test given to a large class, errors in measurements. The normal distribution is always symmetrical about the mean.	Does the data follow a normal distribution
7137	Softmax is an activation function that outputs the probability for each class and these probabilities will sum up to one. Cross Entropy loss is just the sum of the negative logarithm of the probabilities.  Therefore, Softmax loss is just these two appended together.	Is Softmax a loss function
5058	A Poisson distribution assumes a ratio of 1 (i.e., the mean and variance are equal). Therefore, we can see that before we add in any explanatory variables there is a small amount of overdispersion. However, we need to check this assumption when all the independent variables have been added to the Poisson regression.	What are the assumptions of Poisson regression
42	Neural network regularization is a technique used to reduce the likelihood of model overfitting. There are several forms of regularization. The most common form is called L2 regularization.  L2 regularization tries to reduce the possibility of overfitting by keeping the values of the weights and biases small.	What is l2 regularization in neural networks
212	In addition every algorithm must satisfy the following criteria:input: there are zero or more quantities which are externally supplied;output: at least one quantity is produced;definiteness: each instruction must be clear and unambiguous;More items	What are two important criteria for algorithms
1171	The normal distribution is used when the population distribution of data is assumed normal.  A sample of the population is used to estimate the mean and standard deviation. The t statistic is an estimate of the standard error of the mean of the population or how well known is the mean based on the sample size.	What is the difference between a normal distribution and a t distribution
840	To understand potential interaction effects, compare the lines from the interaction plot:If the lines are parallel, there is no interaction.If the lines are not parallel, there is an interaction.	How do you find the interaction effect
4880	Downsides of Multivariate Testing The most difficult challenge in executing multivariate tests is the amount of visitor traffic required to reach meaningful results. Because of the fully factorial nature of these tests, the number of variations in a test can add up quickly.	What is the downside of a multivariate test
5047	"In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (""hidden units""), with connections between the layers but not between units within each layer."	How does deep belief network work
54	MLP usually means many layers and can be supervised with labels. RBM (Restricted Boltzmann Machine) consists of only 2 layers: input layer & hidden layer, and it is un-supervised (no labels).  RBM (Restricted Boltzmann Machine) consists of only 2 layers: input layer & hidden layer, and it is un-supervised (no labels).	Whats the difference between Multilayer Perceptron and Restricted Boltzmann Machine
1451	Definition. A Binned Variable (also Grouped Variable) in the context of Quantitative Risk Management is any variable that is generated via the discretization of Numerical Variable into a defined set of bins (intervals).	What is a binned variable
1066	The Kaplan-Meier estimate is the simplest way of computing the survival over time in spite of all these difficulties associated with subjects or situations. For each time interval, survival probability is calculated as the number of subjects surviving divided by the number of patients at risk.	How do you calculate survival analysis
7518	16 Best Resources to Learn AI & Machine Learning in 2019Introduction to Machine Learning Problem Framing from Google.  Artificial Intelligence: Principles and Techniques from Stanford University.  Daily email list of AI and ML coding tasks from GeekForge.  CS405: Artificial Intelligence from Saylor Academy.  Intro to Artificial Intelligence at Udacity.More items•	What are some good resources to understand Machine Learning theories and algorithms
4515	Also, the rule-based analysis permits an individual's risk to be predicted on the basis of only one, or at most a few, risk factors, whereas scores derived from regression models require that all covariates be available.	What is rule based analysis
3689	Photo by Gareth Thompson, some rights reserved.Allocate More Memory.  Work with a Smaller Sample.  Use a Computer with More Memory.  Change the Data Format.  Stream Data or Use Progressive Loading.  Use a Relational Database.  Use a Big Data Platform.  Summary.	How do I train on very large datasets
305	Pattern recognition is the process of recognizing patterns by using machine learning algorithm. Pattern recognition can be defined as the classification of data based on knowledge already gained or on statistical information extracted from patterns and/or their representation.	What is pattern recognition algorithm
1522	This two-step approach actually combines two different anomaly detection techniques: univariate and multivariate. Univariate anomaly detection looks for anomalies in each individual metric, while multivariate anomaly detection learns a single model for all the metrics in the system.	What is multivariate anomaly detection
18	1:1111:18Suggested clip · 91 secondsLimits of Functions of Two Variables - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the limit of a function with two variables
3133	Optimization lies at the heart of machine learning.  Then the model is typically trained by solving a core optimization problem that optimizes the variables or parameters of the model with respect to the selected loss function and possibly some regularization function.	What is optimization problem in machine learning
126	Entropy can be calculated for a random variable X with k in K discrete states as follows: H(X) = -sum(each k in K p(k) * log(p(k)))	How do you calculate entropy of information
787	A proposition of the form “if p then q” or “p implies q”, represented “p → q” is called a conditional proposition.  The proposition p is called hypothesis or antecedent, and the proposition q is the conclusion or consequent. Note that p → q is true always except when p is true and q is false.	What does P → Q mean
222	Variance: Var(X) To calculate the Variance: square each value and multiply by its probability. sum them up and we get Σx2p. then subtract the square of the Expected Value μ	How do you find a variance of a function
3720	When we think about the English word “Attention”, we know that it means directing your focus at something and taking greater notice. The Attention mechanism in Deep Learning is based off this concept of directing your focus, and it pays greater attention to certain factors when processing the data.	What is attention mechanism in deep learning
3765	Response bias can be defined as the difference between the true values of variables in a study's net sample group and the values of variables obtained in the results of the same study.  Nonresponse bias occurs when some respondents included in the sample do not respond.	What is the difference between nonresponse and response bias
1970	No. A universal Turing machine is a Turing machine that takes as its input a string of the form where is the representation of the transition table of Turing machine and is a string over the input alphabet of .	Is a multi head Turing Machine a Universal Turing Machine
1315	Overall, Sentiment analysis may involve the following types of classification algorithms: Linear Regression. Naive Bayes. Support Vector Machines.	Which algorithm is used for sentiment analysis
4668	This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set. Interchanging the training and test sets also adds to the effectiveness of this method.	What statistics does cross validation reduce
7352	An endogenous variable is a variable in a statistical model that's changed or determined by its relationship with other variables within the model.  Endogenous variables are the opposite of exogenous variables, which are independent variables or outside forces.	What does it mean if a variable is endogenous
7599	In its simplest form, the sigmoid is a representation of time (on the horizontal axis) and activity (on the vertical axis). The wonder of this curve is that it really describes most phenomena, regardless of type.  The phenomenon experiences sharp growth. It hits a maturity phase where growth slows, and then stops.	What does a sigmoid curve mean
5472	Probability density function (PDF) is a statistical expression that defines a probability distribution (the likelihood of an outcome) for a discrete random variable (e.g., a stock or ETF) as opposed to a continuous random variable.	What does probability density function mean
6364	The tool of normal approximation allows us to approximate the probabilities of random variables for which we don't know all of the values, or for a very large range of potential values that would be very difficult and time consuming to calculate.	Why do we use the normal approximation
247	Random oversampling involves randomly selecting examples from the minority class, with replacement, and adding them to the training dataset. Random undersampling involves randomly selecting examples from the majority class and deleting them from the training dataset.	What is the difference between oversampling and resampling in statistics
1321	Markov models are often used to model the probabilities of different states and the rates of transitions among them. The method is generally used to model systems. Markov models can also be used to recognize patterns, make predictions and to learn the statistics of sequential data.	What are Markov models used for
4480	During the experiment, they found that one of the useful way to do text augmentation is replacing words or phrases with their synonyms . Leverage existing thesaurus help to generate lots of data in a short time. Zhang et al. select a word and replace it by synonyms according to geometric distribution.	How do you augment text data
4058	Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect.	What is correlation in statistics
5377	EM is an iterative method which alternates between two steps, expectation (E) and maximization (M). For clustering, EM makes use of the finite Gaussian mixtures model and estimates a set of parameters iteratively until a desired convergence value is achieved.	What is Expectation Maximization clustering
836	A confidence level refers to the percentage of all possible samples that can be expected to include the true population parameter.  For example, suppose all possible samples were selected from the same population, and a confidence interval were computed for each sample.	What is confidence level in statistics
1937	In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification).	What is multiclass classification in machine learning
4419	simple random sample	What type of sample is it if every unit within the population has an equal chance of being selected
6024	As the name implies, multivariate regression is a technique that estimates a single regression model with more than one outcome variable. When there is more than one predictor variable in a multivariate regression model, the model is a multivariate multiple regression.	What is meant by multivariate regression analysis
6753	The prerequisites for really understanding deep learning are linear algebra, calculus and statistics, as well as programming and some machine learning. The prerequisites for applying it are just learning how to deploy a model.	Is Machine Learning a prerequisite for deep learning
936	To solve the problem using logistic regression we take two parameters w, which is n dimensional vector and b which is a real number. The logistic regression model to solve this is : Equation for Logistic Regression. We apply sigmoid function so that we contain the result of ŷ between 0 and 1 (probability value).	What is W and B in logistic regression
2666	Logistic regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative distribution function of logistic distribution.	What kind of distribution does logistic regression follow
36	Cluster-Based Similarity Partitioning Algorithm For each input partition, an N × N binary similarity matrix encodes the piecewise similarity between any two objects, that is, the similarity of one indicates that two objects are grouped into the same cluster and a similarity of zero otherwise.	What is similarity matrix in clustering
7434	Given an image or a video stream, an object detection model can identify which of a known set of objects might be present and provide information about their positions within the image.	What is an object detection model
6314	Binomial counts successes in a fixed number of trials, while Negative binomial counts failures until a fixed number successes. The Bernoulli and Geometric distributions are the simplest cases of the Binomial and Negative Binomial distributions.	What is the difference between binomial and negative binomial
6472	In short, security is contested because of the politically mobilising and powerful connotations associated with the term (Booth 1991: 318; Buzan 1983: 2; McDonald 2012: 24).  In contrast to realism, theoretical approaches like the Welsh School tradition understand security differently.	Why is the concept of security contested
1150	Experience replay enables reinforcement learning agents to memorize and reuse past experiences, just as humans replay memories for the situation at hand. Contemporary off-policy algorithms either replay past experiences uniformly or utilize a rule- based replay strategy, which may be sub-optimal.	What is experience replay in reinforcement learning
22	Time series forecasting is an important area of machine learning that is often neglected. It is important because there are so many prediction problems that involve a time component.  Standard definitions of time series, time series analysis, and time series forecasting.	Is time series forecasting machine learning
4535	The term that does not apply to cluster analysis is factorization. Cluster analysis is a way of grouping data, based on obvious similarities. It is also called as classification analysis or numerical taxonomy.  Hierarchical cluster analysis tends to build a hierarchy within clusters.	Which of the following terms does not apply to cluster analysis
840	We can define a neural network that can learn to recognize objects in less than 100 lines of code.  In analogy, we conjecture that rules for development and learning in brains may be far easier to understand than their resulting properties.	What does it mean to understand a neural network
3088	The difference between nonprobability and probability sampling is that nonprobability sampling does not involve random selection and probability sampling does. At least with a probabilistic sample, we know the odds or probability that we have represented the population well.	What is the difference between non probability sampling and probability sampling
1892	normal distribution	Which distribution has maximum entropy
4396	Decision Tree can be used both in classification and regression problem. This article present the Decision Tree Regression Algorithm along with some advanced topics.	Can decision tree be used for regression
1048	To conclude, it can be said that residual networks have become quite popular for image recognition and classification tasks because of their ability to solve vanishing and exploding gradients when adding more layers to an already deep neural network. A ResNet with thousand layers has not much practical use as of now.	What is the significance of residual networks
245	In statistics, stepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. In each step, a variable is considered for addition to or subtraction from the set of explanatory variables based on some prespecified criterion.	What do you think of step wise regression
63	For example, a p-value of 0.01 would mean there is a 1% chance of committing a Type I error. However, using a lower value for alpha means that you will be less likely to detect a true difference if one really exists (thus risking a type II error).	What is the relationship between the p value of a t test and the Type I and Type II errors
6170	A tensor is a vector or matrix of n-dimensions that represents all types of data. All values in a tensor hold identical data type with a known (or partially known) shape. The shape of the data is the dimensionality of the matrix or array.	What is tensor size
6738	What problems is humanity facing currently & can AI help to solve them?Energy.Environment.Transporation.Food and water.Disease and Human Suffering.Education.Population.	What problems can be solved with AI
545	"The geometric distribution describes the probability of ""x trials are made before a success"", and the negative binomial distribution describes that of ""x trials are made before r successes are obtained"", where r is fixed. So you see that the latter is a particular case of the former, namely, when r=1."	What is the difference between geometric and negative binomial
5152	"In statistics, the phrase ""correlation does not imply causation"" refers to the inability to legitimately deduce a cause-and-effect relationship between two variables solely on the basis of an observed association or correlation between them."	If correlation does not imply causation what does it do
2085	Linear discriminant analysis (LDA) is used here to reduce the number of features to a more manageable number before the process of classification. Each of the new dimensions generated is a linear combination of pixel values, which form a template.	Why do we use linear discriminant analysis
7331	Text mining (also referred to as text analytics) is an artificial intelligence (AI) technology that uses natural language processing (NLP) to transform the free (unstructured) text in documents and databases into normalized, structured data suitable for analysis or to drive machine learning (ML) algorithms.	How NLP is used in text mining
8453	Weights(Parameters) — A weight represent the strength of the connection between units. If the weight from node 1 to node 2 has greater magnitude, it means that neuron 1 has greater influence over neuron 2. A weight brings down the importance of the input value.	Why weight is used in neural network
6132	While the chi-squared test relies on an approximation, Fisher's exact test is one of exact tests. Especially when more than 20% of cells have expected frequencies < 5, we need to use Fisher's exact test because applying approximation method is inadequate.	When should Fisher's exact test be used
701	Examples of ordinal variables include: socio economic status (“low income”,”middle income”,”high income”), education level (“high school”,”BS”,”MS”,”PhD”), income level (“less than 50K”, “50K-100K”, “over 100K”), satisfaction rating (“extremely dislike”, “dislike”, “neutral”, “like”, “extremely like”).	What are examples of ordinal variables
723	Benchmarking Sentiment Analysis Algorithms (Algorithmia) – “Sentiment Analysis, also known as opinion mining, is a powerful tool you can use to build smarter products. It's a natural language processing algorithm that gives you a general idea about the positive, neutral, and negative sentiment of texts.	What is sentiment analysis algorithm
578	A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable.  The slope of the line is b, and a is the intercept (the value of y when x = 0).	What does B represent in linear regression
8655	Stratified sampling offers several advantages over simple random sampling. A stratified sample can provide greater precision than a simple random sample of the same size. Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money.	Which is better simple random sampling or stratified sampling
6665	Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image.	What do you mean by image processing
4054	There are two types of probability distribution which are used for different purposes and various types of the data generation process.Normal or Cumulative Probability Distribution.Binomial or Discrete Probability Distribution.	How many types of probability distribution are there
652	In computer science, an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content).	What is inverted index in information retrieval
4275	Probability and the Normal Curve The normal distribution is a continuous probability distribution.  The total area under the normal curve is equal to 1. The probability that a normal random variable X equals any particular value is 0.	How does the normal curve relate to probability
2676	Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.	Why does normalization work in batch
6689	Univariate statistics summarize only one variable at a time. Bivariate statistics compare two variables. Multivariate statistics compare more than two variables.	What is the difference between univariate and bivariate data
325	There are two main types of criterion validity: concurrent validity and predictive validity. Concurrent validity is determined by comparing tests scores of current employees to a measure of their job performance.	What are the two types of criterion related validity
3382	Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics. The term applies both to mental processes used by humans when reading text, and to artificial processes implemented in computers, which are the subject of natural language processing.	What is segmentation in NLP
1111	Events A and B are independent if the equation P(A∩B) = P(A) · P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together.	How do you know if something is independent in probability
600	The median is a measure of center (location) of a list of numbers.  This will be the median. If there are an even number on the list then average the n/2 and the (N + 2)/2 numbers. In general, the median is at position (n + 1)/2. If this position is a whole number then you have the median at that position in the list.	What is N 2 in median
2206	For most common hierarchical clustering software, the default distance measure is the Euclidean distance. This is the square root of the sum of the square differences. However, for gene expression, correlation distance is often used. The distance between two vectors is 0 when they are perfectly correlated.	What distance metric is used in hierarchical clustering
568	The mean is the average of the numbers. It is easy to calculate: add up all the numbers, then divide by how many numbers there are. In other words it is the sum divided by the count.	How do we find average
1065	"Ground truth is a term used in statistics and machine learning that means checking the results of machine learning for accuracy against the real world. The term is borrowed from meteorology, where ""ground truth"" refers to information obtained on site."	What is ground truth in AI
3202	Minimax is a kind of backtracking algorithm that is used in decision making and game theory to find the optimal move for a player, assuming that your opponent also plays optimally.  In Minimax the two players are called maximizer and minimizer.	What is Minimax search procedure
735	Adaptive Gradient Algorithm (Adagrad) is an algorithm for gradient-based optimization.  It performs smaller updates As a result, it is well-suited when dealing with sparse data (NLP or image recognition) Each parameter has its own learning rate that improves performance on problems with sparse gradients.	What is an AdaGrad algorithm in machine learning
7415	A multinomial experiment is almost identical with one main difference: a binomial experiment can have two outcomes, while a multinomial experiment can have multiple outcomes.  A binomial experiment will have a binomial distribution.	What is difference between binomial and multinomial distribution
2415	Natural language processing helps computers communicate with humans in their own language and scales other language-related tasks. For example, NLP makes it possible for computers to read text, hear speech, interpret it, measure sentiment and determine which parts are important.	What is NLP used for
4194	Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression.  The advantages of Stochastic Gradient Descent are: Efficiency.	What is SGD in deep learning
5087	Multiply the Grand total by the Pretest probability to get the Total with disease. Compute the Total without disease by subtraction. Multiply the Total with disease by the Sensitivity to get the number of True positives.	How do you calculate true positives
240	Adam optimizer. Implements the Adam optimization algorithm. Adam is a stochastic gradient descent method that computes individual adaptive learning rates for different parameters from estimates of first- and second-order moments of the gradients.	What is Adam Optimizer Tensorflow
951	For the coin flip example, N = 2 and π = 0.5. The formula for the binomial distribution is shown below: where P(x) is the probability of x successes out of N trials, N is the number of trials, and π is the probability of success on a given trial.Number of HeadsProbability21/42 more rows	What is the formula for a binomial probability distribution
2766	dB‟ is the area of elementary strip of B –H curve shown in the figure above, Therefore, Energy consumed per cycle = volume of the right x area of hysteresis loop. The hysteresis loss per second is given by the equation[20]: Hysteresis loss, Ph= (Bmax)1.6f V joules per second (or) watts.	What is the formula of hysteresis loss
116	One common method of probability sampling is random sampling, which assumes that each member of a population has an equal chance of being selected.  In a quota sample, a researcher deliberately sets the proportions of levels of members chosen within the sample.	What is the difference between a random sample and a quota sample
729	When dealing with Machine Learning models, it is usually recommended that you store them somewhere. At the private sector, you oftentimes train them and store them before production, while in research and for future model tuning it is a good idea to store them locally.	Where are machine learning models stored
6401	The main difference between Independant and Independent is that the Independant is a misspelling of independent and Independent is a Not dependent; free; not subject to control by others; not relying on others.	What is the difference between independent and independant
873	A little bit of coding skills is enough, but it's better to have knowledge of data structures, algorithms, and OOPs concept. Some of the popular programming languages to learn machine learning in are Python, R, Java, and C++.	Is coding involved in machine learning
1879	Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing (NLP), speech recognition and machine vision.	Is AI Artificial Intelligence
161	5 Most Important Methods For Statistical Data AnalysisMean. The arithmetic mean, more commonly known as “the average,” is the sum of a list of numbers divided by the number of items on the list.  Standard Deviation.  Regression.  Sample Size Determination.  Hypothesis Testing.	What are some of the statistical methods that are useful for data analyst
8541	"In artificial intelligence and computational cognitive science, ""the action selection problem"" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment.  The term is also sometimes used in ethology or animal behavior."	What is action in artificial intelligence
413	For the vanishing gradient problem, the further you go through the network, the lower your gradient is and the harder it is to train the weights, which has a domino effect on all of the further weights throughout the network. That was the main roadblock to using Recurrent Neural Networks.	What is vanishing gradient problem in RNN
7939	The advantage to image-based backups is that all of the information can be collected in a single pass, providing an updated bare metal restore (BMR) capability with each file-based backup.	What is an advantage of image based backup approach
230	A logistic regression model allows us to establish a relationship between a binary outcome variable and a group of predictor variables. It models the logit-transformed probability as a linear relationship with the predictor variables.	What does logit regression mean
941	In some cases, the measurement scale for data is ordinal, but the variable is treated as continuous. For example, a Likert scale that contains five values - strongly agree, agree, neither agree nor disagree, disagree, and strongly disagree - is ordinal.	What type of variable is a Likert scale
2016	The normal distribution can be used as an approximation to the binomial distribution, under certain circumstances, namely: If X ~ B(n, p) and if n is large and/or p is close to ½, then X is approximately N(np, npq)	Can the binomial distribution be approximated by a normal distribution
3327	Stratified sampling is a probability sampling technique wherein the researcher divides the entire population into different subgroups or strata, then randomly selects the final subjects proportionally from the different strata.	Is stratified sampling probability
3141	"Convenience sampling is a type of nonprobability sampling in which people are sampled simply because they are ""convenient"" sources of data for researchers. In probability sampling, each element in the population has a known nonzero chance of being selected through the use of a random selection procedure."	How does convenience sampling work
2277	Feature Selection: Select a subset of input features from the dataset. Unsupervised: Do not use the target variable (e.g. remove redundant variables). Supervised: Use the target variable (e.g. remove irrelevant variables). Wrapper: Search for well-performing subsets of features.	How do you select features from a data set
2263	In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.  Given these hyperparameters, the training algorithm learns the parameters from the data.	What are hyper parameters in machine learning
37	Stochastic Gradient Descent (SGD): Hence, in Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration.  This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration.	How does stochastic gradient descent work
281	Precision is a metric that quantifies the number of correct positive predictions made. Precision, therefore, calculates the accuracy for the minority class. It is calculated as the ratio of correctly predicted positive examples divided by the total number of positive examples that were predicted.	How do you find the accuracy of a precision and recall
1159	The task boils down to computing the distance between two face vectors. As such, appropriate distance metrics are essential for face verification accuracy.  The use of cosine similarity in our method leads to an effective learning algorithm which can improve the generalization ability of any given metric.	Why does face verification identification usally use cosine similarity
6046	binary dependent variable	What type of dependent variable does logistic regression measure
6806	Definition: Entropy is a measure of uncertainty of a random variable. The entropy of a discrete random variable X with alphabet X is H(X) = -) p(x) log p(2) DEX When the base of the logarithm is 2, entropy is measured in bits.	How do you find the entropy of a random variable
7457	"Another strategy OTs typically recommend is something called “backward chaining."" Backward chaining is working backward from the goal. For example, the goal is put on a T-shirt.  Pull shirt over head. Push right arm up through right sleeve."	What is an example of backward chaining
3019	Returning to directed acyclic graphs, the current state of the art in the Bayesian belief networks allows to efficiently deal with undirected cycles, that is, patterns which would be cycles if the arrow directions were not taken into account.	Can Bayesian networks have cycles
7851	Signal Processing means processing any kind of signal whether it is analog or digital in such a manner by which it can be interpreted by any kind of computer. For instance, sound wave is also a type of signal.	What is signal processing machine learning
1697	A logistic regression estimates the mean of your response given that your data is distributed Bernoulli or is a Binomial trial. Since the mean of a Binomial trial is the probability of success, you can interpret the output from a Logistic regression (after logit transformation) as a probability of success.	Why does the logistic function gives the probability
6669	Model-based collaborative filtering algorithms provide item recommendation by first developing a model of user ratings. Algorithms in this category take a probabilistic approach and envision the collaborative filtering process as computing the expected value of a user prediction, given his/her ratings on other items.	What is model based collaborative filtering
4496	"A: Bootstrap aggregation, or ""bagging,"" in machine learning decreases variance through building more advanced models of complex data sets. Specifically, the bagging approach creates subsets which are often overlapping to model the data in a more involved way."	Why does bagging reduce variance
4848	The law of averages is not a mathematical principle, whereas the law of large numbers is.  According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed.	What is the difference between the law of large numbers and the law of averages
3681	In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a	Classification machine learning What is confusion matrix
167	In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.	Can you Factorise matrices
6407	A residual sum of squares (RSS) is a statistical technique used to measure the amount of variance in a data set that is not explained by a regression model.  The residual sum of squares measures the amount of error remaining between the regression function and the data set.	What does residual sum of squares mean
8038	Dive into this post for some overview of the right resources and a little bit of advice.By Pulkit Khandelwal, VIT University.  Step 1 - Background Check.  Step 2 - Digital Image Processing.  Step 3 - Computer Vision.  Step 4 - Advanced Computer Vision.  Step 5 - Bring in Python and Open Source.More items	How do you use computer vision
377	Events A and B are independent if: knowing whether A occured does not change the probability of B. Mathematically, can say in two equivalent ways: P(B|A) = P(B) P(A and B)  Important to distinguish independence from mutually exclusive which would say B ∩ A is empty (cannot happen).	Are A and B independent events
6901	In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.  Such models are called linear models.	How do you explain linear regression
6531	The two sample Kolmogorov-Smirnov test is a nonparametric test that compares the cumulative distributions of two data sets(1,2).  The KS test report the maximum difference between the two cumulative distributions, and calculates a P value from that and the sample sizes.	What does the Kolmogorov Smirnov test show
1170	In many tests, including diagnostic medical tests, sensitivity is the extent to which true positives are not overlooked, thus false negatives are few, and specificity is the extent to which true negatives are classified as such, thus false positives are few.	How does sensitivity and specificity relate to false positives negatives and true positives negatives
2818	An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate.	What does the ROC curve tell us
1370	Backward-chaining is based on modus ponens inference rule. In backward chaining, the goal is broken into sub-goal or sub-goals to prove the facts true. It is called a goal-driven approach, as a list of goals decides which rules are selected and used.	What is used in a backward chaining algorithm
5846	Probability mass functions (pmf) are used to describe discrete probability distributions. While probability density functions (pdf) are used to describe continuous probability distributions.	What is the difference between probability density function probability mass function and probability distribution function
2532	A Poisson Process is a model for a series of discrete event where the average time between events is known, but the exact timing of events is random . The arrival of an event is independent of the event before (waiting time between events is memoryless).	What is Poisson process in statistics
6991	"Advertisements. Searching in data-strucutre refers to the process of finding a desired element in set of items. The desired element is called ""target"". The set of items to be searched in, can be any data-structure like − list, array, linked-list, tree or graph."	What is searching in data structures
1196	Explanation: Correlation is the process of studying the cause and effect relationship that exists between two variables. Correlation coefficient is the measure of the correlation that exists between two variables.	What is the difference between correlation coefficient and correlation
893	Z-tests are statistical calculations that can be used to compare population means to a sample's. T-tests are calculations used to test a hypothesis, but they are most useful when we need to determine if there is a statistically significant difference between two independent sample groups.	Why do we use t test and Z test
1870	The significance level for a given hypothesis test is a value for which a P-value less than or equal to is considered statistically significant. Typical values for are 0.1, 0.05, and 0.01. These values correspond to the probability of observing such an extreme value by chance.	What does a significance level of 0.01 mean
2783	In the chapter on Human Development Indicators, there should be a table that includes the Gini coefficient. For example, in the 2004 edition, they are in table number 14. See also the “Get Indicators” portion of their web site, where you can download an Excel table with the Gini index.	Where is the Gini coefficient data
7770	The Central limit Theorem states that when sample size tends to infinity, the sample mean will be normally distributed. The Law of Large Number states that when sample size tends to infinity, the sample mean equals to population mean.	What is the difference between the central limit theorem and the law of large numbers
7991	The following types of inferential statistics are extensively used and relatively easy to interpret: One sample test of difference/One sample hypothesis test. Confidence Interval. Contingency Tables and Chi Square Statistic.	What is the main type of inferential statistics
607	Chi-squared test	What tests use categorical data
3958	Modes, medians, and frequencies are the appropriate statistical tools to use. If you have designed a series of questions that when combined measure a particular trait, you have created a Likert scale. Use means and standard deviations to describe the scale.	What statistical analysis should I use for Likert scale
7267	The confidence interval (CI) is a range of values that's likely to include a population value with a certain degree of confidence. It is often expressed a % whereby a population means lies between an upper and lower interval.	What is a confidence interval in simple terms
1347	The average value becomes more and more precise as the number of measurements N increases. Although the uncertainty of any single measurement is always Δ��, the uncertainty in the mean Δ��avg becomes smaller (by a factor of 1/ N) as more measurements are made. You measure the length of an object five times.	What happens to uncertainty when you average
3472	Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space.	What is a clustering algorithm
627	Ensemble learning is usually used to average the predictions of different models to get a better prediction. Ensemble methods is like using the predictions of small expert models in different parts of the input space.	When should you use ensemble methods
8236	Deep NN is just a deep neural network, with a lot of layers. It can be CNN, or just a plain multilayer perceptron. CNN, or convolutional neural network, is a neural network using convolution layer and pooling layer.	What is the difference between a CNN and deep neural network
1443	Backward chaining is a type of AI program that starts with a defined end point (or goal) and works backward to figure out the best way to get there. For example, if a person wants to save $1 million for retirement, backward chaining can help them figure out how much they need to save each month to get there.	What is the starting point of backward chaining for the solution
7340	AI Strategy is a road plan for the adoption and implementation of artificial intelligence, machine learning, or deep learning technologies within your organization. An AI Strategy defines your AI priorities, goals, milestones, mission and vision.  AI Strategies are being used in corporations around the world.	What is AI strategy
6433	A Second Order Low Pass Filter is to be design around a non-inverting op-amp with equal resistor and capacitor values in its cut-off frequency determining circuit. If the filters characteristics are given as: Q = 5, and ƒc = 159Hz, design a suitable low pass filter and draw its frequency response.	How do you create a second order low pass filter
3828	The accuracy is calculated by taking the percentage of correct predictions over the total number of examples. Correct prediction means the examples where the value of the prediction attribute is equal to the value of label attribute.	How does Rapidminer calculate accuracy
5566	A continuous random variable can take on an infinite number of values. The probability that it will equal a specific value (such as a) is always zero.	What is the probability of a continuous random variable
2780	If we multiply the variances by then the sample variances would also match the population variance. In statistics, we take a sample of a population and say that the sample mean and sample variance are the same as the population mean and variance.	What is the relationship between sample variance and population variance
3128	OLS cannot be used because the regression function is not a linear function of the regression coefficients (the coefficients appear inside the nonlinear functions Φ or Λ).	Why are the coefficients of probit and logit models estimated by maximum likelihood instead of OLS
898	The Matrix represents a system of control that operates completely in the mind. As a complex, machine-driven program, it appropriates any personal, political, or ideological leanings and renders them wholly false. It allows illusions but no action.	What is the Matrix and what does it represent
3723	Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset.  Large numbers of input features can cause poor performance for machine learning algorithms. Dimensionality reduction is a general field of study concerned with reducing the number of input features.	What is dimensionality reduction technique
4635	There are four types of artificial intelligence: reactive machines, limited memory, theory of mind and self-awareness.	What are the types of artificial intelligence *
3012	Cross tabulationCross tabulations require that the two data columns be adjacent. You can drag columns by selecting them, and moving the cursor so it's immediately between two columns.  Once you have the columns adjacent, select both of them including the variable names all the way to the bottom.	How do you do cross tabulation
3702	Neural networks are sets of algorithms intended to recognize patterns and interpret data through clustering or labeling. In other words, neural networks are algorithms. A training algorithm is the method you use to execute the neural network's learning process.	What are neural networks used for
4643	Share on. Statistics Definitions > Paired data is where natural matching or coupling is possible. Generally this would be data sets where every data point in one independent sample would be paired—uniquely—to a data point in another independent sample.	What does it mean to have paired data
2108	The difference between forward and backward chaining is: Backward chaining starts with a goal and then searches back through inference rules to find the facts that support the goal. Forward chaining starts with facts and searches forward through the rules to find a desired goal.	What is the difference between forward and backward chaining in artificial intelligence
360	How to Test HypothesesState the hypotheses. Every hypothesis test requires the analyst to state a null hypothesis and an alternative hypothesis.  Formulate an analysis plan. The analysis plan describes how to use sample data to accept or reject the null hypothesis.  Analyze sample data.  Interpret the results.	What are the two ways to test a hypothesis
6131	A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss).	What is generative neural network
1089	An activation function is a function used in artificial neural networks which outputs a small value for small inputs, and a larger value if its inputs exceed a threshold.  Activation functions are useful because they add non-linearities into neural networks, allowing the neural networks to learn powerful operations.	What do you mean by activation function
3654	R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.  100% indicates that the model explains all the variability of the response data around its mean.	What does the R squared value mean
4086	The parameters of a neural network are typically the weights of the connections. In this case, these parameters are learned during the training stage. So, the algorithm itself (and the input data) tunes these parameters. The hyper parameters are typically the learning rate, the batch size or the number of epochs.	What are the parameters of a neural network
8378	Latent semantic indexing (LSI) is a concept used by search engines to discover how a term and content work together to mean the same thing, even if they do not share keywords or synonyms.  Basically, though, you often need specific keywords on your pages to boost your website traffic.	What do you understand by latent semantic indexing
8308	The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities .	How do I really understand conditional and Bayesian probability
5351	We can use classification performance metrics such as Log-Loss, Accuracy, AUC(Area under Curve) etc. Another example of metric for evaluation of machine learning algorithms is precision, recall, which can be used for sorting algorithms primarily used by search engines.	What are metrics for evaluating classifier performance
6294	Calculate output size of ConvolutionOutput height = (Input height + padding height top + padding height bottom - kernel height) / (stride height) + 1.Output width = (Output width + padding width right + padding width left - kernel width) / (stride width) + 1.	How do you calculate convolution output size
1024	Cluster sampling refers to a type of sampling method . With cluster sampling, the researcher divides the population into separate groups, called clusters. Then, a simple random sample of clusters is selected from the population. The researcher conducts his analysis on data from the sampled clusters.	How does cluster sampling work
5970	The Poisson distribution is used to describe the distribution of rare events in a large population. For example, at any particular time, there is a certain probability that a particular cell within a large population of cells will acquire a mutation.	When should I use Poisson distribution
3440	The two-sample t-test is a method used to test whether the unknown population means of two groups are equal or not.	What is a two sample t test
6708	N-gram is probably the easiest concept to understand in the whole machine learning space, I guess. An N-gram means a sequence of N words. So for example, “Medium blog” is a 2-gram (a bigram), “A Medium blog post” is a 4-gram, and “Write on Medium” is a 3-gram (trigram). Well, that wasn't very interesting or exciting.	What is N gram in machine learning
5005	The reason n-1 is used is because that is the number of degrees of freedom in the sample. The sum of each value in a sample minus the mean must equal 0, so if you know what all the values except one are, you can calculate the value of the final one.	Why do we divide by N 1 rather than by N when calculating a sample standard deviation
861	Statistical SignificanceUsually, statistical significance is determined by calculating the probability of error (p value) by the t ratio.The difference between two groups (such as an experiment vs. control group) is judged to be statistically significant when p = 0.05 or less.	How do you determine if a difference is statistically significant
4825	Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.	What is the point of bootstrapping
1222	Matrix decomposition refers to the transformation of a given matrix into a product of matrices. They are used to implement efficient matrix algorithms. Decomposing a matrix breaks it up into two matrix factors. This can be helpful when solving equations of the form Ax=b for x when multiple b vectors are to be used.	Why is matrix decomposition useful
5194	Many time series show periodic behavior. This periodic behavior can be very complex. Spectral analysis is a technique that allows us to discover underlying periodicities. To perform spectral analysis, we first must transform data from time domain to frequency domain.	What is spectral analysis in time series
1880	A relative frequency distribution shows the proportion of the total number of observations associated with each value or class of values and is related to a probability distribution, which is extensively used in statistics.	What is a relative frequency distribution in statistics
3610	Statistical Decision for Hypothesis Testing In Hypothesis testing, if the significance value of the test is greater than the predetermined significance level, then we accept the null hypothesis. If the significance value is less than the predetermined value, then we should reject the null hypothesis.	How do you reject or accept a hypothesis in statistics
6135	In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.  Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex.	What is deep convolutional neural network
6986	Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.	What is Gaussian distribution
7585	Consider the function f(x) = |x| on [−1,1]. • The Mean Value Theorem does not apply because the derivative is not defined at x = 0.	What does mean value theorem not apply
8548	The logistic function is the inverse of the natural logit function and so can be used to convert the logarithm of odds into a probability. In mathematical notation the logistic function is sometimes written as expit in the same form as logit.	What is the logistic function used for
598	The exponential distribution predicts the wait time until the *very first* event. The gamma distribution, on the other hand, predicts the wait time until the *k-th* event occurs.	What is the difference between gamma distribution and exponential distribution
4009	In a positively skewed distribution, the mean is usually greater than the median because the few high scores tend to shift the mean to the right.  In a positively skewed distribution, the mode is always less than the mean and median.	Which is typical of a positively skewed distribution
1298	Deep learning is a type of machine learning in which a model learns to perform classification tasks directly from images, text or sound. Deep learning is usually implemented using neural network architecture. The term deep refers to the number of layers in the network—the more the layers, the deeper the network.	What is deep learning in image processing
4718	The computation of a single layer perceptron is performed over the calculation of sum of the input vector each with the value multiplied by corresponding element of vector of the weights.  The value which is displayed in the output will be the input of an activation function.	How does single layer Perceptron function
2302	An embedding is a mapping of a discrete — categorical — variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables.  As input to a machine learning model for a supervised task.	What is embedding in neural network
7030	Absolute standardized differences for baseline covariates comparing treated to untreated subjects in the original and the matched sample.  Thus, when the standardized difference is equal to 0.10, the percentage of non-overlap between the distributions of the continuous covariate in the two groups is 7.7 per cent.	What is absolute standardized difference
684	A second type of quantitative variable is called a continuous variable . This is a variable where the scale is continuous and not made up of discrete steps. For example, if playing a game of trivia, the length of time it takes a player to give an answer might be represented by a continuous variable.	What is a continuous quantitative variable
895	"volves the estimation of some components for some dates by interpola- tion. between values (""benchmarks"") for earlier and later dates. This is often done by using a related series known for all relevant dates. In practice, the bulk of such interpolation uses only a single related."	What is interpolation time series
7805	In deep learning, transfer learning is a technique whereby a neural network model is first trained on a problem similar to the problem that is being solved. One or more layers from the trained model are then used in a new model trained on the problem of interest.	What is transfer learning in neural networks
4777	7 Techniques to Handle Imbalanced DataUse the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	What do you do with an unbalanced data set
11	The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts occurrences of gradient orientation in localized portions of an image.	What is hog computer vision
6466	Disadvantages of Sampling Since choice of sampling method is a judgmental task, there exist chances of biasness as per the mindset of the person who chooses it. Improper selection of sampling techniques may cause the whole process to defunct. Selection of proper size of samples is a difficult job.	What are the disadvantages of sampling
2964	In regression with multiple independent variables, the coefficient tells you how much the dependent variable is expected to increase when that independent variable increases by one, holding all the other independent variables constant. Remember to keep in mind the units which your variables are measured in.	What does the coefficient in regression tell you
345	Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).	What do we mean by regression
2320	Text analytics is the automated process of translating large volumes of unstructured text into quantitative data to uncover insights, trends, and patterns. Combined with data visualization tools, this technique enables companies to understand the story behind the numbers and make better decisions.	What type of text are processed in text analytics
8625	Linear Regression Analysis consists of more than just fitting a linear line through a cloud of data points. It consists of 3 stages – (1) analyzing the correlation and directionality of the data, (2) estimating the model, i.e., fitting the line, and (3) evaluating the validity and usefulness of the model.	What are the steps in regression analysis
3055	"Q-learning is a model-free reinforcement learning algorithm to learn quality of actions telling an agent what action to take under what circumstances.  ""Q"" names the function that the algorithm computes with the maximum expected rewards for an action taken in a given state."	What is Q learning in machine learning
1166	A Z-score is a numerical measurement that describes a value's relationship to the mean of a group of values. Z-score is measured in terms of standard deviations from the mean. If a Z-score is 0, it indicates that the data point's score is identical to the mean score.	What does the Z in z score stand for
6287	A Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer). While a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non – linear functions. Figure 4 shows a multi layer perceptron with a single hidden layer.	What is single layer Perceptron and Multilayer Perceptron
98	Intel® Movidius™ VPUs enable demanding computer vision and edge AI workloads with efficiency.  VPU technology enables intelligent cameras, edge servers and AI appliances with deep neural network and computer vision based applications in areas such as visual retail, security and safety, and industrial automation.	What is Intel movidius
4226	Vectors have many real-life applications, including situations involving force or velocity. For example, consider the forces acting on a boat crossing a river. The boat's motor generates a force in one direction, and the current of the river generates a force in another direction. Both forces are vectors.	What are some applications of vectors in real life
4114	A Poisson distribution is a tool that helps to predict the probability of certain events from happening when you know how often the event has occurred. It gives us the probability of a given number of events happening in a fixed interval of time.  λ (also written as μ) is the expected number of event occurrences.	Why is the Poisson distribution important
4767	They provide a natural way to handle missing data, they allow combination of data with domain knowledge, they facilitate learning about causal relationships between variables, they provide a method for avoiding overfitting of data (Heckerman, 1995), they can show good prediction accuracy even with rather small sample	What are the advantages of Bayesian networks
3132	If the study is based on a very large sample size, relationships found to be statistically significant may not have much practical significance. Almost any null hypothesis can be rejected if the sample size is large enough.	How might a statistical test be statistically significant but not practically significant
6452	The hazard rate refers to the rate of death for an item of a given age (x). It is part of a larger equation called the hazard function, which analyzes the likelihood that an item will survive to a certain point in time based on its survival to an earlier time (t).	What is hazard rate function
2468	The test statistic is a z-score (z) defined by the following equation. z=(p−P)σ where P is the hypothesized value of population proportion in the null hypothesis, p is the sample proportion, and σ is the standard deviation of the sampling distribution.	How do you find the Z test statistic
2130	Examples of continuous variables are body mass, height, blood pressure and cholesterol. A discrete quantitative variable is one that can only take specific numeric values (rather than any value in an interval), but those numeric values have a clear quantitative interpretation.	What is an example of continuous quantitative variable
1279	Logistic regression is easier to implement, interpret, and very efficient to train. If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting. It makes no assumptions about distributions of classes in feature space.	What is the advantage of logistic regression
4520	Bias is a tendency to lean in a certain direction, either in favor of or against a particular thing. To be truly biased means to lack a neutral viewpoint on a particular topic.  If you're biased toward something, then you lean favorably toward it; you tend to think positively of it.	Whats does bias mean
8138	In probability theory, the multi-armed bandit problem (sometimes called the K- or N-armed bandit problem) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known	What is arm banditry
31	Fisher's exact test is a statistical test used to determine if there are nonrandom associations between two categorical variables. . For each one, calculate the associated conditional probability using (2), where the sum of these probabilities must be 1.	What is Fisher exact test used for
6509	The null hypothesis is the one to be tested and the alternative is everything else. In our example, The null hypothesis would be: The mean data scientist salary is 113,000 dollars. While the alternative: The mean data scientist salary is not 113,000 dollars.	What is null and alternative hypothesis example
400	A class of unsupervised models from Deep Learning called Autoencoders have been used as unsupervised models for time-series data.  A class of unsupervised models from Deep Learning called Autoencoders have been used as unsupervised models for time-series data.	Are there any unsupervised learning algorithms for time series data
8604	Neural network in a nutshell This is done using gradient descent (aka backpropagation), which by definition comprises two steps: calculating gradients of the loss/error function, then updating existing parameters in response to the gradients, which is how the descent is done.	Does backpropagation use gradient descent
717	How to Perform Systematic Sampling: StepsStep 1: Assign a number to every element in your population.  Step 2: Decide how large your sample size should be.  Step 3: Divide the population by your sample size.  Step 1: Assign a number to every element in your population.Step 2: Decide how large your sample size should be.More items•	What are the steps of systematic sampling
4331	Interaction effects occur when the effect of one variable depends on the value of another variable.  In this manner, analysts use models to assess the relationship between each independent variable and the dependent variable. This kind of an effect is called a main effect.	What does it mean when there is an interaction between two variables
2635	0:007:47Suggested clip · 116 seconds[Proof] Sequence is divergent - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you prove divergence
7512	A simple random sample is used to represent the entire data population and. randomly selects individuals from the population without any other consideration. A stratified random sample, on the other hand, first divides the population into smaller groups, or strata, based on shared characteristics.	What's the difference between stratified and random sampling
7255	In short, linear regression is one of the mathematical models to describe the (linear) relationship between input and output. Least squares, on the other hand, is a method to metric and estimate models, in which the optimal parameters have been found.	What is the difference between least squares and linear regression
4630	Also known as a parallel boxplot or comparative boxplot, a side-by-side boxplot is a visual display comparing the levels (the possible values) of one categorical variable by means of a quantitative variable.	How do you describe side by side Boxplots
2727	A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model.	What is the use of confusion matrix in machine learning
6905	To convert a logit ( glm output) to probability, follow these 3 steps:Take glm output coefficient (logit)compute e-function on the logit using exp() “de-logarithimize” (you'll get odds then)convert odds to probability using this formula prob = odds / (1 + odds) .	How do you find the probability of a logit model
3236	The process of dividing each feature by its range is called feature scaling. The process feature scaling is used to standardize each variables individually. The term feature scaling when it comes to data processing is also known as data normalization.	What is the process of dividing each feature by its range
7575	The probability theory provides a means of getting an idea of the likelihood of occurrence of different events resulting from a random experiment in terms of quantitative measures ranging between zero and one. The probability is zero for an impossible event and one for an event which is certain to occur.	What is the need of probability
7674	80% accurate. Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.	What is the precision rate accuracy rate for predicting positives )
2033	As a hypothetical example of systematic sampling, assume that in a population of 10,000 people, a statistician selects every 100th person for sampling. The sampling intervals can also be systematic, such as choosing a new sample to draw from every 12 hours.	What is an example of systematic sampling
4817	A continuous random variable is a function X X X on the outcomes of some probabilistic experiment which takes values in a continuous set V V V. That is, the possible outcomes lie in a set which is formally (by real-analysis) continuous, which can be understood in the intuitive sense of having no gaps.	What's a continuous random variable
198	Naive Bayes algorithm works on Bayes theorem and takes a probabilistic approach, unlike other classification algorithms. The algorithm has a set of prior probabilities for each class. Once data is fed, the algorithm updates these probabilities to form something known as posterior probability.	Which algorithm is used in artificial intelligence
861	Convolutional Neural Networks (CNNs) Image segmentation with CNN involves feeding segments of an image as input to a convolutional neural network, which labels the pixels. The CNN cannot process the whole image at once.	What is segmentation in CNN
81	Multitasking is the display of strengths and positive attributes in a multiple number of ways at the same time.  Time is money, and so multitasking will help you carry out more tasks and be more competitive. Remember to keep multitasking to a limit you can handle, to ensure focus is maximised in the tasks you carry out.	Why is it important to be able to multitask
996	The optimal number of clusters can be defined as follow:Compute clustering algorithm (e.g., k-means clustering) for different values of k.  For each k, calculate the total within-cluster sum of square (wss).Plot the curve of wss according to the number of clusters k.More items	How do you choose K in clustering
1213	The false alarm probability is the probability that exceeds a certain threshold when there is no signal.	What is probability of false alarm
5480	In statistical terminology, this is called skewness. In this case, the average can be significantly influenced by the few values, making it not very representative of the majority of the values in the data set. Under these circumstances, median gives a better representation of central tendency than average.	Why is median better than average
1810	The cross product a × b is defined as a vector c that is perpendicular (orthogonal) to both a and b, with a direction given by the right-hand rule and a magnitude equal to the area of the parallelogram that the vectors span.	What is the cross product of two vectors
5313	As nouns the difference between trial and experiment is that trial is an opportunity to test something out; a test while experiment is a test under controlled conditions made to either demonstrate a known truth, examine the validity of a hypothesis, or determine the efficacy of something previously untried.	What is the difference between trial and experiment
691	An estimator of a given parameter is said to be consistent if it converges in probability to the true value of the parameter as the sample size tends to infinity.	What makes an estimator consistent
7733	It is the sum of the likelihood residuals. At record level, the natural log of the error (residual) is calculated for each record, multiplied by minus one, and those values are totaled.	What is the log likelihood in logistic regression
8231	The natural logarithm function is negative for values less than one and positive for values greater than one. So yes, it is possible that you end up with a negative value for log-likelihood (for discrete variables it will always be so).	Can the likelihood be negative
1296	In statistics, the p-value is the probability of obtaining results at least as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is correct.  A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.	What does P value mean
775	The standard deviation is a statistic that measures the dispersion of a dataset relative to its mean and is calculated as the square root of the variance. The standard deviation is calculated as the square root of variance by determining each data point's deviation relative to the mean.	What is deviation and standard deviation
372	The likelihood ratio test (LRT) is a statistical test of the goodness-of-fit between two models. A relatively more complex model is compared to a simpler model to see if it fits a particular dataset significantly better. If so, the additional parameters of the more complex model are often used in subsequent analyses.	What is LRT in statistics
2651	The standard normal or z-distribution assumes that you know the population standard deviation. The t-distribution is based on the sample standard deviation.	How does the t distribution differ from the z distribution
8096	AIC and BIC are Information criteria methods used to assess model fit while penalizing the number of estimated parameters.	What is AIC and BIC in statistics
150	The test statistic is used to calculate the p-value. A test statistic measures the degree of agreement between a sample of data and the null hypothesis. Its observed value changes randomly from one random sample to a different sample.  This causes the test's p-value to become small enough to reject the null hypothesis.	What does a test statistic tell you
61	In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones.	What are the uses of the boosting method for machine learning
605	Hidden layers allow for the function of a neural network to be broken down into specific transformations of the data.  For example, a hidden layer functions that are used to identify human eyes and ears may be used in conjunction by subsequent layers to identify faces in images.	Why hidden layers are required in neural networks
822	The histogram is used for variables whose values are numerical and measured on an interval scale. It is generally used when dealing with large data sets (greater than 100 observations). A histogram can also help detect any unusual observations (outliers) or any gaps in the data.	What type of data is best for a histogram
2196	The loss function of SVM is very similar to that of Logistic Regression. Looking at it by y = 1 and y = 0 separately in below plot, the black line is the cost function of Logistic Regression, and the red line is for SVM. Please note that the X axis here is the raw model output, θᵀx.	What is the loss function for SVM
3330	The total degrees of freedom (dfT) are equal to nT – 1, where nT is the total number of subjects in the design. The between-group degrees of freedom (dfB), which are not absolutely necessary to find, are equal to (j)(k) – 1, where j is the number of levels of variable J and k is the number of levels of variable K.	How do you calculate degrees of freedom for a factorial Anova
2069	"Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a dataset. MDS is used to translate ""information about the pairwise 'distances' among a set of n objects or individuals"" into a configuration of n points mapped into an abstract Cartesian space."	In general terms what is multidimensional scaling
79	The original AlphaGo demonstrated superhuman Go-playing ability, but needed the expertise of human players to get there. Namely, it used a dataset of more than 100,000 Go games as a starting point for its own knowledge. AlphaGo Zero, by comparison, has only been programmed with the basic rules of Go.	Why was AlphaGo able to play go so well
1157	7:3021:58Suggested clip · 120 secondsStatQuest: Principal Component Analysis (PCA), Step-by-Step YouTubeStart of suggested clipEnd of suggested clip	How do you use principal component analysis
975	Linear regression is one of the most common techniques of regression analysis. Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables.	What is the difference between multiple and linear regression
586	Accuracy in Machine Learning Accuracy is the number of correctly predicted data points out of all the data points.  Often, accuracy is used along with precision and recall, which are other metrics that use various ratios of true/false positives/negatives.	What is accuracy in machine learning
72	Key TakeawaysThe least squares method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve.Least squares regression is used to predict the behavior of dependent variables.	What is least square method in regression
4160	It is a Markov random field. It was translated from statistical physics for use in cognitive science. The Boltzmann machine is based on stochastic spin-glass model with an external field, i.e., a Sherrington–Kirkpatrick model that is a stochastic Ising Model and applied to machine learning.	Is there a relation between Boltzmann machines and Markov random fields
5554	The most basic way to use a SVC is with a linear kernel, which means the decision boundary is a straight line (or hyperplane in higher dimensions).	What is kernel in SVC
726	Machine learning uses neural networks and automated algorithms to predict outcomes. Accuracy of data mining depends on how data is collected. Data Mining produces accurate results which are used by machine learning making machine learning produce better results.	What is the difference between artificial intelligence machine learning statistics and data mining
297	“Bayesian statistics is a mathematical procedure that applies probabilities to statistical problems. It provides people the tools to update their beliefs in the evidence of new data.”	How do you explain Bayesian statistics
5367	The 2nd moment around the mean = Σ(xi – μx)2. The second is the variance. In practice, only the first two moments are ever used in statistics.	How do you find second moment in statistics
6659	Gaussian Distribution Function The nature of the gaussian gives a probability of 0.683 of being within one standard deviation of the mean. The mean value is a=np where n is the number of events and p the probability of any integer value of x (this expression carries over from the binomial distribution ).	What is the mean of a Gaussian distribution
1719	For example, create a time vector and signal:t = 0:1/100:10-1/100; % Time vector x = sin(2*pi*15*t) + sin(2*pi*40*t); % Signal.y = fft(x); % Compute DFT of x m = abs(y); % Magnitude y(m<1e-6) = 0; p = unwrap(angle(y)); % Phase.More items	How do you find the DFT of a signal in Matlab
520	A random variable is a numerical description of the outcome of a statistical experiment.  For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x).	What is random in probability
8239	The cumulative distribution function (CDF) of random variable X is defined as FX(x)=P(X≤x), for all x∈R.SolutionTo find the CDF, note that.  To find P(2<X≤5), we can write P(2<X≤5)=FX(5)−FX(2)=3132−34=732.  To find P(X>4), we can write P(X>4)=1−P(X≤4)=1−FX(4)=1−1516=116.	How do you find the probability of a cumulative distribution function
1091	"The term ""negative binomial"" is likely due to the fact that a certain binomial coefficient that appears in the formula for the probability mass function of the distribution can be written more simply with negative numbers."	Why is the negative binomial distribution important
3222	SVMs assume that the data it works with is in a standard range, usually either 0 to 1, or -1 to 1 (roughly). So the normalization of feature vectors prior to feeding them to the SVM is very important.  Some libraries recommend doing a 'hard' normalization, mapping the min and max values of a given dimension to 0 and 1.	Is normalization required for SVM
7138	A Moving-Average (MA) Process Has a Limited Memory An observation of a moving-average process (the MA in ARIMA) consists of a constant, μ (the long-term mean of the process), plus independent random noise minus a fraction of the previous random noise.	What is an MA process
100	Supervised learning can be used to teach an algorithm to distinguish spam mail from normal correspondence. Unsupervised: In this type of learning, no training data is provided. The algorithm analyzes a body of data for patterns or common elements. Large amounts of unstructured data can then be sorted and categorized.	What is predictive learning Is it the same as unsupervised learning
276	Bootstrapping is building a company from the ground up with nothing but personal savings, and with luck, the cash coming in from the first sales. The term is also used as a noun: A bootstrap is a business an entrepreneur with little or no outside cash or other support launches.	What is the meaning of bootstrapping
1183	Regression is a return to earlier stages of development and abandoned forms of gratification belonging to them, prompted by dangers or conflicts arising at one of the later stages. A young wife, for example, might retreat to the security of her parents' home after her…	What is an example of regression
572	Tensors are simply mathematical objects that can be used to describe physical properties, just like scalars and vectors. In fact tensors are merely a generalisation of scalars and vectors; a scalar is a zero rank tensor, and a vector is a first rank tensor.	What do you mean by tensor
2793	Unsupervised learning is commonly used for finding meaningful patterns and groupings inherent in data, extracting generative features, and exploratory purposes.	What is unsupervised learning used for
2358	"The Poisson distribution can be used to calculate the probabilities of various numbers of ""successes"" based on the mean number of successes.  The mean of the Poisson distribution is μ. The variance is also equal to μ. Thus, for this example, both the mean and the variance are equal to 8."	What is the mean of a Poisson distribution
132	To get a p-value we compare our observed test- statistic to the randomization distribution of test- statistics obtained by assuming the null is true. The p-value will be the proportion of test- statistics in the randomization distribution that are as or more extreme than the observed test- statistic.	How do you find the p value for a randomization test
5887	"Many loss or cost functions are designed with an absolute minimum of 0 possible for ""no error"" results.  So in supervised learning problems of regression and classification, you will rarely see a negative cost function value. But there is no absolute rule against negative costs in principle."	Can loss function negative
2888	Yes, residual learning is achieved by simply adding an identity mapping parallel to a layer.	Does Residual Learning work without batch normalization
7159	Negative values for the skewness indicate data that are skewed left and positive values for the skewness indicate data that are skewed right. By skewed left, we mean that the left tail is long relative to the right tail.	What does the skewness value tell us
2613	Multi-objective optimization (also known as multi-objective programming, vector optimization, multicriteria optimization, multiattribute optimization or Pareto optimization) is an area of multiple criteria decision making that is concerned with mathematical optimization problems involving more than one objective	What is multi objective optimization problem
6569	The value of the odds ratio tells you how much more likely someone under 25 might be to make a claim, for example, and the associated confidence interval indicates the degree of uncertainty associated with that ratio.	How do you interpret confidence intervals and odds ratio
6490	Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.  Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.	Why optimizers are used in neural network
6218	When p is greater than 0.5, the distribution will be positively skewed (the peak will be on the left side of the distribution, with relatively fewer observations on the right).	When binomial distribution is positively skewed
5655	2:107:35Suggested clip · 110 secondsLinear Regression R Program Make Predictions - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you predict a value in linear regression in R
535	Microsoft Excel has a few statistical functions that can help you to do linear regression analysis such as LINEST, SLOPE, INTERCPET, and CORREL. Because the LINEST function returns an array of values, you must enter it as an array formula.	Which statistical functions relate to regression analysis
6950	1| Fast R-CNN Written in Python and C++ (Caffe), Fast Region-Based Convolutional Network method or Fast R-CNN is a training algorithm for object detection. This algorithm mainly fixes the disadvantages of R-CNN and SPPnet, while improving on their speed and accuracy.	Which algorithm is best for object detection
1577	Dropout is a regularization technique for neural network models proposed by Srivastava, et al. in their 2014 paper Dropout: A Simple Way to Prevent Neural Networks from Overfitting (download the PDF). Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly.	How does dropout work in neural networks
1168	The term general linear model (GLM) usually refers to conventional linear regression models for a continuous response variable given continuous and/or categorical predictors. It includes multiple linear regression, as well as ANOVA and ANCOVA (with fixed effects only).	Is linear regression A GLM
7365	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution.	Why is it important that we learn about the normal distribution
1349	When the population contains higher dimensions or more random variables, a matrix is used to describe the relationship between different dimensions. In a more easy-to-understand way, covariance matrix is to define the relationship in the entire dimensions as the relationships between every two random variables.	What is a covariance matrix used for
4069	In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.  Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one.	What does logistic model mean
765	Sequence Modeling is the task of predicting what word/letter comes next. Unlike the FNN and CNN, in sequence modeling, the current output is dependent on the previous input and the length of the input is not fixed.	What is sequence model in deep learning
834	The mean Average Precision or mAP score is calculated by taking the mean AP over all classes and/or overall IoU thresholds, depending on different detection challenges that exist. In PASCAL VOC2007 challenge, AP for one object class is calculated for an IoU threshold of 0.5.	How do you calculate average precision score
1492	Two key benefits of Stochastic Gradient Descent are efficiency and the ease of implementation. In a situation when data is less, classifiers in the module are scaled to problems with more than 10^5 training examples and more than 10^5 features.	What is the benefit of stochastic gradient descent
7369	A simple perceptron. Each input is connected to the neuron, shown in gray. Each connection has a weight, the value of which evolves over time, and is used to modify the input. Weighted inputs are summed, and this sum determines the output of the neuron, which is a classification (in this case, either 0 or 1).	What are weights in Perceptron
589	Given that very large datasets are often used to train deep learning neural networks, the batch size is rarely set to the size of the training dataset. Smaller batch sizes are used for two main reasons: Smaller batch sizes are noisy, offering a regularizing effect and lower generalization error.	How does batch size affect training
2591	Different classifiers are then added on top of this feature extractor to classify images.Support Vector Machines. It is a supervised machine learning algorithm used for both regression and classification problems.  Decision Trees.  K Nearest Neighbor.  Artificial Neural Networks.  Convolutional Neural Networks.	How do you classify images in machine learning
8262	A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function.	What is the difference between probability distribution function and probability density function
470	Definition. Conceptually, a data stream is a sequence of data items that collectively describe one or more underlying signals.  A stream model explains how to reconstruct the underlying signals from individual stream items. Thus, understanding the model is a prerequisite for stream processing and stream mining.	What is stream data model
8010	A statistic is a characteristic of a sample. Generally, a statistic is used to estimate the value of a population parameter. For instance, suppose we selected a random sample of 100 students from a school with 1000 students. The average height of the sampled students would be an example of a statistic.	What is a statistic in statistics
6891	Expectations via joint densities: Given a function of x and y (e.g., g(x, y) = xy, or g(x, y) = x, etc.), E(g(X, Y )) = ∫∫ g(x, y)f(x, y)dxdy. Independence: X and Y are called independent if the joint p.d.f. is the product of the individual p.d.f.'s, i.e., if f(x, y) = fX(x)fY (y) for all x, y.	Are X and Y independent joint density function
1252	The critical value approach and the P-value approach give the same results when testing hypotheses.  The P-value is the probability of obtaining a test statistic as extreme as the one for the current sample under the assumption that the null hypothesis is true.	Is critical value and p value the same
7785	"In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for ""maximum-margin"" classification, most notably for support vector machines (SVMs). For an intended output t = ±1 and a classifier score y, the hinge loss of the prediction y is defined as."	What is hinge loss in SVM
5394	The beam search strategy generates the translation word by word from left-to-right while keeping a fixed number (beam) of active candidates at each time step. By increasing the beam size, the translation performance can increase at the expense of significantly reducing the decoder speed.	What is beam search decoding
3291	A Boltzmann Machine is a network of symmetrically connected, neuron- like units that make stochastic decisions about whether to be on or off. Boltz- mann machines have a simple learning algorithm that allows them to discover interesting features in datasets composed of binary vectors.	What are Boltzmann machines used for
4792	In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error.	Is RMSE the same as standard error
299	How to choose the size of the convolution filter or Kernel size1x1 kernel size is only used for dimensionality reduction that aims to reduce the number of channels. It captures the interaction of input channels in just one pixel of feature map.  2x2 and 4x4 are generally not preferred because odd-sized filters symmetrically divide the previous layer pixels around the output pixel.	How do I know what size filter for CNN
2435	Quantization is representing the sampled values of the amplitude by a finite set of levels, which means converting a continuous-amplitude sample into a discrete-time signal.  The discrete amplitudes of the quantized output are called as representation levels or reconstruction levels.	What is Quantisation level
1521	Decision tree algorithms use information gain to split a node. Both gini and entropy are measures of impurity of a node. A node having multiple classes is impure whereas a node having only one class is pure. Entropy in statistics is analogous to entropy in thermodynamics where it signifies disorder.	What is Gini and entropy in decision tree
796	The Loss Function is one of the important components of Neural Networks. Loss is nothing but a prediction error of Neural Net. And the method to calculate the loss is called Loss Function. In simple words, the Loss is used to calculate the gradients. And gradients are used to update the weights of the Neural Net.	What does loss mean in neural network
2530	Data science is the field of study that combines domain expertise, programming skills, and knowledge of mathematics and statistics to extract meaningful insights from data.	What is data science in simple words
261	Three keys to managing bias when building AIChoose the right learning model for the problem. There's a reason all AI models are unique: Each problem requires a different solution and provides varying data resources.  Choose a representative training data set.  Monitor performance using real data.	How does machine learning remove bias
6445	The mean used here is referred to as the arithmetic mean – the sum of all values divided by the number of cases. When working with grouped data, this mean is sometimes referred to as the weighted mean or, more properly, the weighted arithmetic mean. Ungrouped and group methods.	Is a weighted mean and a grouped data mean the same
6580	1). Now the difference is that Confusion Matrix is used to evaluate the performance of a classifier, and it tells how accurate a classifier is in making predictions about classification, and contingency table is used to evaluate association rules.	What is the difference between contingency table and confusion matrix
809	Decision Tree node splitting is an important step, the core issue is how to choose the splitting attribute.  5, the splitting criteria is calculating information gain of each attribute, then the attribute with the maximum information gain or information gain ratio is selected as splitting attribute.	What is splitting criterion in data mining
5950	'Inverse matrix is a measure of how tightly clustered the variables are around the mean (the diagonal elements) and the extent to which they do not co-vary with the other variables (the off-diagonal elements). Thus, the higher the diagonal element, the tighter the variable is clustered around the mean.	What is the inverse covariance matrix What is its statistical meaning
2981	The purpose of hypothesis testing is to determine whether there is enough statistical evidence in favor of a certain belief, or hypothesis, about a parameter.	What is the purpose of hypothesis testing in statistics
6716	Non-random Variables. A non-random (deterministic, non-stochastic variable) is one whose value is known ahead of time or one whose past value is known. EX: Tomorrow's date, yesterday's temperature. Randomness & Time are linked.	What is not a random variable
1499	An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”.	What is Autoencoder in deep learning
57	Partition divides large amount of data into multiple slices based on value of a table column(s).  Bucketing decomposes data into more manageable or equal parts. With partitioning, there is a possibility that you can create multiple small partitions based on column values.	What is difference between bucketing and partitioning
116	Generally, every constraint satisfaction problem which has clear and well-defined constraints on any objective solution, that incrementally builds candidate to the solution and abandons a candidate (“backtracks”) as soon as it determines that the candidate cannot possibly be completed to a valid solution, can be solved	When can you use a backtracking algorithm
6009	Model Decay (also Model Failure) is an informal characterization of pathologies of models already deployed (in operation), whereby the model performance may deteriorate to the point of the model not being any longer fit for purpose.	What is model decay
3354	Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values. For machine learning, every dataset does not require normalization.	Does normalization improve performance machine learning
5108	A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc., although generally computational applications use more fine-grained POS tags like 'noun-plural'.	What is POS tagging in NLP
847	the t-test is robust against non-normality; this test is in doubt only when there can be serious outliers (long-tailed distributions – note the finite variance assumption); or when sample sizes are small and distributions are far from normal. 10 / 20 Page 20 . . .	Is t test robust to violations of normality
232	1 Answers found. A recursive filter has a system in which the output is directly dependent on one or more of its past outputs. But in a non recursive filter the system followed is the one in which the output is independent of any of the past outputs like, the feed-forward system where the system is having no feedback.	What is the main difference between recursive and non recursive filters in DSP
5611	Sudharsan also noted that deep meta reinforcement learning will be the future of artificial intelligence where we will implement artificial general intelligence (AGI) to build a single model to master a wide variety of tasks. Thus each model will be capable to perform a wide range of complex tasks.	Is reinforcement learning the future of AI
1333	The Bellman equation is important because it gives us the ability to describe the value of a state s, with the value of the s' state, and with an iterative approach that we will present in the next post, we can calculate the values of all states.	What is the signficance of the Bellman equation
950	Dissimilarity Measure Numerical measure of how different two data objects are range from 0 (objects are alike) to (objects are different) Proximity refers to a similarity or dissimilarity.	What is dissimilarity measure
6165	You can zoom in TensorBoard by dragging on the chart. You can also make the display larger by pressing the small blue box in the lower-left corner of the chart.	How do you zoom in on TensorBoard
6959	In statistics, a Poisson distribution is a statistical distribution that shows how many times an event is likely to occur within a specified period of time. It is used for independent events which occur at a constant rate within a given interval of time.	What do you mean by Poisson distribution
6013	With dropout (dropout rate less than some small value), the accuracy will gradually increase and loss will gradually decrease first(That is what is happening in your case). When you increase dropout beyond a certain threshold, it results in the model not being able to fit properly.	Does dropout increase accuracy
1156	Association Rule Mining, as the name suggests, association rules are simple If/Then statements that help discover relationships between seemingly independent relational databases or other data repositories. Most machine learning algorithms work with numeric datasets and hence tend to be mathematical.	What are association rules in data mining
654	When two events are dependent events, one event influences the probability of another event. A dependent event is an event that relies on another event to happen first.	What are the dependent events
782	Because the Lagrangian multiplier can be considered as a penalty term, and a negative penalty does not make sense. When is zero you are not violating any constraint, however when is infinity you have to satisfy the constraint (i.e - ) or else your objective function will be unbounded.	Can Lagrangian multiplier be negative
92	A frequency is the number of times a data value occurs. For example, if ten students score 80 in statistics, then the score of 80 has a frequency of 10. Frequency is often represented by the letter f. A frequency chart is made by arranging data values in ascending order of magnitude along with their frequencies.	How do you find the frequency
8260	The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(∗). Both functions will take any number and rescale it to fall between 0 and 1.	What is the difference between probit and logistic regression
1133	The distribution of a variable is a description of the relative numbers of times each possible outcome will occur in a number of trials.  If the measure is a Radon measure (which is usually the case), then the statistical distribution is a generalized function in the sense of a generalized function.	How is a variable distributed
1292	The least squares method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve. Least squares regression is used to predict the behavior of dependent variables.	What are the uses of least square method
6010	In simple linear regression a single independent variable is used to predict the value of a dependent variable. In multiple linear regression two or more independent variables are used to predict the value of a dependent variable. The difference between the two is the number of independent variables.	What should I choose simple linear regression or multiple linear regression
8368	Softmax is an activation function that outputs the probability for each class and these probabilities will sum up to one.  Cross Entropy loss is just the sum of the negative logarithm of the probabilities. They are both commonly used together in classifications.	What is Softmax loss function
3006	The moment generating function (MGF) of a random variable X is a function MX(s) defined as MX(s)=E[esX]. We say that MGF of X exists, if there exists a positive constant a such that MX(s) is finite for all s∈[−a,a]. Before going any further, let's look at an example.	What is moment generating function of a random variable
2003	Here are 13 ways you can naturally increase your eagerness to learn and keep feeding your curiosity to stay on your learning goals.Just Show Your Eagerness.  Stay Updated.  Don't Stop Developing Your Skills.  Look for Challenges.  Learn Lateral Thinking.  Be Open to New Experiences.  Start to Be Interesting.  Gain Initial Knowledge.More items•	In what ways do I become more eager to learn and improve myself
824	When those data are biased, model accuracy and fidelity are compromised. Biased models can limit credibility with important stakeholders. At worst, biased models will actively discriminate against certain groups of people. Being aware of these risks allows a Data Scientist to better eliminate bias.	Why does it matter if the statistical information that we use in our model is biased
1129	Systematic random samplingCalculate the sampling interval (the number of households in the population divided by the number of households needed for the sample)Select a random start between 1 and sampling interval.Repeatedly add sampling interval to select subsequent households.	How do you do a systematic random sample
1062	"Conditional random fields (CRFs) are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering ""neighboring"" samples, a CRF can take context into account."	What is CRF in machine learning
260	Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.	How are machine learning and statistics related
492	By using these midpoints as the categorical response values, the researcher can easily calculate averages. Granted, this average will only be an estimate or a “ballpark” value but is still extremely useful for the purpose of data analysis.	Can you average categorical data
4981	A Type II error is committed when we fail to believe a true condition. Continuing our shepherd and wolf example. Again, our null hypothesis is that there is “no wolf present.” A type II error (or false negative) would be doing nothing (not “crying wolf”) when there is actually a wolf present.	How do you determine Type 2 error
526	AUC and accuracy are fairly different things.  For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set. AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else.	What is the difference between accuracy and AUC
3036	Support Vector Machine can also be used as a regression method, maintaining all the main features that characterize the algorithm (maximal margin).  In the case of regression, a margin of tolerance (epsilon) is set in approximation to the SVM which would have already requested from the problem.	Is SVM good for regression
502	The bolder the probabilities, the better will be your Log Loss — closer to zero. It is a measure of uncertainty (you may call it entropy), so a low Log Loss means a low uncertainty/entropy of your model.	What is a good log loss score
4927	In unsupervised learning, there is no training data set and outcomes are unknown. Essentially the AI goes into the problem blind – with only its faultless logical operations to guide it.	Does unsupervised learning need training data
4723	Optimizing Neural Networks — Where to Start?Start with learning rate;Then try number of hidden units, mini-batch size and momentum term;Lastly, tune number of layers and learning rate decay.	How do you optimize a neural network
8450	You have not been infected with COVID-19 previously.You had COVID-19 in the past but you did not develop or have not yet developed detectable antibodies.The result may be wrong, known as a false negative.	What does a negative antibody test mean
67	There are a number of equations that can generate an S curve, the most common is logistics function with the equation (in Excel notation): S(x) = (1/(1+exp(-kx))^a is the simple form of the equation, where the minimum value is 0 and the maximum value is 1, k and a both >0 and control the shape.	How do you calculate S curve
1155	If we use non - standard units then we may not be able to express our measurement internationally as mainly standard units are used and accepted internationally. The non- standard units do not have the same dimensions all over the world.	What are the disadvantages of using non standard units
7044	The mn Rule Consider an experiment that is performed in two stages. If the first stage can be accomplished in m different ways and for each of these ways, the second stage can be accomplished in n different ways, then there are to- tal mn different ways to accomplish the experiment.	What is the MN rule in statistics
672	Independent and dependent variablesThe independent variable is the cause. Its value is independent of other variables in your study.The dependent variable is the effect. Its value depends on changes in the independent variable.	How do you identify independent and dependent variables
176	The “moments” of a random variable (or of its distribution) are expected values of powers or related functions of the random variable. The rth moment of X is E(Xr). In particular, the first moment is the mean, µX = E(X). The mean is a measure of the “center” or “location” of a distribution.	What is the moment of a random variable
4702	Prior probability, in Bayesian statistical inference, is the probability of an event before new data is collected. This is the best rational assessment of the probability of an outcome based on the current knowledge before an experiment is performed.	What is prior probability in naive Bayes
1072	The task of object localization is to predict the object in an image as well as its boundaries.  Simply, object localization aims to locate the main (or most visible) object in an image while object detection tries to find out all the objects and their boundaries.	What is localization in image processing
4637	Extreme Value AnalysisFocus on univariate methods.Visualize the data using scatterplots, histograms and box and whisker plots and look for extreme values.Assume a distribution (Gaussian) and look for values more than 2 or 3 standard deviations from the mean or 1.5 times from the first or third quartile.More items•	How do you find outliers in machine learning
7502	The name suggests that layers are fully connected (dense) by the neurons in a network layer.  In other words, the dense layer is a fully connected layer, meaning all the neurons in a layer are connected to those in the next layer.	What is a dense layer in a neural network
791	split testing	What does AB testing stand for
5274	This paper explains that to be a potential confounder, a variable needs to satisfy all three of the following criteria: (1) it must have an association with the disease, that is, it should be a risk factor for the disease; (2) it must be associated with the exposure, that is, it must be unequally distributed between	How do you choose a confounding variable
6706	Option D is correct. Q25. Instead of trying to achieve absolute zero error, we set a metric called bayes error which is the error we hope to achieve.	In which neural net architecture does weight sharing occur
667	Neural networks take input data, train themselves to recognize patterns found in the data, and then predict the output for a new set of similar data. Therefore, a neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems.	What is a neural network in programming
554	The resulting image after applying Canny operator (b). The primary advantages of the Sobel operator lie in its simplicity. The Sobel method provides a approximation to the gradient magnitude. Another advantage of the Sobel operator is it can detect edges and their orientations.	What is the advantage of Sobel operator over Prewitt operator
133	So, the rule of thumb is: use linear SVMs (or logistic regression) for linear problems, and nonlinear kernels such as the Radial Basis Function kernel for non-linear problems.	What kernel is used in SVM
545	Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable).	What is stochastic gradient descent method
1895	A probability distribution may be either discrete or continuous. A discrete distribution means that X can assume one of a countable (usually finite) number of values, while a continuous distribution means that X can assume one of an infinite (uncountable) number of different values.	What is the difference between discrete and continuous probability
3255	To visualize a small data set containing multiple categorical (or qualitative) variables, you can create either a bar plot, a balloon plot or a mosaic plot.  These methods make it possible to analyze and visualize the association (i.e. correlation) between a large number of qualitative variables.	What is a recommended way to visualize categorical data
6758	The idea is to separate the image into two parts; the background and foreground.Select initial threshold value, typically the mean 8-bit value of the original image.Divide the original image into two portions;  Find the average mean values of the two new images.Calculate the new threshold by averaging the two means.More items	How do you select threshold value in image processing
7797	Both methods are data dependent.  Statistical Learning is math intensive which is based on the coefficient estimator and requires a good understanding of your data. On the other hand, Machine Learning identifies patterns from your dataset through the iterations which require a way less of human effort.	Is statistical learning the same as machine learning
1383	In statistics and control theory, Kalman filtering, also known as linear quadratic estimation (LQE), is an algorithm that uses a series of measurements observed over time, containing statistical noise and other inaccuracies, and produces estimates of unknown variables that tend to be more accurate than those based on a	What is Kalman filter used for
549	Apriori is useable with large datasets and Eclat is better suited to small and medium datasets. Apriori scans the original (real) dataset, whereas Eclat scan the currently generated dataset. Apriori is slower than Eclat.	What is the difference between Apriori and Eclat algorithms in association rule mining
7673	In statistics, the Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. A variant for classification is also sometimes used.	What is Huber regression
7803	Stepwise regression is an appropriate analysis when you have many variables and you're interested in identifying a useful subset of the predictors. In Minitab, the standard stepwise regression procedure both adds and removes predictors one at a time.	When should I use stepwise regression
984	quality	What does Q stand for in Q learning
1964	to safeguard against the researcher problem of experimenter bias, researchers employ blind observers, single and double blind study, and placebos. to control for ethnocentrism, they use cross cultural sampling.	What are two methods researchers use to avoid experimenter bias
6785	0:404:05Suggested clip · 100 secondsHow to interpret a survival plot - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you read survival curves
6849	Some business analysts at claim that AI is a game changer for the personal device market. By 2020, about 60 percent of personal-device technology vendors will depend on AI-enabled Cloud platforms to deliver enhanced functionality and personalized services. AI technology will deliver an “emotional user experience.”	What is the future of AI and machine learning
144	Anomaly Detection MethodsSupervised methods. As the name suggests, this anomaly detection method requires the existence of a labeled dataset that contains both normal and anomalous data points.  Unsupervised methods.  Intrusion detection.  Mobile sensor data.  Network server or app failure.  Statistical Process Control.	What are the known ways of anomaly detection using machine learning
561	Not including the null hypothesis in your research is considered very bad practice by the scientific community. If you set out to prove an alternate hypothesis without considering it, you are likely setting yourself up for failure. At a minimum, your experiment will likely not be taken seriously.	Is null hypothesis good or bad
4265	Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).	What is a Seq2Seq model
2678	Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.	What is meant by Gaussian distributed
196	The probability density function (PDF) is defined for probability distributions of continuous random variables. The probability at a certain point of a continuous variable is zero. The cumulative distribution function (CDF) is a non-decreasing function as the probabilities can never be less than 0.	What is the difference between CDF and PDF in statistics
259	The resulting learned residual allows our network to theoretically do no worse (than without it). Peephole connections redirect the cell state as input to the LSTM input, output, and forget gates.  These connections are used to learn precise timings.	What is the difference between skip peephole and residual connections in neural networks
641	The frequency of a class interval is the number of data values that fall in the range specified by the interval. The size of the class interval is often selected as 5, 10, 15 or 20 etc. Each class interval starts at a value that is a multiple of the size.	What is class interval size
592	Data = true signal + noise Noisy data are data with a large amount of additional meaningless information in it called noise. This includes data corruption and the term is often used as a synonym for corrupt data. It also includes any data that a user system cannot understand and interpret correctly.	What does noise mean in statistics
1145	Sensitivity refers to a test's ability to designate an individual with disease as positive. A highly sensitive test means that there are few false negative results, and thus fewer cases of disease are missed. The specificity of a test is its ability to designate an individual who does not have a disease as negative.	What does it mean if a test is sensitive but not specific
1058	In-group favoritism, sometimes known as in-group–out-group bias, in-group bias, intergroup bias, or in-group preference, is a pattern of favoring members of one's in-group over out-group members. This can be expressed in evaluation of others, in allocation of resources, and in many other ways.	What does having biased groups mean
734	Control Charts: A discrete distribution is one in which the data can only take on certain values, for example integers. A continuous distribution is one in which data can take on any value within a specified range (which may be infinite).	What is the difference between discrete and continuous distribution
265	The definition of an endogenous variable, exogenous variable and parameter are as follows: An Endogenous Variable- is a variable whose value is determined within the model itself.  An Exogenous Variable – is a variable whose value is assumed to be determined outside the model.	What is the difference between endogenous exogenous and parameter
7234	The Hidden Markov Model (HMM) is a relatively simple way to model sequential data. A hidden Markov model implies that the Markov Model underlying the data is hidden or unknown to you. More specifically, you only know observational data and not information about the states.	How do hidden Markov models work
337	Every probability pi is a number between 0 and 1, and the sum of all the probabilities is equal to 1. Examples of discrete random variables include: The number of eggs that a hen lays in a given day (it can't be 2.3) The number of people going to a given soccer match.	What is an example of a discrete random variable
5642	occurs when individuals or groups in a study differ systematically from the population of interest leading to a systematic error in an association or outcome.	What is selection bias in research
4860	This makes it easy for you to quickly see which variable is independent and which is dependent when looking at a graph or chart. The independent variable always goes on the x-axis, or the horizontal axis. The dependent variable goes on the y-axis, or vertical axis.	How do you find the relationship between independent and dependent variables
1216	The first benefit of time series analysis is that it can help to clean data. This makes it possible to find the true “signal” in a data set, by filtering out the noise. This can mean removing outliers, or applying various averages so as to gain an overall perspective of the meaning of the data.	What are the advantages of time series analysis
5257	So the difference is in the way the future reward is found. In Q-learning it's simply the highest possible action that can be taken from state 2, and in SARSA it's the value of the actual action that was taken.	What is the difference between Q learning and Sarsa
7310	In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph.  The underlying graph of a Markov random field may be finite or infinite.	What is the definition of the undirected Markov property
99	Change in either of Proximity function, no. of data points or no. of variables will lead to different clustering results and hence different dendrograms.	What could be the possible reason s for producing two different Dendrograms using agglomerative clustering algorithm for the same dataset
1334	How to Protect Against Confirmation Bias Find someone who disagrees with a decision you're about to make. Ask them why they disagree with you. Carefully listen to what they have to say.   Continue listening until you can honestly say, “I now understand why you believe that.”	How do you protect yourself from confirmation bias
755	Bootstrapping is a technique used by individuals in business to overcome obstacles, achieve goals and make improvements through organic, self-sustainable means with no assistance from outside.	What are bootstrapping strategies
1241	Relative frequencies can be written as fractions, percents, or decimals. The column should add up to 1 (or 100%). The only difference between a relative frequency distribution graph and a frequency distribution graph is that the vertical axis uses proportional or relative frequency rather than simple frequency.	What is a relative frequency distribution and how is it different from just a frequency distribution
7190	Difference between multi-class classification & multi-label classification is that in multi-class problems the classes are mutually exclusive, whereas for multi-label problems each label represents a different classification task, but the tasks are somehow related.	What is the difference between multi label and multi class classification
5270	"The CVT is an automatic transmission that uses two pulleys with a steel belt running between them. To continuously vary its gear ratios, the CVT simultaneously adjusts the diameter of the ""drive pulley"" that transmits torque from the engine and the ""driven pulley"" that transfers torque to the wheels."	What is continuously variable transmission How does it work
484	Machine Learning is a set of algorithms that parse data and learns from the parsed data and use those learnings to discover patterns of interest. Neural Network or Artificial Neural Network is one set of algorithms used in machine learning for modeling the data using graphs of Neurons.	What is the difference between machine learning and neural networks
1377	jackknifing is calculation with data sets sampled randomly from the original data.  Bootstrapping is similar to jackknifing except that the position chosen at random may include multiple copies of the same position, to form data sets of the same size as original, to preserve statistical properties of data sampling.	Why is it called bootstrapping statistics
7152	In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.	What is a good explanation of Latent Dirichlet Allocation
6693	In Convolutional neural network, the kernel is nothing but a filter that is used to extract the features from the images. The kernel is a matrix that moves over the input data, performs the dot product with the sub-region of input data, and gets the output as the matrix of dot products.	What is a kernel in CNN
4579	1. Why is the XOR problem exceptionally interesting to neural network researchers?  Explanation: Linearly separable problems of interest of neural network researchers because they are the only class of problem that Perceptron can solve successfully.	Why is the XOR problem exceptionally
4654	The Paired Samples t Test compares two means that are from the same individual, object, or related units. The two means can represent things like: A measurement taken at two different times (e.g., pre-test and post-test with an intervention administered between the two time points)5 päivää sitten	What is the comparison mean for a paired sample t test
6561	"The binomial distribution model allows us to compute the probability of observing a specified number of ""successes"" when the process is repeated a specific number of times (e.g., in a set of patients) and the outcome for a given patient is either a success or a failure.  The binomial equation also uses factorials."	Why is binomial distribution useful
5259	Simple Random Sample vs. Random Sample A simple random sample is similar to a random sample. The difference between the two is that with a simple random sample, each object in the population has an equal chance of being chosen. With random sampling, each object does not necessarily have an equal chance of being chosen.	What is the difference between a random sample and a simple random sample
7413	The Range is the difference between the lowest and highest values. Example: In {4, 6, 9, 3, 7} the lowest value is 3, and the highest is 9. So the range is 9 − 3 = 6.	How do you interpret the range in statistics
5193	A bandit is a robber, thief, or outlaw.  A bandit typically belongs to a gang of bandits who commit crimes in remote, lawless, or out-of-the-way places.	What is a group of bandits called
291	A discrete variable is a variable which can only take a countable number of values. In this example, the number of heads can only take 4 values (0, 1, 2, 3) and so the variable is discrete. The variable is said to be random if the sum of the probabilities is one.	Which of the following is considered a discrete random variable
1752	Note: a Markov chain (of any order) is a stochastic recursive sequence of finite order, or equivalently an auto-regressive process of finite order (possibly nonlinear). In contrast, the martingale property does not put constraints on the order of recursion, while imposing a linear projection condition.	Is a martingale a Markov process
5206	The standard deviation is the square root of the variance. Use a calculator to find the square root, and the result is the standard deviation. Report your result. Using this calculation, the precision of the scale can be represented by giving the mean, plus or minus the standard deviation.	How do you calculate precision from standard deviation
8616	"The length of time between each transit is the planet's ""orbital period"", or the length of a year on that particular planet. Not all planets have years as long as a year on the Earth! Some planets discovered by Kepler orbit around their stars so quickly that their years only last about four hours!"	What is the relationship between transit duration and orbital period
719	Statistical learning plays a key role in many areas of science, finance and industry.  Some more examples of the learning problems are: Predict whether a patient, hospitalized due to a heart attack, will have a second heart attack.	Which is an example of statistical learning
3462	The Boruta algorithm is a wrapper built around the random forest classification algorithm. It tries to capture all the important, interesting features you might have in your dataset with respect to an outcome variable. First, it duplicates the dataset, and shuffle the values in each column.	How does Boruta algorithm work
7804	Accuracy refers to the closeness of a measured value to a standard or known value.  Precision refers to the closeness of two or more measurements to each other. Using the example above, if you weigh a given substance five times, and get 3.2 kg each time, then your measurement is very precise.	What is the difference between accuracy and precision with example
722	"The maximum or minimum over the entire function is called an ""Absolute"" or ""Global"" maximum or minimum. There is only one global maximum (and one global minimum) but there can be more than one local maximum or minimum. Assuming this function continues downwards to left or right: The Global Maximum is about 3.7."	Is it possible to have an absolute minimum value but not a local minimum value
4731	Multivariate data analysis is a set of statistical models that examine patterns in multidimensional data by considering, at once, several data variables. It is an expansion of bivariate data analysis, which considers only two variables in its models.	What are the multivariate data analysis procedures
349	Greedy Search This strategy selects the most probable word (i.e. argmax) from the model's vocabulary at each decoding time-step as the candidate to output sequence.	What is greedy decoding
7843	Harmonic means are often used in averaging things like rates (e.g., the average travel speed given a duration of several trips). The weighted harmonic mean is used in finance to average multiples like the price-earnings ratio because it gives equal weight to each data point.	When should harmonic mean be used
6023	If you are broadcasting or reinforcing sound outside, and even your best windscreen can't keep out the persistent low-frequency rumble from wind noise, then stopping it right at the source may be your best option. Highpass filters are excellent for this application.	When should I use high pass filter
7164	1 Answer. Normalized discounted cumulative gain is one of the standard method of evaluating ranking algorithms. You will need to provide a score to each of the recommendations that you give. If your algorithm assigns a low (better) rank to a high scoring entity, your NDCG score will be higher, and vice versa.	How do you evaluate a rank algorithm
2355	Bayesian hyperparameter tuning allows us to do so by building a probabilistic model for the objective function we are trying to minimize/maximize in order to train our machine learning model. Examples of such objective functions are not scary - accuracy, root mean squared error and so on.	What is Bayesian Hyperparameter optimization
6451	Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series. There are several autocorrelation coefficients, corresponding to each panel in the lag plot.	What is the autocorrelation for a time series
7001	Typically, with neural networks, we seek to minimize the error. As such, the objective function is often referred to as a cost function or a loss function and the value calculated by the loss function is referred to as simply “loss.”	What is objective function in neural network
1247	SummaryLoad EMNIST digits from the Extra Keras Datasets module.Prepare the data.Define and train a Convolutional Neural Network for classification.Save the model.Load the model.Generate new predictions with the loaded model and validate that they are correct.	How do you load a model and predict keras
7320	TL; DR: The naive Bayes classifier is an approximation to the Bayes classifier, in which we assume that the features are conditionally independent given the class instead of modeling their full conditional distribution given the class. A Bayes classifier is best interpreted as a decision rule.	What is the difference between the Naive Bayes Classifier and the Bayes classifier
8003	Estimation is the process used to calculated these population parameters by analyzing only a small random sample from the population. The value or range of values used to approximate a parameter is called an estimate.	What is the concept of estimation
2503	An independent variable is defined within the context of a dependent variable. In the context of a model the independent variables are input whereas the dependent variables are the targets (Input vs Output). An exogenous variable is a variable whose state is independent of the state of other variables in a system.	Is exogenous independent variable
4949	Lasso Regression Another Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather than the L2 norm as a measure of complexity.	Does Lasso regression take care of Multicollinearity
2863	In the study of probability theory, the central limit theorem (CLT) states that the distribution of sample approximates a normal distribution (also known as a “bell curve”) as the sample size becomes larger, assuming that all samples are identical in size, and regardless of the population distribution shape.	What is central limit theorem in probability
12	Correlation is the process of moving a filter mask often referred to as kernel over the image and computing the sum of products at each location. Correlation is the function of displacement of the filter.	What is correlation in computer vision
2007	Why You Should Care About the Classical OLS Assumptions In a nutshell, your linear model should produce residuals that have a mean of zero, have a constant variance, and are not correlated with themselves or other variables.	What are the assumptions of OLS regression
7321	In the machine learning world, offline learning refers to situations where the program is not operating and taking in new information in real time. Instead, it has a static set of input data. The opposite is online learning, where the machine learning program is working in real time on data that comes in.	What is the meaning of offline learning
5390	Outgroup homogeneity is the tendency for members of a group to see themselves as more diverse and heterogeneous than they are seen by an outgroup. Thus, for example, whereas Italians see themselves as quite diverse and different from one another, Americans view Italians as more similar to each other, or more alike.	Which statement is an example of the outgroup homogeneity effect
2879	6 Freebies to Help You Increase the Performance of Your Object Detection ModelsVisually Coherent Image Mix-up for Object Detection (+3.55% mAP Boost)Classification Head Label Smoothening (+2.16% mAP Boost)Data Pre-processing (Mixed Results)Training Scheduler Revamping (+1.44% mAP Boost)More items	How can object detection be improved
5312	In statistics and probability analysis, the expected value is calculated by multiplying each of the possible outcomes by the likelihood each outcome will occur and then summing all of those values. By calculating expected values, investors can choose the scenario most likely to give the desired outcome.	How do you find the expected value
3729	4. A size of 100 means the vector representing each document will contain 100 elements - 100 values. The vector maps the document to a point in 100 dimensional space. A size of 200 would map a document to a point in 200 dimensional space. The more dimensions, the more differentiation between documents.	What is vector size in Doc2Vec
8233	One reason this is done is because the normal distribution often describes the actual distribution of the random errors in real-world processes reasonably well.  Some methods, like maximum likelihood, use the distribution of the random errors directly to obtain parameter estimates.	Why error terms should be normally distributed
5023	In statistics, a type of probability distribution in which all outcomes are equally likely.  A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same.	What does it mean if a distribution is uniform
1973	Separating data into training and testing sets is an important part of evaluating data mining models.  By using similar data for training and testing, you can minimize the effects of data discrepancies and better understand the characteristics of the model.	Why do you separate training data from testing data
3721	Advantages and disadvantagesAre simple to understand and interpret.  Have value even with little hard data.  Help determine worst, best and expected values for different scenarios.Use a white box model.  Can be combined with other decision techniques.	What are the advantages and disadvantages of decision trees
1654	Probability density function (PDF) is a statistical expression that defines a probability distribution (the likelihood of an outcome) for a discrete random variable (e.g., a stock or ETF) as opposed to a continuous random variable.	What does probability density mean
1372	Low-shot learning deep learning is based on the concept that reliable algorithms can be created to make predictions from minimalist datasets.	What is low shot learning
6515	Definition. Multivariate statistics refers to methods that examine the simultaneous effect of multiple variables. Traditional classification of multivariate statistical methods suggested by Kendall is based on the concept of dependency between variables (Kendall 1957).	What does multivariate mean in statistics
7899	A local minimum of a function is a point where the function value is smaller than at nearby points, but possibly greater than at a distant point. A global minimum is a point where the function value is smaller than at all other feasible points.	What is meant by local and global minima
3193	P > 0.05 is the probability that the null hypothesis is true. 1 minus the P value is the probability that the alternative hypothesis is true. A statistically significant test result (P ≤ 0.05) means that the test hypothesis is false or should be rejected. A P value greater than 0.05 means that no effect was observed.	What does P value of 0.05 mean
1348	In Average linkage clustering, the distance between two clusters is defined as the average of distances between all pairs of objects, where each pair is made up of one object from each group. D(r,s) = Trs / ( Nr * Ns) Where Trs is the sum of all pairwise distances between cluster r and cluster s.	Do clustering using hierarchical with average linkage
1168	The only difference between proportionate and disproportionate stratified random sampling is their sampling fractions.  If the researcher commits mistakes in allotting sampling fractions, a stratum may either be overrepresented or underrepresented which will result in skewed results.	What is the difference between proportionate and disproportionate stratified sampling
594	pooling layers are used to down sample the volume of convolution neural network by reducing the small translation of the features. pooling layer also provides a parameter reduction.	Why is the pooling layer used in a convolution neural network image sensing
8692	Definition: A hash algorithm is a function that converts a data string into a numeric string output of fixed length. The output string is generally much smaller than the original data.  Two of the most common hash algorithms are the MD5 (Message-Digest algorithm 5) and the SHA-1 (Secure Hash Algorithm).	What is hash algorithm
8485	Maximum likelihood estimation refers to using a probability model for data and optimizing the joint likelihood function of the observed data over one or more parameters.  Bayesian estimation is a bit more general because we're not necessarily maximizing the Bayesian analogue of the likelihood (the posterior density).	What is the difference between Bayesian estimate and maximum likelihood estimation MLE
2430	The standard deviation (SD) measures the amount of variability, or dispersion, from the individual data values to the mean, while the standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean. The SEM is always smaller than the SD.	What is difference between standard deviation and standard error
109	Restricted Boltzmann machines (RBMs) have been used as generative models of many different types of data. RBMs are usually trained using the contrastive divergence learning procedure. This requires a certain amount of practical experience to decide how to set the values of numerical meta-parameters.	What is the practical application of Restricted Boltzmann machines 1
2758	It cannot be maintained that explanation and prediction are identical from the standpoint of their logical structure, the sole point of difference between them being one of content, in that the hypothesis of a prediction concerns the future, while explanations concern the past.	How is an explanation different from a prediction
6312	Machine Learning(ML) generally means that you're training the machine to do something(here, image processing) by providing set of training data's.	Is Image Processing a part of machine learning
1138	Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data.  In k-fold cross-validation, you split the input data into k subsets of data (also known as folds).	What is ML validation
1136	Traditionally in linear regression your predictors must either be continuous or binary. Ordinal variables are often inserted using a dummy coding scheme. This is equivalent to conducting an ANOVA and the baseline ordinal level will be represented by the intercept.	Can ordinal variables be used in regression
4827	Standardized effect size statistics remove the units of the variables in the effect. The second type is simple. These statistics describe the size of the effect, but remain in the original units of the variables. So for example, say you're comparing the mean temperature of soil under two different conditions.	What is standardized effect
8659	Motivation. Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization.  Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.	Why do we normalize a feature
7524	While machine learning uses simpler concepts, deep learning works with artificial neural networks, which are designed to imitate how humans think and learn.  It can be used to solve any pattern recognition problem and without human intervention. Artificial neural networks, comprising many layers, drive deep learning.	How does deep learning work best
8080	t-test is used to test if two sample have the same mean. The assumptions are that they are samples from normal distribution. f-test is used to test if two sample have the same variance. Same assumptions hold.	What is the difference between t statistic and F statistic
3649	Part 6: Improve Deep Learning Models performance & network tuning.Increase model capacity.To increase the capacity, we add layers and nodes to a deep network (DN) gradually.  The tuning process is more empirical than theoretical.  Model & dataset design changes.Dataset collection & cleanup.Data augmentation.More items	How can we improve deep learning
1905	AI is used to augment human thinking and solve complex problems. It concentrates more on providing accurate results. Cognitive thinking, on the other hand, aims at mimicking human behavior and adapting to human reasoning, aiming to solve complex problems in a manner similar to the way humans would solve them.	How does cognitive computing differ from ordinary AI techniques
4273	Deep-learning software by nameSoftwareCreatorInterfacePlaidMLVertex.AI, IntelPython, C++PyTorchAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan (Facebook)Python, C++, JuliaApache SINGAApache Software FoundationPython, C++, JavaTensorFlowGoogle BrainPython (Keras), C/C++, Java, Go, JavaScript, R, Julia, Swift18 riviä lisää	Which software is used for deep learning
157	There is no equivalent. A Kruskal Wallis is a non-parametric test. You say “is this difference larger than I would expect by chance”. You don't have a parameter, which is the size of the difference.	What is the non parametric equivalent of the two way ANOVA Kruskal Wallis test does not allow us to check the effect of interaction term
4977	There are two main types of image processing: image filtering and image warping.  Two commonly implemented filters are the moving average filter and the image segmentation filter. The moving average filter replaces each pixel with the average pixel value of it and a neighborhood window of adjacent pixels.	What is filter computer vision
533	An autoregressive model is when a value from a time series is regressed on previous values from that same time series.  The order of an autoregression is the number of immediately preceding values in the series that are used to predict the value at the present time.	What is autoregression time series
1266	In a dataset a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set.	What is the difference between a training set and a test set
829	Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms.	What is the difference between statistics and data science
34	The major factor affects standard error of the mean is sample size. The size of the sample increases the standard error of the mean decreases. Another factor affecting the standard error of the mean is the size of the population standard deviation.	What factors affect the standard error of the mean
6912	Simple linear regression is useful for finding relationship between two continuous variables. One is predictor or independent variable and other is response or dependent variable. It looks for statistical relationship but not deterministic relationship.	When and where does linear regression become useful
512	A machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data.  See Get ONNX models for Windows ML for more information.	What is a ML model
2187	What are the five steps in the backpropagation learning algorithm?Initialize weights with random values and set other parameters.Read in the input vector and the desired output.Compute the actual output via the calculations, working forward through the layers.	What are the five steps in the backpropagation learning algorithm
793	Each class will have a “lower class limit” and an “upper class limit” which are the lowest and highest numbers in each class. The “class width” is the distance between the lower limits of consecutive classes. The range is the difference between the maximum and minimum data entries.	What is the difference between class interval and class width in statistics
520	Techniques for performance improvement with model optimizationFine tuning the model with subset data >> Dropping few data samples for some of the overly sampled data classes.Class weights >> Used to train highly imbalanced (biased) database, class weights will give equal importance to all the classes during training.More items	How do you improve the accuracy of a CNN model
6195	Checking if two categorical variables are independent can be done with Chi-Squared test of independence. This is a typical Chi-Square test: if we assume that two variables are independent, then the values of the contingency table for these variables should be distributed uniformly.	How can I measure independence correlation between two categorical variables
4673	Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned.  Deep learning is a subfield of machine learning. While both fall under the broad category of artificial intelligence, deep learning is what powers the most human-like artificial intelligence.	Is deep learning the same as machine learning
933	The ability to slide the signal is the what gives Engineers a more accurate representation of the signal and therefore a better resolution in time.  So when you use a Wavelet Transform the signal is deconstructed using the same wavelet at different scales, rather than the same sin() wave at different frequencies.	Why do we use wavelet transform
4524	Equality of result- making certain that people achieve the same result. An example is making sure that all students get the same grade no matter the race. Equality of opportunity- giving people an equal chance to succeed.	What is the difference between equality of opportunity and equality of results quizlet
34	The coefficient of variation represents the ratio of the standard deviation to the mean, and it is a useful statistic for comparing the degree of variation from one data series to another, even if the means are drastically different from one another.	What is the coefficient of variation used for
1220	Text classification also known as text tagging or text categorization is the process of categorizing text into organized groups. By using Natural Language Processing (NLP), text classifiers can automatically analyze text and then assign a set of pre-defined tags or categories based on its content.	How do you classify text into categories
326	Truncated SVD are the singular values of the matrix A with rank r. We can find truncated SVD to A by setting all but the first k largest singular values equal to zero and using only the first k columns of U and V.	What is truncated SVD
3857	Types of Clustering Methods: Overview and Quick Start R Code Hierarchical clustering. Fuzzy clustering. Density-based clustering. Model-based clustering.	What are different clustering techniques
7604	The decision for converting a predicted probability or scoring into a class label is governed by a parameter referred to as the “decision threshold,” “discrimination threshold,” or simply the “threshold.” The default value for the threshold is 0.5 for normalized predicted probabilities or scores in the range between 0	What is threshold machine learning
2388	Deviation means change or distance.  Hence standard deviation is a measure of change or the distance from a measure of central tendency - which is normally the mean. Hence, standard deviation is different from a measure of central tendency.	Why is standard deviation not a measure of central tendency
6288	A function whose value increases more slowly to infinity than any nonconstant polynomial is said to be a logarithmically increasing function.	What does logarithmic increase mean
3954	The second derivative may be used to determine local extrema of a function under certain conditions. If a function has a critical point for which f′(x) = 0 and the second derivative is positive at this point, then f has a local minimum here.  This technique is called Second Derivative Test for Local Extrema.	Why do you use the second derivative test
960	A 1-gram (or unigram) is a one-word sequence.  A 2-gram (or bigram) is a two-word sequence of words, like “I love”, “love reading”, or “Analytics Vidhya”. And a 3-gram (or trigram) is a three-word sequence of words like “I love reading”, “about data science” or “on Analytics Vidhya”.	What is Unigrams and Bigrams in Python
7735	Computational Learning Theory (CoLT) is a field of AI research studying the design of machine learning algorithms to determine what sorts of problems are “learnable.” The ultimate goals are to understand the theoretical underpinnings of deep learning programs, what makes them work or not, while improving accuracy and	What is Computational Learning Theory in Machine Learning
2926	6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case StudyGet a basic understanding of the algorithm.Find some different learning sources.Break the algorithm into chunks.Start with a simple example.Validate with a trusted implementation.Write up your process.	How do you write an algorithm for machine learning
4498	Correlation is the concept of linear relationship between two variables.  Whereas correlation coefficient is a measure that measures linear relationship between two variables.	What is the difference between correlation and correlation coefficient
6629	6 Types of Artificial Neural Networks Currently Being Used in Machine LearningFeedforward Neural Network – Artificial Neuron:  Radial basis function Neural Network:  Kohonen Self Organizing Neural Network:  Recurrent Neural Network(RNN) – Long Short Term Memory:  Convolutional Neural Network:  Modular Neural Network:	What are the different types of neural networks
1166	Here are some important considerations while choosing an algorithm.Size of the training data. It is usually recommended to gather a good amount of data to get reliable predictions.  Accuracy and/or Interpretability of the output.  Speed or Training time.  Linearity.  Number of features.	How do you know which classification algorithm to use
1214	68%	What percentage of the scores in a normal distribution fall within 1 standard deviation of the mean
427	What is a Convolutional Neural Network (CNN) A neural network consists of several different layers such as the input layer, at least one hidden layer, and an output layer. They are best used in object detection for recognizing patterns such as edges (vertical/horizontal), shapes, colours, and textures.	What is CNN in object detection
955	Sampling error is one of two reasons for the difference between an estimate and the true, but unknown, value of the population parameter.  The sampling error for a given sample is unknown but when the sampling is random, the maximum likely size of the sampling error is called the margin of error.	What is the difference between sampling error and margin of error
4624	The notation for the uniform distribution is X ~ U(a, b) where a = the lowest value of x and b = the highest value of x. The probability density function is f(x)=1b−a f ( x ) = 1 b − a for a ≤ x ≤ b. For this example, X ~ U(0, 23) and f(x)=123−0 f ( x ) = 1 23 − 0 for 0 ≤ X ≤ 23.	How do you identify a uniform distribution
1443	In spite of being linear, the Fourier transform is not shift invariant. In other words, a shift in the time domain does not correspond to a shift in the frequency domain.	Is DFT shift invariant
1205	Descriptive statistics are limited in so much that they only allow you to make summations about the people or objects that you have actually measured. You cannot use the data you have collected to generalize to other people or objects (i.e., using data from a sample to infer the properties/parameters of a population).	What are the disadvantages of descriptive statistics
5046	An error term represents the margin of error within a statistical model; it refers to the sum of the deviations within the regression line, which provides an explanation for the difference between the theoretical value of the model and the actual observed results.	What are error terms
2814	Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling.  Others sustain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks.	What does batch normalization do
3534	A certain continuous random variable has a probability density function (PDF) given by: f ( x ) = C x ( 1 − x ) 2 , f(x) = C x (1-x)^2, f(x)=Cx(1−x)2, where x x x can be any number in the real interval [ 0 , 1 ] [0,1] [0,1].	How do you find the continuous random variable of a PDF
1745	For example, a random variable could be the outcome of the roll of a die or the flip of a coin. A probability distribution is a list of all of the possible outcomes of a random variable along with their corresponding probability values.	What is the difference between random variable and probability distribution
7715	Genetic algorithms are stochastic search algorithms which act on a population of possible solutions.  Genetic algorithms are used in artificial intelligence like other search algorithms are used in artificial intelligence — to search a space of potential solutions to find one which solves the problem.	Are genetic algorithms artificial intelligence
1502	Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor. For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms.	Why is there a Bayesian network
6265	1 : having, involving, or exhibiting symmetry. 2 : having corresponding points whose connecting lines are bisected by a given point or perpendicularly bisected by a given line or plane symmetrical curves.	What is the meaning of symmetric
2305	There are four main types of probability sample.Simple random sampling. In a simple random sample, every member of the population has an equal chance of being selected.  Systematic sampling.  Stratified sampling.  Cluster sampling.	What are the types of simple random sampling
6088	Algorithms are often elegant and incredibly useful tools used to accomplish tasks. They are mostly invisible aids, augmenting human lives in increasingly incredible ways. However, sometimes the application of algorithms created with good intentions leads to unintended consequences.	What are algorithms useful for
3679	In statistics, Studentization, named after William Sealy Gosset, who wrote under the pseudonym Student, is the adjustment consisting of division of a first-degree statistic derived from a sample, by a sample-based estimate of a population standard deviation.	What does Studentized mean
110	RGB formats are usually straightforward: red, green, and blue with a given pixel size. RGB24 is the most common, allowing 8 bits and a value of 0-255 per color component.  YUV color-spaces are a more efficient coding and reduce the bandwidth more than RGB capture can.	What is the difference between RGB and YUV
622	Clean, augment, and preprocess the data into a convenient form, if needed. Conduct an exploratory analysis of the data to get a better sense of it. Using what you find as a guide, construct a model of some aspect of the data. Use the model to answer the question you started with, and validate your results.	How do you make a predictive model in R
5766	Applications. The discrete wavelet transform has a huge number of applications in science, engineering, mathematics and computer science. Most notably, it is used for signal coding, to represent a discrete signal in a more redundant form, often as a preconditioning for data compression.	Why discrete wavelet transform is used
6037	Stochastic processes appear in many different fields, including the physical sciences such as biology, chemistry, ecology, neuroscience, and physics as well as technology and engineering fields such as image processing, signal processing, information theory, computer science,, cryptography and telecommunications.	Where is stochastic processes used
1277	You can find the decision boundary analytically. For Bayesian hypothesis testing, the decision boundary corresponds to the values of X that have equal posteriors, i.e., you need to solve: for X = (x1, x2).	How do you find the decision boundary
3376	Weighted least squares (WLS), also known as weighted linear regression, is a generalization of ordinary least squares and linear regression in which the errors covariance matrix is allowed to be different from an identity matrix.	What is weighted least square method
910	How to create probability distribution plots in MinitabChoose Graph > Probability Distribution Plot > View Probability.Click OK.From Distribution, choose Normal.In Mean, type 100.In Standard deviation, type 15.	How do you plot probability distribution
6533	FeaturesAssign a numerical value to each possible outcome on the tree.  Label the likelihood of each outcome.  Make a separate list for each decision and its possible outcomes.  Review each branch on the tree for costs.More items	How do you evaluate a decision tree performance
7538	A learning algorithm is a method used to process data to extract patterns appropriate for application in a new situation. In particular, the goal is to adapt a system to a specific input-output transformation task.	What is the learning algorithm
3977	Key concepts include probability distributions, statistical significance, hypothesis testing, and regression. Furthermore, machine learning requires understanding Bayesian thinking.	What statistics is required for machine learning
899	The Least Squares Regression Line is the line that makes the vertical distance from the data points to the regression line as small as possible. It's called a “least squares” because the best line of fit is one that minimizes the variance (the sum of squares of the errors).	Why is linear regression referred to as least squares
1979	Chaos theory describes the qualities of the point at which stability moves to instability or order moves to disorder. For example, unlike the behavior of a pendulum, which adheres to a predictable pattern a chaotic system does not settle into a predictable pattern due to its nonlinear processes.	What is an example of chaos theory
3518	In the real world, an impulse function is a pulse that is much shorter than the time response of the system. The system's response to an impulse can be used to determine the output of a system to any input using the time-slicing technique called convolution.	What is meant by impulse function
2938	While a linear equation has one basic form, nonlinear equations can take many different forms.  Literally, it's not linear. If the equation doesn't meet the criteria above for a linear equation, it's nonlinear.	What is the difference between linear and nonlinear
2083	Once you find a correlation, you can test for causation by running experiments that “control the other variables and measure the difference.” Two such experiments or analyses you can use to identify causation with your product are: Hypothesis testing. A/B/n experiments.	How do you test causation
468	verb (used with object), quan·tized, quan·tiz·ing. Mathematics, Physics. to restrict (a variable quantity) to discrete values rather than to a continuous set of values.	What is the meaning of the word Quantised
5899	Even with the use of pre-pruning, they tend to overfit and provide poor generalization performance. Therefore, in most applications, by aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of decision trees can be substantially improved.	How can decision tree performance be improved
910	We know that non-significant intercept can be interpreted as result for which the result of the analysis will be zero if all other variables are equal to zero and we must consider its removal for theoretical reasons.	What does it mean when the intercept is not significant
3916	Dimensionality Reduction and PCA. Dimensionality reduction refers to reducing the number of input variables for a dataset. If your data is represented using rows and columns, such as in a spreadsheet, then the input variables are the columns that are fed as input to a model to predict the target variable.	What is PCA dimensionality reduction
4174	It is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on.	What is part of speech tagging in NLP
1083	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is the difference between supervised and unsupervised machine learning
402	Quantum computers could enable an artificial life protocol that encodes quantum behaviours belonging to living systems, including self-replication, mutation, interaction between individuals, birth and death. The researchers executed such a model on an IBM ibmqx4 cloud quantum computer.	Is quantum artificial life possible
1524	A research hypothesis is a statement of an expected or predicted relationship between two or more variables.  It's what the experimenter believes will happen in her research study.	What is a research hypothesis in statistics
1334	Definition 1.1 A Decision rule is a formal rule that states, based on the data obtained, when to reject the null hypothesis H0. Generally, it specifies a set of values based on the data to be collected, which are contradictory to the null H0 and which favor the alternative hypothesis H1.	What is decision rule in statistics
3467	Softmax is used for multi-classification in the Logistic Regression model, whereas Sigmoid is used for binary classification in the Logistic Regression model. This is similar to the Sigmoid function. The difference is that, in the denominator, we sum together all of the values.	What is the difference between sigmoid and Softmax
7873	There is always at least one such optimal policy[8]. The so called greedy policy is following the currently best path of actions. During learning however, for the values to converge into good estimates it is required that the agent visits all available states to gain information about them.	Does a greedy agent always find an optimal policy
4752	Random forests perform well for multi-class object detection and bioinformatics, which tends to have a lot of statistical noise. Gradient Boosting performs well when you have unbalanced data such as in real time risk assessment.	Why is gradient boosting better than random forest
2138	Last Updated on Decem. Cross-entropy is commonly used in machine learning as a loss function. Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions.	What is the cross entropy loss function
3135	Squared hinge loss is nothing else but a square of the output of the hinge's max(…) function. It generates a loss function as illustrated above, compared to regular hinge loss.	What is squared hinge loss
7571	In statistics, a sequence (or a vector) of random variables is homoscedastic /ˌhoʊmoʊskəˈdæstɪk/ if all its random variables have the same finite variance. This is also known as homogeneity of variance.	What does Homoscedasticity mean in statistics
6411	Correlation measures the relationship between two variables.  Correlation refers to an increase/decrease in a dependent variable with an increase/decrease in an independent variable. Collinearity refers to two or more independent variables acting in concert to explain the variation in a dependent variable.	What is the difference between correlation and Collinearity
249	all provides a way to leverage binary classification. -all solution consists of N separate binary classifiers—one binary classifier for each possible outcome.  During training, the model runs through a sequence of binary classifiers, training each to answer a separate classification question.	What is one vs all classification
2397	There are multiple uses of eigenvalues and eigenvectors: Eigenvalues and Eigenvectors have their importance in linear differential equations where you want to find a rate of change or when you want to maintain relationships between two variables.	Why do we find eigenvalues and eigenvectors
214	Connectionism is the philosophy of Edward Thorndike, which says that learning is a product between stimulus and response.  Thorndike proposed three laws of connectionism: The law of effect, which says that a positive outcome strengthens an S-R bond, while a negative outcome weakens it.	What is connectionism in learning
7278	Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target.	Which of the method gives the best fit for logistic regression model
7848	Bayesian decision making is the process in which a decision is made based on the probability of a successful outcome, where this probability is informed by both prior information and new evidence that the decision maker obtains.	What is Bayesian decision making
6185	In short, the problem with neural networks is that a number of parameter have to be set before any training can begin. However, there are no clear rules how to set these parameters.  By combining genetic algorithms with neural networks (GANN), the genetic algorithm is used to find these parameters.	Can we incorporate genetic algorithm concept to artificial neural network
955	The basic idea is to use existing IDSs as an alert source and then apply either off-line (using data mining) or on-line (using machine learning) alert processing to reduce the number of false positives. Moreover, owing to their complementary nature, both approaches can also be used together.	How can machine learning reduce false positives
498	The Least Squares AssumptionsUseful Books for This Topic:  ASSUMPTION #1: The conditional distribution of a given error term given a level of an independent variable x has a mean of zero.  ASSUMPTION #2: (X,Y) for all n are independently and identically distributed.  ASSUMPTION #3: Large outliers are unlikely.More items•	What are the least squares assumptions
1155	LDA stands for Latent Dirichlet Allocation, and it is a type of topic modeling algorithm. The purpose of LDA is to learn the representation of a fixed number of topics, and given this number of topics learn the topic distribution that each document in a collection of documents has.	How do you explain LDA
6873	Examples of texts to create.  Live multimodal texts include dance, performance, oral storytelling, and presentations. Meaning is conveyed through combinations of various modes such as gestural, spatial, audio, and oral language.	What is an example of multimodal
715	Gradient descent is used because it guarantees that, on a convex surface, every modification of the parameters will take you in the right direction toward optimization. Genetic algorithms have no such guarantee.	Why do machine learning algorithms use gradient descent for optimization and not other optimization techniques such as genetic algorithms What makes gradient descent more suitable for machine learning
6111	The null hypothesis is generally denoted as H0. It states the exact opposite of what an investigator or an experimenter predicts or expects. It basically defines the statement which states that there is no exact or actual relationship between the variables. The alternative hypothesis is generally denoted as H1.	What is the difference between a null hypothesis h0 and an alternative hypothesis h1
1100	A Lorenz curve is a graphical representation of income inequality or wealth inequality developed by American economist Max Lorenz in 1905. The graph plots percentiles of the population on the horizontal axis according to income or wealth.	What does the Lorenz curve represent
2131	The exponential distribution is a continuous probability distribution used to model the time we need to wait before a given event occurs. It is the continuous counterpart of the geometric distribution, which is instead discrete.	Why do we use exponential distribution
5399	Neural networks are widely used in unsupervised learning in order to learn better representations of the input data.  This process doesn't give you clusters, but it creates meaningful representations that can be used for clustering. You could, for instance, run a clustering algorithm on the hidden layer's activations.	Can we use neural network for clustering
444	So here are some signs you're highly intelligent, even if you don't feel like it.You're Empathetic And Compassionate. Andrew Zaeh for Bustle.  You're Curious About The World.  You're Observant.  You Have Self-Control.  You Have A Good Working Memory.  You Like To Go With The Flow.More items•	How can you tell if someone is highly intelligent
5018	You do not need to learn linear algebra before you get started in machine learning, but at some time you may wish to dive deeper.  It will give you the tools to help you with the other areas of mathematics required to understand and build better intuitions for machine learning algorithms.	Do you need to know Linear Algebra for machine learning
408	Backpropagation and computing gradients.  In other words, backpropagation aims to minimize the cost function by adjusting network's weights and biases. The level of adjustment is determined by the gradients of the cost function with respect to those parameters.	What is adjusted in back propagation to minimize cost in deep neural network
104	For course 2018 - Take a look at @hiromi post on that: The rule of thumb for determining the embedding size is the cardinality size divided by 2, but no bigger than 50.	How is embed size calculated
4628	Theoretically, convolution are linear operations on the signal or signal modifiers, whereas correlation is a measure of similarity between two signals.  Also, correlation or auto-correlation is the measure of similarity of signal with itself which has a different time lag between them.	What is difference between correlation and convolution
4146	Examples of Unbiased Sample Kathy wants to know how many students in her city use the internet for learning purposes. She used an email poll. Based on the replies to her poll, she found that 83% of those surveyed used the internet. Kathy's sample is biased as she surveyed only the students those who use the internet.	What is an example of an unbiased sample
5878	PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix.	Is PCA A multivariate analysis
433	Word2Vec, Doc2Vec and Glove are semi-supervised learning algorithms and they are Neural Word Embeddings for the sole purpose of Natural Language Processing. Specifically Word2vec is a two-layer neural net that processes text.	Is Doc2Vec supervised or unsupervised
3509	The top-1 error:- The percentage of time that the classifier did not give the correct class the highest probability score. The top-5 error:- The percentage of time that the classifier did not involve the correct class among the top 5 probabilities or guesses.	What is top1 and top5 error
590	As part of the GAN series, this article looks into ways on how to improve GAN.In particular,Change the cost function for a better optimization goal.Add additional penalties to the cost function to enforce constraints.Avoid overconfidence and overfitting.Better ways of optimizing the model.Add labels.	How can I improve my GAN performance
6011	The difference is in the method of removing the negative values, while variance squares it, the mean deviation takes the absolute values (mod). Mean deviation is basically average of the absolute distance of all data points from the mean.	Are variance and mean absolute deviation the same
5748	Caffe2 is was intended as a framework for production edge deployment whereas TensorFlow is more suited towards server production and research. TensorFlow is aimed for researchers and servers while Caffe2 is aimed towards mobile phones and other (relatively) computationally constrained platforms.	How does Caffe 2 compare to TensorFlow
4783	Machine Learning will revolutionize Psychometrics. IRT psychometrics are usually based upon logistic regression techniques.  Machine Learning can be utilized to reveal candidate's strengths in the in the social components of collaborative problem solving, such as perspective taking, participation, and social regulation.	How will machine learning change psychometrics
1425	The Chi square test is used to compare a group with a value, or to compare two or more groups, always using categorical data.	Can Chi Square be used for categorical data
2753	Offline Education – Also referred to as traditional training. Offline Education means a student needs to go in a school, in a classroom, and attend a class face to face with a teacher. So you got it, the main difference between online education vs offline education is the location of the the learning process.26‏/08‏/2020	What is offline learning in education
1288	More precisely, if we consider repeated sampling from our population, for large sample sizes, the distribution (across repeated samples) of the ordinary least squares estimates of the regression coefficients follow a normal distribution.	Are regression coefficients normally distributed
858	"Gradient boosting is a technique for building an ensemble of weak models such that the predictions of the ensemble minimize a loss function.  Gradient descent ""descends"" the gradient by introducing changes to parameters, whereas gradient boosting descends the gradient by introducing new models."	What is the difference between gradient descent and gradient boosting
4762	The outcome variable and dependent variable are used synonymously. However, they are not exactly the same: the outcome variable is defined as the presumed effect in a non-experimental study, where the dependent variable is the presumed effect in an experimental study1.	Is the outcome variable the same as a dependent variable
2836	Bayesian probability is an interpretation of probability whereby we treat facts about the world, or hypotheses, as inherently random. They might be true or they might not be true; it depends on the fixed evidence we have available.	Whats the intuition behind Bayesian probabilities
5393	Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes	How decision tree induction is used in classification
3754	How to Conduct Hypothesis TestsState the hypotheses. Every hypothesis test requires the analyst to state a null hypothesis and an alternative hypothesis.  Formulate an analysis plan. The analysis plan describes how to use sample data to accept or reject the null hypothesis.  Analyze sample data.  Interpret the results.	How do you test a hypothesis in statistics
7881	The collaborative filtering algorithm uses “User Behavior” for recommending items. This is one of the most commonly used algorithms in the industry as it is not dependent on any additional information.	Which algorithms are used in recommendation system
637	"StepsStep 1: For each (x,y) point calculate x2 and xy.Step 2: Sum all x, y, x2 and xy, which gives us Σx, Σy, Σx2 and Σxy (Σ means ""sum up"")Step 3: Calculate Slope m:m = N Σ(xy) − Σx Σy N Σ(x2) − (Σx)2Step 4: Calculate Intercept b:b = Σy − m Σx N.Step 5: Assemble the equation of a line."	How do you find the least squares regression line
7875	5 industries that are using Artificial Intelligence the mostHealthcare. This is one area that tops the list when it comes to the extent of AI application.  Education. Those days are long gone when parent-teacher meetings used to happen in haste without many insights.  Marketing.  Retail and E-commerce.  Financial markets and services.	Which industries are using AI
962	Backtracking is a technique based on algorithm to solve problem. It uses recursive calling to find the solution by building a solution step by step increasing values with time. It removes the solutions that doesn't give rise to the solution of the problem based on the constraints given to solve the problem.	What is backtracking algorithm in data structure
3696	Dimensionality reduction refers to techniques for reducing the number of input variables in training data. When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the “essence” of the data.	What is dimensionality reduction and explain why it is required
7521	Markov model is a state machine with the state changes being probabilities. In a hidden Markov model, you don't know the probabilities, but you know the outcomes.	What is the difference between Markov model and hidden Markov model
6505	jobs. deep learning performs better when sequential processing is used.	Is deep learning performs better when sequential processing is used
5068	K-nearest neighbor (KNN) decision boundary K-nearest neighbor is an algorithm based on the local geometry of the distribution of the data on the feature hyperplane (and their relative distance measures). The decision boundary, therefore, comes up as nonlinear and non-smooth.	What is decision boundary in Knn
5909	Receptive fields are defined portion of space or spatial construct containing units that provide input to a set of units within a corresponding layer. The receptive field is defined by the filter size of a layer within a convolution neural network.	What is a receptive field in a convolutional neural network
316	Ambiguity. The main challenge of NLP is the understanding and modeling of elements within a variable context. In a natural language, words are unique but can have different meanings depending on the context resulting in ambiguity on the lexical, syntactic, and semantic levels.	What is the main challenge s of NLP
1414	In linear regression, coefficients are the values that multiply the predictor values.  The sign of each coefficient indicates the direction of the relationship between a predictor variable and the response variable. A positive sign indicates that as the predictor variable increases, the response variable also increases.	What do coefficients tell you in regression
5631	Implementing Deep Q-Learning using TensorflowPrerequisites: Deep Q-Learning.Step 1: Importing the required libraries.Step 2: Building the Environment.Step 3: Building the learning agent.Step 4: Finding the Optimal Strategy.The agent tries different methods to reach the top and thus gaining knowledge from each episode.Step 5: Testing the Learning Agent.More items•	How is deep Q learning implemented
1261	A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results.	What is the difference between random forest and decision tree
114	Hidden Layers, which are neuron nodes stacked in between inputs and outputs, allowing neural networks to learn more complicated features (such as XOR logic)	Which layer in a neural network allows it to learn more complicated features
1344	Metric learning aims to measure the similarity among samples while using an optimal distance metric for learning tasks.  In recent years, deep metric learning, which provides a better solution for nonlinear data through activation functions, has attracted researchers' attention in many different areas.	What is deep metric learning
3460	From the network operations perspective, streaming telemetry can improve efficiency in many use cases, including: Detecting problems by setting up network monitors and alerts based on pre-configured thresholds or network performance baselines. Troubleshooting connectivity and performance issues.	What value does streaming telemetry bring to network analytics
3452	The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.	What type of data does bag of words represent
958	The skip-gram model. Both the input vector x and the output y are one-hot encoded word representations. The hidden layer is the word embedding of size N.	Which layer of the skip gram model has an actual word embedding representation
2991	1. Agglomerative approach: This method is also called a bottom-up approach shown in Figure 6.7. In this method, each node represents a single cluster at the beginning; eventually, nodes start merging based on their similarities and all nodes belong to the same cluster.	Which of clustering algorithms is called bottom up approach
4094	One way to par- allelize neural network training is to use a technique called Network Parallel Training (NPT). In this approach the neu- rons of the ANN are divided across machines in the cluster, so that each machine holds a portion of the neural network.	How do you parallelize neural network training
2050	Process: In the process of Artificial Intelligence (AI), Future events are forecasted using the predictive model. But Data Science involves the process of prediction, visualization, analysis, and pre-processing of data.  But the primary goal of Data Science is to find the patterns that are hidden in the data.	How is data science related to AI
5993	With two-way ANOVA, you have one continuous dependent variable and two categorical grouping variables for the independent variables. MANOVA models several dependent variables simultaneously and you can include a variety of independent variables.	What is the difference between a 2 way ANOVA and a MANOVA
1195	A ratio scale is a quantitative scale where there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales.	What is ratio scale measurement
3487	The basic assumption of factor analysis is that for a collection of observed variables there are a set of underlying variables called factors (smaller than the observed variables), that can explain the interrelationships among those variables.	What is factor analysis What is the basic purpose of factor analysis what assumptions should be fulfilled to use factor analysis
1610	For a good regression model, you want to include the variables that you are specifically testing along with other variables that affect the response in order to avoid biased results.  Cross-validation determines how well your model generalizes to other data sets by partitioning your data.	What makes a good regression model
1735	A latent variable is a variable that is inferred using models from observed data.  Approaches to inferring latent variables from data include: using a single observed variable, multi-item scales, predictive models, dimension reduction techniques such as factor analysis, structural equation models, and mixture models.	What is a latent variable in factor analysis
1313	To help you get started in the field, we've assembled a list of the best Big Data courses available.Simplilearn. Simplilearn's Big Data Course catalogue is known for their large number of courses, in subjects as varied as Hadoop, SAS, Apache Spark, and R.  Cloudera.  Big Data University.  Hortonworks.  Coursera.	How do you learn big data
4332	Training deep learning neural networks is very challenging. The best general algorithm known for solving this problem is stochastic gradient descent, where model weights are updated each iteration using the backpropagation of error algorithm. Optimization in general is an extremely difficult task.	What are the challenges in training a neural network
105	The lognormal distribution is a probability distribution whose logarithm has a normal distribution. The mean m and variance v of a lognormal random variable are functions of the lognormal distribution parameters µ and σ: m = exp ( μ + σ 2 / 2 ) v = exp ( 2 μ + σ 2 ) ( exp ( σ 2 ) − 1 )	What is the mean and variance of lognormal distribution
5295	Informally, a neural attention mechanism equips a neural network with the ability to focus on a subset of its inputs (or features): it selects specific inputs.	What is Attention neural network
1095	Joint prediction Crucially, Bayesian networks can also be used to predict the joint probability over multiple outputs (discrete and or continuous). This is useful when it is not enough to predict two variables separately, whether using separate models or even when they are in the same model.	What do Bayesian networks predict
1465	So, we have listed some of the ways where you can achieve trade-off between the two. Both bias and variance are related to each other, if you increase one the other decreases and vice versa. By a trade-off, there is an optimal balance in the bias and variance which gives us a model that is neither underfit nor overfit.	How do you achieve bias variance trade off
7923	There are two big reasons why you want homoscedasticity: While heteroscedasticity does not cause bias in the coefficient estimates, it does make them less precise. Lower precision increases the likelihood that the coefficient estimates are further from the correct population value.	Why is homoscedasticity important in regression analysis
2096	The false discovery rate (FDR) is a method of conceptualizing the rate of type I errors in null hypothesis testing when conducting multiple comparisons.  Thus, FDR-controlling procedures have greater power, at the cost of increased numbers of Type I errors.	What is FDR correction
2097	Hypothesis testing is used to assess the plausibility of a hypothesis by using sample data. The test provides evidence concerning the plausibility of the hypothesis, given the data. Statistical analysts test a hypothesis by measuring and examining a random sample of the population being analyzed.	What methods are used to test hypothesis
5265	Machine learning can be automated when it involves the same activity again and again. However, the fundamental nature of machine learning deals with the opposite: variable conditions. In this regard, machine learning needs to be able to function independently and with different solutions to match different demands.	Can machine learning be used for automation
1501	Gradient Descent with Momentum considers the past gradients to smooth out the update.  It computes an exponentially weighted average of your gradients, and then use that gradient to update your weights instead. It works faster than the standard gradient descent algorithm.	What is momentum in gradient descent
853	An autonomous agent is an intelligent agent operating on an owner's behalf but without any interference of that ownership entity.  Non-biological examples include intelligent agents, autonomous robots, and various software agents, including artificial life agents, and many computer viruses.	What is an autonomous agent in artificial intelligence
411	Bootstrap Aggregating is an ensemble method. First, we create random samples of the training data set with replacment (sub sets of training data set). Then, we build a model (classifier or Decision tree) for each sample. Finally, results of these multiple models are combined using average or majority voting.	How do you make an ensemble model
1861	Based on these definitions, EBPH and EBM differ in the following three ways: First, EBM focuses on individual patients, whereas EBPH focuses on community and residents [13]. Second, the EBM intervention is disease treatment, whereas the EBPH intervention is disease prevention and health promotion [14].	How is EBPH different from evidence based medicine
3761	In general, you should probably use the divergence theorem whenever you wish to evaluate a vector surface integral over a closed surface. The divergence theorem can also be used to evaluate triple integrals by turning them into surface integrals.	How do you know when to use the divergence theorem
7764	Cluster analysis is the task of grouping a set of data points in such a way that they can be characterized by their relevancy to one another.  These types are Centroid Clustering, Density Clustering Distribution Clustering, and Connectivity Clustering.	What is cluster and its types
1063	Using P values and Significance Levels Together If your P value is less than or equal to your alpha level, reject the null hypothesis. The P value results are consistent with our graphical representation. The P value of 0.03112 is significant at the alpha level of 0.05 but not 0.01.	Is the P value the same as Alpha
5879	We input the data in the learning algorithm as a set of inputs, which is called as Features, denoted by X along with the corresponding outputs, which is indicated by Y, and the algorithm learns by comparing its actual production with correct outputs to find errors. It then modifies the model accordingly.	What data would be used as input to the machine learning algorithms
1639	Evolution is not a random process. The genetic variation on which natural selection acts may occur randomly, but natural selection itself is not random at all. The survival and reproductive success of an individual is directly related to the ways its inherited traits function in the context of its local environment.	How is evolution a random process
1798	The various metrics used to evaluate the results of the prediction are :Mean Squared Error(MSE)Root-Mean-Squared-Error(RMSE).Mean-Absolute-Error(MAE).R² or Coefficient of Determination.Adjusted R²	Which evaluation metrics can be used for regression problems
51	To plot the probability density function for a log normal distribution in R, we can use the following functions: dlnorm(x, meanlog = 0, sdlog = 1) to create the probability density function. curve(function, from = NULL, to = NULL) to plot the probability density function.	How do you plot a lognormal distribution
7863	The shape of the t distribution depends on the degrees of freedom (df) that went into the estimate of the standard deviation. With very few degrees of freedom, the t distribution is very leptokurtic. With 100 or more degrees of freedom, the t distribution is almost indistinguishable from the normal distribution.	What are the factors that impact the shape of the T distribution
8005	As the formula shows, the standard score is simply the score, minus the mean score, divided by the standard deviation.	How do you work out Standardised scores
1779	In statistics, the mode is the most commonly observed value in a set of data. For the normal distribution, the mode is also the same value as the mean and median. In many cases, the modal value will differ from the average value in the data.	What are the uses of mode in statistics
8290	Both tests relate the mean difference to the variance (variability of measurements) (and to the sample size). The z-test assumes that the variance is known, whereas the t-test does not make this assumption. Usually one does not know the variance, so one needs to estimate it from the available data.	Why is Z test more powerful than T test
550	There are two straightforward ways to solve the optimal control problem: (1) the method of Lagrange multipliers and (2) dynamic programming. We have already outlined the idea behind the Lagrange multipliers approach. The second way, dynamic programming, solves the constrained problem directly.	How do you solve optimal control problems
6658	Assumptions. The assumptions of discriminant analysis are the same as those for MANOVA. The analysis is quite sensitive to outliers and the size of the smallest group must be larger than the number of predictor variables. Multivariate normality: Independent variables are normal for each level of the grouping variable.	What are the assumptions in discriminant analysis
4629	Whereas AI is preprogrammed to carry out a task that a human can but more efficiently, artificial general intelligence (AGI) expects the machine to be just as smart as a human. This is the kind of AI we're used to seeing in blockbuster movies.	What is artificial intelligence how it is different than general intelligence
5735	A positive test result indicates that a person has inherited a known harmful mutation in BRCA1 or BRCA2 and, therefore, has an increased risk of developing certain cancers. However, a positive test result cannot tell whether or when an individual will actually develop cancer.	What decisions would you make if you tested positive for brca1 or brca2
908	"""The Gini coefficient provides an index to measure inequality,"" says Antonio Cabrales, a professor of economics at University College London. It is a way of comparing how distribution of income in a society compares with a similar society in which everyone earned exactly the same amount."	What is the Gini coefficient
6414	Another way to look at the difference is that a p-value of 0.05 implies that 5% of all tests will result in false positives. An FDR adjusted p-value (or q-value) of 0.05 implies that 5% of significant tests will result in false positives. The latter will result in fewer false positives.	What is the difference between P value and adjusted p value
3492	When Longitudinal data looks like a time series is when we measure the same thing over time. The big difference is that in a time series we can measure the overall change in the measurement over time (or by group) while in a longitudinal analysis you actually have the measurement of change at the individual level.	What is the difference between time series and longitudinal data
220	Negentropy is reverse entropy. It means things becoming more in order. By 'order' is meant organisation, structure and function: the opposite of randomness or chaos. One example of negentropy is a star system such as the Solar System.  The opposite of entropy is negentropy.	What is the opposite of entropy
1261	2:1812:46Suggested clip 110 secondsStructural Equation Modeling with SPSS AMOS PART1: by G N YouTubeStart of suggested clipEnd of suggested clip	How do you create a structural equation model in SPSS
5161	Canonical discriminant analysis is a dimension-reduction technique related to principal component analysis and canonical correlation.  This maximal multiple correlation is called the first canonical correlation. The coefficients of the linear combination are the canonical coefficients or canonical weights.	What is canonical discriminant analysis
8258	Adaptive learning systems are designed to dynamically adjust to the level or type of course content based on an individual student's abilities or skill attainment, in ways that accelerate a learner's performance with both automated and instructor interventions.	What is an adaptive learning system
5276	A support vector machine is a machine learning model that is able to generalise between two different classes if the set of labelled data is provided in the training set to the algorithm. The main function of the SVM is to check for that hyperplane that is able to distinguish between the two classes.	How do support vector machines work
627	Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.	What is tokenization in NLP
3107	This term is used in two different senses; one related to multi-stage sampling and the other to multi-phase sampling. In multi-stage sampling the process of selecting sample units, say, at the second stage from any selected first stage unit is called subsampling of the first stage unit.	What is subsampling in statistics
2225	Preparing Your Dataset for Machine Learning: 8 Basic Techniques That Make Your Data BetterArticulate the problem early.Establish data collection mechanisms.Format data to make it consistent.Reduce data.Complete data cleaning.Decompose data.Rescale data.Discretize data.	How does machine learning collect data
6506	The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.	What is AUC machine learning
7241	A machine learning pipeline is used to help automate machine learning workflows. They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative.	What is data pipeline in machine learning
3592	In probability theory and statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d. or iid or IID.	What does independently and identically distributed mean
1248	Moderation distinguishes between the roles of the two variables involved in the interaction.  They are both considered predictor variables. The interaction tells us that the effect of X on Y is different at different values of Z. It also tells us that the effect of Z on Y is different at different values of X.	What is the difference between moderation and interaction
4999	Multinomial logistic regression does have assumptions, such as the assumption of independence among the dependent variable choices. This assumption states that the choice of or membership in one category is not related to the choice or membership of another category (i.e., the dependent variable).	What are the assumptions of multinomial logistic regression
4252	Text classification also known as text tagging or text categorization is the process of categorizing text into organized groups. By using Natural Language Processing (NLP), text classifiers can automatically analyze text and then assign a set of pre-defined tags or categories based on its content.	How does text classification work
57	Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are. Instead, they need to be recoded into a series of variables which can then be entered into the regression model.	Can you use categorical variables in multiple regression
2392	In statistics and regression analysis, moderation occurs when the relationship between two variables depends on a third variable. The third variable is referred to as the moderator variable or simply the moderator.	If you control for a variable and examine the relationship between two others is this moderation
1597	The most common functional form is parametric linear model, as a type of parametric regression, is frequently used to describe the relationship between a dependent variable and explanatory variables. Parametric linear models require the estimation of a finite number of parameters, β.	What is parametric regression model
1518	The relative frequencies add up to 1.	Aside in a relative frequency distribution what should the relative frequencies add up to
2008	For our objective function, which measures the quality of a clustering, we use the sum of the squared error (SSE), which is also known as scatter. In other words, we calculate the error of each data point, i.e., its Euclidean distance to the closest centroid, and then compute the total sum of the squared errors.	What is the objective function for measuring the quality of clustering in case of the K Means algorithm with Euclidean distance
3476	A probability distribution is a statistical function that describes all the possible values and likelihoods that a random variable can take within a given range.	What is probability distribution function
564	Any object, function, or statistic that doesn't change when scales are multiplied by a common factor is scale invariant. In statistics, it can also mean a statistic that tends not to change (i.e. 99% of the time, it will stay the same). Some specific statistics are scale invariant.	What is invariance in statistics
2741	Statistical inference is the process through which inferences about a population are made based on certain statistics calculated from a sample of data drawn from that population.	What does statistical inference mean
6620	Definition: Union of Events. The union of events A and B, denoted A∪B, is the collection of all outcomes that are elements of one or the other of the sets A and B, or of both of them.	What is a union in statistics
3316	7.2. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems. Radial basis function networks are distinguished from other neural networks due to their universal approximation and faster learning speed.	Which function popularly we used in RBF network
6232	Statistically significant means a result is unlikely due to chance. The p-value is the probability of obtaining the difference we saw from a sample (or a larger one) if there really isn't a difference for all users.  Statistical significance doesn't mean practical significance.	What does it mean when sample results are not statistically significant
2487	Because neural networks work internally with numeric data, binary data (such as sex, which can be male or female) and categorical data (such as a community, which can be suburban, city or rural) must be encoded in numeric form.	Can neural network handle categorical data
5634	The difference between these two statistical measurements is that correlation measures the degree of a relationship between two variables (x and y), whereas regression is how one variable affects another.	What is the difference between regression and correlation
1097	A mixture of two normal distributions with equal standard deviations is bimodal only if their means differ by at least twice the common standard deviation. Estimates of the parameters is simplified if the variances can be assumed to be equal (the homoscedastic case).	How do you know if a distribution is bimodal
7712	"His later short story ""The Last Question"", however, expands the AC suffix to be ""analog computer"". In possibly the most famous Multivac story, ""The Last Question"", two slightly drunken technicians ask Multivac if humanity can reverse the increase of entropy."	What is AC in the last question
4286	Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the	What is PLS in statistics
3796	Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks.  This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped.	What is gradient clipping
6341	No. If the learning rate is too high, then the model can diverge.  If the validation error consistently goes up, that means the model could be diverging because of high learning rate.	Do all gradient descent algorithms lead to the same model provided you let them run long enough
7233	It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder. A clustering network transforms the data into another space and then selects one of the clusters. Next, the autoencoder associated with this cluster is used to reconstruct the data-point.	How do I use Autoencoder for clustering
438	Deep learning is getting a lot of hype right now, but neural networks aren't the answer to everything.Disadvantages of Neural NetworksBlack Box.  Duration of Development.  Amount of Data.  Computationally Expensive.	What are the limitations of neural networks
7393	Nonparametric tests are sometimes called distribution-free tests because they are based on fewer assumptions (e.g., they do not assume that the outcome is approximately normally distributed).  There are several statistical tests that can be used to assess whether data are likely from a normal distribution.	Why would you use a nonparametric statistic
937	The hypergeometric distribution has the following properties: The mean of the distribution is equal to n * k / N . The variance is n * k * ( N - k ) * ( N - n ) / [ N2 * ( N - 1 ) ] .	What is the mean of a hypergeometric distribution
3505	1 Answer. Transfer learning is when a model developed for one task is reused to work on a second task. Fine tuning is one approach to transfer learning.	What is the difference between transfer learning and fine tuning
1247	The reason for preferring L2 norm is that it corresponds to Hilbert space .	Why is squared of l2 norm preferred in ML than just l2 norm
1881	Can it solve any problem that a person would solve by thinking? Are human intelligence and machine intelligence the same?	What is the philosophy behind artificial intelligence
3366	the condition or quality of being true, correct, or exact; freedom from error or defect; precision or exactness; correctness. Chemistry, Physics. the extent to which a given measurement agrees with the standard value for that measurement. Compare precision (def. 6).	What do you mean accuracy
8126	Image SegmentationNon-contextual thresholding. Simple thresholding. Adaptive thresholding. Colour thresholding.Contextual segmentation: Region growing. Pixel connectivity. Region similarity. Region growing. Split-and-merge segmentation.Texture segmentation: Spectral features.References.	What are the types of image segmentation
5828	3:237:22Suggested clip · 117 secondsCategorical Regression Model - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do a regression with a categorical variable in Excel
7411	The standard deviation is simply the square root of the variance.  The average deviation, also called the mean absolute deviation , is another measure of variability. However, average deviation utilizes absolute values instead of squares to circumvent the issue of negative differences between data and the mean.	What is the difference between standard deviation and average deviation
6970	The Perceptron algorithm learns the weights for the input signals in order to draw a linear decision boundary. This enables you to distinguish between the two linearly separable classes +1 and -1. Note: Supervised Learning is a type of Machine Learning used to learn models from labeled training data.	How does a Perceptron learn
5299	Linear discriminant analysis is primarily used here to reduce the number of features to a more manageable number before classification. Each of the new dimensions is a linear combination of pixel values, which form a template.	Why is linear discriminant analysis used
294	In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code.	How do expert systems relate to artificial intelligence
283	Degrees of Freedom refers to the maximum number of logically independent values, which are values that have the freedom to vary, in the data sample. Degrees of Freedom are commonly discussed in relation to various forms of hypothesis testing in statistics, such as a Chi-Square.	What does degrees of freedom mean in statistics
588	Secondly, there is more than one way to reduce overfitting: Enlarge your data set by using augmentation techniques such as flip, scale, Using regularization techniques like dropout (you already did it), but you can play with dropout rate, try more than or less than 0.5.More items•	How can we prevent Overfitting in transfer learning
5381	The t-test is a method that determines whether two populations are statistically different from each other, whereas ANOVA determines whether three or more populations are statistically different from each other.	What is the difference between an ANOVA and a T test
739	Yes, decoherence does solve the measurement problem of quantum mechanics. Decoherence explains why, after a measurement, you would get the same result if you immediately made the same measurement again.  So the claim being made is that decoherence explains why the wavefunction appears to collapse.	Does decoherence solve the measurement problem
41	In regression analysis, you need to standardize the independent variables when your model contains polynomial terms to model curvature or interaction terms.  This problem can obscure the statistical significance of model terms, produce imprecise coefficients, and make it more difficult to choose the correct model.	Why would you standardize a regression coefficient
3053	Appropriate Problems for Decision Tree LearningInstances are represented by attribute-value pairs.  The target function has discrete output values.  Disjunctive descriptions may be required.  The training data may contain errors.  The training data may contain missing attribute values.	What are the issues in decision tree learning
2840	In this blog we will learn what is calibration and why and when we should use it. We calibrate our model when the probability estimate of a data point belonging to a class is very important. Calibration is comparison of the actual output and the expected output given by a system.	Why do we need calibration in machine learning
1308	One of the ways to help deal with this bias is to avoid shaping participants' ideas or experiences before they are faced with the experimental material. Even stating seemingly innocuous details might prime an individual to form theories or thoughts that could bias their answers or behavior.	How do you get rid of participant bias
7148	The t-test is a method that determines whether two populations are statistically different from each other, whereas ANOVA determines whether three or more populations are statistically different from each other.	What is the difference between Anova and t test
2944	A rolling hash (also known as recursive hashing or rolling checksum) is a hash function where the input is hashed in a window that moves through the input.  At best, rolling hash values are pairwise independent or strongly universal. They cannot be 3-wise independent, for example.	What is rolling hash function
3030	Each feature, or column, represents a measurable piece of data that can be used for analysis: Name, Age, Sex, Fare, and so on. Features are also sometimes referred to as “variables” or “attributes.” Depending on what you're trying to analyze, the features you include in your dataset can vary widely.	What is a feature variable
845	Predictive validity refers to the degree to which scores on a test or assessment are related to performance on a criterion or gold standard assessment that is administered at some point in the future.	What is the predictive validity of a test
669	shutdown point	What is the minimum point on the average variable cost curve called
8323	value is the split of the samples at each node. so at the root node, 32561 samples are divided into two child nodes of  samples each. –	What is value in decision tree
291	Social media plays an important role in every student's life. It is often easier and more convenient to access information, provide information and communicate via social media. Tutors and students can be connected to each other and can make good use of these platforms for the benefit of their learning and teaching.	What is the importance of studying social network
2191	We have compiled a list of best practices and strategies that you can use to improve your TensorFlow Lite model performance.Choose the best model for the task.  Profile your model.  Profile and optimize operators in the graph.  Optimize your model.  Tweak the number of threads.  Eliminate redundant copies.More items	How can I improve my TensorFlow performance
8331	A cross-sectional study selects a single group, for whose members the presence or absence of the condition is initially unknown, and looks for correlations between current characteristics (and/or retrospectively recalled past characteristics) and presence/absence of the condition of interest.	What is cross sectional correlation
8302	Correlation is a statistical method used to determine whether a relationship between variables exists. Regression is a statistical method used to describe the nature of the relationship between variables — i.e., a positive or negative, linear or nonlinear relationship.	Which statistics can be used to test relationships among variables
2158	Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a dataset.  More technically, MDS refers to a set of related ordination techniques used in information visualization, in particular to display the information contained in a distance matrix.	What do you mean by multidimensional scaling
3595	deep learning - a name for an algorithm in machine learning (just like SVM, Regression etc.) transfer learning - as you may know, in order to train a Neural network it might take long time. So, we use a Neural Network that is already trained and in this way we can extract some features of new sample.	What is the difference between deep learning and transfer learning
6043	Advantages of Neural Networks:Neural Networks have the ability to learn by themselves and produce the output that is not limited to the input provided to them.The input is stored in its own networks instead of a database, hence the loss of data does not affect its working.More items•	What are the advantages of artificial neural network
5240	Probability theory is the mathematical study of phenomena characterized by randomness or uncertainty. More precisely, probability is used for modelling situations when the result of an experiment, realized under the same circumstances, produces different results (typically throwing a dice or a coin).	What is probability theory used for
2137	Random event/process/variable: an event/process that is not and cannot be made exact and, consequently, whose outcome cannot be predicted, e.g., the sum of the numbers on two rolled dice.	What random events mean
7416	This task of identifying the best subset of predictors to include in the model, among all possible subsets of predictors, is referred to as variable selection.	What is variable selection in regression
8276	Linear programming: The most widely used application of linear algebra is definitely optimization, and the most widely used kind of optimization is linear programming. You can optimize budgets, your diet, and your route to work using linear programming, and this only scratches the surface of the applications.	What is the application of linear algebra
7417	Computer vision, however, is more than machine learning applied. It involves tasks as 3D scene modeling, multi-view camera geometry, structure-from-motion, stereo correspondence, point cloud processing, motion estimation and more, where machine learning is not a key element.	Is computer vision part of machine learning
865	For example, in regression analysis, many researchers say that there should be at least 10 observations per variable. If we are using three independent variables, then a clear rule would be to have a minimum sample size of 30. Some researchers follow a statistical formula to calculate the sample size.	What is a good sample size for regression analysis
591	Information gain can also be used for feature selection, by evaluating the gain of each variable in the context of the target variable. In this slightly different usage, the calculation is referred to as mutual information between the two random variables.	How is information gain used in feature selection
8508	In simple words, stemming technique only looks at the form of the word whereas lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word.	What is the difference between stemming and Lemmatization
5114	Assumptions for the Kruskal Wallis Test One independent variable with two or more levels (independent groups). The test is more commonly used when you have three or more levels. For two levels, consider using the Mann Whitney U Test instead. Ordinal scale, Ratio Scale or Interval scale dependent variables.	What are the conditions for using a Kruskal Wallis test
929	The output from the logistic regression analysis gives a p-value of , which is based on the Wald z-score. Rather than the Wald method, the recommended method to calculate the p-value for logistic regression is the likelihood-ratio test (LRT), which for this data gives .	What is the output of logistic regression
2194	Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)	What is meant by machine learning algorithms
1666	Cost function(J) of Linear Regression is the Root Mean Squared Error (RMSE) between predicted y value (pred) and true y value (y). Gradient Descent: To update θ1 and θ2 values in order to reduce Cost function (minimizing RMSE value) and achieving the best fit line the model uses Gradient Descent.	What is a cost function in linear regression
3459	Below are the steps to implement the handwritten digit recognition project:Import the libraries and load the dataset. First, we are going to import all the modules that we are going to need for training our model.  Preprocess the data.  Create the model.  Train the model.  Evaluate the model.  Create GUI to predict digits.	How do you make a digit recognizer
1088	Some of the autonomous driving tasks where reinforcement learning could be applied include trajectory optimization, motion planning, dynamic pathing, controller optimization, and scenario-based learning policies for highways. For example, parking can be achieved by learning automatic parking policies.	Where is reinforcement learning used
220	"In machine learning, the hinge loss is a loss function used for training classifiers. The hinge loss is used for ""maximum-margin"" classification, most notably for support vector machines (SVMs). For an intended output t = ±1 and a classifier score y, the hinge loss of the prediction y is defined as."	What is hinge loss in machine learning
8614	Stream processing is the processing of data in motion, or in other words, computing on data directly as it is produced or received. The majority of data are born as continuous streams: sensor events, user activity on a website, financial trades, and so on – all these data are created as a series of events over time.	What is data stream processing
103	Bayesian networks are a type of Probabilistic Graphical Model that can be used to build models from data and/or expert opinion. They can be used for a wide range of tasks including prediction, anomaly detection, diagnostics, automated insight, reasoning, time series prediction and decision making under uncertainty.	How the Bayesian network can be used
5294	Humans are error-prone and biased, but that doesn't mean that algorithms are necessarily better.  But these systems can be biased based on who builds them, how they're developed, and how they're ultimately used. This is commonly known as algorithmic bias.	How are algorithms biased
557	Dropout is a regularization method where input and recurrent connections to LSTM units are probabilistically excluded from activation and weight updates while training a network. This has the effect of reducing overfitting and improving model performance.	What is dropout in Lstm
4528	in a test involving multiple comparisons, the probability of making at least one Type I error over an entire research study. The experiment-wise error rate differs from the testwise error rate, which is the probability of making a Type I error when performing a specific test or comparison.	What is the experiment wise error rate
2002	Gamma–Poisson mixture That is, we can view the negative binomial as a Poisson(λ) distribution, where λ is itself a random variable, distributed as a gamma distribution with shape = r and scale θ = p/(1 − p) or correspondingly rate β = (1 − p)/p.	What does a negative binomial distribution look like
1899	(Select all that apply.) Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next. Class limits specify the span of data values that fall within a class.	What is the difference between class limits and class boundaries
2060	Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data.	What does the term estimation theory means
3199	Classification and regression trees are machine-learning methods for constructing. prediction models from data. The models are obtained by recursively partitioning. the data space and fitting a simple prediction model within each partition.	What is classification and regression tree
276	"In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract ""topics"" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body."	What is topic Modelling used for
3926	An SVM performs classification tasks by constructing hyperplanes in a multidimensional space that separates cases of different class labels. You can use an SVM when your data has exactly two classes, e.g. binary classification problems, but in this article we'll focus on a multi-class support vector machine in R.	Can SVM be used for multi class classification
6109	In these situations, the median is generally considered to be the best representative of the central location of the data. The more skewed the distribution, the greater the difference between the median and mean, and the greater emphasis should be placed on using the median as opposed to the mean.	Why use median instead of mean
5591	The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time. A large F ratio means that the variation among group means is more than you'd expect to see by chance.	What does an F ratio mean
1337	Keras is a neural network library while TensorFlow is the open-source library for a number of various tasks in machine learning. TensorFlow provides both high-level and low-level APIs while Keras provides only high-level APIs.  Keras is built in Python which makes it way more user-friendly than TensorFlow.	What is difference between TensorFlow and keras
6276	NHST is difficult to describe in one sentence, particularly here.	What does Null Hypothesis significance testing NHST mean
921	ProcedureFrom the cluster management console, select Workload > Spark > Deep Learning.Select the Datasets tab.Click New.Create a dataset from Images for Object Detection.Provide a dataset name.Specify a Spark instance group.Provide a training folder.  Provide the percentage of training images for validation.More items	How do you create a dataset for object detection
4267	Based on Deep convolutional neural networks, DeepFace is a deep learning face recognition system. Created by Facebook, it detects and determines the identity of an individual's face through digital images, reportedly with an accuracy of 97.35%.	Which machine learning algorithm is used in face recognition
1206	Statistics Definitions > A random walk is a sequence of discrete, fixed-length steps in random directions. Random walks may be 1-dimensional, 2-dimensional, or n-dimensional for any n. A random walk can also be confined to a lattice.	What is a random walk in statistics
2	"Chunking is used to add more structure to the sentence by following parts of speech (POS) tagging. It is also known as shallow parsing.  Shallow Parsing is also called light parsing or chunking. The primary usage of chunking is to make a group of ""noun phrases."" The parts of speech are combined with regular expressions."	What is chunking in NLTK
7305	c. Deep Learning lacks common sense. This makes the systems fragile and when errors are made, the errors can be very large. These are part of concerns and thus, there is a growing feeling in the field that deep learning's shortcomings require some fundamentally new ideas.	What are the limitations of deep learning
241	Static final variables 2) The variable MY_VAR is public which means any class can use it. It is a static variable so you won't need any object of class in order to access it. It's final so the value of this variable can never be changed in the current or in any class.	Can we change the value of static variable
1001	Random forest improves on bagging because it decorrelates the trees with the introduction of splitting on a random subset of features. This means that at each split of the tree, the model considers only a small subset of features rather than all of the features of the model.	Why does random forest perform well
1865	A two-sided hypothesis is an alternative hypothesis which is not bounded from above or from below, as opposed to a one-sided hypothesis which is always bounded from either above or below. In fact, a two-sided hypothesis is nothing more than the union of two one-sided hypotheses.	What is a two sided hypothesis
361	"Step 1: Find the mean of x, and the mean of y. Step 2: Subtract the mean of x from every x value (call them ""a""), and subtract the mean of y from every y value (call them ""b"") Step 3: Calculate: ab, a2 and b2 for every value. Step 4: Sum up ab, sum up a2 and sum up b."	How do you find the correlation of a data set
1175	Rejection region/Significance: P(x in rejection region|H0) = α. The p-value is a tool to check if the test statistic is in the rejection region. It is also a measure of the evidence for rejecting H0. “Data at least as extreme” is defined by the sidedness of the rejection region.	How is the rejection region related to the P value
5842	Feature extraction identifies those product aspects which are being commented by customers, sentiment prediction identifies the text containing sentiment or opinion by deciding sentiment polarity as positive, negative or neutral and finally summarization module aggregates the results obtained from previous two steps.	What is feature extraction in sentiment analysis
4819	Support vectors are the elements of the training set that would change the position of the dividing hyperplane if removed. d+ = the shortest distance to the closest positive point d- = the shortest distance to the closest negative point The margin (gutter) of a separating hyperplane is d+ + d–.	How do you find the support vector
1605	Divide the number of subjects by 2, and round down. In the example 5 ÷ 2 = 2.5 and rounding down gives 2. Find the first-ordered survival time that is greater than this number. This is the median survival time.	How do you find the median in survival time
1359	Natural language refers to speech analysis in both audible speech, as well as text of a language. NLP systems capture meaning from an input of words (sentences, paragraphs, pages, etc.) in the form of a structured output (which varies greatly depending on the application).	What are the input and output of an NLP system
4766	In a Data Mining sense, the similarity measure is a distance with dimensions describing object features. That means if the distance among two data points is small then there is a high degree of similarity among the objects and vice versa. The similarity is subjective and depends heavily on the context and application.	What are the measures of similarity in data mining
998	The confidence of an association rule is a percentage value that shows how frequently the rule head occurs among all the groups containing the rule body.  Thus, the confidence of a rule is the percentage equivalent of m/n, where the values are: m. The number of groups containing the joined rule head and rule body.	What is confidence in association rule
719	Random errors in experimental measurements are caused by unknown and unpredictable changes in the experiment.  Examples of causes of random errors are: electronic noise in the circuit of an electrical instrument, irregular changes in the heat loss rate from a solar collector due to changes in the wind.	What is an example of a random error
6396	A bias vector is an additional set of weights in a neural network that require no input, and this it corresponds to the output of an artificial neural network when it has zero input. Bias represents an extra neuron included with each pre-output layer and stores the value of “1,” for each action.	What is bias in convolutional neural network
592	Below are the methods to convert a categorical (string) input to numerical nature:Label Encoder: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables).  Convert numeric bins to number: Let's say, bins of a continuous variable are available in the data set (shown below).	How do you convert categorical data to numerical data
2240	The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value. So the median is the value of the quantile at the probability value of 0.5. A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75.	What is the difference between quartile and quantile
1118	In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.  In the adaptive control literature, the learning rate is commonly referred to as gain.	What is meant by learning rate
5182	Because our sample size is greater than 30, the Central Limit Theorem tells us that the sampling distribution will approximate a normal distribution.  Because we know the population standard deviation and the sample size is large, we'll use the normal distribution to find probability.	Why is normal distribution used in sampling distribution
7175	Cross Entropy is definitely a good loss function for Classification Problems, because it minimizes the distance between two probability distributions - predicted and actual. Consider a classifier which predicts whether the given animal is dog, cat or horse with a probability associated with each.	When should you use cross entropy loss and why
4607	Detection accuracy as discussed in this section refers to the agreement between the emotional states detected by different sets of emotion measurement equipment (e.g., multiple modalities), one of which is being used as the “grounded truth” (i.e., standard) for determining the correct emotion.	What is detection accuracy
3546	In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer.	What does a positively skewed distribution mean
4571	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution.	Why does everything in statistics approach a normal distribution
1988	To analyze this data follow these steps:Open the file KAPPA.SAV.  Select Analyze/Descriptive Statistics/Crosstabs.Select Rater A as Row, Rater B as Col.Click on the Statistics button, select Kappa and Continue.Click OK to display the results for the Kappa test shown here:	How do I report a kappa statistic
4370	A feature vector is a vector containing multiple elements about an object. Putting feature vectors for objects together can make up a feature space. The features may represent, as a whole, one mere pixel or an entire image. The granularity depends on what someone is trying to learn or represent about the object.	What features do vectors have
754	The variance is the average of the squared differences from the mean. Standard deviation is the square root of the variance so that the standard deviation would be about 3.03.  Because of this squaring, the variance is no longer in the same unit of measurement as the original data.	Is sample variance and standard deviation the same
657	In behavioral finance, base rate fallacy is the tendency for people to erroneously judge the likelihood of a situation by not taking into account all relevant data. Instead, investors might focus more heavily on new information without acknowledging how this impacts original assumptions.	What is the base rate fallacy and why is it important to avoid it
3684	Boosting refers to any Ensemble method that can combine several weak learners into a strong learner and is used to reduce bias and variance.  Bagging otherwise known as bootstrap aggregating, is used to reduce variance which helps avoid overfitting.	What is the difference between boost ensemble bootstrap and bagging
5534	The problem is a paradox of the veridical type, because the correct choice (that one should switch doors) is so counterintuitive it can seem absurd, but is nevertheless demonstrably true.	Is the Monty Hall problem a paradox
4492	"Meaning of Entropy At a conceptual level, Shannon's Entropy is simply the ""amount of information"" in a variable. More mundanely, that translates to the amount of storage (e.g. number of bits) required to store the variable, which can intuitively be understood to correspond to the amount of information in that variable."	What is meant by Shannon entropy
492	Generative Adversarial Networks, or GANs, are a deep-learning-based generative model. More generally, GANs are a model architecture for training a generative model, and it is most common to use deep learning models in this architecture.	Are GANs deep learning
685	The term linear comes from algebra, because it's used to solve linear equation sets with multiple variables through operations such as subtracting one linear equation from another or multiplying it by a constant, results of which are linear equations. The left part of the equation is also known as a linear function.	Why linear transformation is called linear
1380	A training dataset is a dataset of examples used during the learning process and is used to fit the parameters (e.g., weights) of, for example, a classifier.	What is meant by training data set
5594	Definition: Stratified sampling is a type of sampling method in which the total population is divided into smaller groups or strata to complete the sampling process.  Stratified sampling is used when the researcher wants to understand the existing relationship between two groups.	What is meant by stratified sampling
5456	Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the	Is Random Forest a classification technique
640	Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well. Bag of Words vectors are easy to interpret.	Is TF IDF bag of words
6976	The monty hall problem has 3 doors instead of 100. It is still more likely that you pick a goat.  If a person picks door 1 which is wrong the Monty Hall will close door 3 and give you chance to switch to the right answer, so it means they want always people win the prize.	How does the Monty Hall problem work
4674	In order to assess practical significance, you would also want to know the effect size, strength of any relationship (through a correlation coefficient), and confidence intervals. That said, you would want to be careful not to “sanctify” any results (e.g. an effect size of .	What are the requirements for practical significance in statistics
7781	For an idea we are all familiar with, randomness is surprisingly hard to formally define. We think of a random process as something that evolves over time but in a way we can't predict.	Can we predict randomness
536	Latent Semantic Analysis is an efficient way of analysing the text and finding the hidden topics by understanding the context of the text. Latent Semantic Analysis(LSA) is used to find the hidden topics represented by the document or text. This hidden topics then are used for clustering the similar documents together.	How does latent semantic analysis work
4314	Examples of sampling bias include self-selection, pre-screening of trial participants, discounting trial subjects/tests that did not run to completion and migration bias by excluding subjects who have recently moved into or out of the study area.	What is selection bias example
513	Statistical inference can be divided into two areas: estimation and hypothesis testing. In estimation, the goal is to describe an unknown aspect of a population, for example, the average scholastic aptitude test (SAT) writing score of all examinees in the State of California in the USA.	What are examples of statistical inference
852	The degrees of freedom for the chi-square are calculated using the following formula: df = (r-1)(c-1) where r is the number of rows and c is the number of columns. If the observed chi-square test statistic is greater than the critical value, the null hypothesis can be rejected.	How do you find the degrees of freedom for a chi square distribution
3251	Testing approach: The answers lie in the data set. In order to test a machine learning algorithm, tester defines three different datasets viz. Training dataset, validation dataset and a test dataset (a subset of training dataset).	How do you test machine learning models
2680	The standard error of estimate, Se indicates approximately how much error you make when you use the predicted value for Y (on the least-squares line) instead of the actual value of Y.	What does the standard error of the estimate represent
1458	Basically, you're just pre-setting some of the weights of the new network. Be sure to initialize the new connections to have similar distributions. Make the last layer a concatenation of their results and then add another few layers. Make the last layer a concatenation of their results and the original input.	How do you merge neural networks
6822	They are basically equivalent: the linear time invariant systems refers to an analog system and shift-invariant system refers to a discrete-time system.  The shift-invariant is the same as time invariant: if we delay the input, the output that we get is the original input to the signal that wasn't delayed.	What is difference between Linear time invariant system and Linear shift invariant system 1
7259	1 Answer. Transfer learning is when a model developed for one task is reused to work on a second task. Fine tuning is one approach to transfer learning.	What is the difference between transfer learning and fine tuning
4108	A feedforward neural network is a biologically inspired classification algorithm. It consist of a (possibly large) number of simple neuron-like processing units, organized in layers. Every unit in a layer is connected with all the units in the previous layer.  This is why they are called feedforward neural networks.	What is feedforward in neural network algorithm
506	Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood.	What is pruning in decision trees Why is it important
7862	In the case of information retrieval, the cosine similarity of two documents will range from 0 to 1, since the term frequencies (using tf–idf weights) cannot be negative. The angle between two term frequency vectors cannot be greater than 90°.	Can cosine similarity be negative
337	Cohen's kappa coefficient (κ) is a statistic that is used to measure inter-rater reliability (and also Intra-rater reliability) for qualitative (categorical) items.	What does Cohen's kappa measure
2457	The algorithm is used to effectively train a neural network through a method called chain rule. In simple terms, after each forward pass through a network, backpropagation performs a backward pass while adjusting the model's parameters (weights and biases).	What is the use of back propagation algorithm
1171	Because learning grammatical regularities requires infants to be able to determine boundaries between individual words, this indicates that infants who are still quite young are able to acquire multiple levels of language knowledge (both lexical and syntactical) simultaneously, indicating that statistical learning is a	Why is statistical learning important for language acquisition
2679	A relative frequency distribution shows the proportion of the total number of observations associated with each value or class of values and is related to a probability distribution, which is extensively used in statistics.	What is relative frequency distribution in statistics
1220	To implement stratified sampling, first find the total number of members in the population, and then the number of members of each stratum. For each stratum, divide the number of members by the total number in the entire population to get the percentage of the population represented by that stratum.	How do you find percent in stratified random sampling
1453	The correlation between two true dichotomous variables is called a phi coefficient. This can be computed either by just obtaining the Pearson's r between your X and Y variables (each of them with scores of 1 and 0, or for that matter, any two numbers that differ).	How do you find the correlation between binary variables
3369	Stratified random sampling refers to a sampling method that has the following properties.The population consists of N elements.The population is divided into H groups, called strata.Each element of the population can be assigned to one, and only one, stratum.More items	What are the properties of stratified random sampling
3943	All descriptive statistics are either measures of central tendency or measures of variability, also known as measures of dispersion.  Range, quartiles, absolute deviation and variance are all examples of measures of variability. Consider the following data set: 5, 19, 24, 62, 91, 100.	What is an example of a descriptive statistic
3002	Anomaly detection (aka outlier analysis) is a step in data mining that identifies data points, events, and/or observations that deviate from a dataset's normal behavior. Anomalous data can indicate critical incidents, such as a technical glitch, or potential opportunities, for instance a change in consumer behavior.	What is anomaly detection used for
75	If the dice is thrown repeatedly until the first time a three appears. The probablility distribution of the number of times it is thrown not getting a three (not-a-threes number of failures to get a three) is a geometric distribution with the success_fraction = 1/6 = 0.1666 ̇.	What are examples of Geometric distribution in real life
6576	For example, The number of cases of a disease in different towns; The number of mutations in given regions of a chromosome; The number of dolphin pod sightings along a flight path through a region; The number of particles emitted by a radioactive source in a given time; The number of births per hour during a given day.	What is an example of Poisson distribution
4849	"Regression. Regression analysis attempts to determine the best ""fit"" between two or more variables. The independent variable in a regression analysis is a continuous variable, and thus allows you to determine how one or more independent variables predict the values of a dependent variable."	What kind of analysis would you use to explore the relationship between two sets of variables
357	For example, if n = 100 and p = 0.25 then we are justified in using the normal approximation. This is because np = 25 and n(1 - p) = 75. Since both of these numbers are greater than 10, the appropriate normal distribution will do a fairly good job of estimating binomial probabilities.	What is an example of the normal approximation of the binomial distribution
3136	Using batch normalisation allows much higher learning rates, increasing the speed at which networks train. Makes weights easier to initialise — Weight initialisation can be difficult, especially when creating deeper networks. Batch normalisation helps reduce the sensitivity to the initial starting weights.	Why should we use batch normalization
7095	Intelligence Quotient	What does IQ mean
2044	There are many moving parts in a Machine Learning (ML) model that have to be tied together for an ML model to execute and produce results successfully. This process of tying together different pieces of the ML process is known as a pipeline. A pipeline is a generalized but very important concept for a Data Scientist.	What is an AI pipeline
1336	"Backpropagation, short for ""backward propagation of errors,"" is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights."	What is Backpropagation in neural network
1836	A random variable is a variable whose value is unknown or a function that assigns values to each of an experiment's outcomes.	How can I understand the definition of a random variable
909	Systematic vs. Random errors are (like the name suggests) completely random. They are unpredictable and can't be replicated by repeating the experiment again. Systematic Errors produce consistent errors, either a fixed amount (like 1 lb) or a proportion (like 105% of the true value).	What is the difference between random error and systematic error
2575	Answer to Try It! Variables that give a straight line with a constant slope are said to have a linear relationship. In this case, however, the relationship is nonlinear. The slope changes all along the curve.	What is a linear and nonlinear association
1533	In general, there is no universal rule of thumb indicating that the accuracy of a learner is directly proportional to the number of features used to train it.	Does increasing the number of feature variables of the dataset improve the accuracy of the training model
5235	Time series analysis can be useful to see how a given asset, security, or economic variable changes over time. It can also be used to examine how the changes associated with the chosen data point compare to shifts in other variables over the same time period.	What is the importance of time series analysis
6619	The bias is the value of the difference between two techniques (reading A – reading B) and this value is plotted on the y axis, against the mean of the two techniques (reading A+reading B/2) on the x‐axis) (Fig. 1). If the two methods you are comparing give very similar results, your bias should be very close to zero.	How do you find the bias between two methods
7073	This cheat sheet demonstrates 11 different classical time series forecasting methods; they are:Autoregression (AR)Moving Average (MA)Autoregressive Moving Average (ARMA)Autoregressive Integrated Moving Average (ARIMA)Seasonal Autoregressive Integrated Moving-Average (SARIMA)More items•	What are the methods of time series
5083	As in classification, support vector regression (SVR) is characterized by the use of kernels, sparse solution, and VC control of the margin and the number of support vectors. Although less popular than SVM, SVR has been proven to be an effective tool in real-value function estimation.	What is SVR regression
5281	1:246:12Suggested clip · 104 secondsBuilding Statistical Models - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you make a statistical model
847	Deep learning models are trained by using large sets of labeled data and neural network architectures that learn features directly from the data without the need for manual feature extraction. Figure 1: Neural networks, which are organized in layers consisting of a set of interconnected nodes.	How deep can learning be developed
3113	Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.	Is AI all about simulating human intelligence
3831	A neural network is considered to be an effort to mimic human brain actions in a simplified manner. Attention Mechanism is also an attempt to implement the same action of selectively concentrating on a few relevant things, while ignoring others in deep neural networks.	What is the attention mechanism
6126	A P value is also affected by sample size and the magnitude of effect. Generally the larger the sample size, the more likely a study will find a significant relationship if one exists. As the sample size increases the impact of random error is reduced.	Why does P value change with sample size
394	A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term. Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.	What is l1 and l2 regularization
376	The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance.	Why are the formulas for sample variance and population variance different
7322	Decision trees can handle both categorical and numerical variables at the same time as features, there is not any problem in doing that. Edit: Every split in a decision tree is based on a feature. If the feature is categorical, the split is done with the elements belonging to a particular class.	How do Decision Trees handle categorical variables
6169	Every box is composed of four parts (or areas), defined by their respective edges: the content edge, padding edge, border edge, and margin edge.	What are the 4 areas of the box model
679	Structural equation modeling is a multivariate statistical analysis technique that is used to analyze structural relationships. This technique is the combination of factor analysis and multiple regression analysis, and it is used to analyze the structural relationship between measured variables and latent constructs.	What is structural equation modeling used for
851	To quickly summarize: Image Classification helps us to classify what is contained in an image. Image Localization will specify the location of single object in an image whereas Object Detection specifies the location of multiple objects in the image.	What is the difference between object detection and classification
4510	The concept of exclusion restrictions denotes that some of the exogenous variables are not in some of the equations. Often this idea is expressed by saying the coefficient next to that exogenous variable is zero.	What is the exclusion restriction instrumental variable
311	Multinomial theorem is actually not in the syllabus but learning this topic well can save your time as well as will make the problems of permutations and combinations look easy.	Is multinomial theorem in JEE mains
7522	AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability.  By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.	What is ROC AUC score
5639	SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs.	How does support vector algorithm work
8253	Autocorrelation (for sound signals)(1) finding the value of the signal at a time t,(2) finding the value of the signal at a time t + τ,(3) multiplying those two values together,(4) repeating the process for all possible times, t, and then.(5) computing the average of all those products.	How do you calculate autocorrelation of a signal
7078	The linear Discriminant analysis estimates the probability that a new set of inputs belongs to every class.  LDA uses Bayes' Theorem to estimate the probabilities. If the output class is (k) and the input is (x), here is how Bayes' theorem works to estimate the probability that the data belongs to each class.	How does linear discriminant analysis work
8575	Machine learning is a form of artificial intelligence (AI) that teaches computers to think in a similar way to how humans do: learning and improving upon past experiences. It works by exploring data, identifying patterns, and involves minimal human intervention.	How does a machine learning model work
615	Attention Mechanism in Neural Networks - 1.  Attention is arguably one of the most powerful concepts in the deep learning field nowadays. It is based on a common-sensical intuition that we “attend to” a certain part when processing a large amount of information.	What is Attention based neural network
1737	SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs.	Why SVM is used for classification
1293	Simply head on over to www.canva.com to start creating your decision tree design. You don't need to download Canva, just create an account and log in.	Where can I make a decision tree
3104	Boltzmann machine is an unsupervised machine learning algorithm. It helps discover latent features present in the dataset. Dataset is composed of binary vectors. Connection between nodes are undirected.	Is RBM supervised or unsupervised
6518	A greedy algorithm is used to construct a Huffman tree during Huffman coding where it finds an optimal solution. In decision tree learning, greedy algorithms are commonly used, however they are not guaranteed to find the optimal solution. One popular such algorithm is the ID3 algorithm for decision tree construction.	Where greedy algorithm is used
288	To do so, you can completely opt out from the automatic change detection in your component, and handle things yourself, by injecting in your component a ChangeDetectorRef . This class offers a few methods: detach() detectChanges()	How do you stop change detection in angular 4
396	Here is a brief review of our original seven techniques for dimensionality reduction:Missing Values Ratio.  Low Variance Filter.  High Correlation Filter.  Random Forests/Ensemble Trees.  Principal Component Analysis (PCA).  Backward Feature Elimination.  Forward Feature Construction.	What are the effective methods of dimension reduction
1549	A recurrent neural network (RNN) is a type of artificial neural network commonly used in speech recognition and natural language processing (NLP). RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario.	What is recurrent neural network used for
8164	formal parameter — the identifier used in a method to stand for the value that is passed into the method by a caller. actual parameter — the actual value that is passed into the method by a caller.	What is the difference between formal parameter and actual parameter
8351	"To recap the differences between the two: Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned. Deep learning structures algorithms in layers to create an ""artificial neural network” that can learn and make intelligent decisions on its own."	What is deep learning vs Machine Learning
3226	"In mathematical optimization and decision theory, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some ""cost"" associated with the event.  In optimal control, the loss is the penalty for failing to achieve a desired value."	What does loss function mean
5548	"Binning is a way to group a number of more or less continuous values into a smaller number of ""bins"". For example, if you have data about a group of people, you might want to arrange their ages into a smaller number of age intervals."	What is the purpose of binning
7837	A (non-mathematical) definition I like by Miller (2017)3 is: Interpretability is the degree to which a human can understand the cause of a decision.  The higher the interpretability of a machine learning model, the easier it is for someone to comprehend why certain decisions or predictions have been made.	What does interpretable machine learning mean
1442	In probability theory and statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d. or iid or IID.	What is meant by Independent and Identically Distributed Random Variables
1709	AI is not one technology; it's a set of technologies and building blocks, all using data to unlock intelligent value across industries and business functions. AI Consulting Services from IBM help you leverage AI to drive smart reinvention of your workflows and technology.	What is IBM artificial intelligence
4647	Basically, we can think of TensorFlow as the Lego bricks (similar to NumPy and SciPy) that we can use to implement machine learning algorithms whereas Scikit-Learn comes with off-the-shelf algorithms, e.g., algorithms for classification such as SVMs, Random Forests, Logistic Regression, and many, many more.	What are the main differences between TensorFlow and SciKit Learn
7247	With a continuous variable, the hazard ratio indicates the change in the risk of death if the parameter in question rises by one unit, for example if the patient is one year older on diagnosis. For every additional year of patient age on diagnosis, the risk of death falls by 7% (hazard ratio 0.93).	How do you interpret a hazard ratio for a continuous variable
7487	The INPUT function returns the value produced when a SAS expression is read using a specified informat. You must use an assignment statement to store that value in a variable. The INPUT statement uses an informat to read a data value and then optionally stores that value in a variable. Examples.	What is an input function
501	“Deep learning is a branch of machine learning that uses neural networks with many layers. A deep neural network analyzes data with learned representations similarly to the way a person would look at a problem,” Brock says. “In traditional machine learning, the algorithm is given a set of relevant features to analyze.	How do you explain deep learning
3665	Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.	How are decision trees used for regression
758	Generally, a machine learning pipeline describes or models your ML process: writing code, releasing it to production, performing data extractions, creating training models, and tuning the algorithm. An ML pipeline should be a continuous process as a team works on their ML platform.	What is a pipeline in machine learning
1886	Sample variance Concretely, the naive estimator sums the squared deviations and divides by n, which is biased.  The sample mean, on the other hand, is an unbiased estimator of the population mean μ. Note that the usual definition of sample variance is. , and this is an unbiased estimator of the population variance.	Is the sample variance an unbiased estimator
454	Data analysis has two prominent methods: qualitative research and quantitative research. Each method has their own techniques. Interviews and observations are forms of qualitative research, while experiments and surveys are quantitative research.	What are the methods of data analysis
2024	Convolutional Neural Networks (CNNs) is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough.	What type of deep learning models are best suited for image recognition
467	In Computer science (especially Machine learning) Pruning means simplifying/compressing and optimizing a Decision tree by removing sections of the tree that are uncritical and redundant to classify instances.	How does pruning work in decision trees
2709	"To work out the probability that a discrete random variable X takes a particular value x, we need to identify the event (the set of possible outcomes) that corresponds to ""X=x"". pX(x)=Pr(X=x). In general, the probability function pX(x) may be specified in a variety of ways."	What is the function of probability
3170	Data mining has several types, including pictorial data mining, text mining, social media mining, web mining, and audio and video mining amongst others.Read: Data Mining vs Machine Learning.Learn more: Association Rule Mining.Check out: Difference between Data Science and Data Mining.Read: Data Mining Project Ideas.	What are the two types of data mining
1067	The tobit model, also called a censored regression model, is designed to estimate linear relationships between variables when there is either left- or right-censoring in the dependent variable (also known as censoring from below and above, respectively).	What is Tobit model used for
7253	Y hat (written ŷ ) is the predicted value of y (the dependent variable) in a regression equation. It can also be considered to be the average value of the response variable.  The equation is calculated during regression analysis.	What is Y hat in regression
7888	The optimal number of clusters can be defined as follow: Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters. For each k, calculate the total within-cluster sum of square (wss). Plot the curve of wss according to the number of clusters k.	How do you define K in K means clustering
2987	Output is defined as the act of producing something, the amount of something that is produced or the process in which something is delivered. An example of output is the electricity produced by a power plant. An example of output is producing 1,000 cases of a product.	What output means
70	In artificial intelligence, eager learning is a learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system.	What is the difference between eager learning and lazy learning
938	1:006:34Suggested clip · 111 secondsPoisson regression interpreting SPSS results (brief demo) - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you report Poisson regression results
6277	for research.  Instead, the bottleneck often is the lack of a solid research design and a credible theory, both of which are essential to develop, test, and accumulate causal explanations. This does not mean that big data has no benefits.	Can Big Data solve the fundamental problem of causal inference
7063	Most implementations of random forest (and many other machine learning algorithms) that accept categorical inputs are either just automating the encoding of categorical features for you or using a method that becomes computationally intractable for large numbers of categories. A notable exception is H2O.	Can random forest handle categorical data
7480	Deep neural networks. A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed.	What makes a neural network a deep neural network
8356	- YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the median for continuous data
2089	In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters.  The bias error is an error from erroneous assumptions in the learning algorithm.	What is bias in bias variance tradeoff
5549	The k-means clustering algorithm attempts to split a given anonymous data set (a set containing no information as to class identity) into a fixed number (k) of clusters.  The resulting classifier is used to classify (using k = 1) the data and thereby produce an initial randomized set of clusters.	How does cluster algorithm work
5122	Gladwell's purpose for writing The Outliers was to inform reader's on how successful people achieve success through the help of others, practice, and opportunity. He also wanted to get rid of our society's crude perspective on how outliers become successful.	What is Gladwell's purpose in outliers
5059	The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above . 05 indicates normality.	How can you tell if data is normally distributed
768	The size of the sample space is the total number of possible outcomes. For example, when you roll 1 die, the sample space is 1, 2, 3, 4, 5, or 6. So the size of the sample space is 6.	How do you find the sample space
8446	Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus.	What is TF IDF used for
1149	In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success/yes/true/one (with probability p)	What type of distributions is the binomial distribution
4961	Residual = Observed – Predicted positive values for the residual (on the y-axis) mean the prediction was too low, and negative values mean the prediction was too high; 0 means the guess was exactly correct. That is, (1) they're pretty symmetrically distributed, tending to cluster towards the middle of the plot.	How do you interpret a residual scatter plot
717	Use systematic sampling when there's low risk of data manipulation. Systematic sampling is the preferred method over simple random sampling when a study maintains a low risk of data manipulation.	What is systematic sampling used for
1128	0:031:06Suggested clip · 42 secondsLatent dirichlet allocation distortions - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you pronounce latent Dirichlet allocation
8316	Convenience sampling (also known as grab sampling, accidental sampling, or opportunity sampling) is a type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand.	What is a convenience sample in statistics
826	You calculate the mean, say it's 10. You calculate the standard deviation: it's 12. That means that any number from 10 to 22 is within one standard deviation away from the mean. Now if your data are symmetric (say normal), any number from -2 to 10 is also within a standard deviation from the mean.	How do you find how many standard deviations away from the mean
5664	Here are the four most common ways of measuring reliability for any empirical method or metric:inter-rater reliability.test-retest reliability.parallel forms reliability.internal consistency reliability.	What are 2 ways to test reliability
818	An autocorrelation plot is designed to show whether the elements of a time series are positively correlated, negatively correlated, or independent of each other. (The prefix auto means “self”— autocorrelation specifically refers to correlation among the elements of a time series.)	What is an autocorrelation plot
2014	Sentiment analysis also means you'll be able to detect changes in the overall opinion towards your brand. Because it provides insight into the way your customers are feeling when they approach you, you can monitor trends and see if overall opinion towards your company drops or rises.	What are the benefits of sentiment analysis
8397	Mini Batch Gradient Descent Batch : A CompromiseEasily fits in the memory.It is computationally efficient.Benefit from vectorization.If stuck in local minimums, some noisy steps can lead the way out of them.Average of the training samples produces stable error gradients and convergence.	What are the benefits of mini batch gradient descent
3295	"Jeffrey Jacob Abrams (born June 27, 1966), more commonly known as J.J. Abrams, is one of the creators and executive producers of Lost. He is also credited with being the driving force behind the show, along with writing and directing the episodes ""Pilot, Part 1"" and ""Pilot, Part 2""."	How involved was JJ Abrams in Lost
5200	Sample space is all the possible outcomes of an event. Sometimes the sample space is easy to determine. For example, if you roll a dice, 6 things could happen. You could roll a 1, 2, 3, 4, 5, or 6.	What is sample space in probability examples
1548	The probability distribution of a discrete random variable can always be represented by a table. For example, suppose you flip a coin two times.  The probability of getting 0 heads is 0.25; 1 head, 0.50; and 2 heads, 0.25. Thus, the table is an example of a probability distribution for a discrete random variable.	What is an example of probability distribution
7396	Random Forest uses bootstrap sampling and feature sampling, i.e row sampling and column sampling. Therefore Random Forest is not affected by multicollinearity that much since it is picking different set of features for different models and of course every model sees a different set of data points.	Is Multicollinearity a problem in random forest
7927	Because a chi-square test is a univariate test; it does not consider relationships among multiple variables at the same time. Therefore, dependencies detected by chi-square analyses may be unrealistic or non-causal. There may be other unseen factors that make the variables appear to be associated.	Is Chi square univariate analysis
8222	The loss function is usually either the mean-squared error or cross-entropy between the output and the input, known as the reconstruction loss, which penalizes the network for creating outputs different from the input.	What is reconstruction loss
7947	The probability distribution for a random variable describes how the probabilities are distributed over the values of the random variable. For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x).	What is the probability distribution of a random variable
875	Definition Quantile. A quantile defines a particular part of a data set, i.e. a quantile determines how many values in a distribution are above or below a certain limit. Special quantiles are the quartile (quarter), the quintile (fifth) and percentiles (hundredth).	What is a quantile in statistics
3178	"Discrete Variable. Discrete Variable. Variables that can only take on a finite number of values are called ""discrete variables."" All qualitative variables are discrete. Some quantitative variables are discrete, such as performance rated as 1,2,3,4, or 5, or temperature rounded to the nearest degree."	Which are discrete variables
979	Sampling Frame vs. A sampling frame is a list of things that you draw a sample from. A sample space is a list of all possible outcomes for an experiment. For example, you might have a sampling frame of names of people in a certain town for a survey you're going to be conducting on family size.	What is the difference between sample frame and sample size
8461	A random variable Xk is referred to as a kth-order Erlang (or Erlang-k) random variable with parameter λ if its PDF is given by. f X k ( x ) = { λ k x k − 1 e − λ x ( k − 1 ) ! k = 1 , 2 , 3 , … ; x ≥ 0 0 x < 0.	What is an Erlang random variable
7335	First, make a list of the possible outcomes for each flip. Next, count the number of the possible outcomes for each flip. There are two outcomes for each flip of a coin: heads or tails. Then, multiply the number of outcomes by the number of flips.	How do you find the outcome of a sample space
852	Convolution neural network is a type of neural network which has some or all convolution layers. Feed forward neural network is a network which is not recursive. neurons in this layer were only connected to neurons in the next layer.  neurons in this layer were only connected to neurons in the next layer.	What are the differences between a convolutional network and a feedforward neural network
5806	A dataset can be created in three different ways:  As a copy of an existing dataset in the database or on your local computer. As a child dataset from an existing global dataset in the database or on your local computer. The time period and the dataset name cannot be changed in this case.	How are datasets created
227	Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data.	What is detection and estimation theory
500	It might take about 2-4 hours of coding and 1-2 hours of training if done in Python and Numpy (assuming sensible parameter initialization and a good set of hyperparameters). No GPU required, your old but gold CPU on a laptop will do the job. Longer training time is expected if the net is deeper than 2 hidden layers.	How long do neural networks take to train
7871	Stochastic (from from Greek στόχος (stókhos) 'aim, guess'.) is any randomly determined process. In mathematics the terms stochastic process and random process are interchangeable.	What is meant by stochastic
2197	For linear algebra, it's very helpful to prepare by doing simple practice problems with the basic axioms of vector spaces and inner products. I was always mediocre at algebra, but good at visualizing 2D and 3D things.	How do you prepare linear algebra
1158	We can interpret the Poisson regression coefficient as follows: for a one unit change in the predictor variable, the difference in the logs of expected counts is expected to change by the respective regression coefficient, given the other predictor variables in the model are held constant.	How do you interpret Poisson regression results
8150	Here are the steps to split a decision tree using reduction in variance:For each split, individually calculate the variance of each child node.Calculate the variance of each split as the weighted average variance of child nodes.Select the split with the lowest variance.More items•	How do you split a regression tree
2354	The fact that two variables are strongly correlated does not in itself imply a cause-and-effect relationship between the variables.	What does Correlation does not imply causation quizlet mean
4756	"A point estimate is the value of a statistic that estimates the value of a parameter. For example, the sample mean is a point estimate of the population mean. The arithmetic mean is a single value meant to ""sum up"" a data set. To calculate the mean, add up all the values and divide by the number of values."	How do you calculate population mean
4176	Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem.	What is the learning rate in neural network
780	To recap the differences between the two: Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned.  While both fall under the broad category of artificial intelligence, deep learning is what powers the most human-like artificial intelligence.	What is the difference between deep learning and usual machine learning
252	The process of determining the frequency contents of a continuous-time signal in the discrete-time domain is known as spectral analysis.  Hence, the main objective of spectral analysis is the determination of the power spectrum density (PSD) of a random process.	What is spectral analysis of signals
3898	The main difference between Binomial and Poisson Distribution is that the Binomial distribution is only for a certain frame or a probability of success and the Poisson distribution is used for events that could occur a very large number of times.	What are the differences between the Poisson and the Binomial Distribution
51	Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation.  Its goal is to maximize the total reward.	What is reinforced learning in AI
3146	Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.	Why do we need cross validation in machine learning
573	In the context of AB testing experiments, statistical significance is how likely it is that the difference between your experiment's control version and test version isn't due to error or random chance.  It's commonly used in business to observe how your experiments affect your business's conversion rates.	What is statistical significance in AB testing
8173	The t-test is commonly used in statistical analysis. It is an appropriate method for comparing two groups of continuous data which are both normally distributed. The most commonly used forms of the t- test are the test of hypothesis, the single-sample, paired t-test, and the two-sample, unpaired t-test.	How do you Analyse a continuous variable
1120	"The DCT can be used to convert the signal (spatial information) into numeric data (""frequency"" or ""spectral"" information) so that the image's information exists in a quantitative form that can be manipulated for compression. The signal for a graphical image can be thought of as a three-dimensional signal."	Why DCT is useful in compression
1694	Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning.  LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series.	What is Lstm in neural network
3847	Batch means that you use all your data to compute the gradient during one iteration. Mini-batch means you only take a subset of all your data during one iteration.	What is the difference between batch mode and mini batch in machine learning
5703	The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image).	What is the use of median filter
6936	The learning algorithm is called consistent with respect to F and P if the risk R(fn) converges in probability to the risk R(fF) of the best classifier in F, that is for all ε > 0, P(R(fn) − R(fF) > ε) → 0 as n → ∞. 2.	What does it mean to say that a learning algorithm is consistent in the context of statistical learning theory
7219	A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.	What is posterior probability in machine learning
398	In a hypothesis test, we:Evaluate the null hypothesis, typically denoted with H0.  Always write the alternative hypothesis, typically denoted with Ha or H1, using less than, greater than, or not equals symbols, i.e., (≠, >, or <).More items	How do you state the null hypothesis and alternative hypothesis
1691	The difference between multi-task learning and meta-learning is: in multitask learning, your goal would be to try to solve all of the training tasks shown in the gray box (on the left picture); whereas in meta-learning your goal is to use these training tasks in order to solve new tasks with a small amount of data, so	What is the difference between meta learning and multi task learning
7194	In artificial intelligence and operations research, constraint satisfaction is the process of finding a solution to a set of constraints that impose conditions that the variables must satisfy.  Constraint propagation methods are also used in conjunction with search to make a given problem simpler to solve.	What is constraint satisfaction problem in artificial intelligence
8054	We can approximate the flux across Sr using the divergence theorem as follows: ∬SrF·dS=∭BrdivFdV≈∭BrdivF(P)dV=divF(P)V(Br). and we can consider the divergence at P as measuring the net rate of outward flux per unit volume at P.	How does divergence theorem find flux
4502	The tm package utilizes the Corpus as its main structure. A corpus is simply a collection of documents, but like most things in R , the corpus has specific attributes that enable certain types of analysis.  Volitile Corpus (VCorpus) is a temporary object within R and is the default when assigning documents to a corpus.	What is TM package in R
463	The population mean of the distribution of sample means is the same as the population mean of the distribution being sampled from.  Thus as the sample size increases, the standard deviation of the means decreases; and as the sample size decreases, the standard deviation of the sample means increases.	How does standard deviation change with sample size
181	The margin of error increases as the level of confidence increases because the larger the expected proportion of intervals that will contain the​ parameter, the larger the margin of error.  The larger the level of confidence​ is, the larger number of intervals that will contain the parameter.	What increases the margin of error
5064	The workflow for using TensorFlow Lite involves the following steps:Pick a model. Bring your own TensorFlow model, find a model online, or pick a model from our Pre-trained models to drop in or retrain.Convert the model.  Deploy to your device.  Optimize your model.	How do you use TensorFlow Lite
4163	Algorithms consist of instructions that are carried out (performed) one after another. Sequencing is the specific order in which instructions are performed in an algorithm. For example, a very simple algorithm for brushing teeth might consist of these steps: put toothpaste on toothbrush.	What is the difference between sequence and algorithm
2993	Learning rate is a hyper-parameter th a t controls how much we are adjusting the weights of our network with respect the loss gradient.  Furthermore, the learning rate affects how quickly our model can converge to a local minima (aka arrive at the best accuracy).	Does learning rate affect accuracy
7299	A distribution in statistics is a function that shows the possible values for a variable and how often they occur.	What is the definition of data distribution
5666	If I know a programming language, where is a great place to start practicing algorithms?  Become proficient at written communication.  Learn Functional Programming.  Learn Object Oriented Analysis and Design.  Free Code Camp.More items•	How can I begin to learn algorithms
7431	You are hereTraining an Artificial Neural Network.The Iterative Learning Process.Feedforward, Back-Propagation.Structuring the Network.Rule One: As the complexity in the relationship between the input data and the desired output increases, the number of the processing elements in the hidden layer should also increase.More items	How do I train artificial neural network
1159	To find the relative frequency, divide the frequency by the total number of data values. To find the cumulative relative frequency, add all of the previous relative frequencies to the relative frequency for the current row.	How do you find the cumulative relative frequency
8433	"A decision tree is a flowchart-like structure in which each internal node represents a ""test"" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes)."	What is decision tree example
8676	Pattern recognition is a process in which we use multiple senses in order to make decisions. As we go through our day, our brain's pattern recognition abilities help us recognise certain objects and situations.	What are pattern recognition skills
3677	In probability, two events are independent if the incidence of one event does not affect the probability of the other event. If the incidence of one event does affect the probability of the other event, then the events are dependent. There is a red 6-sided fair die and a blue 6-sided fair die.	What is meant by independent events in probability
8136	0:0010:07Suggested clip 119 secondsProbability Exponential Distribution Problems - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you solve exponential distributions
6865	Fundamentally, classification is about predicting a label and regression is about predicting a quantity.  That classification is the problem of predicting a discrete class label output for an example. That regression is the problem of predicting a continuous quantity output for an example.	What is classification regression
8688	NLP is short for natural language processing while NLU is the shorthand for natural language understanding. Similarly named, the concepts both deal with the relationship between natural language (as in, what we as humans speak, not what computers understand) and artificial intelligence.	What is NLU and NLP
4858	Random assignment of participants helps to ensure that any differences between and within the groups are not systematic at the outset of the experiment. Thus, any differences between groups recorded at the end of the experiment can be more confidently attributed to the experimental procedures or treatment.	What is the main reason study participants are randomly assigned to the study groups
3110	Subsampling is the process of sampling a signal with a frequency lower than twice the highest signal frequency, but higher than two times the signal bandwidth.	What is subsampling in signal processing
827	The pdf represents the relative frequency of failure times as a function of time. The cdf is a function, F(x)\,\!, of a random variable X\,\!, and is defined for a number x\,\!	What is the relationship between PDF and CDF
6293	These are the steps we are going to do:Make a stupid model as an example, train and store it.Fetch the variables you need from your stored model.Build the tensor info from them.Create the model signature.Create and save a model builder.Download a Docker image with TensorFlow serving already compile on it.More items•	How do you deploy TensorFlow in production
53	"My main criticism of Bayes' Theorem is that it is stating the obvious, and much like the ""Law of Large Numbers"" (which essentially states that N * x = N*x = Nx) doesn't deserve to have a name."	What are some criticisms of Bayes Theorem
6325	Mean Squared Error, commonly used for linear regression models, isn't convex for logistic regression. This is because the logistic function isn't always convex. The logarithm of the likelihood function is however always convex.	Why mean square error is not used in logistic regression
4976	The chi-square test is the most commonly used to test the goodness of fit tests and is used for discrete distributions like the binomial distribution and the Poisson distribution, whereas The Kolmogorov-Smirnov and Anderson-Darling goodness of fit tests are used for continuous distributions.	Which of the following function is used to test goodness of fit of a continuous distribution to data
6713	The Loss Function is one of the important components of Neural Networks. Loss is nothing but a prediction error of Neural Net. And the method to calculate the loss is called Loss Function. In simple words, the Loss is used to calculate the gradients. And gradients are used to update the weights of the Neural Net.	What is the loss function in a neural network
8278	Two random variables are independent if they convey no information about each other and, as a consequence, receiving information about one of the two does not change our assessment of the probability distribution of the other.	Are two random variables independent
5290	"The letter ""x"" is often used in algebra to mean a value that is not yet known. It is called a ""variable"" or sometimes an ""unknown"". In x + 2 = 7, x is a variable, but we can work out its value if we try! A variable doesn't have to be ""x"", it could be ""y"", ""w"" or any letter, name or symbol."	What does X do to an equation
3899	Running the ProcedureOpen the Frequencies window (Analyze > Descriptive Statistics > Frequencies) and double-click on variable Rank.To request the mode statistic, click Statistics. Check the box next to Mode, then click Continue.To turn on the bar chart option, click Charts.  When finished, click OK.	How do you interpret demographic data in SPSS
7750	This is when the kernel trick comes in. It allows us to operate in the original feature space without computing the coordinates of the data in a higher dimensional space.  In essence, what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions.	What is kernel trick and how it is useful
2339	Real-time big data analytics means that big data is processed as it arrives and either a business user gets consumable insights without exceeding a time period allocated for decision-making or an analytical system triggers an action or a notification.	What is real time processing in big data
4812	Here are some examples of discrete variables: Number of children per family. Number of students in a class. Number of citizens of a country.	What is a discrete variable example
3514	Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data.	What is unsupervised learning in machine learning
445	A random process is a time-varying function that assigns the outcome of a random experiment to each time instant: X(t). • For a fixed (sample path): a random process is a time varying function, e.g., a signal.	What does random processes mean
1953	Mini-Max Algorithm in Artificial Intelligence. Mini-max algorithm is a recursive or backtracking algorithm which is used in decision-making and game theory. It provides an optimal move for the player assuming that opponent is also playing optimally.  This Algorithm computes the minimax decision for the current state.	What is Minimax algorithm in AI
7423	"In computer science, locality-sensitive hashing (LSH) is an algorithmic technique that hashes similar input items into the same ""buckets"" with high probability.  It differs from conventional hashing techniques in that hash collisions are maximized, not minimized."	What is LSH
3468	The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer.	How do you determine the number of neurons in a neural network
770	If r is the number of possible character codes on an computer, and if table_size is a prime such that r % table_size equal 1, then hash function h(key) = key % table_size is simply the sum of the binary representation of the characters in the key mod table_size.	How is hash function calculated
7282	The batch size is a hyperparameter of gradient descent that controls the number of training samples to work through before the model's internal parameters are updated. The number of epochs is a hyperparameter of gradient descent that controls the number of complete passes through the training dataset.	What is batch size in gradient descent
1520	Introduction Nonparametric Test: Those procedures that test hypotheses that tests hypotheses that are not statements about population parameters are classified as nonparametric.  Distribution free procedure: Those procedures that make no assumption about the sampled population are called distribution free procedures.	What is the difference between the non parametric and distribution free methods
6486	"The outcome variable is also called the response or dependent variable, and the risk factors and confounders are called the predictors, or explanatory or independent variables. In regression analysis, the dependent variable is denoted ""Y"" and the independent variables are denoted by ""X""."	What are dependent and independent variables in linear regression
1184	“Risk” refers to the probability of occurrence of an event or outcome. Statistically, risk = chance of the outcome of interest/all possible outcomes. The term “odds” is often used instead of risk.	What does risk mean in statistics
7435	Sampling is done because you usually cannot gather data from the entire population. Even in relatively small populations, the data may be needed urgently, and including everyone in the population in your data collection may take too long.	Why is sample survey done
3676	Median filtering A median filter is a nonlinear filter in which each output sample is computed as the median value of the input samples under the window – that is, the result is the middle value after the input values have been sorted. Ordinarily, an odd number of taps is used.	Why median filter is non linear
3339	Lambda is defined as an asymmetrical measure of association that is suitable for use with nominal variables. It may range from 0.0 to 1.0. Lambda provides us with an indication of the strength of the relationship between independent and dependent variables.	What is the value of lambda in statistics
6614	Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE is most useful when large errors are particularly undesirable.	Why root mean squared error is preferred over mean squared error
2936	In statistics, omitted-variable bias (OVB) occurs when a statistical model leaves out one or more relevant variables. The bias results in the model attributing the effect of the missing variables to those that were included.	When you have an omitted variable problem
3084	Quantile plots directly display the quantiles of a set of values. The sample quantiles are plotted against the fraction of the sample they correspond to. There is no built-in quantile plot in R, but it is relatively simple to produce one. Quantile-quantile plots allow us to compare the quantiles of two sets of numbers.	How is a quantile quantile plot different from a quantile plot
1421	Non-response bias is a type of bias that occurs when people are unwilling or unable to respond to a survey due to a factor that makes them differ greatly from people who respond. The difference between non-respondents and respondents is usually an influencing factor for the lack of response.	What is non response bias in stats
1432	In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent).	What is an agent artificial intelligence
808	Attention is simply a vector, often the outputs of dense layer using softmax function.  However, attention partially fixes this problem. It allows machine translator to look over all the information the original sentence holds, then generate the proper word according to current word it works on and the context.	What is an attention layer
5801	Accuracy = TP+TN/TP+FP+FN+TN. Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.	What is accuracy and precision in machine learning
238	Impressive Applications of Deep Learning Natural language processing is not “solved“, but deep learning is required to get you to the state-of-the-art on many challenging problems in the field.	Is natural language processing deep learning
5962	Hierarchical Clustering	Which clustering algorithm uses Dendrogram
707	“And unlike what any one person can analyze, machine learning can take vast amounts of data over time and make predictions to improve the customer experience and provide real value to the end-user.”	What is machine learning good for
4546	A frequency table is a method of organizing raw data in a compact form by displaying a series of scores in ascending or descending order, together with their frequencies—the number of times each score occurs in the respective data set.	What is a frequency table in statistics
516	Covariance Matrix is a measure of how much two random variables gets change together.  The Covariance Matrix is also known as dispersion matrix and variance-covariance matrix. The covariance between two jointly distributed real-valued random variables X and Y with finite second moments is defined as.	What is covariance matrix example
768	Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used.	Which method is used for predicting continuous dependent variable
6311	The optimal number of clusters can be defined as follow: Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters. For each k, calculate the total within-cluster sum of square (wss).	How do you choose K for K means clustering
1038	Statisticians define two types of errors in hypothesis testing. Creatively, they call these errors Type I and Type II errors. Both types of error relate to incorrect conclusions about the null hypothesis. The table summarizes the four possible outcomes for a hypothesis test.	What are the two types of errors in hypothesis testing
5819	A ratio scale is a quantitative scale where there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales.	What is ratio scale of measurement
2912	Consider using a portfolio of technical tools, as well as operational practices such as internal “red teams,” or third-party audits. Third, engage in fact-based conversations around potential human biases.	What do we do about the biases in AI
473	Must-Know: How to evaluate a binary classifierTrue Positive Rate (TPR) or Hit Rate or Recall or Sensitivity = TP / (TP + FN)False Positive Rate(FPR) or False Alarm Rate = 1 - Specificity = 1 - (TN / (TN + FP))Accuracy = (TP + TN) / (TP + TN + FP + FN)Error Rate = 1 – accuracy or (FP + FN) / (TP + TN + FP + FN)Precision = TP / (TP + FP)More items	How do you evaluate binary classification
8458	An example of dimensionality reduction: email classification. Let's set up a specific example to illustrate how PCA works. Assume that you have a database of emails and you want to classify (using some machine learning numerical algorithm) each email as spam/not spam.	What is dimensionality reduction example
3927	This powerful technique is no longer constrained by the limits of human knowledge. Instead, the computer program accumulated thousands of years of human knowledge during a period of just a few days and learned to play Go from the strongest player in the world, AlphaGo.	Why was Alpha Go able to play go so well
688	Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called empirical risk minimization.	What is training machine learning
5874	Info-gap decision theory is a non-probabilistic decision theory that seeks to optimize robustness to failure – or opportuneness for windfall – under severe uncertainty, in particular applying sensitivity analysis of the stability radius type to perturbations in the value of a given estimate of the parameter of interest	What is the info gap decision theory applied in a power system
1027	Smaller MSE generally indicates a better estimate, at the data points in question. As others have said, MSE is the mean of the squared difference between your estimate and the data.  Smaller MSE generally indicates a better estimate, at the data points in question.	How is mean squared error MSE used to compare different estimators Is a larger or smaller MSE better
6008	The NLP Engine is the core component that interprets what users say at any given time and converts that language to structured inputs the system can process.  To interpret the user inputs, NLP engines, based on the business case, use either finite state automata models or deep learning methods.	What is a NLP engine
5077	Validity is important because it can help determine what types of tests to use, and help to make sure researchers are using methods that are not only ethical, and cost-effective, but also a method that truly measures the idea or constructs in question.	What is the purpose of measuring the validity of a test
753	Data Augmentation in play. A convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called invariance. More specifically, a CNN can be invariant to translation, viewpoint, size or illumination (Or a combination of the above).	What is data augmentation in CNN
6514	You can use reinforcement learning for classification problems but it won't be giving you any added benefit and instead slow down your convergence rate. Detailed answer: yes but it's an overkill.  So, if you possess labels, it would be a LOT more faster and easier to use regular supervised learning.	Can reinforcement learning be used for classification
6930	A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction.	How does a bagging classifier work
655	Most recent answer One way to compare the two different size data sets is to divide the large set into an N number of equal size sets. The comparison can be based on absolute sum of of difference. THis will measure how many sets from the Nset are in close match with the single 4 sample set.	How do you compare data with different sample sizes
1207	This result is known as Graham's law of diffusion after Thomas Graham (1805 to 1869), a Scottish chemist, who discovered it by observing effusion of gases through a thin plug of plaster of paris.  Calculate the relative rates of effusion of He(g) and O2(g) .	How was Graham's law discovered
2046	A discrete quantitative variable is one that can only take specific numeric values (rather than any value in an interval), but those numeric values have a clear quantitative interpretation. Examples of discrete quantitative variables are number of needle punctures, number of pregnancies and number of hospitalizations.	What is an example of discrete quantitative variable
4077	Training Set: this data set is used to adjust the weights on the neural network. Validation Set: this data set is used to minimize overfitting.  Testing Set: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network.	What is the difference between training set validation set and test set
1622	Importance sampling is a useful technique for investigating the properties of a distri- bution while only having samples drawn from a different (proposal) distribution.	Whats the advantage of importance sampling
3301	A Binomial Regression model can be used to predict the odds of an event.  The Logistic Regression model is a special case of the Binomial Regression model in the situation where the size of each group of explanatory variables in the data set is one.	Is binomial regression the same as logistic regression
3785	The output of an LSTM cell or layer of cells is called the hidden state. This is confusing, because each LSTM cell retains an internal state that is not output, called the cell state, or c.	What are hidden units in LSTM
3643	In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable.	What is the role of maximum likelihood estimation
690	The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic	How does back propagation work
204	Simple random sampling: By using the random number generator technique, the researcher draws a sample from the population called simple random sampling. Simple random samplings are of two types.  Cluster sampling: Cluster sampling occurs when a random sample is drawn from certain aggregational geographical groups.	Is cluster sampling random or non random
2317	Fractional scaling helps you to fully utilize your HiDPI monitors, high-resolution laptops by making your desktop not too small or not too big and keep things in balance. Although the resolution settings are there to help they sometimes are not feasible due to the operating system limitations.	What is fractional scaling ubuntu
3001	Latent class regression, where the purpose of the analysis is to identify segments that contain different parameters. This model is most commonly used for creating segments with choice modeling data. Model-based clustering, where a series of numeric, categorical or ranking variables are used to create segments.	What is the difference between latent class analysis and model based clustering
525	The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word. In word2vec, a distributed representation of a word is used.	What is the output of word2vec
406	Enneagram test results are very accurate for determining your enneagram type and the MBTI test results are quite accurate for determining your MBTI type. Neither is in competition with the other. That being said, it can be very interesting to have the results for both of these uniquely different typologies.	Which is more accurate MBTI and Enneagram
272	The result is that the coefficient estimates are unstable and difficult to interpret. Multicollinearity saps the statistical power of the analysis, can cause the coefficients to switch signs, and makes it more difficult to specify the correct model.	What are the effects of multicollinearity and when can I ignore them
1184	The law of averages is the commonly held belief that a particular outcome or event will over certain periods of time occur at a frequency that is similar to its probability. Depending on context or application it can be considered a valid common-sense observation or a misunderstanding of probability.	What is law of averages 1
5380	Memorization and generalization are both important for recommender systems. Wide linear models can effectively memorize sparse feature interactions using cross-product fea- ture transformations, while deep neural networks can gener- alize to previously unseen feature interactions through low- dimensional embeddings.	What are the benefits of wide linear models and deep neural models regarding wide and deep learning recommended systems
8432	2:055:17Suggested clip · 113 secondsWeighted Kappa in IBM SPSS Statistics - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do a weighted kappa in SPSS
1249	A more accurate model postulates that the relative growth rate P /P decreases when P approaches the carrying capacity K of the environment. The corre- sponding equation is the so called logistic differential equation: dP dt = kP ( 1 − P K ) .	What is the equation for logistic function
8594	Structural equation models are often used to assess unobservable 'latent' constructs. They often invoke a measurement model that defines latent variables using one or more observed variables, and a structural model that imputes relationships between latent variables.	When would you use a structural equation model
5990	While the current state-of-the-art method for federated learning, FedAvg (McMahan et al, 2017), has demonstrated empirical success, it does not fully address the underlying challenges associated with heterogeneity, and can diverge in practice.	Has federated learning been successful
1640	"To recap the differences between the two: Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned. Deep learning structures algorithms in layers to create an ""artificial neural network” that can learn and make intelligent decisions on its own."	What is the relationship between machine learning and deep learning
188	Topic Modeling refers to the process of dividing a corpus of documents in two:A list of the topics covered by the documents in the corpus.Several sets of documents from the corpus grouped by the topics they cover.	How do you do a topic model
1317	The coefficients in a linear-log model represent the estimated unit change in your dependent variable for a percentage change in your independent variable. The term on the right-hand-side is the percent change in X, and the term on the left-hand-side is the unit change in Y.	How do you interpret a linear log model
1235	Two events are dependent if the outcome of the first event affects the outcome of the second event, so that the probability is changed.	What makes an event dependent
2669	Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension.	What is meant by dimensionality reduction
2827	A q-value threshold of 0.05 yields a FDR of 5% among all features called significant. The q-value is the expected proportion of false positives among all features as or more extreme than the observed one.	What is FDR q value
2308	Subsampling reduces the image size by removing information all together. Usually when you subsample, you also interpolate or smooth the image so that you reduce aliasing.  Usually, the chrominance values are filtered then subsampled by 1/2 or even 1/4 of that of the intensity.	What is image subsampling
1415	A fundamental difference between mean and median is that the mean is much more sensitive to extreme values than the median. That is, one or two extreme values can change the mean a lot but do not change the the median very much. Thus, the median is more robust (less sensitive to outliers in the data) than the mean.	Why is median more robust than mean
954	Logistic regression, also called a logit model, is used to model dichotomous outcome variables. In the logit model the log odds of the outcome is modeled as a linear combination of the predictor variables.	Is logit a linear model
534	The bits of linguistic information that enter into one person's mind, from another, cause people to entertain a new thought with profound effects on his world knowledge, inferencing, and subsequent behavior. Language neither creates nor distorts conceptual life. Thought comes first, while language is an expression.	What is the relationship between language and thought
2507	Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron's input is relevant for the model's prediction.	What is activation function of neuron
541	"The idea behind importance sampling is that certain values of the input random variables in a simulation have more impact on the parameter being estimated than others. If these ""important"" values are emphasized by sampling more frequently, then the estimator variance can be reduced."	Why is sampling important
2282	Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).	How do you combine two ML models
267	Calculate the derivative of g(x)=ln(x2+1). Solution: To use the chain rule for this problem, we need to use the fact that the derivative of ln(z) is 1/z. Then, by the chain rule, the derivative of g is g′(x)=ddxln(x2+1)=1x2+1(2x)=2xx2+1.	How do you solve chain rule problems
2402	Interaction effects occur when the effect of one variable depends on the value of another variable. Interaction effects are common in regression analysis, ANOVA, and designed experiments.  Interaction effects indicate that a third variable influences the relationship between an independent and dependent variable.	What does it mean to have an interaction effect
4157	It depends on the data you want and the project you're doing. You could use even your twitter data for sentiment analysis. Request your archive in twitter -> download -> analyse sentiment through supervised learning techniques.	Where can one find a training set for sentiment analysis
3848	The Least Squares Regression Line is the line that makes the vertical distance from the data points to the regression line as small as possible. It's called a “least squares” because the best line of fit is one that minimizes the variance (the sum of squares of the errors).	Why linear regression is sometimes referred to as least squares
7590	Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them.	What is data augmentation in deep learning
7180	The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.	What is the bag of words algorithm
1516	In other words, discriminative models are used to specify outputs based on inputs (by models such as Logistic regression, Neural networks and Random forests), while generative models generate both inputs and outputs (for example, by Hidden Markov model, Bayesian Networks and Gaussian mixture model).	Is Random Forest discriminative
8553	Prior probability, in Bayesian statistical inference, is the probability of an event before new data is collected.	What is prior probability in machine learning
709	By reversing the words in the source sentence, the average distance between corresponding words in the source and target language is unchanged. However, the first few words in the source language are now very close to the first few words in the target language, so the problem's minimal time lag is greatly reduced.	In the paper Sequence to Sequence Learning with Neural Networks why does reversing the source sentence allow better performance on longer sentences
7197	Additive interaction means the effect of two chemicals is equal to the sum of the effect of the two chemicals taken separately.  Synergistic interaction means that the effect of two chemicals taken together is greater than the sum of their separate effect at the same doses.	What is the difference between additive and synergistic effect
4027	Bivariate analysis means the analysis of bivariate data. It is one of the simplest forms of statistical analysis, used to find out if there is a relationship between two sets of values. It usually involves the variables X and Y. Univariate analysis is the analysis of one (“uni”) variable.	When will we use bivariate analysis
3524	Typically, a regression analysis is done for one of two purposes: In order to predict the value of the dependent variable for individuals for whom some information concerning the explanatory variables is available, or in order to estimate the effect of some explanatory variable on the dependent variable.	What is the purpose of a regression
8220	Genetic algorithm is used in optimum design because of its efficient optimum capabilities. The genetic algorithm is an efficient tool in the field of engineering education (Bütün, 2005).	Which method is used for optimum design
8280	By using some mathematics it can be shown that there are a few conditions that we need to use a normal approximation to the binomial distribution. The number of observations n must be large enough, and the value of p so that both np and n(1 - p) are greater than or equal to 10.	What conditions need to be satisfied to use the normal approximation for proportions
3880	A Type I is a false positive where a true null hypothesis that there is nothing going on is rejected. A Type II error is a false negative, where a false null hypothesis is not rejected – something is going on – but we decide to ignore it.	What are the type I and type II decision errors costs
2059	Semantic similarity: this scores words based on how similar they are, even if they are not exact matches. It borrows techniques from Natural Language Processing (NLP), such as word embeddings.	What is semantic similarity in NLP
380	Data bias in machine learning is a type of error in which certain elements of a dataset are more heavily weighted and/or represented than others. A biased dataset does not accurately represent a model's use case, resulting in skewed outcomes, low accuracy levels, and analytical errors.	What is bias in machine learning
2630	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution.	Why is the standard normal distribution important in statistical analysis
5493	Perhaps the biggest problem with using the historical LCGs for generating random numbers is that their periods are too short, even if they manage to hit the maximal period. Given the scale of simulations being conducted today, even a period of 232 would likely be too short to appear sufficiently random.	What is the problem with using statistical random number generators
5445	In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time.	What is the difference between stationary and non stationary time series
2932	Discriminant validity (or divergent validity) tests that constructs that should have no relationship do, in fact, not have any relationship. If a research program is shown to possess both of these types of validity, it can also be regarded as having excellent construct validity.	What is divergent validity in research
5191	The basic premise of transfer learning is simple: take a model trained on a large dataset and transfer its knowledge to a smaller dataset. For object recognition with a CNN, we freeze the early convolutional layers of the network and only train the last few layers which make a prediction.	What is transfer learning in CNN
1016	The Most Simple Ways to Build an Interactive Decision TreeLog in to your Zingtree account, go to My Trees and select Create New Tree.  After naming your decision tree, choosing your ideal display style and providing a description, just click the Create Tree button to move on to the next step.More items•	How do you make an interactive decision tree
501	The basic idea behind a neural network is to simulate (copy in a simplified but reasonably faithful way) lots of densely interconnected brain cells inside a computer so you can get it to learn things, recognize patterns, and make decisions in a humanlike way.	How does a neural network function
841	Another cause of skewness is start-up effects. For example, if a procedure initially has a lot of successes during a long start-up period, this could create a positive skew on the data. (On the opposite hand, a start-up period with several initial failures can negatively skew data.)	What causes positive skewness
6247	When they are positively skewed (long right tail) taking logs can sometimes help. Sometimes logs are taken of the dependent variable, sometimes of one or more independent variables. Substantively, sometimes the meaning of a change in a variable is more multiplicative than additive. For example, income.	What does log of a variable mean
5229	If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier.	What is linear and nonlinear classifier
1183	The mean is also to the left of the peak. A right-skewed distribution has a long right tail.  Next, you'll see a fair amount of negatively skewed distributions. For example, household income in the U.S. is negatively skewed with a very long left tail.	What is an example of skewed data
7567	DeepDream is an experiment that visualizes the patterns learned by a neural network. Similar to when a child watches clouds and tries to interpret random shapes, DeepDream over-interprets and enhances the patterns it sees in an image.	What is deep dream in deep learning
2688	It works in part because it doesn't require unbiased estimators; While least squares produces unbiased estimates, variances can be so large that they may be wholly inaccurate. Ridge regression adds just enough bias to make the estimates reasonably reliable approximations to true population values.	Why does ridge regression improve over Least Squares explain
6736	The sensitivity of the test reflects the probability that the screening test will be positive among those who are diseased. In contrast, the specificity of the test reflects the probability that the screening test will be negative among those who, in fact, do not have the disease.	How do you interpret sensitivity and specificity
507	It is named after Andrey Kolmogorov and Nikolai Smirnov. The Kolmogorov–Smirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples.	What is a KS score
6257	When the standard deviation or the mean change, something unusual is happening. To detect such changes, for each upcoming point “p” we create of window from “p” to “p-100″. Then, we calculate the standard deviation and mean of this window. If it changes too much, an anomaly has been detected.	How would you find an anomaly in a distribution
2431	RNN is recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation.  Unlike feed-forward neural networks, RNNs can use their internal state (memory) to process sequences of inputs.	Which is a type of recurrent neural network
5884	One standard deviation or one-sigma, plotted either above or below the average value, includes 68 percent of all data points. Two-sigma includes 95 percent and three-sigma includes 99.7 percent. Higher sigma values mean that the discovery is less and less likely to be accidentally a mistake or 'random chance'.	What is sigma value
2684	Classification table. The classification table is another method to evaluate the predictive accuracy of the logistic regression model. In this table the observed values for the dependent outcome and the predicted values (at a user defined cut-off value, for example p=0.50) are cross-classified.	What is the classification table used for in logistic regression
7052	Deep learning really shines when it comes to complex tasks, which often require dealing with lots of unstructured data, such as image classification, natural language processing, or speech recognition, among others.	Does deep learning require a lot of data
1245	7 Techniques to Handle Imbalanced DataUse the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	How do you deal with imbalanced datasets
5366	Because data science is a broad term for multiple disciplines, machine learning fits within data science. Machine learning uses various techniques, such as regression and supervised clustering. On the other hand, the data' in data science may or may not evolve from a machine or a mechanical process.	How is data science and machine learning related
2708	A decision tree is a simple representation for classifying examples. Decision tree learning is one of the most successful techniques for supervised classification learning.  A decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature.	How Decision trees are used in learning
6502	Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data.	What is unsupervised learning algorithm
539	In simple terms, deep learning is when ANNs learn from large amounts of data. Similar to how humans learn from experience, a deep learning algorithm performs a task repeatedly, each time tweaking it slightly to improve the outcome.	How does a deep neural network learn
41	Weights control the signal (or the strength of the connection) between two neurons. In other words, a weight decides how much influence the input will have on the output. Biases, which are constant, are an additional input into the next layer that will always have the value of 1.	How do neural networks choose the weights and biases
842	It is one of several methods statisticians and researchers use to extract a sample from a larger population; other methods include stratified random sampling and probability sampling. The advantages of a simple random sample include its ease of use and its accurate representation of the larger population.	What are the advantages of simple random sampling
6910	Definition. Univariate analyses are used extensively in quality of life research. Univariate analysis is defined as analysis carried out on only one (“uni”) variable (“variate”) to summarize or describe the variable (Babbie, 2007; Trochim, 2006).	What do you mean by univariate analysis
1036	Interaction effects occur when the effect of one variable depends on the value of another variable. Interaction effects are common in regression analysis, ANOVA, and designed experiments.  Interaction effects indicate that a third variable influences the relationship between an independent and dependent variable.	What is the interaction effect in Anova
2544	Vanishing Gradient problem arises while training an Artificial Neural Network. This mainly occurs when the network parameters and hyperparameters are not properly set. Parameters could be weights and biases while hyperparameters could be learning rate, the number of epochs, the number of batches, etc.	Why vanishing gradient occurs in RNN
179	For example, a perfect precision and recall score would result in a perfect F-Measure score:F-Measure = (2 * Precision * Recall) / (Precision + Recall)F-Measure = (2 * 1.0 * 1.0) / (1.0 + 1.0)F-Measure = (2 * 1.0) / 2.0.F-Measure = 1.0.	How do you measure precision and recall
6971	Logistic regression is used to obtain odds ratio in the presence of more than one explanatory variable.  The result is the impact of each variable on the odds ratio of the observed event of interest. The main advantage is to avoid confounding effects by analyzing the association of all variables together.	What does a logistic regression tell you
2126	The geometric mean must be used when working with percentages, which are derived from values, while the standard arithmetic mean works with the values themselves. The harmonic mean is best used for fractions such as rates or multiples.	What is the difference between arithmetic mean and harmonic mean
8649	The Slovin's Formula is given as follows: n = N/(1+Ne2), where n is the sample size, N is the population size and e is the margin of error to be decided by the researcher.	How do you calculate population sample size
2125	Definition. The class intervals are the subsets into which the data is grouped. The width of the class intervals will be a compromise between having intervals short enough so that not all of the observations fall in the same interval, but long enough so that you do not end up with only one observation per interval.	What is class interval in statistics
1269	In Regression Clustering (RC), K (>1) regression functions are applied to the dataset simultaneously which guide the clustering of the dataset into K subsets each with a simpler distribution matching its guiding function. Each function is regressed on its own subset of data with a much smaller residue error.	What is clustering in regression
2650	Binary Variables A simple version of a categorical variable is called a binary variable. This type of variable lists two distinct, mutually exclusive choices. True-or-false and yes-or-no questions are examples of binary variables.	What type of variable is binary
2663	Data Analytics is a Bigger picture of the same thing which is referred as Machine learning. Like Data Analytics has various categories based on the Data used, similarly, Machine Learning, expresses the way one machine learns a code or work in supervised,unsupervised,semi supervised and reinforcement manner.	Is machine learning and data analytics same
3111	"edit: More explanation - sigma basically controls how ""fat"" your kernel function is going to be; higher sigma values blur over a wider radius. Since you're working with images, bigger sigma also forces you to use a larger kernel matrix to capture enough of the function's energy."	What is Sigma in Gaussian filter
1334	The conversion of a frequency distribution to a probability distribution is also called an adjusted histogram. This is true for continuous random variables. To convert a frequency distribution to a probability distribution, divide area of the bar or interval of x by the total area of all the Bars.	How do you convert a histogram to a probability distribution curve
4930	Loss curves are a standard actuarial technique for helping insurance companies assess the amount of reserve capital they need to keep on hand to cover claims from a line of business. Claims made and reported for a given accounting period are tracked seperately over time.	What is a loss curve
4732	the state of being likely or probable; probability. a probability or chance of something: There is a strong likelihood of his being elected.	What does likelihood mean
1410	(When does a random variable have a Poisson YouTubeStart of suggested clipEnd of suggested clip	How do you find the Poisson distribution
727	CRF is a discriminant model. MEMM is not a generative model, but a model with finite states based on state classification. HMM and MEMM are a directed graph, while CRF is an undirected graph. HMM directly models the transition probability and the phenotype probability, and calculates the probability of co-occurrence.	What is the major difference between CRF Conditional Random Field and HMM hidden Markov model
7088	In the frequency or Fourier domain, the value and location are represented by sinusoidal relationships that depend upon the frequency of a pixel occurring within an image. In this domain, pixel location is represented by its x- and y-frequencies and its value is represented by an amplitude.	How image is represent in frequency domain
7117	Binomial Approximation The normal distribution can be used as an approximation to the binomial distribution, under certain circumstances, namely: If X ~ B(n, p) and if n is large and/or p is close to ½, then X is approximately N(np, npq)	What are the conditions for using the normal approximation to the binomial distribution
2242	A Sampling unit is one of the units selected for the purpose of sampling. Each unit being regarded as individual and indivisible when the selection is made. CONTEXT: Many times the Sampling frame and the Sampling unit are derived from Administrative data.	What is sampling unit and sampling frame
3830	After you collect the data, one way to check whether your data are random is to use a runs test to look for a pattern in your data over time. To perform a runs test in Minitab, choose Stat > Nonparametrics > Runs Test. There are also other graphs that can identify whether a sample is random.	How do you know if data is random
1046	In the Stepwise regression technique, we start fitting the model with each individual predictor and see which one has the lowest p-value. Then pick that variable and then fit the model using two variable one which we already selected in the previous step and taking one by one all remaining ones.	How do you select a regression feature
1260	"In this paper, we propose the ""adversarial autoencoder"" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior"	What is adversarial Autoencoder
4451	Artificial intelligence is a technology which enables a machine to simulate human behavior. Machine learning is a subset of AI which allows a machine to automatically learn from past data without programming explicitly. The goal of AI is to make a smart computer system like humans to solve complex problems.	What is the difference between AI and IT
101	This tutorial is divided into four parts; they are:Regression Dataset.Numerical Feature Selection. Correlation Feature Selection. Mutual Information Feature Selection.Modeling With Selected Features. Model Built Using All Features. Model Built Using Correlation Features.  Tune the Number of Selected Features.	How do you perform feature selection for regression data
8217	Last Updated on Septem. Multioutput regression are regression problems that involve predicting two or more numerical values given an input example. An example might be to predict a coordinate given an input, e.g. predicting x and y values.	What is multi output regression
1954	Introduction Statistical discrete processes – for example, the number of accidents per driver, the number of insects per leaf in an orchard, the number of thunderstorms per year, the number of earthquakes per year, the number of patients visit emergency room in a certain hospital per day - often occur in real life.	What are uses of discrete distributions in real life
3950	Try to see the difference between an estimator and an estimate. An estimator is a random variable and an estimate is a number (that is the computed value of the estimator).  Similarly, the sample median would be a natural point estimator for the population median.	What is the difference between estimator and estimate
6809	Multivariate analysis is conceptualized by tradition as the statistical study of experiments in which multiple measurements are made on each experimental unit and for which the relationship among multivariate measurements and their structure are important to the experiment's understanding.	What is meant by multivariate analysis
1139	Object is a copy of the class. Instance is a variable that holds the memory address of the object. You can also have multiple objects of the same class and then multiple instances of each of those objects. In these cases, each object's set of instances are equivalent in value, but the instances between objects are not.	What is the difference between object and instance
676	There are two stages to prediction. The first stage is training the model—this is where the tree is built, tested, and optimized by using an existing collection of data. In the second stage, you actually use the model to predict an unknown outcome.	How do you make predictions in decision trees
4866	Joint probability is calculated by multiplying the probability of event A, expressed as P(A), by the probability of event B, expressed as P(B). For example, suppose a statistician wishes to know the probability that the number five will occur twice when two dice are rolled at the same time.	How do you calculate joint probability
8487	"It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action. Some academic sources reserve the term ""strong AI"" for machines that can experience consciousness."	Does artificial general intelligence exist
6415	5.2 RIPPER. The next algorithm we used, RIPPER [3], is an inductive rule learner.  This algorithm used libBFD information as features. RIPPER is a rule-based learner that builds a set of rules that identify the classes while minimizing the amount of error.	What is Ripper algorithm
1402	Today, neural networks are used for solving many business problems such as sales forecasting, customer research, data validation, and risk management. For example, at Statsbot we apply neural networks for time-series predictions, anomaly detection in data, and natural language understanding.	What are the types of problems in which artificial neural network can be applied
4016	7 Techniques to Handle Imbalanced DataUse the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	How do you deal with imbalanced dataset in classification
5669	Linear programming is a special case of mathematical programming (mathematical optimization). Now linear programming is a subset of machine learning known as supervised learning. In a supervised learning, the system knows the patterns and the pattern is well defined based on previous data and information.	Is Machine Learning linear programming
3409	Lasso regression stands for Least Absolute Shrinkage and Selection Operator.  The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero.	What is the difference between Lasso and Ridge regression
3464	A decision tree is a non-linear classifier. If your dataset contains consistent samples, namely you don't have the same input features and contradictory labels, decision trees can classify the data entirely and overfit it.	Why is decision tree a non linear classifier
2432	"The ReLU activation solves the problem of vanishing gradient that is due to sigmoid-like non-linearities (the gradient vanishes because of the flat regions of the sigmoid). The other kind of ""vanishing"" gradient seems to be related to the depth of the network (e.g. see this for example)."	How does Resnet solve vanishing gradient
1409	AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine multiple “weak classifiers” into a single “strong classifier”.  → AdaBoost algorithms can be used for both classification and regression problem.	Can AdaBoost be used for regression
7921	Compare the P-value to the α significance level stated earlier. If it is less than α, reject the null hypothesis. If the result is greater than α, fail to reject the null hypothesis. If you reject the null hypothesis, this implies that your alternative hypothesis is correct, and that the data is significant.	How do you interpret t test results
6657	Logistic regression models are a great tool for analysing binary and categorical data, allowing you to perform a contextual analysis to understand the relationships between the variables, test for differences, estimate effects, make predictions, and plan for future scenarios.	Can logistic regression be used as a means of predicting categorical outcomes
118	Categorical data works well with Decision Trees, while continuous data work well with Logistic Regression. If your data is categorical, then Logistic Regression cannot handle pure categorical data (string format).  Therefore, if you have lots of categorical data, go with a Decision Tree.	Which is better logistic regression or decision tree
3415	The low-pass filter has a gain response with a frequency range from zero frequency (DC) to ωC. Any input that has a frequency below the cutoff frequency ωC gets a pass, and anything above it gets attenuated or rejected. The gain approaches zero as frequency increases to infinity.	What is the frequency response of low pass filter
1967	In probability theory, a continuity correction is an adjustment that is made when a discrete distribution is approximated by a continuous distribution.	What is continuity correction probability
284	The seeds of modern AI were planted by classical philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning.	How did artificial intelligence develop
354	You should put it after the non-linearity (eg. relu layer). If you are using dropout remember to use it before.	Where should I put batch normalization
1368	Given two random variables X and Y, the correlation is scale and location invariant in the sense that cor(X,Y)=cor(XT,YT), if XT=a+bX, and YT=c+dY, and b and d have the same sign (either both positive or both negative).	Are correlation scales invariant
383	Technically, the probability density of variable X , means the probability per unit increment of X . The units of probability density are the reciprocal of the units of X — if the units of X are dollars, the units of probability density are probability per dollar increment.	What are the units of a probability density function
4659	LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate's activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.	How Lstm networks solve the problem of vanishing gradients
63	The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities .	What is the difference between Bayes theorem and conditional probability
2378	The Formula for the Slope For paired data (x,y) we denote the standard deviation of the x data by sx and the standard deviation of the y data by sy. The formula for the slope a of the regression line is: a = r(sy/sx)	How do you find the slope of the regression line in R
508	Logistic regression is a statistical analysis method used to predict a data value based on prior observations of a data set.  Based on historical data about earlier outcomes involving the same input criteria, it then scores new cases on their probability of falling into a particular outcome category.	What is logistic regression in data science
6523	The loss is calculated on training and validation and its interpretation is how well the model is doing for these two sets. Unlike accuracy, a loss is not a percentage. It is a sum of the errors made for each example in training or validation sets.	What is validation loss
8444	The function scipy. linalg. eig computes eigenvalues and eigenvectors of a square matrix .	What is the function to get both eigenvalues and eigenvectors of a matrix
208	Simply put, a z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it's a measure of how many standard deviations below or above the population mean a raw score is. A z-score can be placed on a normal distribution curve.	What is the relationship between Z scores and the normal distribution
3686	Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are. Instead, they need to be recoded into a series of variables which can then be entered into the regression model.	How are categorical variables used in regression
4051	Definition: An image processing method that creates a bitonal (aka binary) image based on setting a threshold value on the pixel intensity of the original image.  The thresholding process is sometimes described as separating an image into foreground values (black) and background values (white).	What is threshold value in image processing
4564	Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.	What is Adam optimizer
5614	A loss function is a measure of how good a prediction model does in terms of being able to predict the expected outcome. A most commonly used method of finding the minimum point of function is “gradient descent”.  Loss functions can be broadly categorized into 2 types: Classification and Regression Loss.	What is loss function in regression
5938	Definition. In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived.  Model validation is carried out after model training.	What is validation machine learning
416	Batch normalization is a layer that allows every layer of the network to do learning more independently. It is used to normalize the output of the previous layers. The activations scale the input layer in normalization.	What is batch normalization CNN
921	In image processing, a Gaussian blur (also known as Gaussian smoothing) is the result of blurring an image by a Gaussian function (named after mathematician and scientist Carl Friedrich Gauss). It is a widely used effect in graphics software, typically to reduce image noise and reduce detail.	What is Gaussian blur in image processing
5359	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What are the differences between supervised and unsupervised classification
718	We propose that especially in the context of introducing automated decision aids to explicitly reduce human error, people become primed to use decision aids in biased ways. Rather than necessarily leading to fewer errors, automated decision aids may simply lead to di!erent kinds or classes of errors.	Does automation bias decision making
8030	Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class. For multiclass problem you will need to reduce it into multiple binary classification problems. Random Forest works well with a mixture of numerical and categorical features.	When would you use random forests vs SVM and why
778	In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function ( ) that expresses how the shape of one is modified by the other. The term convolution refers to both the result function and to the process of computing it.	What is the convolution of two functions
3421	In statistics, the p-value is the probability of obtaining results at least as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is correct.  A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.	What is p value in t test
223	load_data function Loads the MNIST dataset. This is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. More info can be found at the MNIST homepage.	What is Mnist Load_data ()
4779	Results of a study can be made more accurate by controlling for the variation in the covariate. So, a covariate is in fact, a type of control variable.  A control variable is a nominal variable (not continuous) and although it has more than one value, the values are categorical and not infinite.	Is a covariate a control variable
527	The Hidden Markov Model (HMM) is a relatively simple way to model sequential data. A hidden Markov model implies that the Markov Model underlying the data is hidden or unknown to you. More specifically, you only know observational data and not information about the states.	How does Hidden Markov work
8154	Creating A Target VariableFrom the menu: Click View > User Variables. The Variables dialog box appears. Click Add Target.From the Target pane: Right-click a linked field and select Edit Lookup Criteria. The Edit Lookup Criteria for the selected field appears. Click Edit Lookup Formula. The Edit Formula for the selected field appears.	How do you create a target variable
803	Bootstrapping is a type of resampling where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample.	How do you explain bootstrapping
5791	A pair of computer scientists have created a neural network that can self-replicate. “Self-replication is a key aspect of biological life that has been largely overlooked in Artificial Intelligence systems,” they argue in a paper popped onto arXiv this month.	Can Artificial Intelligence reproduce
246	In artificial intelligence and operations research, constraint satisfaction is the process of finding a solution to a set of constraints that impose conditions that the variables must satisfy.  Constraint propagation methods are also used in conjunction with search to make a given problem simpler to solve.	What is constraint satisfaction problem in AI
5370	Specificity (True negative rate) Specificity (SP) is calculated as the number of correct negative predictions divided by the total number of negatives. It is also called true negative rate (TNR). The best specificity is 1.0, whereas the worst is 0.0.	How do you find the true negative from a confusion matrix
1333	How to Calculate a Confusion MatrixStep 1) First, you need to test dataset with its expected outcome values.Step 2) Predict all the rows in the test dataset.Step 3) Calculate the expected predictions and outcomes:	How do you analyze a confusion matrix
5308	Use Simple Random Sampling One of the most effective methods that can be used by researchers to avoid sampling bias is simple random sampling, in which samples are chosen strictly by chance. This provides equal odds for every member of the population to be chosen as a participant in the study at hand.	How do you avoid bias in random sampling
2997	Because the standard normal distribution is used to calculate critical values for the test, this test is often called the one-sample z-test.	Why is the 1 sample z test in statistics called the 1 sample Z test emphasis on the Z
3769	The Sobel operator, sometimes called the Sobel–Feldman operator or Sobel filter, is used in image processing and computer vision, particularly within edge detection algorithms where it creates an image emphasising edges.	What is Sobel operator in image processing
2072	How TensorFlow works. TensorFlow allows developers to create dataflow graphs—structures that describe how data moves through a graph, or a series of processing nodes. Each node in the graph represents a mathematical operation, and each connection or edge between nodes is a multidimensional data array, or tensor.	What is TensorFlow and how it works
3466	Tensorflow is the most popular and apparently best Deep Learning Framework out there.  Tensorflow can be used to achieve all of these applications. The reason for its popularity is the ease with which developers can build and deploy applications.	Is TensorFlow good for deep learning
2136	Logarithm, the exponent or power to which a base must be raised to yield a given number. Expressed mathematically, x is the logarithm of n to the base b if bx = n, in which case one writes x = logb n. For example, 23 = 8; therefore, 3 is the logarithm of 8 to base 2, or 3 = log2 8.	What is the concept of logarithm
1352	1 Answer. In word2vec, you train to find word vectors and then run similarity queries between words. In doc2vec, you tag your text and you also get tag vectors.  If two authors generally use the same words then their vector will be closer.	What is the difference between word2vec and Doc2Vec
4039	A statistical hypothesis is a formal claim about a state of nature structured within the framework of a statistical model. For example, one could claim that the median time to failure from (acce]erated) electromigration of the chip population described in Section 6.1.	What is a statistical hypothesis example
3711	Normalization is a systematic approach of decomposing tables to eliminate data redundancy(repetition) and undesirable characteristics like Insertion, Update and Deletion Anomalies. It is a multi-step process that puts data into tabular form, removing duplicated data from the relation tables.	What is normalization explain in detail
793	Probability RulesEvery probability is between zero and one. In other words, if A is an event, then 0≤P(A)≤1.The sum of the probabilities of all of the outcomes is one. In other words, if all of the outcomes in the sample space are denoted by Ai, then ∑Ai=1.Impossible events have probability zero.  Certain events have probability one.	What are the probability rules
5851	A linear regression model attempts to explain the relationship between two or more variables using a straight line. Consider the data obtained from a chemical process where the yield of the process is thought to be related to the reaction temperature (see the table below).	What is a linear regression test
585	Conclusion – Standard Deviation vs Mean Standard deviation is the deviation from the mean, and a standard deviation is nothing but the square root of the variance. Mean is an average of all set of data available with an investor or company.	What is the difference between mean deviation and standard deviation
102	Anyhow, you could start by reading Introduction to Artificial Neural Networks by Jacek M. Zurada. It's a very good and tells you all you need to know about neural networks. Try to follow the Machine Learning course offered by Stanford on Coursera.	How do I start learning neural networks
241	Active learning is an approach to instruction that involves actively engaging students with the course material through discussions, problem solving, case studies, role plays and other methods.	What is meant by active learning
3288	Gaussian smoothing filters are commonly used to reduce noise.  Gaussian filters are generally isotropic, that is, they have the same standard deviation along both dimensions. An image can be filtered by an isotropic Gaussian filter by specifying a scalar value for sigma .	What is Gaussian filter Matlab
2562	In your case, with three groups, you'd run ANOVA. If you need to compare the 5-point scales one at a time, then non-parametric statistics are more appropriate. To compare two groups use the Mann-Whitney U test. To compare three or more groups use the Kruskal–Wallis H test.	Which statistical test should I use to compare 3 ordinal variables
8597	The odds ratio tells us how much higher the odds of exposure are among case-patients than among controls. An odds ratio of • 1.0 (or close to 1.0) indicates that the odds of exposure among case-patients are the same as, or similar to, the odds of exposure among controls. The exposure is not associated with the disease.	What does an odds ratio of 1.0 mean
3052	When we run studies we want to be confident in the results from our sample. Confidence intervals show us the likely range of values of our population mean. When we calculate the mean we just have one estimate of our metric; confidence intervals give us richer data and show the likely values of the true population mean.	How are confidence intervals used in the real world
1407	Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.	What's the difference between a prior probability and a posterior probability
6100	OLS (linear regression, linear model) assumes normally distributed residuals.  Ordinary least squares assumes things like equal variance of the noise at every x location. Generalized least squares does not assume a diagonal co-variance matrix.	Regression statistics What is the difference between Ordinary least square and generalized least squares
8643	A Classification and Regression Tree(CART) is a predictive algorithm used in machine learning. It explains how a target variable's values can be predicted based on other values. It is a decision tree where each fork is a split in a predictor variable and each node at the end has a prediction for the target variable.	What is a classification and regression tree CART
6592	If you want to control for the effects of some variables on some dependent variable, you just include them into the model. Say, you make a regression with a dependent variable y and independent variable x. You think that z has also influence on y too and you want to control for this influence.	How do you control for variables in regression
2152	5 Ways to Avoid Being Fooled By Statistics.  Do A Little Bit of Math and apply Common Sense.  Always Look for the Source and check the authority of the source.  Question if the statistics are biased or statistically insignificant.  Question if the statistics are skewed purposely or Misinterpreted.More items•	How can we avoid misleading statistics
745	The input() method reads a line from the input (usually from the user), converts the line into a string by removing the trailing newline, and returns it.	What is the purpose of the input () function
1191	In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways.  The term concept refers to the quantity to be predicted.	What is Concept drift in machine learning
250	A 95% confidence interval for βi has two equivalent definitions: The interval is the set of values for which a hypothesis test to the level of 5% cannot be rejected. The interval has a probability of 95% to contain the true value of βi .	What is 95 confidence interval in regression
5789	Big data is a big deal. From reducing their costs and making better decisions, to creating products and services that are in demand by customers, businesses will increasingly benefit by using big-data analytics.	What's the big deal about Big Data
8066	Add More Layers: If you have a complex dataset, you should utilize the power of deep neural networks and smash on some more layers to your architecture. These additional layers will allow your network to learn a more complex classification function that may improve your classification performance. Add more layers! III.	How can you improve the classification of an image
1527	Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.	What is clustering and how it works
2760	The Cramér-Rao Inequality provides a lower bound for the variance of an unbiased estimator of a parameter. It allows us to conclude that an unbiased estimator is a minimum variance unbiased estimator for a parameter.	What is the significance of the Cramer Rao inequality
4207	The F-statistic is the test statistic for F-tests. In general, an F-statistic is a ratio of two quantities that are expected to be roughly equal under the null hypothesis, which produces an F-statistic of approximately 1.  In order to reject the null hypothesis that the group means are equal, we need a high F-value.	What does an F statistic tell you
1838	AI is a bigger concept to create intelligent machines that can simulate human thinking capability and behavior, whereas, machine learning is an application or subset of AI that allows machines to learn from data without being programmed explicitly.	What is the difference between machine learning
4774	Cluster analysis can be a powerful data-mining tool for any organisation that needs to identify discrete groups of customers, sales transactions, or other types of behaviors and things. For example, insurance providers use cluster analysis to detect fraudulent claims, and banks use it for credit scoring.	Why do we need cluster analysis
582	In image processing, a Gaussian blur (also known as Gaussian smoothing) is the result of blurring an image by a Gaussian function (named after mathematician and scientist Carl Friedrich Gauss). It is a widely used effect in graphics software, typically to reduce image noise and reduce detail.	Why do we use Gaussian blur
3249	Amortized VI is the idea that instead of optimizing a set of free parameters, we can introduce a parameterized function that maps from observation space to the parameters of the approximate posterior distribution.	What is amortized variational inference
7938	1 degree	How many degrees of freedom does a chi square test have
7743	An invertible matrix is a square matrix that has an inverse. We say that a square matrix is invertible if and only if the determinant is not equal to zero. In other words, a 2 x 2 matrix is only invertible if the determinant of the matrix is not 0.	What makes a matrix invertible
656	A hierarchical linear regression is a special form of a multiple linear regression analysis in which more variables are added to the model in separate steps called “blocks.” This is often done to statistically “control” for certain variables, to see whether adding variables significantly improves a model's ability to	What is a hierarchical regression analysis
3091	A simple definition of a sampling frame is the set of source materials from which the sample is selected. The definition also encompasses the purpose of sampling frames, which is to provide a means for choosing the particular members of the target population that are to be interviewed in the survey.	What is the purpose of a sampling frame
178	List of Common Machine Learning AlgorithmsLinear Regression.Logistic Regression.Decision Tree.SVM.Naive Bayes.kNN.K-Means.Random Forest.More items•	Which of the following are machine learning algorithms
12	The standard deviation is simply the square root of the variance.  The average deviation, also called the mean absolute deviation , is another measure of variability. However, average deviation utilizes absolute values instead of squares to circumvent the issue of negative differences between data and the mean.	What is standard deviation and mean deviation
791	The sample variance is not always smaller than the population variance.	Is sample variance always smaller than population variance
2171	The Gamma distribution can be thought of as a generalization of the Chi-square distribution. If a random variable has a Chi-square distribution with degrees of freedom and is a strictly positive constant, then the random variable defined as has a Gamma distribution with parameters and .	What is a gamma random variable
2657	The formula for calculating a z-score is is z = (x-μ)/σ, where x is the raw score, μ is the population mean, and σ is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation. Figure 2.	How do you find the z score with the mean and standard deviation
323	The degree of freedom defines as the capability of a body to move. Consider a rectangular box, in space the box is capable of moving in twelve different directions (six rotational and six axial). Each direction of movement is counted as one degree of freedom. i.e. a body in space has twelve degree of freedom.	What are the 12 degrees of freedom
6697	The presence of serial correlation can be detected by the Durbin-Watson test and by plotting the residuals against their lags. The subscript t represents the time period.	How do you determine serial correlation
6600	In my opinion the LVM partition is more usefull cause then after installation you can later change partition sizes and number of partitions easily. In standard partition also you can do resizing, but total number of physical partitions are limited to 4. With LVM you have much greater flexibility.	What is the difference between LVM and standard partition
1422	No, the same values are reported. A researcher computes a one-sample z test in two studies. Both studies used the same alpha level, placed the rejection region in both tails, and measured the same sample mean.	Is a one sample z test reported differently for one tailed and two tailed tests
5776	Action selection in AI systems is a basic system in which the problem can be analyzed by the AI machine to understand what it has to do next to get closer to the solution of the problem.  AI agents and action selection form to be very important entities to help devise an intelligent solution to a problem.	What is action in artificial intelligence
820	Machine learning is more than neural networks and deep learning. It is a field with a legion of smart algorithms that deduce complex patterns and make predictions about the unknown. The robustness of Random forests is contributed to its collection of distinct decision trees, each trying to solve part of the problem.	What have you learned about machine learning
5285	Particle filters or Sequential Monte Carlo (SMC) methods are a set of Monte Carlo algorithms used to solve filtering problems arising in signal processing and Bayesian statistical inference.  Particle filters update their prediction in an approximate (statistical) manner.	What is a particle filter used for
3838	In mathematics, the empty set is the unique set having no elements; its size or cardinality (count of elements in a set) is zero.	Is 0 an element of a set
1394	The Mann-Whitney U test is used to compare differences between two independent groups when the dependent variable is either ordinal or continuous, but not normally distributed.  The Mann-Whitney U test is often considered the nonparametric alternative to the independent t-test although this is not always the case.	Why would you use a Mann Whitney U test
474	Logistic regression is easier to implement, interpret, and very efficient to train. If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting. It makes no assumptions about distributions of classes in feature space.	What are the advantages of logistic regression
7451	Bounding boxes is one of the most popular and recognizable image annotation method used in machine learning and deep learning. Using bounding boxes annotators are asked to outline the object in a box as per the machine learning project requirements.	What is bounding box in object detection
1346	Backward elimination, which involves starting with all candidate variables, testing the deletion of each variable using a chosen model fit criterion, deleting the variable (if any) whose loss gives the most statistically insignificant deterioration of the model fit, and repeating this process until no further variables	What is backward elimination regression
1329	Interpolation search works better than Binary Search for a sorted and uniformly distributed array. On average the interpolation search makes about log(log(n)) comparisons (if the elements are uniformly distributed), where n is the number of elements to be searched.	Why would you prefer an interpolation search to a binary search
2878	A) Simple regression uses more than one dependent and independent variables, whereas multiple regression uses only one dependent and independent variable.	What is the major difference between simple regression and multiple regression quizlet
5222	Reinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation.  Its goal is to maximize the total reward.	How does reinforced learning work
1000	R squared, the proportion of variation in the outcome Y, explained by the covariates X, is commonly described as a measure of goodness of fit. This of course seems very reasonable, since R squared measures how close the observed Y values are to the predicted (fitted) values from the model.	How do you calculate goodness of fit in regression
4417	r text-mining natural-language. According the documentation of the removeSparseTerms function from the tm package, this is what sparsity entails: A term-document matrix where those terms from x are removed which have at least a sparse percentage of empty (i.e., terms occurring 0 times in a document) elements.	What is sparsity in document term matrix
4000	Let's explore 5 common techniques used for extracting information from the above text.Named Entity Recognition. The most basic and useful technique in NLP is extracting the entities in the text.  Sentiment Analysis.  Text Summarization.  Aspect Mining.  Topic Modeling.	What are the techniques used in NLP
3344	Cross-sectional data, or a cross section of a study population, in statistics and econometrics is a type of data collected by observing many subjects (such as individuals, firms, countries, or regions) at the one point or period of time. The analysis might also have no regard to differences in time.	What is cross sectional data analysis
1424	Uncertainty, means simply that there is a lack of certainty, caused by some information being hidden.	What is uncertainty in information theory
2038	Under simple random sampling, a sample of items is chosen randomly from a population, and each item has an equal probability of being chosen. Meanwhile, systematic sampling involves selecting items from an ordered population using a skip or sampling interval.	What is the difference between simple random sampling and systematic sampling
4657	Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration. The batch size can be one of three options: batch mode: where the batch size is equal to the total dataset thus making the iteration and epoch values equivalent.	What is batch size neural network
1303	Random forest has nearly the same hyperparameters as a decision tree or a bagging classifier.  Random forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features.	What is random in Random Forest algorithm
5661	"The Bag-of-words model is an orderless document representation — only the counts of words matter. For instance, in the above example ""John likes to watch movies. Mary likes movies too"", the bag-of-words representation will not reveal that the verb ""likes"" always follows a person's name in this text."	What Is the following an example of Bag of Words
382	The definition of data misuse is pretty simple: using information in a way it wasn't intended to be used.  The most common reasons for misuse are lack of awareness, personal gain, silent data collection, and using trade secrets in order to start a new business. In some cases, misuse can lead to a data breach.	How can data be misused
1333	Simply put, homoscedasticity means “having the same scatter.” For it to exist in a set of data, the points must be about the same distance from the line, as shown in the picture above. The opposite is heteroscedasticity (“different scatter”), where points are at widely varying distances from the regression line.	What does Homoscedasticity mean in regression
8527	A number of Machine Learning Algorithms - Supervised or Unsupervised, use Distance Metrics to know the input data pattern in order to make any Data Based decision. A good distance metric helps in improving the performance of Classification, Clustering and Information Retrieval process significantly.	What is the role of the distance function and why is it important to the accuracy of your machine learning algorithm
1355	Advantages. The coefficient of variation is useful because the standard deviation of data must always be understood in the context of the mean of the data. In contrast, the actual value of the CV is independent of the unit in which the measurement has been taken, so it is a dimensionless number.	Why is the coefficient of variation better than standard deviation
5269	Hyperparameters are the variables which determines the network structure(Eg: Number of Hidden Units) and the variables which determine how the network is trained(Eg: Learning Rate). Hyperparameters are set before training(before optimizing the weights and bias).	What are the Hyperparameters of a neural network
4355	We often divide the distribution at 99 centiles or percentiles . The median is thus the 50th centile. For the 20th centile of FEV1, i =0.2 times 58 = 11.6, so the quantile is between the 11th and 12th observation, 3.42 and 3.48, and can be estimated by 3.42 + (3.48 - 3.42) times (11.6 - 11) = 3.46.	How do you calculate quantile
434	R is a highly extensible and easy to learn language and fosters an environment for statistical computing and graphics. All of this makes R an ideal choice for data science, big data analysis, and machine learning.	Why is R used for data science
5705	Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter.  An example of a decision tree can be explained using above binary tree.	What is decision trees in machine learning
5317	Cluster Sampling: Advantages and Disadvantages Assuming the sample size is constant across sampling methods, cluster sampling generally provides less precision than either simple random sampling or stratified sampling. This is the main disadvantage of cluster sampling.	What is cluster sampling advantages and disadvantages
7460	The anti-Martingale, or reverse Martingale, system is a trading methodology that involves halving a bet each time there is a trade loss and doubling it each time there is a gain. This technique is the opposite of the Martingale system, whereby a trader (or gambler) doubles down on a losing bet and halves a winning bet.	What is reverse Martingale
1958	The determinant is a unique number associated with a square matrix. If the determinant of a matrix is equal to zero: The matrix is less than full rank. The matrix is singular.	What is a determinant in statistics
4876	The important limitations of statistics are: (1) Statistics laws are true on average. Statistics are aggregates of facts, so a single observation is not a statistic.  (2) Statistical methods are best applicable to quantitative data. (3) Statistics cannot be applied to heterogeneous data.	What are statistical limitations
8047	The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively.	How do you calculate AUC from ROC curve
5771	Now living under the identity of Scarecrow, Hide helped Koutarou Amon flee from Akihiro Kanou after he was turned into a one-eyed ghoul.	Did hide become a ghoul
1255	The consistency of the sampling distribution is dependent on the sample size not on the distribution of the population. As the sample size decreases the absolute value of the skewness and kurtosis of the sampling distribution increases. This sample size relationship is expressed in the central limit theorem.	How does sample size affect skewness
6714	The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.	What is the difference between statistics and machine learning
269	You'll get the same answer, but the technical difference is glm uses likelihood (if you want AIC values) whereas lm uses least squares. Consequently lm is faster, but you can't do as much with it.	What is the difference between a linear model using glm and a linear model using lm in R
4671	If X takes values in [a, b] and Y takes values in [c, d] then the pair (X, Y ) takes values in the product [a, b] × [c, d]. The joint probability density function (joint pdf) of X and Y is a function f(x, y) giving the probability density at (x, y).	How do you find the density of a joint function
3200	A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value.	What is a decision tree in Python
4061	The population mean of the distribution of sample means is the same as the population mean of the distribution being sampled from.  Thus as the sample size increases, the standard deviation of the means decreases; and as the sample size decreases, the standard deviation of the sample means increases.	How does population size affect standard deviation
5296	Maximum likelihood estimation involves defining a likelihood function for calculating the conditional probability of observing the data sample given a probability distribution and distribution parameters. This approach can be used to search a space of possible distributions and parameters.	What is maximum likelihood hypothesis in machine learning
6296	An artificial neural network is an attempt to simulate the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a humanlike manner. ANNs are created by programming regular computers to behave as though they are interconnected brain cells.	How do artificial neural networks work
7140	In a mechanical system, energy is dissipated when two surfaces rub together. Work is done against friction which causes heating of the two surfaces – so the internal (thermal) energy store of the surfaces increases and this is then transferred to the internal energy store of the surroundings.	How is dissipated energy transferred
358	GAN models can suffer badly in the following areas comparing to other deep networks. Non-convergence: the models do not converge and worse they become unstable. Slow training: the gradient to train the generator vanished.	Why is Gan hard to train
7141	A standard deviation is a measure of variability for a distribution of scores in a single sample or in a population of scores. A standard error is the standard deviation in a distribution of means of all possible samples of a given size from a particular population of individual scores.	What is the difference between standard deviation and standard error quizlet
1077	ReLU is differentiable at all the point except 0. the left derivative at z = 0 is 0 and the right derivative is 1.  Hidden units that are not differentiable are usually non-differentiable at only a small number of points.	Is ReLU function differentiable
4568	"The questionable cause—also known as causal fallacy, false cause, or non causa pro causa (""non-cause for cause"" in Latin)—is a category of informal fallacies in which a cause is incorrectly identified. For example: ""Every time I go to sleep, the sun goes down."	What is an example of a false cause fallacy
1914	Neural network momentum is a simple technique that often improves both training speed and accuracy. Training a neural network is the process of finding values for the weights and biases so that for a given set of input values, the computed output values closely match the known, correct, target values.	What is momentum in neural network
6138	Centroid-based clustering organizes the data into non-hierarchical clusters, in contrast to hierarchical clustering defined below. k-means is the most widely-used centroid-based clustering algorithm. Centroid-based algorithms are efficient but sensitive to initial conditions and outliers.	Which algorithm can perform clustering
1143	A simple definition of a sampling frame is the set of source materials from which the sample is selected. The definition also encompasses the purpose of sampling frames, which is to provide a means for choosing the particular members of the target population that are to be interviewed in the survey.	What is the purpose of sampling frame
175	"The Random Variable is X = ""The sum of the scores on the two dice"". Let's count how often each value occurs, and work out the probabilities: 2 occurs just once, so P(X = 2) = 1/36. 3 occurs twice, so P(X = 3) = 2/36 = 1/18."	How do you find the random variable
371	The integral sign ∫ represents integration. The symbol dx, called the differential of the variable x, indicates that the variable of integration is x. The function f(x) to be integrated is called the integrand.	What does the integration symbol mean
7014	Here's how we can do it.Step 1: Choose the number of clusters k.  Step 2: Select k random points from the data as centroids.  Step 3: Assign all the points to the closest cluster centroid.  Step 4: Recompute the centroids of newly formed clusters.  Step 5: Repeat steps 3 and 4.	How do you implement clustering
7468	Some variables, such as social security numbers and zip codes, take numerical values, but are not quantitative: They are qualitative or categorical variables. The sum of two zip codes or social security numbers is not meaningful. The average of a list of zip codes is not meaningful.	Is zip code a quantitative or categorical variable
6624	String Interpolation is a one-way databinding technique which is used to output the data from a TypeScript code to HTML template (view). It uses the template expression in double curly braces to display the data from the component to the view.	What is string interpolation in angular
1125	Hypothesis Tests with the Repeated-Measures t (cont.) In words, the null hypothesis says that there is no consistent or systematic difference between the two treatment conditions. Note that the null hypothesis does not say that each individual will have a difference score equal to zero.	What is the null hypothesis for a repeated measures test
4218	Bootstrapping is a type of resampling where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample.  You randomly draw three numbers 5, 1, and 49. You then replace those numbers into the sample and draw three numbers again.	How is a bootstrap sample obtained
5747	"In mathematical optimization and decision theory, a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some ""cost"" associated with the event."	How do you define a loss function
7285	Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron.	How does the deep belief network DBN solve the vanishing gradient
6319	Edward Lorenz, from the Massachusetts Institute of Technology (MIT) is the official discoverer of chaos theory.  Lorenz had rediscovered the chaotic behavior of a nonlinear system, that of the weather, but the term chaos theory was only later given to the phenomenon by the mathematician James A. Yorke, in 1975.	How was the chaos theory discovered
362	It is simply not possible to use the k-means clustering over categorical data because you need a distance between elements and that is not clear with categorical data as it is with the numerical part of your data.	Can categorical variables be used in clustering
5250	Predicting Google's Stock Price using Linear RegressionTake a value of x (say x=0)Find the corresponding value of y by putting x=0 in the equation.Store the (x,y) value pair in a table.Repeat the process once or twice or as many times as we want.Plot the points on the graph to obtain the straight line.	How do you use linear regression to predict stock prices
3763	ReLU is important because it does not saturate; the gradient is always high (equal to 1) if the neuron activates. As long as it is not a dead neuron, successive updates are fairly effective. ReLU is also very quick to evaluate.	Why is ReLU used in CNN
7034	The correlation of X and Y is the normalized covariance: Corr(X,Y) = Cov(X,Y) / σXσY .  (Notice that the covariance of X with itself is Var(X), and therefore the correlation of X with itself is 1.) Correlation is a measure of the strength of the linear relationship between two variables.	Are X and Y correlated
95	In the nonparametric bootstrap a sample of the same size as the data is take from the data with replacement. What does this mean? It means that if you measure 10 samples, you create a new sample of size 10 by replicating some of the samples that you've already seen and omitting others.	What is nonparametric bootstrapping
689	In probability theory and statistics, a categorical distribution (also called a generalized Bernoulli distribution, multinoulli distribution) is a discrete probability distribution that describes the possible results of a random variable that can take on one of K possible categories, with the probability of each	What is a Multinoulli distribution
3621	The machine learning perspective on the Ising model The Ising model is an undirected graphical model or Markov random field.  These random variables are the spins of the Ising model, so two nodes are connected by an edge if they interact.	How is the Ising model related to machine learning
924	The Delta rule in machine learning and neural network environments is a specific type of backpropagation that helps to refine connectionist ML/AI networks, making connections between inputs and outputs with layers of artificial neurons. The Delta rule is also known as the Delta learning rule.	What is Delta rule in neural network
4475	Because it arises from consistency between parts of a test, split-half reliability is an “internal consistency” approach to estimating reliability. This result is an estimate of the reliability of the test scores, and it provides some support for the quality of the test scores.	Why is split half reliability important
64	If there are only two variables, one is continuous and another one is categorical, theoretically, it would be difficult to capture the correlation between these two variables.	Can you capture correlation between continuous and categorical variables
7849	As the name stepwise regression suggests, this procedure selects variables in a step-by-step manner. The procedure adds or removes independent variables one at a time using the variable's statistical significance. Stepwise either adds the most significant variable or removes the least significant variable.	How do you explain stepwise regression
3367	Bellman's principle of optimality Principle of Optimality: An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.	What is Bellman's principle of optimality
3076	When we think of data structures, there are generally four forms:Linear: arrays, lists.Tree: binary, heaps, space partitioning etc.Hash: distributed hash table, hash tree etc.Graphs: decision, directed, acyclic etc.	What are the types of data structures
7844	The Gaussian Processes Classifier is a classification machine learning algorithm. Gaussian Processes are a generalization of the Gaussian probability distribution and can be used as the basis for sophisticated non-parametric machine learning algorithms for classification and regression.	What is Gaussian process classifier
1167	Batch normalization may be used on the inputs to the layer before or after the activation function in the previous layer. It may be more appropriate after the activation function if for s-shaped functions like the hyperbolic tangent and logistic function.	When should I use batch normalization
5630	Sampling errors can be reduced by the following methods: (1) by increasing the size of the sample (2) by stratification. Increasing the size of the sample: The sampling error can be reduced by increasing the sample size. If the sample size n is equal to the population size N, then the sampling error is zero.	What is sampling error and how can it be reduced
1132	Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration.  Usually, a number that can be divided into the total dataset size. stochastic mode: where the batch size is equal to one.	What is batch size
5992	K-nearest neighbors K- nearest neighbor (kNN) is a simple supervised machine learning algorithm that can be used to solve both classification and regression problems. kNN stores available inputs and classifies new inputs based on a similar measure i.e. the distance function.	Which algorithm is used for both classification and regression predictive problems
4266	Regular Markov Chains. ○ A transition matrix P is regular if some power of P has only positive entries. A Markov chain is a regular Markov chain if its transition matrix is regular. For example, if you take successive powers of the matrix D, the entries of D will always be positive (or so it appears).	What is a regular Markov chain
4925	The primary advantage of CRFs over hidden Markov models is their conditional nature, resulting in the relaxation of the independence assumptions required by HMMs in order to ensure tractable inference.	What are the advantage of using hidden Markov models an conditional random fields for sequence labeling
4908	The data used in cluster analysis can be interval, ordinal or categorical. However, having a mixture of different types of variable will make the analysis more complicated.	What are the different types of data used in cluster analysis
45	"Prediction bias is a quantity that measures how far apart those two averages are. That is: prediction bias = average of predictions − average of labels in data set. Note: ""Prediction bias"" is a different quantity than bias (the b in wx + b)."	What is bias in classification
3865	The input X provides the initial information that then propagates to the hidden units at each layer and finally produce the output y^. The architecture of the network entails determining its depth, width, and activation functions used on each layer. Depth is the number of hidden layers.	What is the forward propagation in a neural network and what is its output
494	A batch size of 32 means that 32 samples from the training dataset will be used to estimate the error gradient before the model weights are updated.	Why is batch size 32
566	The determinant is related to the volume of the space occupied by the swarm of data points represented by standard scores on the measures involved.  When the measures are correlated, the space occupied becomes an ellipsoid whose volume is less than 1.	What does the determinant of the correlation matrix represent
1596	"To recap the differences between the two: Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned. Deep learning structures algorithms in layers to create an ""artificial neural network” that can learn and make intelligent decisions on its own."	What are the key differences between machine learning ml and deep learning
5271	Nucleus is a library of Python and C++ code designed to make it easy to read, write and analyze data in common genomics file formats like SAM and VCF. A library from DeepMind for constructing neural networks. A learning framework to train neural networks by leveraging structured signals in addition to feature inputs.	What is the name of the TensorFlow library containing common data that you can use to train and test neural networks
757	A cost function is something you want to minimize. For example, your cost function might be the sum of squared errors over your training set. Gradient descent is a method for finding the minimum of a function of multiple variables. So you can use gradient descent to minimize your cost function.	What is the difference between cost function and gradient descent
5109	4:0054:57Suggested clip · 116 secondsMaximum Likelihood Estimation Derivation Properties  - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you derive the maximum likelihood estimator
5126	Factor-Label Method	What does dimensional analysis mean
663	Artificial intelligence (AI) is evolving—literally. Researchers have created software that borrows concepts from Darwinian evolution, including “survival of the fittest,” to build AI programs that improve generation after generation without human input.	What is evolution of artificial intelligence
3271	In column-oriented NoSQL databases, data is stored in cells grouped in columns of data rather than as rows of data. Columns are logically grouped into column families. Column families can contain a virtually unlimited number of columns that can be created at runtime or while defining the schema.	What represents column in NoSQL
328	ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.	When and how are ROC and AUC curves used in machine learning
6030	In some research studies one variable is used to predict or explain differences in another variable. In those cases, the explanatory variable is used to predict or explain differences in the response variable. In an experimental study, the explanatory variable is the variable that is manipulated by the researcher.	What does the explanatory variable represent
632	Explanation: The objective of perceptron learning is to adjust weight along with class identification.	What is the objective of Perceptron learning
2255	Supervised machine learning builds a model that makes predictions based on evidence in the presence of uncertainty. A supervised learning algorithm takes a known set of input data and known responses to the data (output) and trains a model to generate reasonable predictions for the response to new data.	How does prediction work in machine learning
7385	An AB test is an example of statistical hypothesis testing, a process whereby a hypothesis is made about the relationship between two data sets and those data sets are then compared against each other to determine if there is a statistically significant relationship or not.	What is AB testing in statistics
2408	Accuracy Assessment for Image ClassificationOpen the Create Accuracy Assessment Points tool and set the Target Field to Ground Truth.Select a sampling strategy.  Open the Update Accuracy Assessment Points tool.Set the Input Raster or Feature Class data as the classified dataset.More items	How do you do accuracy assessment in Arcgis
948	Cross-entropy can be calculated using the probabilities of the events from P and Q, as follows: H(P, Q) = – sum x in X P(x) * log(Q(x))	How do you calculate cross entropy loss
858	Zero-shot learning aims at predicting a large number of unseen classes using only labeled data from a small set of classes and external knowledge about class relations. Moreover, the number of categories keeps increasing as well as the difficulty to collect new data for each new category.	What is zero shot learning and what are its advantages
3515	You now know that: Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance.	What is the bias variance trade off in machine learning
7105	A tensor is a quantity, for example a stress or a strain, which has magnitude, direction, and a plane in which it acts. Stress and strain are both tensor quantities.  A tensor is a quantity, for example a stress or a strain, which has magnitude, direction, and a plane in which it acts.	What is tensor quantity
1154	Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.  That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.	What is cross validation in machine learning
6302	When a data set has a negative value, the axis will be shifted upward by –MIN(R) where R is the data range containing the data. Thus if R ranges from -10 to 20, the range in the chart will range from 0 to 30.	How can the box plot chart have negative values
1098	The distribution margin is an accountancy term that describes the degree of profit or loss with respect to a good that is bought wholesale.  You will need the wholesale price of the good, as well as the average sales price for which you are selling the good on the market.	What is a distribution margin
5626	"In the binomial distribution, the number of trials is fixed, and we count the number of ""successes"". Whereas, in the geometric and negative binomial distributions, the number of ""successes"" is fixed, and we count the number of trials needed to obtain the desired number of ""successes""."	What are the differences between binomial and negative binomial distributions
1335	To calculate the standard deviation of those numbers:Work out the Mean (the simple average of the numbers)Then for each number: subtract the Mean and square the result.Then work out the mean of those squared differences.Take the square root of that and we are done!	How do you find the sample standard deviation of the differences
611	Main MenuContinuous Optimization.Bound Constrained Optimization.Constrained Optimization.Derivative-Free Optimization.Discrete Optimization.Global Optimization.Linear Programming.Nondifferentiable Optimization.More items	What are the different types of optimization techniques
5148	One of the drawbacks of SGD is that it uses a common learning rate for all parameters. For optimization problems with huge number of parameters, this might be problematic: Let's say your objective function contours look like the above.	What is the disadvantage of stochastic gradient descent SGD )
767	In sampling with replacement the mean of all sample means equals the mean of the population:  Whatever the shape of the population distribution, the distribution of sample means is approximately normal with better approximations as the sample size, n, increases.	What is sampling distribution of mean with replacement
3879	Five Common Types of Sampling ErrorsPopulation Specification Error—This error occurs when the researcher does not understand who they should survey.  Sample Frame Error—A frame error occurs when the wrong sub-population is used to select a sample.More items	What are the types of sampling error
546	If both variables tend to increase or decrease together, the coefficient is positive, and the line that represents the correlation slopes upward. If one variable tends to increase as the other decreases, the coefficient is negative, and the line that represents the correlation slopes downward.	How do you interpret correlation results
1399	As the degrees of freedom of a Chi Square distribution increase, the Chi Square distribution begins to look more and more like a normal distribution. Thus, out of these choices, a Chi Square distribution with 10 df would look the most similar to a normal distribution.	Which chi square distribution looks the most like a normal distribution
6529	The geometric mean differs from the arithmetic average, or arithmetic mean, in how it is calculated because it takes into account the compounding that occurs from period to period. Because of this, investors usually consider the geometric mean a more accurate measure of returns than the arithmetic mean.	What is the difference between mean and geometric mean
6695	Regression to the mean is all about how data evens out. It basically states that if a variable is extreme the first time you measure it, it will be closer to the average the next time you measure it. In technical terms, it describes how a random variable that is outside the norm eventually tends to return to the norm.	How do you explain regression to the mean
4850	Derivative RulesCommon FunctionsFunctionDerivativeSquarex22xSquare Root√x(½)x-½Exponentialexexaxln(a) ax24 more rows	What is the derivative of E X
990	Logistic regression is a model for binary classification predictive modeling.  Under this framework, a probability distribution for the target variable (class label) must be assumed and then a likelihood function defined that calculates the probability of observing the outcome given the input data and the model.	What is likelihood function in logistic regression
663	Epsilon is used when we are selecting specific actions base on the Q values we already have.  In conclusion learning rate is associated with how big you take a leap and epsilon is associated with how random you take an action.	What is Epsilon in reinforcement learning
373	The main difference is the one of focus. Data Engineers are focused on building infrastructure and architecture for data generation. In contrast, data scientists are focused on advanced mathematics and statistical analysis on that generated data.  Simply put, data scientists depend on data engineers.	How data engineering is different from data science
6995	Stratified random sampling is a method of sampling that involves the division of a population into smaller sub-groups known as strata. In stratified random sampling, or stratification, the strata are formed based on members' shared attributes or characteristics such as income or educational attainment.	What is a stratified random sample
5183	In statistics, the t-statistic is the ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error.  For example, the T-statistic is used in estimating the population mean from a sampling distribution of sample means if the population standard deviation is unknown.	What does the t statistic mean
4404	Univariate analysis, looking at single variables, is typically the first procedure one does when examining first time data.  The SPSS tools for looking at single variables include the following procedures: Frequencies, Descriptives and Explore all located under the Analyze menu.	What is univariate analysis in SPSS
8069	Operating system is a system software. Kernel is a part of operating system. Operating system acts as an interface between user and hardware. Kernel acts as an interface between applications and hardware.	How is kernel different from operating system
7830	TensorFlow is a free and open-source software library for machine learning. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. Tensorflow is a symbolic math library based on dataflow and differentiable programming.	What is TensorFlow and why it is used
7527	For each value x, multiply the square of its deviation by its probability. (Each deviation has the format x – μ). The mean, μ, of a discrete probability function is the expected value. The standard deviation, Σ, of the PDF is the square root of the variance.	How do you find probability given mean and standard deviation
897	Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space.	What is clustering in machine learning
6741	Just as ordinary least square regression is the method used to estimate coefficients for the best fit line in linear regression, logistic regression uses maximum likelihood estimation (MLE) to obtain the model coefficients that relate predictors to the target.	Which method gives the best fit for logistic regression model
4056	Bayesian decision theory is a fundamental statistical approach to the problem of pattern classification.  This approach is based on quantifying the tradeoffs between various classification decisions using probability and the costs that accompany such decisions.	What is Bayesian decision rule
4542	The steps in grouping may be summarized as follows:Decide on the number of classes.Determine the range, i.e., the difference between the highest and lowest observations in the data.Divide range by the number of classes to estimate approximate size of the interval (h).More items	How do you find the class interval in a frequency table
582	You CAN use linear regression with ordinal data, because you can regress any set of numbers against any other. The problems come when interpreting the results.  You CAN use linear regression with ordinal data, because you can regress any set of numbers against any other.	Can you use linear regression for ordinal data
5198	For example for a t-test, we assume that a random variable follows a normal distribution. For discrete data key distributions are: Bernoulli, Binomial, Poisson and Multinomial.	Is Bernoulli distribution discrete or continuous
1018	Rank one matrices The rank of a matrix is the dimension of its column (or row) space. The matrix. 1 4 5 A = 2 8 10 2 Page 3 � � has rank 1 because each of its columns is a multiple of the first column.	What is a rank one matrix
871	A scatterplot is a type of data display that shows the relationship between two numerical variables. Each member of the dataset gets plotted as a point whose x-y coordinates relates to its values for the two variables.	What type of data can be displayed in a scatter plot
7157	Preventing the error gradients from vanishing The presence of the forget gate's activations allows the LSTM to decide, at each time step, that certain information should not be forgotten and to update the model's parameters accordingly. and the gradient doesn't vanish.	How does LSTM help prevent the vanishing and exploding gradient problem in a recurrent neural network
8577	Step 1: Learn the fundamental data structures and algorithms. First, pick a favorite language to focus on and stick with it.  Step 2: Learn advanced concepts, data structures, and algorithms.  Step 1+2: Practice.  Step 3: Lots of reading + writing.  Step 4: Contribute to open-source projects.  Step 5: Take a break.	How do I start learning algorithms
1172	In physics, mathematics and statistics, scale invariance is a feature of objects or laws that do not change if scales of length, energy, or other variables, are multiplied by a common factor, and thus represent a universality.	What does scale invariant mean
694	Bayesian classification is based on Bayes' Theorem. Bayesian classifiers are the statistical classifiers. Bayesian classifiers can predict class membership probabilities such as the probability that a given tuple belongs to a particular class.	How Bayes theorem is used for classification
1804	Sample variance Dividing instead by n − 1 yields an unbiased estimator.  In other words, the expected value of the uncorrected sample variance does not equal the population variance σ2, unless multiplied by a normalization factor. The sample mean, on the other hand, is an unbiased estimator of the population mean μ.	Why is sample variance an unbiased estimator
6630	Types of InferencePoint Estimation.Interval Estimation.Hypothesis Testing.	What are the three forms of statistical inference
4788	Contrastive Loss: Contrastive refers to the fact that these losses are computed contrasting two or more data points representations. This name is often used for Pairwise Ranking Loss, but I've never seen using it in a setup with triplets. Triplet Loss: Often used as loss name when triplet training pairs are employed.	What is contrastive loss
4105	“Statistical significance helps quantify whether a result is likely due to chance or to some factor of interest,” says Redman. When a finding is significant, it simply means you can feel confident that's it real, not that you just got lucky (or unlucky) in choosing the sample.	What is statistical significance and why is it important
35	Definition. Univariate analyses are used extensively in quality of life research. Univariate analysis is defined as analysis carried out on only one (“uni”) variable (“variate”) to summarize or describe the variable (Babbie, 2007; Trochim, 2006).	What does univariate analysis mean
7283	Reinforcement Learning WorkflowCreate the Environment. First you need to define the environment within which the agent operates, including the interface between agent and environment.  Define the Reward.  Create the Agent.  Train and Validate the Agent.  Deploy the Policy.	How do you teach reinforcement to learning
4493	3 layers	How many layers are in deep neural network
6481	The derivative of the sigmoid is ddxσ(x)=σ(x)(1−σ(x)).	What is the derivative of the sigmoid function
2459	"The consequent of a conditional statement is the part that usually follows ""then"". The part that usually follows ""if"" is called the ""antecedent"".  To affirm the consequent is, of course, to claim that the consequent is true. Thus, affirming the consequent in the example would be to claim that I have logic class."	What is the difference between affirming the consequent and denying the antecedent
8469	In machine learning, the delta rule is a gradient descent learning rule for updating the weights of the inputs to artificial neurons in a single-layer neural network. It is a special case of the more general backpropagation algorithm.	What is Delta learning rule
712	10 things to consider before choosing enterprise analytics platformAnalytic approach and data accuracy.Features and Tracking Types.Connectivity and Integration.Professional Services and Support.Data Storage Options: SaaS vs. Self-Hosting.Legal compliance.Supplier and Software Reliability.Ongoing and future costs.More items•	What should I look for in an analytics tool
3678	⏩ optimal policy: the best action to take at each state, for maximum rewards over time. To help our agent do this, we need two things: A way to determine the value of a state in MDP. An estimated value of an action taken at a particular state.	What is optimal policy in reinforcement learning
8431	Non-hierarchical cluster analysis aims to find a grouping of objects which maximises or minimises some evaluating criterion. Many of these algorithms will iteratively assign objects to different groups while searching for some optimal value of the criterion.	What is non hierarchical clustering
1366	Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example.	What error metric would you use to evaluate how good a binary classifier is
6455	Adding Noise into Neural Network Neural networks are capable of learning output functions that can change wildly with small changes in input. Adding noise to inputs randomly is like telling the network to not change the output in a ball around your exact input.	What is noise in neural network
2632	"In natural language processing, word sense disambiguation (WSD) is the problem of determining which ""sense"" (meaning) of a word is activated by the use of the word in a particular context, a process which appears to be largely unconscious in people."	What is word sense disambiguation in NLP
8299	Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant.	What is the problems associated with multicollinearity
1572	Strong AI has a complex algorithm that helps it act in different situations, while all the actions in weak AIs are pre-programmed by a human. Strong AI-powered machines have a mind of their own. They can process and make independent decisions, while weak AI-based machines can only simulate human behavior.	What is the difference between weak AI and strong AI
228	In Electrical Engineering, Calculus (Integration) is used to determine the exact length of power cable needed to connect two substations, which are miles away from each other. Space flight engineers frequently use calculus when planning for long missions.	What is the application of integration in real life
207	A hidden Markov model (HMM) is an augmentation of the Markov chain to include observations. Just like the state transition of the Markov chain, an HMM also includes observations of the state.  The observations are modeled using the variable Ot for each time t whose domain is the set of possible observations.	What is hidden Markov model in artificial intelligence
1045	A compound proposition that is always True is called a tautology. Two propositions p and q are logically equivalent if their truth tables are the same. Namely, p and q are logically equivalent if p ↔ q is a tautology. If p and q are logically equivalent, we write p ≡ q.	What is logically equivalent to P → Q
4855	Backward chaining (or backward reasoning) is an inference method described colloquially as working backward from the goal. It is used in automated theorem provers, inference engines, proof assistants, and other artificial intelligence applications.  Both rules are based on the modus ponens inference rule.	What is backward chaining in artificial intelligence
1234	Basically, predicting a continuous variable is termed as regression. There are a no of regression algorithms like ridge and lasso regression you may want to check out.Linear Regression.Logistic Regression.Polynomial Regression.Stepwise Regression.Ridge Regression.Lasso Regression.ElasticNet Regression,	What are some good models for predicting continuous response variables
3751	A regression model will have unit changes between the x and y variables, where a single unit change in x will coincide with a constant change in y. Taking the log of one or both variables will effectively change the case from a unit change to a percent change.	Why do we take log of variables in regression
8075	When we do further analysis, like multivariate linear regression, for example, the attributed income will intrinsically influence the result more due to its larger value. But this doesn't necessarily mean it is more important as a predictor. So we normalize the data to bring all the variables to the same range.	Should I normalize data for linear regression
2091	The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map.	How do convolutions work
467	It results in a biased sample, a non-random sample of a population (or non-human factors) in which all individuals, or instances, were not equally likely to have been selected. If this is not accounted for, results can be erroneously attributed to the phenomenon under study rather than to the method of sampling.	What is a biased sample in research
2393	Parametric tests assume underlying statistical distributions in the data. Nonparametric tests do not rely on any distribution.  They can thus be applied even if parametric conditions of validity are not met.	What is the difference between parametric and nonparametric tests
5376	In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data.  The input at the bottom layer is raw data, and the output of the final layer is the final low-dimensional feature or representation.	What is representation in deep learning
4652	Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on. If your training/validation loss are about equal then your model is underfitting. Increase the size of your model (either number of layers or the raw number of neurons per layer)	How do neural networks reduce loss
463	Standardizing Neural Network Data.  In theory, it's not necessary to normalize numeric x-data (also called independent data). However, practice has shown that when numeric x-data values are normalized, neural network training is often more efficient, which leads to a better predictor.	Is normalization required for neural networks
8542	Types of Activation FunctionsSigmoid Function. In an ANN, the sigmoid function is a non-linear AF used primarily in feedforward neural networks.  Hyperbolic Tangent Function (Tanh)  Softmax Function.  Softsign Function.  Rectified Linear Unit (ReLU) Function.  Exponential Linear Units (ELUs) Function.	What are the different activation functions in neural network
5987	"Use Fisher's exact test when you have two nominal variables.  Fisher's exact test will tell you whether this difference between 81 and 31% is statistically significant. A data set like this is often called an ""R×C table,"" where R is the number of rows and C is the number of columns."	What does Fisher exact test tell you
5071	"The word ""deep"" in ""deep learning"" refers to the number of layers through which the data is transformed.  Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively."	Why is the word deep used in deep learning
4312	There is a layer of input nodes, a layer of output nodes, and one or more intermediate layers. The interior layers are sometimes called “hidden layers” because they are not directly observable from the systems inputs and outputs.	Why is it called hidden layer
4134	Moments are a set of statistical parameters to measure a distribution. Four moments are commonly used: 1st, Mean: the average.	What does moment mean in statistics
532	We define the cross-entropy cost function for this neuron by C=−1n∑x[ylna+(1−y)ln(1−a)], where n is the total number of items of training data, the sum is over all training inputs, x, and y is the corresponding desired output. It's not obvious that the expression (57) fixes the learning slowdown problem.	What is cross entropy cost function
109	AdaBoost algorithm, short for Adaptive Boosting, is a Boosting technique that is used as an Ensemble Method in Machine Learning. It is called Adaptive Boosting as the weights are re-assigned to each instance, with higher weights to incorrectly classified instances.	What is AdaBoost algorithm in machine learning
5452	Regression toward the mean occurs for two reasons. First, it results because you asymmetrically sampled from the population. If you randomly sample from the population, you would observe (subject to random error) that the population and your sample have the same pretest average.	What causes regression to the mean
1488	Face validity is only considered to be a superficial measure of validity, unlike construct validity and content validity because is not really about what the measurement procedure actually measures, but what it appears to measure. This appearance is only superficial.	What is face validity and why is it not considered evidence of validity
6892	Here are some important considerations while choosing an algorithm.Size of the training data. It is usually recommended to gather a good amount of data to get reliable predictions.  Accuracy and/or Interpretability of the output.  Speed or Training time.  Linearity.  Number of features.	How can I choose among classification algorithms to work with
1923	Based on a rule of thumb, it can be said that RMSE values between 0.2 and 0.5 shows that the model can relatively predict the data accurately. In addition, Adjusted R-squared more than 0.75 is a very good value for showing the accuracy. In some cases, Adjusted R-squared of 0.4 or more is acceptable as well.	What is an acceptable RMS error
4293	Generally, the data is arranged from smallest to largest: First quartile: the lowest 25% of numbers. Second quartile: between 25.1% and 50% (up to the median) Third quartile: 51% to 75% (above the median)	What are the quartiles of a normal distribution
2417	When the sample size is small, we use the t-distribution to calculate the p-value. In this case, we calculate the degrees of freedom, df= n-1. We then use df, along with the test statistic, to calculate the p-value. If the sample is greater than 30 (n>30), we consider this a large sample size.	How do you find the p value from a test statistic and sample size
1023	Significance level and p-value α is the maximum probability of rejecting the null hypothesis when the null hypothesis is true. If α = 1 we always reject the null, if α = 0 we never reject the null hypothesis.  If we choose to compare the p-value to α = 0.01, we are insisting on a stronger evidence!	How small of an alpha value can you choose and still have sufficient evidence to reject the null hypothesis
1724	Binning, bagging, and stacking, are basic parts of a data scientist's toolkit and a part of a series of statistical techniques called ensemble methods.  Bagging to decrease the model's variance; Boosting to decreasing the model's bias, and; Stacking to increasing the predictive force of the classifier.	What is bagging boosting and stacking
1371	The simplest solution is to use other activation functions, such as ReLU, which doesn't cause a small derivative. Residual networks are another solution, as they provide residual connections straight to earlier layers.	How do you solve the vanishing gradient problem
1681	Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks.  In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly.	What is multi task learning in the context of deep learning
676	One of the ways to help deal with this bias is to avoid shaping participants' ideas or experiences before they are faced with the experimental material. Even stating seemingly innocuous details might prime an individual to form theories or thoughts that could bias their answers or behavior.	How do you avoid participant bias
7208	Linear regression is a classical model for predicting a numerical quantity.  Coefficients of a linear regression model can be estimated using a negative log-likelihood function from maximum likelihood estimation. The negative log-likelihood function can be used to derive the least squares solution to linear regression.	What is log likelihood in regression
2117	It all depends on your end goal, if you want to experience the power of modern computer then go for Deep learning, but in DL you need some basic machine learning concepts. If you want to know how machines predict the weather or make their own artificial intelligence, then learn ML.	Should I start with machine learning or deep learning
3341	regression of y on x - the equation representing the relation between selected values of one variable (x) and observed values of the other (y); it permits the prediction of the most probable values of y. regression equation.	What does regression of Y on X mean
520	Basically, it takes between 365 days (1 year) to 1,825 days (5 years) to learn artificial intelligence (assuming you put in 4 – 0.5 learning hours a day). And how fast you learn also affects how long it takes you to be an expert.	How long does it take to learn artificial intelligence
225	Communalities – This is the proportion of each variable's variance that can be explained by the factors (e.g., the underlying latent continua). It is also noted as h2 and can be defined as the sum of squared factor loadings for the variables.  They are the reproduced variances from the factors that you have extracted.	What does Communalities mean in factor analysis
3138	Hyperparameters are the variables which determines the network structure(Eg: Number of Hidden Units) and the variables which determine how the network is trained(Eg: Learning Rate). Hyperparameters are set before training(before optimizing the weights and bias).	What are Hyperparameters in deep learning
1044	A Markov chain is a regular Markov chain if its transition matrix is regular. For example, if you take successive powers of the matrix D, the entries of D will always be positive (or so it appears). So D would be regular.	How can you tell if a Markov chain is regular
2150	Anomaly detection, also known as outlier detection is the process of identifying extreme points or observations that are significantly deviating from the remaining data. Whereas in unsupervised learning, no labels are presented for data to train upon.	Is anomaly detection supervised or unsupervised
89	Select a File for Image ChangeFrom the Toolbox, select Change Detection > Image Change Workflow. Select an input file from the File Selection dialog.  To apply a mask, select the Input Mask tab in the File Selection panel.  Select the Input Files tab again.Enter the path and filename for the Time 2 File.  Click Next.	How do you do change detection ENVI
333	k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.	What is the point of K means clustering
597	To find the shortest path, all you have to do is start from the source and perform a breadth first search and stop when you find your destination Node. The only additional thing you need to do is have an array previous[n] which will store the previous node for every node visited. The previous of source can be null.	How do I use BFS to find shortest path
4585	Take a labelled dataset, cluster it with the algorithm and interpret the results so expectation is to have same label instances in the same clusters. Use some kind of precision-recall, purity or entropy metrics for empirical results.Cluster data and compare with the randomly clustered data.	How do you know if clustering is accurate
3027	Generalization refers to your model's ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model. Estimated Time: 5 minutes Learning Objectives.	What is Generalisation in machine learning
8279	Some regression models are already classification models - e.g. logistic regression.  Regression trees turn into classification trees if the dependent variable changes.  Similarly, if you cateogorize the dependent variable, a linear regression is inappopriate and a logistic regression model is better.	What are some possible ways to turn a regression model to a classification model
1444	Focus on these key areas to lay the groundwork for successful AI implementations in your organizationExplore business opportunities.Assess your data needs.Examine your infrastructure.Determine your talent or vendor needs.Be prepared for inevitable risk.	How can I prepare for Artificial Intelligence
4784	Blocking refers to classifying experimental units into blocks whereas stratification refers to classifying individuals of a population into strata. The samples from the strata in a stratified random sample can be the blocks in an experiment.	What is the difference between blocking and stratification
1281	2 Answers. measures of central tendency are mean, mode and median , whereas measures of dispersion are variance, standard deviation and interquartile range (it explains the extent to which distribution stretched or squeezed).	What is the difference between measures of central tendency and standard deviation
7539	The dimensional equations have got the following uses: To check the correctness of a physical relation. To derive the relation between various physical quantities. To convert value of physical quantity from one system of unit to another system.	What are the uses of dimensional equation
850	You now know that: Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance.	What is tradeoff between bias and variance
609	The formula is:P(A|B) = P(A) P(B|A)P(B)P(Man|Pink) = P(Man) P(Pink|Man)P(Pink)P(Man|Pink) = 0.4 × 0.1250.25 = 0.2.Both ways get the same result of ss+t+u+v.P(A|B) = P(A) P(B|A)P(B)P(Allergy|Yes) = P(Allergy) P(Yes|Allergy)P(Yes)P(Allergy|Yes) = 1% × 80%10.7% = 7.48%More items	How do you calculate Bayesian probability
167	Prevalence thus impacts the positive predictive value (PPV) and negative predictive value (NPV) of tests. As the prevalence increases, the PPV also increases but the NPV decreases. Similarly, as the prevalence decreases the PPV decreases while the NPV increases.	What happens to sensitivity of prevalence increases
3385	Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.	What is the exact meaning of artificial intelligence
1071	Overfitting is a modeling error that occurs when a function is too closely fit to a limited set of data points.  Thus, attempting to make the model conform too closely to slightly inaccurate data can infect the model with substantial errors and reduce its predictive power.	What is meant by Overfitting of data
1122	"In the binomial distribution, the number of trials is fixed, and we count the number of ""successes"". Whereas, in the geometric and negative binomial distributions, the number of ""successes"" is fixed, and we count the number of trials needed to obtain the desired number of ""successes""."	What is the difference between the Binomial Bernoulli Negative Binomial Geometric Hypergeometric and Negative Hypergeometric distributions
5179	Len Gould. Answered November 6, 2016 · Author has 6.4K answers and 3M answer views. Outgroups are simply the people who are not members of your ingroup. Obvious examples of bases for forming ingroups are according to their race, culture, gender, age or religion.	What is ingroup and outgroup examples
731	The t-value is specific thing for a specific statistical test, that means little by itself. The p-value tells you the statistical significance of the difference; the t-value is an intermediate step.  This is the p-value. If p < alpha = 0.05, you have a statistically significant difference.	What is the difference between P value and t statistic
3244	Generative adversarial networks (GANs) are an exciting recent innovation in machine learning. GANs are generative models: they create new data instances that resemble your training data. For example, GANs can create images that look like photographs of human faces, even though the faces don't belong to any real person.	What are GANs in machine learning
6110	Stationary Time Series Time series are stationary if they do not have trend or seasonal effects. Summary statistics calculated on the time series are consistent over time, like the mean or the variance of the observations.	What makes data stationary during time series analysis
1246	Definition A.I (fuzzy set) A fuzzy set A on universe (domain) X is defined by the membership function ILA{X) which is a mapping from the universe X into the unit interval:  F{X) denotes the set of all fuzzy sets on X. Fuzzy set theory allows for a partial membership of an element in a set.	What is fuzzy sets in artificial intelligence
4667	Skewed data show a lopsided boxplot, where the median cuts the box into two unequal pieces. If the longer part of the box is to the right (or above) the median, the data is said to be skewed right.  If one side of the box is longer than the other, it does not mean that side contains more data.	What does it mean when a box plot is skewed
7370	"The distributional hypothesis in linguistics is derived from the semantic theory of language usage, i.e. words that are used and occur in the same contexts tend to purport similar meanings. The underlying idea that ""a word is characterized by the company it keeps"" was popularized by Firth in the 1950s."	What is distributional information
5966	Decision trees use multiple algorithms to decide to split a node into two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes.  The decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.	How does the decision tree algorithm work
8464	To calculate the similarity between two examples, you need to combine all the feature data for those two examples into a single numeric value. For instance, consider a shoe data set with only one feature: shoe size. You can quantify how similar two shoes are by calculating the difference between their sizes.	How do you calculate similarity
1397	The shape of any distribution can be described by its various 'moments'. The first four are: 1) The mean, which indicates the central tendency of a distribution. 2) The second moment is the variance, which indicates the width or deviation.	What are the moments of a distribution
2968	Data Streaming Explained Also known as event stream processing, streaming data is the continuous flow of data generated by various sources. By using stream processing technology, data streams can be processed, stored, analyzed, and acted upon as it's generated in real-time.	What does it mean to stream data
7429	Applications of association rule mining are stock analysis, web log mining, medical diagnosis, customer market analysis bioinformatics etc. In past, many algorithms were developed by researchers for Boolean and Fuzzy association rule mining such as Apriori, FP-tree, Fuzzy FP-tree etc.	What are different applications of association rule mining
2806	VGGAcronymDefinitionVGGVisual Geometry Group (UK)VGGVancouver Gaming Guild (Canada)VGGVery Great GameVGGVeterans Gaming Group3 more rows	What does Vgg stand for
85	Factor analysis is a multivariant mathematical technique traditionally used in psychometrics to construct measures of psychologic and behavioral characteristics, such as intellectual abilities or personality traits (12).	What is factor analysis in psychometrics
786	For a discrete random variable, the expected value, usually denoted as or , is calculated using: μ = E ( X ) = ∑ x i f ( x i )	How do you find the expected value of a random variable
8497	Definition 1 (Minimal Sufficiency). A sufficient statistic T is minimal if for every sufficient statistic T and for every x, y ∈ X, T(x) = T(y) whenever T (x) = T (y). In other words, T is a function of T (there exists f such that T(x) = f(T (x)) for any x ∈ X).	How do you prove minimal sufficient statistics
189	Concepts in Feature Space Given a set of features for a concept learning problem, we can interpret the feature set as a feature space. Given some data, a feature space is just the set of all possible values for a chosen set of features from that data.	What is concept space in machine learning
705	Chi-squared test for nominal (categorical) data. The c2 test is used to determine whether an association (or relationship) between 2 categorical variables in a sample is likely to reflect a real association between these 2 variables in the population.	What statistical test is used for categorical data
7227	Random Forest	Which ML algorithm is based on the idea of bagging
8601	A simple random sample is used to represent the entire data population and. randomly selects individuals from the population without any other consideration. A stratified random sample, on the other hand, first divides the population into smaller groups, or strata, based on shared characteristics.	What is the difference between random sampling and stratified sampling
2534	A kNN algorithm is an extreme form of instance-based methods because all training observations are retained as a part of the model. It is a competitive learning algorithm because it internally uses competition between model elements (data instances) to make a predictive decision.	Why is KNN called instance based learning
4196	The pca. explained_variance_ratio_ parameter returns a vector of the variance explained by each dimension.  That will return a vector x such that x[i] returns the cumulative variance explained by the first i+1 dimensions.	What is PCA Explained_variance_ratio_
577	Non parametric do not assume that the data is normally distributed.  For example: the Kruskal Willis test is the non parametric alternative to the One way ANOVA and the Mann Whitney is the non parametric alternative to the two sample t test. The main nonparametric tests are: 1-sample sign test.	Which is an example of non parametric statistic
2398	Relative Risk is calculated by dividing the probability of an event occurring for group 1 (A) divided by the probability of an event occurring for group 2 (B). Relative Risk is very similar to Odds Ratio, however, RR is calculated by using percentages, whereas Odds Ratio is calculated by using the ratio of odds.	How do you calculate risk in statistics
2977	A rule-based system (e.g., production system, expert system) uses rules as the knowledge representation. These rules are coded into the system in the form of if-then-else statements.  So, let's regard rule-based systems as the simplest form of AI.	How does rule based AI model work
8142	These models, when used as inputs of ensemble methods, are called ”base models”. In this blog post I will cover ensemble methods for classification and describe some widely known methods of ensemble: voting, stacking, bagging and boosting.	What are the different Ensembling methods
8256	Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.	What is a gradient in deep learning
1991	The expected value of the sample mean is equal to the population mean µ. Therefore, the sample mean is an unbiased estimator of the population mean.  Since only a sample of observations is available, the estimate of the mean can be either less than or greater than the true population mean.	Is the sample mean always unbiased
318	Answer. A negative path loading is basically the same as a negative regression coefficient. I.e., For a path loading from X to Y it is the predicted increase in Y for a one unit increase on X holding all other variables constant. So a negative coefficient just means that as X increases, Y is predicted to decrease.	What does a negative path coefficient mean
2521	Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron.	What is the purpose of activation function
1174	P > 0.05 is the probability that the null hypothesis is true. 1 minus the P value is the probability that the alternative hypothesis is true. A statistically significant test result (P ≤ 0.05) means that the test hypothesis is false or should be rejected. A P value greater than 0.05 means that no effect was observed.	What does P 0.05 level of significance mean
7202	One of the most common structures that text mining packages work with is the document-term matrix (or DTM). This is a matrix where: each row represents one document (such as a book or article), each column represents one term, and. each value (typically) contains the number of appearances of that term in that document.	What is DTM in text analytics
4372	This article lists out 10 comprehensive data mining tools widely used in the big data industry.Rapid Miner.  Oracle Data Mining.  IBM SPSS Modeler.  KNIME.  Python.  Orange.  Kaggle.  Rattle.More items•	What are the data mining tools
6981	The aim of distributional semantics is to learn the meanings of linguistic expressions from a corpus of text. The core idea, known as the distributional hy- pothesis, is that the contexts in which an expression appears give us information about its meaning.	What are the goals of distributional semantics
7587	“The benefit to using a one-tailed test is that it requires fewer subjects to reach significance. A two-tailed test splits your significance level and applies it in both directions. Thus, each direction is only half as strong as a one-tailed test, which puts all the significance in one direction.	What is the advantage of a one tailed test
836	Stratified random sampling involves first dividing a population into subpopulations and then applying random sampling methods to each subpopulation to form a test group. A disadvantage is when researchers can't classify every member of the population into a subgroup.	What are pros and cons for stratified random sampling
5645	A pooling layer is a new layer added after the convolutional layer. Specifically, after a nonlinearity (e.g. ReLU) has been applied to the feature maps output by a convolutional layer; for example the layers in a model may look as follows: Input Image. Convolutional Layer.	What is pooling in deep learning
539	- Mode-The most repetitive number! - Median:The number in the MIDDLE when they are IN ORDER! - Mean- The AVERAGE OF ALL NUMBERS: You add up all the numbers then you divide it by the TOTAL NUMBER of NUMBERS! - Range - THE BIGGEST minus the Smallest!	What is the purpose of mean median mode and range
2439	The experts predict that AI will outperform humans in the next 10 years in tasks such as translating languages (by 2024), writing high school essays (by 2026), and driving trucks (by 2027). But many other tasks will take much longer for machines to master.	Will artificial intelligence supersede human intelligence
6183	A classification problem is when the output variable is a category, such as “red” or “blue” or “disease” and “no disease”. A classification model attempts to draw some conclusion from observed values. Given one or more inputs a classification model will try to predict the value of one or more outcomes.	What are classification problems in machine learning
1188	Neural networks are designed to work just like the human brain does. In the case of recognizing handwriting or facial recognition, the brain very quickly makes some decisions. For example, in the case of facial recognition, the brain might start with “It is female or male?	What is an example of a neural network
6495	The fundamental reason to use a random forest instead of a decision tree is to combine the predictions of many decision trees into a single model. The logic is that a single even made up of many mediocre models will still be better than one good model.	What is the main reason to use a random forest versus a decision tree
7292	one training example	How many training examples are required by one shot learning for each class
986	Bayesian model averaging (BMA) is an extension of the usual Bayesian inference methods in which one does not only describe parameter uncertainty through the prior distribution but also model uncertainty obtaining posterior distributions for model parameters and for the model themselves using Bayes' theorem, allowing	What is Bayesian model averaging
349	The first order statistic is the smallest sample value (i.e. the minimum), once the values have been placed in order. For example, in the sample 9, 2, 11, 5, 7, 4 the first order statistic is 2. In notation, that's x(1) = 2. The second order statistic x(2) is the next smallest value.	What is first order statistics
5022	Text classification using word embeddings and deep learning in python — classifying tweets from twitterSplit the data into text (X) and labels (Y)Preprocess X.Create a word embedding matrix from X.Create a tensor input from X.Train a deep learning model using the tensor inputs and labels (Y)More items•	How do I use Word embeds for text classification
237	Definition. A study design that randomly assigns participants into an experimental group or a control group. As the study is conducted, the only expected difference between the control and experimental groups in a randomized controlled trial (RCT) is the outcome variable being studied.	Is a randomized controlled trial an experimental design
47	Bayesian inference refers to statistical inference where uncertainty in inferences is quantified using probability.  Statistical models specify a set of statistical assumptions and processes that represent how the sample data is generated. Statistical models have a number of parameters that can be modified.	What are Bayesian models
6896	A generative model on the other hand will be able to produce a new picture of a either class. Typical discriminative models include logistic regression (LR), support vector machines (SVM), conditional random fields (CRFs) (specified over an undirected graph), decision trees, neural networks, and many others.	Is SVM generative or discriminative
2570	The dimension of a data set is the number of columns. The rows are the number of samples, usually.	What are the dimensions of a data set
39	Splitting: Dividing a node into two or more sub-nodes based on if-else conditions. Decision Node: After splitting the sub-nodes into further sub-nodes, then it is called as the decision node. Leaf or Terminal Node: This is the end of the decision tree where it cannot be split into further sub-nodes.	What is decision tree with example
674	Connectionism is a movement in cognitive science that hopes to explain intellectual abilities using artificial neural networks (also known as “neural networks” or “neural nets”).  These weights model the effects of the synapses that link one neuron to another.	What is connectionism in the context of data science and machine learning
1395	Bivariate statistics is a type of inferential statistics that deals with the relationship between two variables.  When bivariate statistics is employed to examine a relationship between two variables, bivariate data is used. Bivariate data consists of data collected from a sample on two different variables.	What is bivariate in statistics
7613	Any sum or difference or independent normal random variables is also normally distributed. A binomial setting arises when we perform several independent trials of the same chance process and record the number of times a particular outcome occurs.	What happens if two independent normal random variables are combined
1259	Parametric tests are used only where a normal distribution is assumed. The most widely used tests are the t-test (paired or unpaired), ANOVA (one-way non-repeated, repeated; two-way, three-way), linear regression and Pearson rank correlation.	Which are the parametric tests
5133	A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers.	What is ResNet neural network
7398	Linear regression is the analysis of two separate variables to define a single relationship and is a useful measure for technical and quantitative analysis in financial markets.  Using linear regression, a trader can identify key price points—entry price, stop-loss price, and exit prices.	What is linear regression in trading
4614	In a courtroom, a Type 2 error is acquitting a guilty person. A Type 1 error is when you incorrectly reject the null when it is true.  If the p -value is small, then you have observed something rare if the null is true. This then provides evidence against the truth of H0 .	How does P value relate to Type 1 and Type 2 errors
1344	2 Answers. The multinomial distribution is when there are multiple identical independent trials where each trial has k possible outcomes. The categorical distribution is when there is only one such trial.	What is the difference between the categorical distribution and the multinomial distribution
7753	Types of selection bias The most common type of selection bias in research or statistical analysis is a sample selection bias.  In principle, the bias can occur through selection effects in other aspects of the research process, such as which variables to use in analysis, and which tools to use to perform measurement.	Are selection devices biased
5563	Vectors can be used to represent physical quantities. Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction, and are drawn as arrows.	What are vectors used for
4340	In mathematics, a nonnegative matrix, written. is a matrix in which all the elements are equal to or greater than zero, that is, A positive matrix is a matrix in which all the elements are strictly greater than zero.	What is non negative matrix
4377	Electroencephalogram (EEG) spectral analysis quantifies the amount of rhythmic (or oscillatory) activity of different frequency in EEGs.  Despite the tremendous amount of research related to its usefulness, EEG spectral analysis still exhibits inconsistent results among studies.	What is spectral analysis EEG
3331	The Poisson(λ) Distribution can be approximated with Normal when λ is large. For sufficiently large values of λ, (say λ>1,000), the Normal(μ = λ,σ2 = λ) Distribution is an excellent approximation to the Poisson(λ) Distribution.	How do you approximate Poisson to normal
5168	Many algorithms have been used in measuring user similarity or item similarity in recommender systems. For example, the k-nearest neighbor (k-NN) approach and the Pearson Correlation as first implemented by Allen.	Which algorithm is used in recommendation system
6919	Data Preprocessing is a technique that is used to convert the raw data into a clean data set.  In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.	What is preprocessing in ML
206	If you establish at least a moderate correlation between X and Y through both a correlation coefficient and a scatterplot, then you know they have some type of linear relationship. Never do a regression analysis unless you have already found at least a moderately strong correlation between the two variables.	When should a regression model not be used to make a prediction
7048	Textual entailment (TE) in natural language processing is a directional relation between text fragments. The relation holds whenever the truth of one text fragment follows from another text. In the TE framework, the entailing and entailed texts are termed text (t) and hypothesis (h), respectively.	How can you explain the concept of Recognizing Textual Entailment in NLP
7239	In the real world, knowledge plays a vital role in intelligence as well as creating artificial intelligence. It demonstrates the intelligent behavior in AI agents or systems. It is possible for an agent or system to act accurately on some input only when it has the knowledge or experience about the input.	What is the role of knowledge in AI
3401	Variational approximations is a body of deterministic tech- niques for making approximate inference for parameters in complex statistical models.	What is variational approximation
4789	Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).	What are statistical methods used for analysis
1009	A frequent problem in estimating logistic regression models is a failure of the likelihood maximization algorithm to converge. In most cases, this failure is a consequence of data patterns known as complete or quasi-complete separation.  Log-likelihood as a function of the slope, quasi-complete separation.	Is logistic regression guaranteed to converge
3322	Logistic Regression not only gives a measure of how relevant a predictor (coefficient size) is, but also its direction of association (positive or negative). 4. Logistic regression is easier to implement, interpret and very efficient to train.	What are some of the advantages of fitting a logistic regression model
7448	Gradient descent techniques are known to be limited by a characteristic referred to as the `local minima' problem. During the search for an optimum solution or global minima, these techniques can encounter local minima from which they cannot escape due to the `steepest descent' nature of the approach.	What is local minima in gradient descent
41	Bayesian Belief Network or Bayesian Network or Belief Network is a Probabilistic Graphical Model (PGM) that represents conditional dependencies between random variables through a Directed Acyclic Graph (DAG).	What is Bayesian belief network in machine learning
7133	So, How Does a Neural Network Work Exactly?Information is fed into the input layer which transfers it to the hidden layer.The interconnections between the two layers assign weights to each input randomly.A bias added to every input after weights are multiplied with them individually.More items•	How neural network works step by step
6961	A false positive state is when the IDS identifies an activity as an attack but the activity is acceptable behavior. A false positive is a false alarm.  This is when the IDS identifies an activity as acceptable when the activity is actually an attack. That is, a false negative is when the IDS fails to catch an attack.	What's detection false positive vs false negative
114	An OUTCOME (or SAMPLE POINT) is the result of a the experiment. The set of all possible outcomes or sample points of an experiment is called the SAMPLE SPACE. An EVENT is a subset of the sample space.	What is the difference between outcome and sample space
7495	average	What is the meaning of mean in statistics
2618	Timestep = the len of the input sequence. For example, if you want to give LSTM a sentence as an input, your timesteps could either be the number of words or the number of characters depending on what you want. Number of hidden units = (well) number of hidden units. Sometimes, people call this number of LSTM cells.	What is the relationship between timestep and number hidden unit in LSTM
4421	In most basic probability theory courses your told moment generating functions (m.g.f) are useful for calculating the moments of a random variable. In particular the expectation and variance. Now in most courses the examples they provide for expectation and variance can be solved analytically using the definitions.	What are some applications of moment generating functions
2670	Multidimensional scaling is a visual representation of distances or dissimilarities between sets of objects.  Objects that are more similar (or have shorter distances) are closer together on the graph than objects that are less similar (or have longer distances).	What is multidimensional scaling in statistics
7886	Suppose it is of interest to estimate the population mean, μ, for a quantitative variable. Data collected from a simple random sample can be used to compute the sample mean, x̄, where the value of x̄ provides a point estimate of μ.  The standard deviation of a sampling distribution is called the standard error.	What sample statistic is used to estimate a population mean
6007	The exponential moving average (EMA) is a technical chart indicator that tracks the price of an investment (like a stock or commodity) over time. The EMA is a type of weighted moving average (WMA) that gives more weighting or importance to recent price data.	What does exponential moving average do
6365	Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.  Bayesian updating is particularly important in the dynamic analysis of a sequence of data.	How does Bayesian inference work
4995	Cross-validation is a technique in which we train our model using the subset of the data-set and then evaluate using the complementary subset of the data-set.  The three steps involved in cross-validation are as follows : Reserve some portion of sample data-set.	What is cross validation set in machine learning
5075	"Multinomial logistic regression deals with situations where the outcome can have three or more possible types (e.g., ""disease A"" vs. ""disease B"" vs. ""disease C"") that are not ordered.  Binary logistic regression is used to predict the odds of being a case based on the values of the independent variables (predictors)."	What's the difference between binary and multinomial logistic regression
6662	There are several situation in which the variable we want to explain can take only two possible values. This is typically the case when we want to model the choice of an individual.  This is why these models are called binary choice models, because they explain a (0/1) dependent variable.	What is a binary choice model
7529	Stack and Queuegeeksforgeeks.org - Stack Data Structure.geeksforgeeks.org - Introduction and Array Implementation.tutorialspoint.com - Data Structures Algorithms.cs.cmu.edu - Stacks.cs.cmu.edu - Stacks and Queues.cs.cmu.edu - Stacks and Queues.	Where can I learn algorithms and data structures
3900	The formula to calculate the test statistic comparing two population means is, Z= ( x - y )/√(σx2/n1 + σy2/n2). In order to calculate the statistic, we must calculate the sample means ( x and y ) and sample standard deviations (σx and σy) for each sample separately. n1 and n2 represent the two sample sizes.	How do you find the test statistic
430	Positive feedback is the opposite of negative feedback in that encourages a physiological process or amplifies the action of a system. Positive feedback is a cyclic process that can continue to amplify your body's response to a stimulus until a negative feedback response takes over.	What is a positive feedback response
2736	A learning model is a description of the mental and physical mechanisms that are involved in the acquisition of new skills and knowledge and how to engage those those mechanisms to encourage and facilitate learning.  Under each of these categories are numerous sub-categories to suit virtually any learning style.	What is model learning
4716	The geometric distribution is a one-parameter family of curves that models the number of failures before one success in a series of independent trials, where each trial results in either success or failure, and the probability of success in any individual trial is constant.	What are the parameters of a geometric distribution
923	If two random variables X and Y are independent, then their covariance Cov(X, Y) = E(XY) − E(X)E(Y) = 0, that is, they are uncorrelated.	When X and Y are statistically independent then I xy is
7125	Hierarchical clustering outputs a hierarchy, ie a structure that is more informa ve than the unstructured set of flat clusters returned by k-‐means. Therefore, it is easier to decide on the number of clusters by looking at the dendrogram (see sugges on on how to cut a dendrogram in lab8).	What are the benefits of hierarchical clustering over K means clustering
3057	In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.	What is CNN in neural network
7302	linear_model . LinearRegression. Ordinary least squares Linear Regression.	Which Scikit learn class is used for linear regression
1090	Simply put, in any application area where you have lots of heterogeneous or noisy data or anywhere you need a clear understanding of your uncertainty are areas that you can use Bayesian Statistics.	Where is Bayesian statistics used
2798	In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.	Why we use generalized linear model
7307	The purpose of singular value decomposition is to reduce a dataset containing a large number of values to a dataset containing significantly fewer values, but which still contains a large fraction of the variability present in the original data.	What is the purpose of singular value decomposition
3860	By “trend value” I mean exactly that: the background level at a given moment.  If it changes while the nature of the fluctuations remains the same, the probability of record-setting extremes will of course change.	What is non linear trend
8026	Bounding-box regression is a popular technique to refine or predict localization boxes in recent object detection approaches. Typically, bounding-box regressors are trained to regress from either region proposals or fixed anchor boxes to nearby bounding boxes of a pre-defined target object classes.	How does a bounding box regression work
3283	"In logic, temporal logic is any system of rules and symbolism for representing, and reasoning about, propositions qualified in terms of time (for example, ""I am always hungry"", ""I will eventually be hungry"", or ""I will be hungry until I eat something"")."	What is temporal logic in artificial intelligence
6709	The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image.  The result of applying it to a pixel on an edge is a vector that points across the edge from darker to brighter values.	How does Sobel filter work
6724	Unsupervised machine learning helps you to finds all kind of unknown patterns in data. Clustering and Association are two types of Unsupervised learning. Four types of clustering methods are 1) Exclusive 2) Agglomerative 3) Overlapping 4) Probabilistic.	What are different types of unsupervised learning
5698	Five Common Types of Sampling Errors. Population Specification Error—This error occurs when the researcher does not understand who they should survey. For example, imagine a survey about breakfast cereal consumption.  Sample Frame Error—A frame error occurs when the wrong sub-population is used to select a sample.	What are the main sampling errors
5643	In this module, we have discussed on various data preprocessing methods for Machine Learning such as rescaling, binarizing, standardizing, one hot encoding, and label encoding.	Which method is used for data preprocessing in machine learning
2266	In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimates across samples can be reduced by increasing the bias in the estimated parameters.	What is the trade off between bias and variance
1857	Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.  But if we instead take steps proportional to the positive of the gradient, we approach a local maximum of that function; the procedure is then known as gradient ascent.	Is gradient descent an optimization algorithm
476	A test statistic is a number calculated by a statistical test. It describes how far your observed data is from the null hypothesis of no relationship between variables or no difference among sample groups.	What is a test statistic in statistics
1789	Random forest will reduce variance part of error rather than bias part, so on a given training data set decision tree may be more accurate than a random forest. But on an unexpected validation data set, Random forest always wins in terms of accuracy.	How is random forest better than decision tree
4389	Think of feature columns as the intermediaries between raw data and Estimators. Feature columns are very rich, enabling you to transform a diverse range of raw data into formats that Estimators can use, allowing easy experimentation. In simple words feature column are bridge between raw data and estimator or model.	What is feature column in TensorFlow
595	Ensemble learning helps improve machine learning results by combining several models.  Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).	What is ensemble learning algorithm
844	Using the entire training set is just using a very large minibatch size, where the size of your minibatch is limited by the amount you spend on data collection, rather than the amount you spend on computation.	In deep learning why dont we use the whole training set to compute the gradient
968	Large numbers are numbers that are significantly larger than those typically used in everyday life, for instance in simple counting or in monetary transactions.  Very large numbers often occur in fields such as mathematics, cosmology, cryptography, and statistical mechanics.More items	Where do we use large numbers in real life
6488	Using batch normalisation allows much higher learning rates, increasing the speed at which networks train. Makes weights easier to initialise — Weight initialisation can be difficult, especially when creating deeper networks. Batch normalisation helps reduce the sensitivity to the initial starting weights.	Why do we use batch normalization
3873	Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.	How is gradient descent used in machine learning
7036	a transformation in which measurements on a linear scale are converted into probabilities between 0 and 1. It is given by the formula y = ex/(1 + ex), where x is the scale value and e is the Eulerian number.	What is logistic transformation
6954	n_estimators : This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower.	What is N_estimators in random forest
576	So, let's have a look at the most common dataset problems and the ways to solve them.How to collect data for machine learning if you don't have any.  Articulate the problem early.  Establish data collection mechanisms.  Format data to make it consistent.  Reduce data.  Complete data cleaning.  Decompose data.  Rescale data.More items•	How do you prepare a dataset for machine learning
278	7 Techniques to Handle Imbalanced DataUse the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	How do you balance an imbalanced dataset
6076	Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks.  This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped.	What is gradient clipping and why is it necessary
1382	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is supervised and unsupervised learning explain with the examples
6823	Cluster analysis is applied in many fields such as the natural sciences, the medical sciences, economics, marketing, etc. There are essentially two types of clustering methods: hierarchical algorithms and partioning algorithms. The hierarchical algorithms can be divided into agglomerative and splitting procedures.	What are the applications of hierarchical clustering
706	Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. Synset instances are the groupings of synonymous words that express the same concept. Some of the words have only one Synset and some have several.	What is a Synset in WordNet
1731	1:254:40Suggested clip · 105 secondsFinding the Input of a Function Given the Output - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the input
5732	Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them.	What are federated learning algorithms
7610	The repeatability is defined as the closeness of agreement between the results of successive measurements of the same measurand carried out subject to the following conditions: • the same measurement procedure, •	What is repeatability in measurement
727	There are a ton of 'smart' algorithms that assist data scientists do the wizardry.  k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification.	Is K means clustering supervised or unsupervised
1008	The Non-Linear Decision Boundary SVM works well when the data points are linearly separable. If the decision boundary is non-liner then SVM may struggle to classify. Observe the below examples, the classes are not linearly separable. SVM has no direct theory to set the non-liner decision boundary models.	What is non linear decision boundary
5273	The ways in which they function Another fundamental difference between traditional computers and artificial neural networks is the way in which they function. While computers function logically with a set of rules and calculations, artificial neural networks can function via images, pictures, and concepts.	How is a neural network different from a computer network
7131	Some of the most popular methods for outlier detection are:Z-Score or Extreme Value Analysis (parametric)Probabilistic and Statistical Modeling (parametric)Linear Regression Models (PCA, LMS)Proximity Based Models (non-parametric)Information Theory Models.More items	What methods do you use to identify outliers within a data set
6251	The prior is, generally speaking, a probability distribution that expresses one's beliefs about a quantity before some evidence is taken into account. If we restrict ourselves to an ML model, the prior can be thought as of the distribution that is imputed before the model starts to see any data.	What is prior in machine learning
815	The probability distribution for a random error that is as likely to move the value in either direction is called a Gaussian distribution. Such a distribution is characterized by two parameters, µ the mean or average value, and σ the standard deviation.	What are Gaussian errors
4621	Conditional probability is defined as the likelihood of an event or outcome occurring, based on the occurrence of a previous event or outcome. Conditional probability is calculated by multiplying the probability of the preceding event by the updated probability of the succeeding, or conditional, event.	What does conditional probability mean
5060	The Unsharp Mask filter adjusts the contrast of the edge detail and creates the illusion of a more focused image.	What does the unsharp mask filter do
3148	two independent variables	How many independent variables can you have in a regression
6397	The interval scale of measurement is a type of measurement scale that is characterized by equal intervals between scale units. A perfect example of an interval scale is the Fahrenheit scale to measure temperature.  For example, suppose it is 60 degrees Fahrenheit on Monday and 70 degrees on Tuesday.	What is interval scale example
2686	1:065:39Suggested clip · 107 secondsMake a Histogram Using Excel's Histogram tool in the Data Analysis YouTubeStart of suggested clipEnd of suggested clip	How do you analyze data from a histogram
2866	Page 1. 1 Order Statistics. Definition The order statistics of a random sample X1,,Xn are the sample values placed in ascending order. They are denoted by X(1),,X(n). The order statistics are random variables that satisfy X(1) ≤ X(2) ≤ ··· ≤ X(n).	What does XN mean in statistics
1250	Alternatively, general dimensionality reduction techniques are used such as:Independent component analysis.Isomap.Kernel PCA.Latent semantic analysis.Partial least squares.Principal component analysis.Multifactor dimensionality reduction.Nonlinear dimensionality reduction.More items	What are the different feature extraction techniques
7488	Supervised learning involves some process which trains the algorithm.  Topic modeling is a form of unsupervised statistical machine learning. It is like document clustering, only instead of each document belonging to a single cluster or topic, a document can belong to many different clusters or topics.	Is Topic Modelling supervised or unsupervised
3551	In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent).	What is agent system in artificial intelligence
1322	There are several approaches to avoiding overfitting in building decision trees. Pre-pruning that stop growing the tree earlier, before it perfectly classifies the training set. Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree.	What are the methods to avoid overfitting in decision trees
496	The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups.	What does analysis of variance tell you
4162	Ensemble learning helps improve machine learning results by combining several models.  Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).	How does ensemble method work
8455	Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.	What is a statistical inference example
7268	Train a neural network with TensorFlowStep 1: Import the data.Step 2: Transform the data.Step 3: Construct the tensor.Step 4: Build the model.Step 5: Train and evaluate the model.Step 6: Improve the model.	How do you train a neural network in TensorFlow
3159	The normal distribution is a continuous probability distribution. This has several implications for probability. The total area under the normal curve is equal to 1. The probability that a normal random variable X equals any particular value is 0.	Is normal distribution continuous or discrete
7661	Loss value implies how poorly or well a model behaves after each iteration of optimization. An accuracy metric is used to measure the algorithm's performance in an interpretable way.  It is the measure of how accurate your model's prediction is compared to the true data.	What is loss and accuracy
7298	Real time processing is usually found in systems that use computer control. This processing method is used when it is essential that the input request is dealt with quickly enough so as to be able to control an output properly. The is called the 'latency'.	What is real time processing used for
2221	This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. Therefore we can work with the simpler log-likelihood instead of the original likelihood.	Why is maximum likelihood estimation important
5231	The first component is the definition: Two variables are independent when the distribution of one does not depend on the the other.  If the probabilities of one variable remains fixed, regardless of whether we condition on another variable, then the two variables are independent.	What does it mean when two variables are independent
1371	The number of input variables or features for a dataset is referred to as its dimensionality.  Large numbers of input features can cause poor performance for machine learning algorithms. Dimensionality reduction is a general field of study concerned with reducing the number of input features.	What is Dimension machine learning
3854	LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate's activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.	How do LSTMs solve the vanishing gradient problem
1725	The aggregate opinion of a multiple models is less noisy than other models. In finance, we called it “Diversification” a mixed portfolio of many stocks will be much less variable than just one of the stocks alone. This is also why your models will be better with ensemble of models rather than individual.	What is the reason behind the better performance of ensemble models
4316	When we calculate probabilities involving one event AND another event occurring, we multiply their probabilities. In some cases, the first event happening impacts the probability of the second event.	Do we multiply probability
922	There is a popular method known as elbow method which is used to determine the optimal value of K to perform the K-Means Clustering Algorithm. The basic idea behind this method is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer elements in the cluster.	How do you find K for K means
7608	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What are the differences between supervised and unsupervised learning
895	The significance level, also denoted as alpha or α, is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference.	What is Alpha in statistics significance level
1826	Learning involves far more than thinking: it involves the whole personality - senses, feelings, intuition, beliefs, values and will.  Learning occurs when we are able to: Gain a mental or physical grasp of the subject. Make sense of a subject, event or feeling by interpreting it into our own words or actions.	What is learning and how do we learn
4447	The short answer is yes—because most regression models will not perfectly fit the data at hand. If you need a more complex model, applying a neural network to the problem can provide much more prediction power compared to a traditional regression.	Are neural networks good for regression
6339	Recurrent Neural Networks (RNNs) are a form of machine learning algorithm that are ideal for sequential data such as text, time series, financial data, speech, audio, video among others.	Does recurrent neural networks are best suited for text processing
7676	Learning rate decay (lrDecay) is a \emph{de facto} technique for training modern neural networks.  We provide another novel explanation: an initially large learning rate suppresses the network from memorizing noisy data while decaying the learning rate improves the learning of complex patterns.	How does learning rate decay help modern neural networks
196	This makes systematic sampling functionally similar to simple random sampling (SRS). However it is not the same as SRS because not every possible sample of a certain size has an equal chance of being chosen (e.g. samples with at least two elements adjacent to each other will never be chosen by systematic sampling).	Why is systematic sampling not random
1819	Conjoint analysis is a survey-based statistical technique used in market research that helps determine how people value different attributes (feature, function, benefits) that make up an individual product or service.	What is conjoint analysis usually used for
6208	You must use the t-distribution table when working problems when the population standard deviation (σ) is not known and the sample size is small (n<30). General Correct Rule: If σ is not known, then using t-distribution is correct. If σ is known, then using the normal distribution is correct.	When should we use the t distribution instead of the Z distribution
761	A simple test of consistency is that all frequencies should be positive. If any frequency is negative, it means that there is inconsistency in the sample data. If the data is consistent, all the ultimate class frequencies will be positive.	How do you know if data is consistent
2409	: a function (such as y = loga x or y = ln x) that is the inverse of an exponential function (such as y = ax or y = ex) so that the independent variable appears in a logarithm.	What is a logarithmic function definition
4224	: a function of a set of variables that is evaluated for samples of events or objects and used as an aid in discriminating between or classifying them.	What is a discriminant function
2167	Back-propagation is just a way of propagating the total loss back into the neural network to know how much of the loss every node is responsible for, and subsequently updating the weights in such a way that minimizes the loss by giving the nodes with higher error rates lower weights and vice versa.	What is backward propagation in neural network
4359	Unlike range and quartiles, the variance combines all the values in a data set to produce a measure of spread.  It is calculated as the average squared deviation of each number from the mean of a data set. For example, for the numbers 1, 2, and 3 the mean is 2 and the variance is 0.667.	What is variance and example
6071	You can start with a bimodal distribution of data and turn it into a standard normal distribution if you want.	Can you transform bimodal data
7096	Covariance: An Overview. Variance and covariance are mathematical terms frequently used in statistics and probability theory. Variance refers to the spread of a data set around its mean value, while a covariance refers to the measure of the directional relationship between two random variables.	What is the difference between covariance and variance
5117	There are multiple ways to select a good starting point for the learning rate. A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. We might start with a large value like 0.1, then try exponentially lower values: 0.01, 0.001, etc.	How does neural network choose learning rate
7466	Sparse matrix is a matrix which contains very few non-zero elements.  For example, consider a matrix of size 100 X 100 containing only 10 non-zero elements. In this matrix, only 10 spaces are filled with non-zero values and remaining spaces of the matrix are filled with zero.	What is sparse matrix example
1364	Spatiotemporal data mining refers to the process of discovering patterns and knowledge from spatiotemporal data.  Other examples of moving-object data mining include mining periodic patterns for one or a set of moving objects, and mining trajectory patterns, clusters, models, and outliers.	What is spatio temporal data mining
384	An experimental group is a test sample or the group that receives an experimental procedure. This group is exposed to changes in the independent variable being tested.  A control group is a group separated from the rest of the experiment such that the independent variable being tested cannot influence the results.	What is the difference between a control group and a test group
1179	The difference between true random number generators(TRNGs) and pseudo-random number generators(PRNGs) is that TRNGs use an unpredictable physical means to generate numbers (like atmospheric noise), and PRNGs use mathematical algorithms (completely computer-generated).	What is the difference between random number and pseudo random number
7201	Compared to simple random sampling, stratified sampling has two main disadvantages.Advantages and DisadvantagesA stratified sample can provide greater precision than a simple random sample of the same size.Because it provides greater precision, a stratified sample often requires a smaller sample, which saves money.More items	What are the advantages and disadvantages of stratified random sampling
2041	Put simply, batch processing is the process by which a computer completes batches of jobs, often simultaneously, in non-stop, sequential order. It's also a command that ensures large jobs are computed in small parts for efficiency during the debugging process.	What is the meaning of batch processing
2199	22 thousand	How many categories are there in ImageNet
1415	Binomial is defined as a math term meaning two expressions connected by a plus or minus sign. An example of a binomial is x – y.  An example of a binomial is Canis familiaris, the scientific name for dog.	What is binomial example
980	The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.  The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better.	What s so special about rectified linear units ReLU activation function
839	In machine learning, however, there's one way to tackle outliers: it's called “one-class classification” (OCC). This involves fitting a model on the “normal” data, and then predicting whether the new data collected is normal or an anomaly.	How does machine learning deal with outliers
4241	The Erlang distribution was developed by A. K. Erlang to examine the number of telephone calls which might be made at the same time to the operators of the switching stations. This work on telephone traffic engineering has been expanded to consider waiting times in queueing systems in general.	What is Erlang distribution used for
6799	A sampling distribution is where you take a population (N), and find a statistic from that population.  This is repeated for all possible samples from the population. Example: You hold a survey about college student's GRE scores and calculate that the standard deviation is 1.	How do you describe the sampling distribution
4149	AdaBoost is one of the first boosting algorithms to be adapted in solving practices. Adaboost helps you combine multiple “weak classifiers” into a single “strong classifier”.  → AdaBoost algorithms can be used for both classification and regression problem.	Can we use AdaBoost for regression
6968	Streaming Data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes).	What does streaming data mean
1932	Artificial intelligence (AI) makes it possible for machines to learn from experience, adjust to new inputs and perform human-like tasks. Most AI examples that you hear about today – from chess-playing computers to self-driving cars – rely heavily on deep learning and natural language processing.	What we can do with artificial intelligence
1196	classifiers, the time for training classifiers may actually decrease, since the training data set for each classifier is much smaller. . This general method can be extended to give a multiclass formulation of various kinds of linear classifiers.	Can SVM be used for multiclass classification
1213	The rank-sum test is a non-parametric hypothesis test that can be used to determine if there is a statistically significant association between categorical survey responses provided for two different survey questions. The use of this test is appropriate even when survey sample size is small.	What kind of statistical analysis should I use for surveys
1484	If your data are missing completely at random, you could consider listwise deletion: just remove the cases with missing values from your analysis. In addition to decision trees, logistic regression is the workhorse in the modelling in order to forecast the occurrence of an event.	What do you do with missing data in logistic regression
6408	A statistic is biased if it is calculated in such a way that it is systematically different from the population parameter being estimated. The following lists some types of biases, which can overlap. Selection bias involves individuals being more likely to be selected for study than others, biasing the sample.	What does it mean if a statistic is biased
613	For example, Q-learning is an off-policy learner.  Q-learning is called off-policy because the updated policy is different from the behavior policy, so Q-Learning is off-policy. In other words, it estimates the reward for future actions and appends a value to the new state without actually following any greedy policy.	Which is an example of off policy method in reinforcement learning
7405	In short, when a dependent variable is not distributed normally, linear regression remains a statistically sound technique in studies of large sample sizes. Figure 2 provides appropriate sample sizes (i.e., >3000) where linear regression techniques still can be used even if normality assumption is violated.	Does the dependent variable in regression need to be normally distributed
6460	You can think of an N-gram as the sequence of N words, by that notion, a 2-gram (or bigram) is a two-word sequence of words like “please turn”, “turn your”, or ”your homework”, and a 3-gram (or trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”	What is Unigram in NLP
6899	Developers can make use of NLP to perform tasks like speech recognition, sentiment analysis, translation, auto-correct of grammar while typing, and automated answer generation. NLP is a challenging field since it deals with human language, which is extremely diverse and can be spoken in a lot of ways.	What is the scope of NLP
4180	The term Markov chain refers to any system in which there are a certain number of states and given probabilities that the system changes from any state to another state.  If it doesn't rain today (N), then there is a 20% chance it will rain tomorrow and 80% chance of no rain.	What do you mean by Markov chains give any 2 examples
1267	The range can also be used to estimate another measure of spread, the standard deviation. Rather than go through a fairly complicated formula to find the standard deviation, we can instead use what is called the range rule. The range is fundamental in this calculation.	What are the uses of the range in statistics and what are the areas that we use range for calculations in statistics
5881	Using too large a batch size can have a negative effect on the accuracy of your network during training since it reduces the stochasticity of the gradient descent.	How does a larger batch size affect your training accuracy
1354	K nearest neighbors is a simple algorithm that stores all available cases and predict the numerical target based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970's as a non-parametric technique.	Can Knn be used for prediction
90	The usual logic of 2SLS doesn't work the same way for logit, since the underlying regression equations are latent (you only observe a categorical indicator instead of the underlying, interval-scaled response).	Can 2SLS be used for a logit model
7725	The global facial recognition market size is projected to grow from USD 3.2 billion in 2019 to USD 7.0 billion by 2024, at a CAGR of 16.6% from 2019 to 2024. The major factors driving the market include increased technological advancements across verticals.	How large is the market for face recognition and object recognition
1271	Although both techniques have certain similarities, the difference lies in the fact that classification uses predefined classes in which objects are assigned, while clustering identifies similarities between objects, which it groups according to those characteristics in common and which differentiate them from other	What is the difference between classification and clustering
3486	The squared hinge loss is differentiable because the term from the chain rule forces the limits to converge to the same number from both sides.	Why is squared hinge loss differentiable
7328	Time Series Forecast in RStep 1: Reading data and calculating basic summary.  Step 2: Checking the cycle of Time Series Data and Plotting the Raw Data.  Step 3: Decomposing the time series data.  Step 4: Test the stationarity of data.  Step 5: Fitting the model.  Step 6: Forecasting.	How do you forecast time series data
1939	The product moment correlation coefficient (pmcc) can be used to tell us how strong the correlation between two variables is. A positive value indicates a positive correlation and the higher the value, the stronger the correlation.  If there is a perfect negative correlation, then r = -1.	What does product moment correlation coefficient mean
8683	Difficulties in NLU Syntax Level ambiguity − A sentence can be parsed in different ways. For example, “He lifted the beetle with red cap.” − Did he use cap to lift the beetle or he lifted a beetle that had red cap? Referential ambiguity − Referring to something using pronouns. For example, Rima went to Gauri.	What are difficulties in NLU
3373	Definition: The trend is the component of a time series that represents variations of low frequency in a time series, the high and medium frequency fluctuations having been filtered out.	What is trend in time series data
522	Conclusion. Human intelligence revolves around adapting to the environment using a combination of several cognitive processes. The field of Artificial intelligence focuses on designing machines that can mimic human behavior. However, AI researchers are able to go as far as implementing Weak AI, but not the Strong AI.	What is the difference between intelligence and artificial intelligence
4117	A matrix is a linear operator acting on the vector space of column vectors. Per linear algebra and its isomorphism theorems, any vector space is isomorphic to any other vector space of the same dimension. As such, matrices can be seen as representations of linear operators subject to some basis of column vectors.	Is a matrix an operator
1575	Statistical inference consists in the use of statistics to draw conclusions about some unknown aspect of a population based on a random sample from that population.  Point estimation is discussed in the statistics section of the encyclopedia.	Which helps to make inferences about a population
7343	Conversely, according to the fundamental theorem of calculus, Eq. (1.7), p(x) = F′(x). Thus, the probability density is the derivative of the cumulative distribution function. This in turn implies that the probability density is always nonnegative, p(x) ≥ 0, because F is monotone increasing.	Why is PDF derivative of CDF
354	Reduce Variance of an Estimate If we want to reduce the amount of variance in a prediction, we must add bias. Consider the case of a simple statistical estimate of a population parameter, such as estimating the mean from a small random sample of data. A single estimate of the mean will have high variance and low bias.	How do you reduce the variance of data
4228	Keras is a high-level interface and uses Theano or Tensorflow for its backend. It runs smoothly on both CPU and GPU. Keras supports almost all the models of a neural network – fully connected, convolutional, pooling, recurrent, embedding, etc. Furthermore, these models can be combined to build more complex models.	Is keras part of TensorFlow
308	0:003:12Suggested clip · 104 secondsBeta distribution: mean - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the mean of a Beta distribution
5652	The False Discovery Rate approach is a more recent development. This approach also determines adjusted p-values for each test.  An FDR adjusted p-value (or q-value) of 0.05 implies that 5% of significant tests will result in false positives. The latter will result in fewer false positives.	What is FDR p value
4101	A feature map is formed by different units in a CNN that share the same weights and biases. For example:  Basically they are feature extractors/filters learned through training. When convolved with the input and passed through the activation function, they generate meaningful inputs for the next layer or output.	What is a convolutional feature map
8151	For distributions that are strongly skewed or have outliers, the median is often the most appropriate measure of central tendency because in skewed distributions the mean is pulled out toward the tail. The median is more resistant to outliers compared to the mean.	What measures should be used in skewed distributions
665	In other words, stream processing receives and analyses data in a continuous stream without delays. In the past, data was stored in a database and prepped for analysis. Stream processing allows users to skip storage and go straight into analysis allowing users to gain insights at a faster rate than before.	What is stream processing in big data and what does it do
3454	Fourier Methods in Signal Processing The Fourier transform and discrete-time Fourier transform are mathematical analysis tools and cannot be evaluated exactly in a computer. The Fourier transform is used to analyze problems involving continuous-time signals or mixtures of continuous- and discrete-time signals.	What is the use of Fourier transform in signal processing
6335	Summing up, a more precise statement of the universality theorem is that neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision.	Can neural networks approximate any function
2328	If there are only two variables, one is continuous and another one is categorical, theoretically, it would be difficult to capture the correlation between these two variables.	Is it possible capture the correlation between continuous and categorical variable
5124	Average-linkage is where the distance between each pair of observations in each cluster are added up and divided by the number of pairs to get an average inter-cluster distance. Average-linkage and complete-linkage are the two most popular distance metrics in hierarchical clustering.	How does Average Linkage work in Hierarchical Agglomerative clustering
7300	It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case.	Which is better random forest or XGBoost
3687	The amount that the weights are updated during training is referred to as the step size or the “learning rate.” Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.	What is step size in machine learning
995	The mean is the average of a group of numbers, and the variance measures the average degree to which each number is different from the mean.	What is the relationship between mean and variance
340	Nevertheless, the same has been delineated briefly below:Step 1: Visualize the Time Series. It is essential to analyze the trends prior to building any kind of time series model.  Step 2: Stationarize the Series.  Step 3: Find Optimal Parameters.  Step 4: Build ARIMA Model.  Step 5: Make Predictions.	How do you do a time series analysis
256	Word embedding and topic modeling come from two different research communities. Word embeddings come from the neural net research tradition, while topic modelings come from Bayesian model research tradition. Word embedding can be used to improve topic models like Lda2Vec.	What is the relationship between word embedding and topic modeling Can we use word embedding to enhance topic modeling
2913	"A confusion matrix is a table that is often used to describe the performance of a classification model (or ""classifier"") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing."	What does confusion matrix mean
6603	The sampling frequency or sampling rate, fs, is the average number of samples obtained in one second (samples per second), thus fs = 1/T.	How do you find the frequency of a sampled signal
177	Creative Ways to Benefit From Social Media AnalyticsEngage Better With Your Audience. Many businesses have a hard time keeping up with the vast amount of social media activity that impacts their brand.  Improve Customer Relations.  Monitor Your Competition.  Identify and Engage With Your Top Customers.  Find Out Where Your Industry is Heading.	What are the benefits of social media analytics
7698	In artificial intelligence, an intelligent agent (IA) refers to an autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent).	What should we call an agent in artificial intelligence
4107	The short answer is yes—because most regression models will not perfectly fit the data at hand. If you need a more complex model, applying a neural network to the problem can provide much more prediction power compared to a traditional regression.	Can recurrent neural networks be used in regression scenario
799	When working with a measurement variable, the Kruskal–Wallis test starts by substituting the rank in the overall data set for each measurement value. The smallest value gets a rank of 1, the second-smallest gets a rank of 2, etc.	How do you rank data for the Kruskal Wallis test
7136	A modern approach to reducing generalization error is to use a larger model that may be required to use regularization during training that keeps the weights of the model small. These techniques not only reduce overfitting, but they can also lead to faster optimization of the model and better overall performance.	How can neural network errors be reduced
5516	While the trials are independent, their outcomes X are dependent because they must be summed to n. ; in this form, a categorical distribution is equivalent to a multinomial distribution over a single trial.	Is categorical distribution the same as multinomial distribution
368	The arithmetic mean is appropriate if the values have the same units, whereas the geometric mean is appropriate if the values have differing units. The harmonic mean is appropriate if the data values are ratios of two variables with different measures, called rates.	What is the difference between arithmetic mean geometric mean and harmonic mean
3064	Asynchronous data is data that is not synchronized when it is sent or received.  This usually refers to data that is transmitted at intermittent intervals rather than in a steady stream, which means that the first parts of the complete file might not always be the first to be sent and arrive at the destination.	What do you mean by asynchronous data transfer
4099	: being or having the shape of a normal curve or a normal distribution.	What does Gaussian mean
1858	"A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for ""unsharp masking"" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."	What does a Gaussian filter do
541	In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a separate vote. The prediction which we get from the majority of the models is used as the final prediction.	What is the purpose of aggregating the predictions of multiple models in data science
550	The item response theory (IRT), also known as the latent response theory refers to a family of mathematical models that attempt to explain the relationship between latent traits (unobservable characteristic or attribute) and their manifestations (i.e. observed outcomes, responses or performance).	What is an IRT model
2639	Boosting is an ensemble modeling technique which attempts to build a strong classifier from the number of weak classifiers. It is done building a model by using weak models in series.  AdaBoost was the first really successful boosting algorithm developed for the purpose of binary classification.	What is boosting technique
6861	Scale Invariant Feature Transform (SIFT) is an image descriptor for image-based matching and recognition developed by David Lowe (1999, 2004).  The SIFT descriptor has also been extended from grey-level to colour images and from 2-D spatial images to 2+1-D spatio-temporal video.	What is a descriptor in the context of a scale invariant feature transform SIFT
1982	Topic modelling refers to the task of identifying topics that best describes a set of documents.  And the goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics.	How does LDA topic Modelling work
294	Data Science vs Business Analytics, often used interchangeably, are very different domains.  Simply put, Data science is the study of Data using statistics which provides key insights but not business changing decisions whereas Business Analytics is the analysis of data to make key business decisions for the company.	How is business analytics different from data science
6937	The Euclidean distance corresponds to the L2-norm of a difference between vectors. The cosine similarity is proportional to the dot product of two vectors and inversely proportional to the product of their magnitudes.	Why cosine similarity is better than Euclidean distance
61	Cross-entropy can be calculated using the probabilities of the events from P and Q, as follows: H(P, Q) = – sum x in X P(x) * log(Q(x))	How do you calculate cross entropy
976	Deep learning requires large amounts of labeled data. For example, driverless car development requires millions of images and thousands of hours of video. Deep learning requires substantial computing power. High-performance GPUs have a parallel architecture that is efficient for deep learning.	What is needed for deep learning
8607	In qualitative research no hypotheses or relationships of variables are tested. Because variables must be defined numerically in hypothesis-testing research, they cannot reflect subjective experience. This leads to hypothesis-generating research using the grounded theory method to study subjective experience directly.	Does a qualitative study have variables
3171	In contrast to the non-stationary process that has a variable variance and a mean that does not remain near, or returns to a long-run mean over time, the stationary process reverts around a constant long-term mean and has a constant variance independent of time.	What is stationary and non stationary time series
7717	An independent variable, sometimes called an experimental or predictor variable, is a variable that is being manipulated in an experiment in order to observe the effect on a dependent variable, sometimes called an outcome variable.	Is predictor variable same as independent variable
2420	Statistical significance is a determination by an analyst that the results in the data are not explainable by chance alone.  A p-value of 5% or lower is often considered to be statistically significant.	What does significance refer to in a statistical test
1446	Arithmetic mean is calculated by dividing the sum of the numbers by number count. However, Geometric means takes into account the compounding effect while calculation.	What is difference between arithmetic mean and geometric mean
5862	Distance MatrixThe proximity between object can be measured as distance matrix.  For example, distance between object A = (1, 1) and B = (1.5, 1.5) is computed as.Another example of distance between object D = (3, 4) and F = (3, 3.5) is calculated as.More items	How do you find the distance of a matrix
7954	Artificial Intelligence (AI) is the ability for an artificial machine to act intelligently. Logic Programming is a method that computer scientists are using to try to allow machines to reason because it is useful for knowledge representation.  The diagram below shows the essence of logic programming.	What is logic programming in artificial intelligence
5010	The only goal PCA and other dimension reduction techniques accomplish is just that; reducing the dimensions of your feature space, thus driving down computational cost and time. Whether or not you decide to normalize the data is a completely independent matter. In short, don't skip on normalization.	Does it need feature normalization after dimension reduction for classification
1398	A statistical model is a mathematical representation (or mathematical model) of observed data. When data analysts apply various statistical models to the data they are investigating, they are able to understand and interpret the information more strategically.	What are statistical models used for
6956	A variable whose Output property is Yes is an output variable. When the script runs, any value assigned to the variable is saved for use outside of the script. Its value is output to external storage when the script executes.	What is the output variable
3413	Both skew and kurtosis can be analyzed through descriptive statistics. Acceptable values of skewness fall between − 3 and + 3, and kurtosis is appropriate from a range of − 10 to + 10 when utilizing SEM (Brown, 2006).	What kurtosis is acceptable
8481	spark. mllib is the first of the two Spark APIs while org.apache.spark.ml is the new API.  mllib carries the original API built on top of RDDs. spark.ml contains higher-level API built on top of DataFrames for constructing ML pipelines.	What is the difference between spark ml and spark Mllib
730	In general, as sample size increases, the difference between expected adjusted r-squared and expected r-squared approaches zero; in theory this is because expected r-squared becomes less biased. the standard error of adjusted r-squared would get smaller approaching zero in the limit.	Is R Squared affected by sample size
1294	"A decision tree is a simple representation for classifying examples. For this section, assume that all of the input features have finite discrete domains, and there is a single target feature called the ""classification"". Each element of the domain of the classification is called a class."	What is a class in decision tree learning
7317	As you prepare to conduct your statistics, it is important to consider testing the assumptions that go with your analysis. Assumption testing of your chosen analysis allows you to determine if you can correctly draw conclusions from the results of your analysis.	Why do we check assumptions before performing statistical tests
8162	A probability density plot simply means a density plot of probability density function (Y-axis) vs data points of a variable (X-axis).  By showing probability density plots, we're only able to understand the distribution of data visually without knowing the exact probability for a certain range of values.	What is a probability density plot
3497	Multivariate ANOVA (MANOVA) extends the capabilities of analysis of variance (ANOVA) by assessing multiple dependent variables simultaneously. ANOVA statistically tests the differences between three or more group means.  This statistical procedure tests multiple dependent variables at the same time.	Is Anova multivariate analysis
1233	Verify that the partial derivative Fxy is correct by calculating its equivalent, Fyx, taking the derivatives in the opposite order (d/dy first, then d/dx). In the above example, the derivative d/dy of the function f(x,y) = 3x^2*y - 2xy is 3x^2 - 2x.	How do you find the partial derivative of fxy
1209	The significance of Matrix is - they represent Linear transformations like rotation/scaling. A Matrix is just a stack of numbers - but very special - you can add them and subtract them and multiply them [restrictions]. The significance of Matrix is - they represent Linear transformations like rotation/scaling.	What is the significance of Matrix
5405	A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features or business intelligence in the input data. Perceptron was introduced by Frank Rosenblatt in 1957.  A Perceptron is an algorithm for supervised learning of binary classifiers.	Is a Perceptron a neural network
5986	"The decision rule is: Reject H0 if Z < 1.645. The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960. The complete table of critical values of Z for upper, lower and two-tailed tests can be found in the table of Z values to the right in ""Other Resources."""	What is the rule for rejecting Ho in terms of Z
7606	Face recognition systems use computer algorithms to pick out specific, distinctive details about a person's face. These details, such as distance between the eyes or shape of the chin, are then converted into a mathematical representation and compared to data on other faces collected in a face recognition database.	How does the facial recognition technology work
7840	A box plot (also known as box and whisker plot) is a type of chart often used in explanatory data analysis to visually show the distribution of numerical data and skewness through displaying the data quartiles (or percentiles) and averages.	What does a box plot represent
8009	Descriptive statistics describes data (for example, a chart or graph) and inferential statistics allows you to make predictions (“inferences”) from that data. With inferential statistics, you take data from samples and make generalizations about a population.	What is descriptive and inferential statistics
713	Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are. Instead, they need to be recoded into a series of variables which can then be entered into the regression model.	How do you deal with categorical variables in regression
742	False negatives — that is, a test that says you don't have the virus when you actually do have the virus — may occur.	What does false negative COVID-19 test result mean
2193	To define an optimal hyperplane we need to maximize the width of the margin (w).  In this situation SVM finds the hyperplane that maximizes the margin and minimizes the misclassifications. The algorithm tries to maintain the slack variable to zero while maximizing margin.	What is W in SVM
2726	A learning curve plots the score over varying numbers of training samples, while a validation curve plots the score over a varying hyper parameter. The learning curve is a tool for finding out if an estimator would benefit from more data, or if the model is too simple (biased).	What are learning and validation curve in Machine learning
1060	R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  For instance, small R-squared values are not always a problem, and high R-squared values are not necessarily good!	Is R Squared a good measure of model performance
4998	Word sense disambiguation, in natural language processing (NLP), may be defined as the ability to determine which meaning of word is activated by the use of word in a particular context.  Lexical ambiguity, syntactic or semantic, is one of the very first problem that any NLP system faces.	What is word sense disambiguation give example
8104	“Covariance” indicates the direction of the linear relationship between variables. “Correlation” on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance.	What is the difference between correlation and covariance
8288	The clear sign of a machine learning overfitting is if its error on testing set is much greater than the error on training set.  For instance if the model accuracy for train data is 85% and the accuracy for test/validation data is 65% then its very obvious that the model has overlearned and you should check that.	How do you know when your learning algorithm has Overfit a model
8611	If there is no relationship between X and Y, the best guess for all values of X is the mean of Y. At any rate, the regression line always passes through the means of X and Y. This means that, regardless of the value of the slope, when X is at its mean, so is Y.	Why does regression line go through mean
458	A statistic is biased if the long-term average value of the statistic is not the parameter it is estimating. More formally, a statistic is biased if the mean of the sampling distribution of the statistic is not equal to the parameter.  Therefore the sample mean is an unbiased estimate of μ.	Is mean a biased estimator
2268	Performance wise SVMs using the radial basis function kernel are more likely to perform better as they can handle non-linearities in the data. Naive Bayes performs best when the features are independent of each other which often does not happen in real.	Which is better SVM or naive Bayes
559	The Word2Vec Model This model was created by Google in 2013 and is a predictive deep learning based model to compute and generate high quality, distributed and continuous dense vector representations of words, which capture contextual and semantic similarity.	Is Word2Vec deep learning
5744	Some applications of unsupervised machine learning techniques are: Clustering automatically split the dataset into groups base on their similarities. Anomaly detection can discover unusual data points in your dataset. It is useful for finding fraudulent transactions.	What is an example application of unsupervised machine learning
2713	Latent semantic analysis (LSA) is a mathematical method for computer modeling and simulation of the meaning of words and passages by analysis of representative corpora of natural text. LSA closely approximates many aspects of human language learning and understanding.	What is the current state of latent semantic analysis
1029	Parametric tests are those that make assumptions about the parameters of the population distribution from which the sample is drawn. This is often the assumption that the population data are normally distributed. Non-parametric tests are “distribution-free” and, as such, can be used for non-Normal variables.	What is parametric and non parametric test in SPSS
215	Association rule mining is a procedure which aims to observe frequently occurring patterns, correlations, or associations from datasets found in various kinds of databases such as relational databases, transactional databases, and other forms of repositories.	What is an association rule in data mining
755	"The term ""negative binomial"" is likely due to the fact that a certain binomial coefficient that appears in the formula for the probability mass function of the distribution can be written more simply with negative numbers."	Why is negative binomial called negative
5785	As the formula shows, the standard score is simply the score, minus the mean score, divided by the standard deviation.	How do you calculate standard score
973	In simple words, Instance refers to the copy of the object at a particular time whereas object refers to the memory address of the class.	What is the difference between class object and class instance
7526	1:5313:32Suggested clip · 105 secondsDimensional Analysis Explained! - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you explain dimensional analysis
421	Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to false positive.	What is the difference between Type 1 and Type 2 error
6103	When class intervals are unequal, we take Frequency Density on Y axis. Based on this find the tallest class interval and follow the regular method of joining top corners of tallest column to the top corners of the opposite adjacent columns. The X-coordinate of Intersection point would give value of Mode.	How do you find the mode in case of unequal class intervals
3760	Decision trees are a classic machine learning technique. The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree. By Narendra Nath Joshi, Carnegie Mellon.  The basic intuition behind a decision tree is to map out all possible decision paths in the form of a tree.	What is an intuitive explanation of a decision tree
8241	Decision theory is an interdisciplinary approach to arrive at the decisions that are the most advantageous given an uncertain environment. Decision theory brings together psychology, statistics, philosophy, and mathematics to analyze the decision-making process.	What do you mean by decision theory
4902	No, logistic regression does not require any particular distribution for the independent variables. They can be normal, skewed, categorical or whatever. No regression method makes assumptions about the shape of the distribution of either the IVs or the DV.	Does logistic regression require normal distribution
1061	The scale-invariant feature transform (SIFT) is an algorithm used to detect and describe local features in digital images.  The descriptors are supposed to be invariant against various transformations which might make images look different although they represent the same object(s).	Why is sift scale invariant
6342	The beta distribution of the first kind, usually written in terms of the incom- plete beta function, can be used to model the distribution of measurements whose values all lie between zero and one. It can also be used to model the distribution for the probability of occurrence of some discrete event.	What is the significance of the beta distribution What are some common applications
5230	Therefore, a low test–retest reliability correlation might be indicative of a measure with low reliability, of true changes in the persons being measured, or both. That is, in the test–retest method of estimating reliability, it is not possible to separate the reliability of measure from its stability.	What does low test retest reliability mean
7842	In machine learning, the term inductive bias refers to a set of (explicit or implicit) assumptions made by a learning algorithm in order to perform induction, that is, to generalize a finite set of observation (training data) into a general model of the domain.	What is inductive bias in machine learning
4062	For example, let's say a child received a scaled score of 8, with a 95% confidence interval range of 7-9. This means that with high certainty, the child's true score lies between 7 and 9, even if the received score of 8 is not 100% accurate.	How do you explain confidence interval to a child
1310	Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.	What is sentiment analysis in natural language processing
6869	Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression.	Is it possible to apply logistic regression algorithm on a class classification problem 3
5994	The Regression Fallacy occurs when one mistakes regression to the mean, which is a statistical phenomenon, for a causal relationship. For example, if a tall father were to conclude that his tall wife committed adultery because their children were shorter, he would be committing the regression fallacy.	What is an example of a regression fallacy
3455	Cumulative relative frequency distribution–showsthe proportionof items with values less than orequal to the upper limit of each class. Cumulative DistributionsCumulative percent frequency distribution–showsthe percentageof items with values less than orequal to the upper limit of each class.	What values are displayed on a cumulative relative frequency distribution
5040	According to research conducted at Cornell University, researchers state that “the traditional phrase-based translation system which consists of many small sub-components that are tuned separately, neural machine translation attempts to build and train a single, large neural network that reads a sentence and outputs a	How does neural machine translation work
653	"The difference is simple and conceptual. A class is a template for objects.  An object is a member or an ""instance"" of a class. An object has a state in which all of its properties have values that you either explicitly define or that are defined by default settings."	What is the difference between class and objects
84	Quantiles are points in a distribution that relate to the rank order of values in that distribution. For a sample, you can find any quantile by sorting the sample. The middle value of the sorted sample (middle quantile, 50th percentile) is known as the median.	What is the quantile of a distribution
1038	The cumulative distribution function (CDF) calculates the cumulative probability for a given x-value. Use the CDF to determine the probability that a random observation that is taken from the population will be less than or equal to a certain value.	What does a cumulative distribution function tell you
924	The Effect Smart Market is a peer-to-peer marketplace for artificial intelligence algorithms. It offers a web-based service that opens up the global AI market to all. It is the easiest way to buy, sell and trade AI-powered solutions with fast and reliable API access.	What is effect AI
7333	Linearity assumption This can be done by visually inspecting the scatter plot between each predictor and the logit values. The smoothed scatter plots show that variables glucose, mass, pregnant, pressure and triceps are all quite linearly associated with the diabetes outcome in logit scale.	How do you check the linearity assumption in logistic regression
1348	The three main metrics used to evaluate a classification model are accuracy, precision, and recall. Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.	How do you evaluate the performance of a machine learning model
3805	Seriously, the p value is literally a confounded index because it reflects both the size of the underlying effect and the size of the sample. Hence any information included in the p value is ambiguous (Lang et al. 1998).  The smaller the sample, the less likely the result will be statistically significant.	Why are p values considered confounded statistics
7756	Linear models, generalized linear models, and nonlinear models are examples of parametric regression models because we know the function that describes the relationship between the response and explanatory variables.  If the relationship is unknown and nonlinear, nonparametric regression models should be used.	Is linear regression non parametric
956	For a binary classification like our example, the typical loss function is the binary cross-entropy / log loss.	What loss is used for binary classification
615	A matrix A is symmetric if it is equal to its transpose, i.e., A=AT. A matrix A is symmetric if and only if swapping indices doesn't change its components, i.e., aij=aji.	What makes a matrix symmetric
7611	Advantages of Systematic SamplingEasy to Execute and Understand.Control and Sense of Process.Clustered Selection Eliminated.Low Risk Factor.Assumes Size of Population Can Be Determined.Need for Natural Degree of Randomness.Greater Risk of Data Manipulation.	What are the advantages of using systematic sampling
1411	The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution.	Is Z score the test statistic
618	Data augmentation is a technique to artificially create new training data from existing training data. This is done by applying domain-specific techniques to examples from the training data that create new and different training examples.  The intent is to expand the training dataset with new, plausible examples.	How does data augmentation work
4526	While the variance and the standard error of the mean are different estimates of variability, one can be derived from the other. Multiply the standard error of the mean by itself to square it. This step assumes that the standard error is a known quantity.	Is variance and standard error the same
1199	Mutual information is a distance between two probability distributions. Correlation is a linear distance between two random variables.  If you are working with variables that are smooth, correlation may tell you more about them; for instance if their relationship is monotonic.	What is the difference between mutual information and correlation
4569	In statistical hypothesis testing, the null distribution is the probability distribution of the test statistic when the null hypothesis is true. For example, in an F-test, the null distribution is an F-distribution. Null distribution is a tool scientists often use when conducting experiments.	What is the distribution of the test statistic under the null hypothesis
44	The Basics of a One-Tailed Test Hypothesis testing is run to determine whether a claim is true or not, given a population parameter. A test that is conducted to show whether the mean of the sample is significantly greater than and significantly less than the mean of a population is considered a two-tailed test.	What do you mean by one tailed test and two tailed test
4898	The primary reason skew is important is that analysis based on normal distributions incorrectly estimates expected returns and risk.  Knowing that the market has a 70% probability of going up and a 30% probability of going down may appear helpful if you rely on normal distributions.	Why is skewness important in statistics
1362	The correlation coefficient is the specific measure that quantifies the strength of the linear relationship between two variables in a correlation analysis. The coefficient is what we symbolize with the r in a correlation report.	What is the correlation coefficient between the variables
7535	A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed.	What do you mean by Markov chain
749	Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable).	What is the dependent variable in multiple regression
7446	Univariate statistics summarize only one variable at a time. Bivariate statistics compare two variables. Multivariate statistics compare more than two variables.	What is the difference between univariate and multivariate analysis
6015	Discriminant analysis is a versatile statistical method often used by market researchers to classify observations into two or more groups or categories. In other words, discriminant analysis is used to assign objects to one group among a number of known groups.	What is the purpose of discriminant analysis
6923	Machine learning algorithms find natural patterns in data that generate insight and help you make better decisions and predictions. They are used every day to make critical decisions in medical diagnosis, stock trading, energy load forecasting, and more.	What can machine learning be used for
708	Supervised learning is the category of machine learning algorithms that require annotated training data. For instance, if you want to create an image classification model, you must train it on a vast number of images that have been labeled with their proper class. “[Deep learning] is not supervised learning.	Is supervised learning deep learning
4321	Squared loss is a loss function that can be used in the learning setting in which we are predicting a real-valued variable y given an input variable x.	What is squared loss
863	The Cox (proportional hazards or PH) model (Cox, 1972) is the most commonly used multivariate approach for analysing survival time data in medical research. It is a survival analysis regression model, which describes the relation between the event incidence, as expressed by the hazard function and a set of covariates.	What is multivariate Cox regression analysis
5668	In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory.	What is the basic idea of instance based learning
804	Confidence intervals and hypothesis tests are similar in that they are both inferential methods that rely on an approximated sampling distribution. Confidence intervals use data from a sample to estimate a population parameter. Hypothesis tests use data from a sample to test a specified hypothesis.	What is the confidence interval method of hypothesis testing
6575	A residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a nonlinear model is more appropriate.	What is the correct residual plot to look at after a multiple regression
309	A variable is said to be continuous if it can assume an infinite number of real values. Examples of a continuous variable are distance, age and temperature. The measurement of a continuous variable is restricted by the methods used, or by the accuracy of the measuring instruments.	Is age a continuous variable
1920	Elastic net regularization adds an additional ridge regression-like penalty which improves performance when the number of predictors is larger than the sample size, allows the method to select strongly correlated variables together, and improves overall prediction accuracy.	What is the purpose of penalties regularization used in Ridge and lasso regression
2073	A class is a blueprint which you use to create objects. An object is an instance of a class - it's a concrete 'thing' that you made using a specific class. So, 'object' and 'instance' are the same thing, but the word 'instance' indicates the relationship of an object to its class.	What is the difference between an object class definition and an object instance
6042	In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well.	What is Embeddings in machine learning
7366	"A probability-predicting regression model can be used as part of a classifier by imposing a decision rule - for example, if the probability is 50% or more, decide it's a cat.  There are also ""true"" classification algorithms, such as SVM, which only predict an outcome and do not provide a probability."	Can regression be used for classification
4113	Average can simply be defined as the sum of all the numbers divided by the total number of values. Average is usually present as mean or arithmetic mean. Mean is simply a method of describing the average of the sample.  The arithmetic mean is considered as a form of average.	What is the difference between arithmetic mean and average
6122	In statistics, a Multimodal distribution is a probability distribution with two different modes, may also be referred to as a bimodal distribution. These appear as distinct peaks (local maxima) in the probability density function, as shown in Figures 1 and 2.	What does a multimodal distribution mean
756	Given two competing hypotheses and some relevant data, Bayesian hypothesis testing begins by specifying separate prior distributions to quantitatively describe each hypothesis. The combination of the likelihood function for the observed data with each of the prior distributions yields hypothesis-specific models.	What is Bayesian hypothesis testing
591	To train a deep neural network to classify sequence data, you can use an LSTM network. An LSTM network enables you to input sequence data into a network, and make predictions based on the individual time steps of the sequence data.	Can we use Lstm for classification
6908	Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and other fields, to find a linear combination of features that characterizes or separates two or more classes	What is discriminant function in Pattern Recognition
2899	"The predicted value of y (""ˆy "") is sometimes referred to as the ""fitted value"" and is computed as ˆyi=b0+b1xi y ^ i = b 0 + b 1 x i ."	How do you find the predicted value of y
6880	The weight of a fire fighter would be an example of a continuous variable; since a fire fighter's weight could take on any value between 150 and 250 pounds.	Is weight a continuous or discrete variable
212	It shows that the dependent variable (IQ) is significantly correlated with all the variables included in the analysis, except for sex, and physical attractiveness is more strongly associated with general intelligence than any other variable.	Is there a correlation between attractiveness and intelligence
5356	Dynamic Programming is used to obtain the optimal solution.  In Dynamic Programming, we choose at each step, but the choice may depend on the solution to sub-problems. 2. In a greedy Algorithm, we make whatever choice seems best at the moment and then solve the sub-problems arising after the choice is made.	How does the dynamic programming differ from greedy algorithm
6207	Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex. The goals of artificial intelligence include learning, reasoning, and perception.	What do you mean by intelligence in AI
597	A Z score is the number of standard deviations a given result is above (positive score) or below (negative score) the age- and sex-adjusted population mean. Results that are within the IGF-1 reference interval will have a Z score between -2.0 and +2.0.	What is Z score in blood test
108	There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. For a robot that is learning to walk, the state is the position of its two legs. For a Go program, the state is the positions of all the pieces on the board.	What is a state in reinforcement learning
113	1. A pattern recognition technique that is used to categorize a huge number of data into different classes.	What is feature classification
3033	The Vanishing Gradient problem, is when the signal parsing of SGD (Stochastic Gradient Descent) or other forms of GD (Gradient Descent) - becomes so small - or the signal becomes so approximative small - that the signal registers as 0.Think of the signal as balancing between two modes:1 and 0.More items	How do you find the vanishing gradient
5414	A joint probability distribution shows a probability distribution for two (or more) random variables. Instead of events being labeled A and B, the norm is to use X and Y. The formal definition is: f(x,y) = P(X = x, Y = y) The whole point of the joint distribution is to look for a relationship between two variables.	What is joint distribution in statistics
143	The dependent variable is the variable that is being measured or tested in an experiment. For example, in a study looking at how tutoring impacts test scores, the dependent variable would be the participants' test scores, since that is what is being measured.	How do you determine the dependent variable
6636	Variance plays a major role in interpreting data in statistics. The most common application of variance is in polls. For opinion polls, the data gathering agencies cannot invest in collecting data from the entire population.	How is variance used in real life
6832	Every probability pi is a number between 0 and 1, and the sum of all the probabilities is equal to 1. Examples of discrete random variables include: The number of eggs that a hen lays in a given day (it can't be 2.3) The number of people going to a given soccer match.	What are examples of discrete random variables
3485	Role of Scaling is mostly important in algorithms that are distance based and require Euclidean Distance. Random Forest is a tree-based model and hence does not require feature scaling.	Is feature scaling required for random forest
6643	Many classification and clustering methods depend upon some measure of distance and similarity or distance between objects. If they do, then they can use cosine similarity. Similarity measures are not machine learning algorithm per se, but they play an integral part.	Is cosine similarity machine learning
721	Principal Component Analysis (PCA) is used to explain the variance-covariance structure of a set of variables through linear combinations. It is often used as a dimensionality-reduction technique.	What is the use of principal component analysis
7634	Deep Belief NetworksTrain the first layer as an RBM that models the raw input.  Use that first layer to obtain a representation of the input that will be used as data for the second layer.  Train the second layer as an RBM, taking the transformed data (samples or mean activations) as training examples (for the visible layer of that RBM).More items	How do you train a deep belief network
4075	Receiver Operating Characteristics (ROC) Curve For classification models, there are many other evaluation methods like Gain and Lift charts, Gini coefficient etc. But the in depth knowledge about the confusion matrix can help to evaluate any classification model very effectively.	How do you evaluate a classification model
864	If your data are missing completely at random, you could consider listwise deletion: just remove the cases with missing values from your analysis. In addition to decision trees, logistic regression is the workhorse in the modelling in order to forecast the occurrence of an event.	How does logistic regression handle missing data
4145	Matrix factorization using the alternating least squares algorithm for collaborative filtering. Alternating least squares (ALS) is an optimization technique to solve the matrix factorization problem. This technique achieves good performance and has proven relatively easy to implement.	What is the significance of alternating least squares in collaborative filtering
642	A random variable is a variable that takes specific values with specific probabilities. It can be thought of as a variable whose value depends on the outcome of an uncertain event. 2. We usually denote random variables by capital letters near the end of the alphabet; e.g., X,Y,Z.	How do you denote a random variable
644	In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer.	What does positively skewed mean in statistics
542	A sampling frame is a list or other device used to define a researcher's population of interest. The sampling frame defines a set of elements from which a researcher can select a sample of the target population.  Comprehensiveness refers to the degree to which a sampling frame covers the entire target population.	What do you mean by sampling frame
4744	Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables: One variable, denoted x, is regarded as the predictor, explanatory, or independent variable.	What does linear regression analysis tell you
2227	If you are working on a classification problem, the best score is 100% accuracy. If you are working on a regression problem, the best score is 0.0 error. These scores are an impossible to achieve upper/lower bound. All predictive modeling problems have prediction error.	What is a good accuracy for machine learning model
5232	Definition of 'average deviation' 1. the difference between an observed value of a variable and its mean. 2. Also: mean deviation from the mean, mean deviation from the median, average deviation.	What is the meaning of average deviation
2406	Bayesian neural networks differ from plain neural networks in that their weights are assigned a probability distribution instead of a single value or point estimate. These probability distributions describe the uncertainty in weights and can be used to estimate uncertainty in predictions.	What are Bayesian neural networks
224	Simpson's paradox can be avoided by selecting an appropriate experimental design and analysis that incorporates the confounding variable in such a way as to obtain unconfounded estimates of treatment effects, thus more accurately answering the research question.	How do you avoid Simpson's paradox
5411	Random Walk states that stock prices cannot be reliably predicted.  In the EMH, prices reflect all the relevant information regarding a financial asset; while in Random Walk, prices literally take a 'random walk' and can even be influenced by 'irrelevant' information.	What is the difference between the Efficient Market Hypothesis and the Random Walk Theory
4559	In mathematics, measurement typically refers to understanding units and precision in problems that deal with most concrete measures such as length, area, and volume. But, in statistics, measurement can be a bit more abstract.  Statistics, however, utilizes inductive reasoning and conclusions are always uncertain.	What is the difference between mathematics and statistics
5704	Let's GO!Step 0 : Pre-requisites. It is recommended that before jumping on to Deep Learning, you should know the basics of Machine Learning.  Step 1 : Setup your Machine.  Step 2 : A Shallow Dive.  Step 3 : Choose your own Adventure!  Step 4 : Deep Dive into Deep Learning.	How do you implement deep learning
1712	The binomial is a type of distribution that has two possible outcomes (the prefix “bi” means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail. A Binomial Distribution shows either (S)uccess or (F)ailure.	What is binomial distribution with example
2368	Eight Steps on How to Reduce Bias in AIDefine and narrow the business problem you're solving.  Structure data gathering that allows for different opinions.  Understand your training data.  Gather a diverse ML team that asks diverse questions.  Think about all of your end-users.  Annotate with diversity.More items•	How can we prevent bias in artificial intelligence
3707	Interpolation refers to using the data in order to predict data within the dataset. Extrapolation is the use of the data set to predict beyond the data set.	What is the difference between interpolation and extrapolation
1448	Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting.	What is bagging technique in ML
1439	Definition: Let X be a continuous random variable with mean µ. The variance of X is Var(X) = E((X − µ)2).	How do you find the variance of a continuous distribution
6524	AdaBoost. AdaBoost is an ensemble machine learning algorithm for classification problems. It is part of a group of ensemble methods called boosting, that add new machine learning models in a series where subsequent models attempt to fix the prediction errors made by prior models.	Which algorithm uses ensemble learning
2769	Backpropagation only works during training the model on a dataset.  You run your model with the learned parameters (from Backpropagation) and best hyperparameters (from validation) once on the Test set and report the accuracy. You never learn anything, be it parameters or hyperparameters on the Test set.	How does backpropagation work between training validation and test sets
524	Hysteresis is the difference between two separate measurements taken at the same point, the first is taken during a series of increasing measurement values, and the other during during a series of decreasing measurement values.	What is hysteresis in measurement
5489	Inverted dropout is a variant of the original dropout technique developed by Hinton et al. Just like traditional dropout, inverted dropout randomly keeps some weights and sets others to zero. In contrast, traditional dropout requires scaling to be implemented during the test phase.	What is inverted dropout
1056	The principal advantage of linear regression is its simplicity, interpretability, scientific acceptance, and widespread availability. Linear regression is the first method to use for many problems.	What are two major advantages for using a regression
5248	Improved response times: Data visualization puts the data into the users' hands allowing them to more quickly identify issues and improve response times.  It allows decision-makers to view data using graphical representations including charts, fever charts, and heat maps.	Can visualization help in decision making
6281	Two types of reinforcement learning are 1) Positive 2) Negative. Two widely used learning model are 1) Markov Decision Process 2) Q learning. Reinforcement Learning method works on interacting with the environment, whereas the supervised learning method works on given sample data or example.	What are the types of reinforcement learning
5738	Given a linear operator it can have associated eigenvectors / functions.  For example, given the differential operator the exponential function is an eigenfunction of it. This is why we can solve linear homogeneous differential equations by solving a characteristic equation.	Is the linear operator an eigenfunction
2867	However, people generally apply this probability to a single study. Consequently, an odds ratio of 5.2 with a confidence interval of 3.2 to 7.2 suggests that there is a 95% probability that the true odds ratio would be likely to lie in the range 3.2-7.2 assuming there is no bias or confounding.	How do you interpret a confidence interval for an odds ratio
1898	An ARIMA model is a class of statistical models for analyzing and forecasting time series data.  The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary. MA: Moving Average.	How is Arima model used in forecasting
759	Keras is a neural network library while TensorFlow is the open-source library for a number of various tasks in machine learning. TensorFlow provides both high-level and low-level APIs while Keras provides only high-level APIs.	What is the relationship between tensorflow with keras
2315	A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example.	What is latent variable
5227	four outcomes	How many outcomes are in the sample space
4216	A significant advantage of a decision tree is that it forces the consideration of all possible outcomes of a decision and traces each path to a conclusion. It creates a comprehensive analysis of the consequences along each branch and identifies decision nodes that need further analysis.	What are the advantages of decision tree
1494	The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data.	What is bootstrapping in machine learning
1173	• Parametric tests are based on assumptions about the distribution of the underlying. population from which the sample was taken. The most common parametric. assumption is that data are approximately normally distributed. • Nonparametric tests do not rely on assumptions about the shape or parameters of the.	What is parametric and non parametric test
8373	This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. A simple relation for linear regression looks like this.	Why is regularization necessary in linear regression
8661	"The binomial distribution model allows us to compute the probability of observing a specified number of ""successes"" when the process is repeated a specific number of times (e.g., in a set of patients) and the outcome for a given patient is either a success or a failure."	What is the importance of binomial distribution
7971	A supervised learning algorithm takes a known set of input data and known responses to the data (output) and trains a model to generate reasonable predictions for the response to new data.  Supervised learning uses classification and regression techniques to develop predictive models.	How does supervised learning algorithm work
1126	Linear regression is the next step up after correlation. It is used when we want to predict the value of a variable based on the value of another variable. The variable we want to predict is called the dependent variable (or sometimes, the outcome variable).	When would you use a linear regression
1	Why Backing Up is Essential: The Top Five Benefits to Data BackupQuick Access to Files. One of the greatest things about backing up data is the ease at which you are able to retrieve files and information.  Protection Against Power Failures.  Added Anti-Virus Protection.  Safeguard Against Failed Hard Drive.  Recovery if Operating System Fails.	What are the advantages of backing up data
7212	The answer is whitening. If x is an n dimensional column vector of zero mean and has an n by n covariance R then x' inv(R) x is chi squared, that is x' inv(R) x is the sum of n unit variance zero mean random variables.	What is the inverse of a covariance matrix
4947	The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.	What is Knn in machine learning
1621	Word2Vec can be used to get actionable metrics from thousands of customers reviews. Businesses don't have enough time and tools to analyze survey responses and act on them thereon. This leads to loss of ROI and brand value. Word embeddings prove invaluable in such cases.	Where is Word2Vec used
392	Symmetrical distribution occurs when the values of variables occur at regular frequencies and the mean, median and mode occur at the same point. In graph form, symmetrical distribution often appears as a bell curve. If a line were drawn dissecting the middle of the graph, it would show two sides that mirror each other.	What is symmetric data distribution
1396	As the df increase, the chi square distribution approaches a normal distribution. The mean of a chi square distribution is its df. The mode is df - 2 and the median is approximately df - 0 .	How do you find the mode of a chi square distribution
7596	CNNs are trained to identify and extract the best features from the images for the problem at hand. That is their main strength. The latter layers of a CNN are fully connected because of their strength as a classifier.	What are the advantages of a CNN over a fully connected DNN for image classification
7759	Ensemble learning helps improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model.	What do you think about Ensemble Learning
8234	A simple random sample is a subset of a statistical population in which each member of the subset has an equal probability of being chosen. A simple random sample is meant to be an unbiased representation of a group.	What is a simple random sample in statistics
4923	Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.	What is gradient boosting machine learning
6779	semi-supervised learning model	Which type of learning has data with labels
2634	Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis.	What is the difference between prior and posterior probabilities
8060	Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model.  It is the best starting point for understanding boosting.	What is boosting in ensemble learning
6803	Response variables are also known as dependent variables, y-variables, and outcome variables. Typically, you want to determine whether changes in the predictors are associated with changes in the response. For example, in a plant growth study, the response variable is the amount of growth that occurs during the study.	What is the response variable
7406	The tobit model, also called a censored regression model, is designed to estimate linear relationships between variables when there is either left- or right-censoring in the dependent variable (also known as censoring from below and above, respectively).	How does Tobit regression work
5567	Generally, the rule of thumb is that the larger the sample size, the more statistically significant it is—meaning there's less of a chance that your results happened by coincidence.	How do you know if a sample size is statistically significant
1734	Tensorflow is the most popular and apparently best Deep Learning Framework out there.  Tensorflow can be used to achieve all of these applications. The reason for its popularity is the ease with which developers can build and deploy applications.	Is Tensorflow good for deep learning
2026	A post hoc test is used only after we find a statistically significant result and need to determine where our differences truly came from. The term “post hoc” comes from the Latin for “after the event”. There are many different post hoc tests that have been developed, and most of them will give us similar answers.	What are post hoc tests in statistics
8535	Categorization is a major component of qualitative data analysis by which investigators attempt to group patterns observed in the data into meaningful units or categories. Through this process, categories are often created by chunking together groups of previously coded data.	What is categorization in data analysis
2548	"While measures of central tendency are used to estimate ""normal"" values of a dataset, measures of dispersion are important for describing the spread of the data, or its variation around a central value. A proper description of a set of data should include both of these characteristics."	Why are measures of dispersion used in addition to measures of central tendency
2577	A probability distribution is a statistical function that describes all the possible values and likelihoods that a random variable can take within a given range.  These factors include the distribution's mean (average), standard deviation, skewness, and kurtosis.	What does probability distribution function mean
4042	In the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available.  An algorithm that implements classification, especially in a concrete implementation, is known as a classifier.	What is classification learning
6757	A polygon is convex if all the interior angles are less than 180 degrees. If one or more of the interior angles is more than 180 degrees the polygon is non-convex (or concave).	What does non convex mean
5162	In this paper we describe a multiagent Q-learning tech- nique, called Sparse Cooperative Q-learning, that al- lows a group of agents to learn how to jointly solve a task when the global coordination requirements of the system (but not the particular action choices of the agents) are known beforehand.	What is sparse Q learning
1187	The Four Assumptions of Linear RegressionLinear relationship: There exists a linear relationship between the independent variable, x, and the dependent variable, y.Independence: The residuals are independent.  Homoscedasticity: The residuals have constant variance at every level of x.Normality: The residuals of the model are normally distributed.	What are the four assumptions of linear regression
4791	A square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is zero.  Non-square matrices (m-by-n matrices for which m ≠ n) do not have an inverse. However, in some cases such a matrix may have a left inverse or right inverse.	What is non invertible matrix
67	Euclidean distance	Which is most commonly used measure of similarity in clustering exercise
1587	The probability of making a type I error is α, which is the level of significance you set for your hypothesis test. An α of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis.  The probability of rejecting the null hypothesis when it is false is equal to 1–β.	What is the probability of a Type I error
1553	For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality.	When would you use dimensionality reduction
81	We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks).	What are the layers in CNN
8471	Gloves are pieces of clothing which cover your hands and wrists and have individual sections for each finger. You wear gloves to keep your hands warm or dry or to protect them.  a pair of white cotton gloves. Synonyms: mitten, gauntlet, mitt More Synonyms of glove. 2.	What is the meaning of glove
1280	Most recent answer. Three different measures of effect size for chi-squared test and Fisher's exact test predominantly used are Phi, Cramer's V, and Odds Ratio. Phi and Odds Ratio are only suitable for a 2x2 contingency table and Cramer's V is suitable for larger contingency tables.	What effect size is appropriate for a chi square test
231	To calculate the Sharpe Ratio, find the average of the “Portfolio Returns (%)” column using the “=AVERAGE” formula and subtract the risk-free rate out of it. Divide this value by the standard deviation of the portfolio returns, which can be found using the “=STDEV” formula.	How do you find the standard deviation of a Sharpe ratio
828	A theorem that explains the shape of a sampling distribution of sample means. It states that if the sample size is large (generally n ≥ 30), and the standard deviation of the population is finite, then the distribution of sample means will be approximately normal.	What will be the shape of the sampling distribution
2184	Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error.	How do you explain gradient boosting
1142	The beta distribution is a continuous probability distribution that can be used to represent proportion or probability outcomes. For example, the beta distribution might be used to find how likely it is that your preferred candidate for mayor will receive 70% of the vote.	What is a beta distribution used for
3998	Active learning promotes recall and deeper understanding of material, as students are engaging with the content rather than simply listening to it.  It helps to maintain student concentration and deepens learning towards the higher-level skills like critical thinking.	Why is active learning
3471	Multivariate data is a set of data with more than two variables per observation.  Multidimensional databases contain fact tables (with observations, events, or transactions, say) and dimension tables, which more fully describe or put into context the variables in the fact tables — such as time hierarchies.	What is the difference between high dimensional data multidimensional data and multivariate data
2954	In qualitative research, there are various sampling techniques that you can use when recruiting participants. The two most popular sampling techniques are purposeful and convenience sampling because they align the best across nearly all qualitative research designs.	What type of sampling is best for qualitative research
18	"q is called the variational approximation to the posterior. The term variational is used because you pick the best q in Q -- the term derives from the ""calculus of variations,"" which deals with optimization problems that pick the best function (in this case, a distribution q)."	Why is it called variational inference
4532	Rabin-Karp is another pattern searching algorithm to find the pattern in a more efficient way. It also checks the pattern by moving window one by one, but without checking all characters for all cases, it finds the hash value. When the hash value is matched, then only it tries to check each character.	What is the basic objective of Rabin Karp pattern matching algorithm
7265	Introduction. Linear regression and logistic regression are two types of regression analysis techniques that are used to solve the regression problem using machine learning. They are the most prominent techniques of regression.	What is regression and its types
7043	An algorithm that uses random numbers to decide what to do next anywhere in its logic is called a Randomized Algorithm. For example, in Randomized Quick Sort, we use a random number to pick the next pivot (or we randomly shuffle the array). And in Karger's algorithm, we randomly pick an edge.	What is randomized algorithm with example
1961	Categorical variables are also known as discrete or qualitative variables. Categorical variables can be further categorized as either nominal, ordinal or dichotomous. Nominal variables are variables that have two or more categories, but which do not have an intrinsic order.	Is a nominal variable a discrete variable
7977	To overcome this prob- lem, the ResNet incorporates skip-connections between layers (He et al., 2016a,b) and the batch-normalization (BN) normalizes the input of activation functions (Ioffe and Szegedy, 2015). These architectures enable an extreme deep neural network to be trained with high performance.	Does ResNet use batch normalization
2205	Sampling is a process used in statistical analysis in which a predetermined number of observations are taken from a larger population. The methodology used to sample from a larger population depends on the type of analysis being performed, but it may include simple random sampling or systematic sampling.	What is meant by sampling method
8593	How to Detect Omitted Variable Bias and Identify Confounding Variables. You saw one method of detecting omitted variable bias in this post. If you include different combinations of independent variables in the model, and you see the coefficients changing, you're watching omitted variable bias in action!	How do you know if a omitted variable is biased
1194	MATLAB is a high-performance language for technical computing. It integrates computation, visualization, and programming in an easy-to-use environment where problems and solutions are expressed in familiar mathematical notation. Typical uses include:  Data analysis, exploration, and visualization.	What is Matlab and why is it used
847	K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970's as a non-parametric technique.	What is K nearest Knn data mining algorithm
3511	0:0116:09Suggested clip · 82 secondsAnalyzing Models with TensorBoard - Deep Learning with Python YouTubeStart of suggested clipEnd of suggested clip	How do you analyze a TensorBoard
6723	This is machine learning in general and almost all ML algorithms are based on this optimization. Curve fitting, on the other hand, is a process of finding a mathematical function on the available data such that the function defines the best fit on the data points.  ML does the same but it needs to generalize it's fit.	Is curve fitting machine learning
613	An endogenous variable is a variable in a statistical model that's changed or determined by its relationship with other variables within the model.  Therefore, its values may be determined by other variables. Endogenous variables are the opposite of exogenous variables, which are independent variables or outside forces.	What is an endogenous variable in regression
6584	Object recognition is a computer vision technique for identifying objects in images or videos. Object recognition is a key output of deep learning and machine learning algorithms.  The goal is to teach a computer to do what comes naturally to humans: to gain a level of understanding of what an image contains.	What is object recognition in image processing
368	In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to).	What is loss function in classification
8093	The least squares approach limits the distance between a function and the data points that the function explains. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data. Mathematicians use the least squares method to arrive at a maximum-likelihood estimate.	What is the least squares method and how is it used to find the estimated regression equation What is the role of least squares method in calculating coefficient of determination explain
7128	Gaussian Distribution Function The nature of the gaussian gives a probability of 0.683 of being within one standard deviation of the mean. The mean value is a=np where n is the number of events and p the probability of any integer value of x (this expression carries over from the binomial distribution ).	How is Gaussian distribution calculated
159	Image processing is a method to perform some operations on an image, in order to get an enhanced image or to extract some useful information from it. It is a type of signal processing in which input is an image and output may be image or characteristics/features associated with that image.	What is image processing used for
145	The Fourier transform of a function of time is a complex-valued function of frequency, whose magnitude (absolute value) represents the amount of that frequency present in the original function, and whose argument is the phase offset of the basic sinusoid in that frequency.	What does Fourier transform represent
6058	Because the distance function used to find the k nearest neighbors is not linear, so it usually won't lead to a linear decision boundary.  kNN does not build a model of your data, it simply assumes that instances that are close together in space are similar.	Can Knn have linear decision boundary
2269	Moments About the MeanFirst, calculate the mean of the values.Next, subtract this mean from each value.Then raise each of these differences to the sth power.Now add the numbers from step #3 together.Finally, divide this sum by the number of values we started with.	How do you calculate moments in statistics
2174	Tensors are a type of data structure used in linear algebra, and like vectors and matrices, you can calculate arithmetic operations with tensors.	What are tensors used for
2511	There are four possible outcomes: hit (signal present and subject says “yes”), miss (signal present and subject says “no”), false alarm (signal absent and subject says “yes”), and correct rejection (signal absent and subject says “no”). Hits and correct rejections are good.	What are the four possible outcomes in signal detection theory
658	According to his theory, S-Shaped curved lines signify liveliness and activity and excite the attention of the viewer as contrasted with straight lines, parallel lines, or right-angled intersecting lines which signify stasis, death, or inanimate objects. He goes on to say that the S curve is the basis of all great art.	What is Hogarth s S Curve
6877	A mathematical function with symbol εijk defined to switch between the discrete values of +1, 0, and -1, depending on the values of the three indices i, j, and k: It is one of the tools used in Einstein's summation notation to handle operations equivalent to cross products in vector notation.	What is alternating tensor
5988	A local minimum is a suboptimal equilibrium point at which system error is non-zero and the hidden output matrix is singular [12]. The complex problem which has a large number of patterns needs as many hidden nodes as patterns in order not to cause a singular hidden output matrix.	What is the local minima problem
1913	Heisenberg's uncertainty principle is a key principle in quantum mechanics. Very roughly, it states that if we know everything about where a particle is located (the uncertainty of position is small), we know nothing about its momentum (the uncertainty of momentum is large), and vice versa.	What do you mean by Heisenberg's uncertainty principle
5412	Lift can be found by dividing the confidence by the unconditional probability of the consequent, or by dividing the support by the probability of the antecedent times the probability of the consequent, so: The lift for Rule 1 is (3/4)/(4/7) = (3*7)/(4 * 4) = 21/16 ≈ 1.31.	How do you calculate lift in association rule mining
6249	Experimental probability is the result of an experiment. Theoretical probability is what is expected to happen. Three students tossed a coin 50 times individually.	What is the difference between theoretical and experimental probability
6108	In statistics, a sampling frame is the source material or device from which a sample is drawn. It is a list of all those within a population who can be sampled, and may include individuals, households or institutions. Importance of the sampling frame is stressed by Jessen and Salant and Dillman.	What should a sampling frame represent
211	By adjusting the rotation of the prism, separated lines of light with different colors could be observed with the telescope on the left. These lines were the spectrum of the substance. Kirchhoff and Bunsen found that elements such as lithium, sodium, and potassium all had their unique spectra.	How did Kirchoff and Bunsen invent spectral analysis
1743	Some popular examples of unsupervised learning algorithms are:k-means for clustering problems.Apriori algorithm for association rule learning problems.	Which algorithms fall under unsupervised learning
6387	In general, having high bias reduces the performance of the algorithm on training set while having high variance reduces performance on unseen data. This is known as Bias Variance Trade off.	What is an intuitive explanation for bias variance tradeoff
7122	KMeans is a clustering algorithm which divides observations into k clusters. Since we can dictate the amount of clusters, it can be easily used in classification where we divide data into clusters which can be equal to or more than the number of classes.	Can K means be used for classification
544	Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.  Often the weights of a neural network are contained within the hidden layers of the network.	What is weights in neural networks
183	A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data.  The mean (often called the average) is most likely the measure of central tendency that you are most familiar with, but there are others, such as the median and the mode.	How do you explain central tendency
1098	Decision tree is a type of supervised learning algorithm that can be used in both regression and classification problems. It works for both categorical and continuous input and output variables.	Can decision trees be used for classification tasks
1038	Global max pooling = ordinary max pooling layer with pool size equals to the size of the input (minus filter size + 1, to be precise). You can see that MaxPooling1D takes a pool_length argument, whereas GlobalMaxPooling1D does not.	What is Global Max pooling
3268	Most scientific calculators only calculate logarithms in base 10 and base e. A logarithm is a mathematical operation that determines how many times a certain number, called the base, is multiplied by itself to reach another number.	What is the purpose of using a logarithm
7098	The IOU is a number between 0 and 1, with larger being better. Ideally, the predicted box and the ground-truth have an IOU of 100% but in practice anything over 50% is usually considered to be a correct prediction. For the above example the IOU is 74.9% and you can see the boxes are a good match.	What is the IOU value of predicted and ground truth boxes in the Yolo algorithm
8608	RBMs were invented by Geoffrey Hinton and can be used for dimensionality reduction, classification, regression, collaborative filtering, feature learning, and topic modeling. RBMs are a special class of Boltzmann Machines and they are restricted in terms of the connections between the visible and the hidden units.	What are restricted Boltzmann machines used for
2399	Endogenous variables are used in econometrics and sometimes in linear regression. They are similar to (but not exactly the same as) dependent variables. Endogenous variables have values that are determined by other variables in the system (these “other” variables are called exogenous variables).	How do you find endogenous variables
373	One tool they can use to do so is a decision tree. Decision trees are flowchart graphs or diagrams that help explore all of the decision alternatives and their possible outcomes.  Decision tree software helps businesses draw out their trees, assigns value and probabilities to each branch and analyzes each option.	How do decision trees help business decision making
3737	(retrogress) Opposite of to develop gradually. retrogress. diminish. regress.	What is the opposite of evolve
825	Right padding of string in Python Right padding a string means adding a given character at the right side of string to make it of a given length.	What is padding in Python
8174	Z Score is free of any scale, hence it is used as a transformation technique while we need to make any variable unit free in various statistical techniques. Also, it is used to identifying outliers in a univarite way.  Z-test is a statistical technique to test the Null Hypothesis against the Alternate Hypothesis.	How is the Z test different from Z score analysis
7226	If an infinite series converges, then the individual terms (of the underlying sequence being summed) must converge to 0. This can be phrased as a simple divergence test: If limn→∞an either does not exist, or exists but is nonzero, then the infinite series ∑nan diverges.	What is the divergence test for series
4197	Definition. Multi-label learning is an extension of the standard supervised learning setting. In contrast to standard supervised learning where one training example is associated with a single class label, in multi-label learning, one training example is associated with multiple class labels simultaneously.	What is multi label learning
7373	"Contrapositive: The contrapositive of a conditional statement of the form ""If p then q"" is ""If ~q then ~p"". Symbolically, the contrapositive of p q is ~q ~p."	What is the Contrapositive of P → Q
957	A greater power requires a larger sample size. Effect size – This is the estimated difference between the groups that we observe in our sample. To detect a difference with a specified power, a smaller effect size will require a larger sample size.	Why does the sample size required change with effect size
1326	Model calibration is the process of adjustment of the model parameters and forcing within the margins of the uncertainties (in model parameters and / or model forcing) to obtain a model representation of the processes of interest that satisfies pre-agreed criteria (Goodness-of-Fit or Cost Function).	What does model calibration mean
8369	Keyhole. Keyhole excels at four key things:  Agorapulse. AgoraPulse is one of the greatest social media analytics tools that helps you identify your best content and see what users need.  Brandwatch. Data is huge these days and BrandWatch is all about it.  BrandMentions.  Meltwater.  Reputology.  TapInfluence.  Hootsuite.More items•	How do I pull social media analytics
959	In a box plot, we draw a box from the first quartile to the third quartile. A vertical line goes through the box at the median. The whiskers go from each quartile to the minimum or maximum.	How do you plot a box plot
1406	Another way of visualizing multivariate data for multiple attributes together is to use parallel coordinates. Basically, in this visualization as depicted above, points are represented as connected line segments. Each vertical line represents one data attribute.	How graphically represents multivariate data
1421	The total number of contravariant and covariant indices of a tensor. The rank of a tensor is independent of the number of dimensions. of the underlying space.	What is tensor rank
190	For example, people may respond similarly to questions about income, education, and occupation, which are all associated with the latent variable socioeconomic status. In every factor analysis, there are the same number of factors as there are variables.	What is factor analysis with example
2323	The number of different treatment groups that we have in any factorial design can easily be determined by multiplying through the number notation. For instance, in our example we have 2 x 2 = 4 groups. In our notational example, we would need 3 x 4 = 12 groups. We can also depict a factorial design in design notation.	How do you calculate factorial design
4220	When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation.	Should I use sample or population standard deviation
885	From our confusion matrix, we can calculate five different metrics measuring the validity of our model.Accuracy (all correct / all) = TP + TN / TP + TN + FP + FN.Misclassification (all incorrect / all) = FP + FN / TP + TN + FP + FN.Precision (true positives / predicted positives) = TP / TP + FP.More items	How do you find the accuracy of a confusion matrix
2523	The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered. In machine learning, one aims to construct algorithms that are able to learn to predict a certain target output.	What is the inductive bias for Instance based learning
2633	This list of requirements prioritization techniques provides an overview of common techniques that can be used in prioritizing requirements.Ranking.  Numerical Assignment (Grouping)  MoScoW Technique.  Bubble Sort Technique.  Hundred Dollar Method.  Analytic Hierarchy Process (AHP)  Five Whys.	What techniques can be used to prioritize changes
4814	In linear regression, the function is a linear (straight-line) equation. In power or exponential regression, the function is a power (polynomial) equation of the form or an exponential function in the form .	What is the difference between linear regression and exponential regression
6761	"Perceptron for XOR: XOR is where if one is 1 and other is 0 but not both.  A ""single-layer"" perceptron can't implement XOR. The reason is because the classes in XOR are not linearly separable. You cannot draw a straight line to separate the points (0,0),(1,1) from the points (0,1),(1,0)."	Why is XOR not linearly separable
5255	A type II error produces a false negative, also known as an error of omission. For example, a test for a disease may report a negative result, when the patient is, in fact, infected. This is a type II error because we accept the conclusion of the test as negative, even though it is incorrect.	What is an example of a Type 2 error
6948	It is known as a top-down approach. Backward-chaining is based on modus ponens inference rule. In backward chaining, the goal is broken into sub-goal or sub-goals to prove the facts true. It is called a goal-driven approach, as a list of goals decides which rules are selected and used.	What technique is used in backward chaining algorithm
3105	Artificial Intelligence (AI) is a kind of simulation that involves a model intended to represent human intelligence or knowledge. An AI-based simulation model typically mimics human intelligence such as reasoning, learning, perception, planning, language comprehension, problem-solving, and decision making.	What is simulation in artificial intelligence
7018	Federated Learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud.  Your phone personalizes the model locally, based on your usage (A).	How does Federated Learning work
253	The mean of a discrete random variable X is a weighted average of the possible values that the random variable can take. Unlike the sample mean of a group of observations, which gives each observation equal weight, the mean of a random variable weights each outcome xi according to its probability, pi.	What is the mean of a discrete random variable
5155	Gaussian RBF(Radial Basis Function) is another popular Kernel method used in SVM models for more. RBF kernel is a function whose value depends on the distance from the origin or from some point.	What is RBF kernel in SVM
165	A partially observable Markov decision process (POMDP) is a generalization of a Markov decision process (MDP). A POMDP models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state.	What is the difference between a Markov decision process MDP and a partially observable Markov decision process Pomdp )
46	Least squares is an estimation technique that allows you to estimate the parameters of models. OLS (ordinary least squares) is the least squares technique used for estimating the parameters of linear regression models.  The problem of linear regression is to fit a line to the data by minimizing the error.	What is the difference between ordinary least squares regression and linear regression with the least squares method
3701	Direct link to this answer. Assuming he spectrogram function plots the power spectral density (PSD) in decibels. The values are relative, not negative, amplitudes, so -150 dB corresponds to an amplitude of about 3.2E-8.	Can power spectral density be negative
253	The term cognitive computing is typically used to describe AI systems that aim to simulate human thought.  A number of AI technologies are required for a computer system to build cognitive models that mimic human thought processes, including machine learning, deep learning, neural networks, NLP and sentiment analysis.	What is cognitive computing in AI
1400	It turns out self-driving cars aren't dissimilar from self-driving humans: It takes about 16 years for them to be ready for the road.	How many years until a car is fully autonomous
7109	If exploding gradients are still occurring, you can check for and limit the size of gradients during the training of your network. This is called gradient clipping. Dealing with the exploding gradients has a simple but very effective solution: clipping gradients if their norm exceeds a given threshold.	What technique is followed to deal with the problem of exploding gradients in recurrent neural net works RNN )
3522	A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function.	What gives non linearity to a neural network
1068	Confidence Levelz0.951.960.962.050.982.330.992.586 more rows	What is the z value for 96 confidence interval
6398	Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron's input is relevant for the model's prediction.	What is the role of activation functions in a neural network
736	Gradient boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems. Gradient boosting is also known as gradient tree boosting, stochastic gradient boosting (an extension), and gradient boosting machines, or GBM for short.	Is gradient boosting an ensemble
4164	In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert.  The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.	Is expert system an artificial intelligence
7281	In statistics, a sampling frame is the source material or device from which a sample is drawn. It is a list of all those within a population who can be sampled, and may include individuals, households or institutions. Importance of the sampling frame is stressed by Jessen and Salant and Dillman.	What does sampling frame mean
693	In-group bias is notoriously difficult to avoid completely, but research shows it can be reduced through interaction with other groups, and by giving people an incentive to act in an unbiased manner.	How can you avoid in group bias
802	In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.  Linear regression has many practical uses.	What is standard linear regression
2241	There are three main steps to deploying on GCP:Upload your model to a Cloud Storage bucket.Create an AI Platform Prediction model resource.Create an AI Platform Prediction version resource, specifying the Cloud Storage path to your saved model.	How do you deploy an AI model
557	Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers.	What kurtosis tells us
929	You can tell if two random variables are independent by looking at their individual probabilities. If those probabilities don't change when the events meet, then those variables are independent. Another way of saying this is that if the two variables are correlated, then they are not independent.	How do you prove independent variables
6127	Updated: 04/26/2017 by Computer Hope. The degree of errors encountered during data transmission over a communications or network connection. The higher the error rate, the less reliable the connection or data transfer will be. The term error rate can refer to anything where errors can occur.	What is error rate in networking
605	To say it informally, the filter size is how many neighbor information you can see when processing the current layer. When the filter size is 3*3, that means each neuron can see its left, right, upper, down, upper left, upper right, lower left, lower right, as a total of 8 neighbor information.	What is filter size in CNN
5827	Lag sequential analysis is a method for analyzing the sequential dependency in a serially sequenced series of dichotomous codes representing different system states.  The analysis assumes that the events are sequenced in time (a time series) but does not assume equal time intervals between events.	What is lag sequential analysis
3783	In statistics, we usually say “random sample,” but in probability it's more common to say “IID.” Identically Distributed means that there are no overall trends–the distribution doesn't fluctuate and all items in the sample are taken from the same probability distribution.	What is IID sample
140	One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture.	Which of the following were introduced to overcome the vanishing gradient problem
367	The gamma distribution is the maximum entropy probability distribution (both with respect to a uniform base measure and with respect to a 1/x base measure) for a random variable X for which E[X] = kθ = α/β is fixed and greater than zero, and E[ln(X)] = ψ(k) + ln(θ) = ψ(α) − ln(β) is fixed (ψ is the digamma function).	What is the support of the gamma distribution
5202	The survival function is a function that gives the probability that a patient, device, or other object of interest will survive beyond any specified time. The survival function is also known as the survivor function or reliability function.	What does survival function mean
3217	In simple linear regression a single independent variable is used to predict the value of a dependent variable. In multiple linear regression two or more independent variables are used to predict the value of a dependent variable. The difference between the two is the number of independent variables.	What is the difference between linear regression and multiple linear regression
8439	P ∧ Q means P and Q. P ∨ Q means P or Q. An argument is valid if the following conditional holds: If all the premises are true, the conclusion must be true.  So, when you attempt to write a valid argument, you should try to write out what the logical structure of the argument is by symbolizing it.	What does P ∧ Q mean
2239	Kernel function A kernel (or covariance function) describes the covariance of the Gaussian process random variables. Together with the mean function the kernel completely defines a Gaussian process. In the first post we introduced the concept of the kernel which defines a prior on the Gaussian process distribution.	What is kernel Gaussian process
1664	The chi-squared test applies an approximation assuming the sample is large, while the Fisher's exact test runs an exact procedure especially for small-sized samples.	What is the difference between chi square test and Fisher's exact test
165	For symmetric and Hermitian matrices, the eigenvalues and singular values are obviously closely related. A nonnegative eigenvalue, λ ≥ 0, is also a singular value, σ = λ. The corresponding vectors are equal to each other, u = v = x.	Are singular values and eigenvalues the same
1048	A conditional probability can always be computed using the formula in the definition. Sometimes it can be computed by discarding part of the sample space. Two events A and B are independent if the probability P(A∩B) of their intersection A∩B is equal to the product P(A)⋅P(B) of their individual probabilities.	What is the conditional probability of A and B are independent
1037	1| Fast R-CNN Written in Python and C++ (Caffe), Fast Region-Based Convolutional Network method or Fast R-CNN is a training algorithm for object detection. This algorithm mainly fixes the disadvantages of R-CNN and SPPnet, while improving on their speed and accuracy.	Which algorithm is used for object detection
8324	The reasoning is the mental process of deriving logical conclusion and making predictions from available knowledge, facts, and beliefs.  In artificial intelligence, the reasoning is essential so that the machine can also think rationally as a human brain, and can perform like a human.	What is reasoning in artificial intelligence
708	Weaknesses. Histograms have many benefits, but there are two weaknesses. A histogram can present data that is misleading. For example, using too many blocks can make analysis difficult, while too few can leave out important data.	What are the disadvantages of using a histogram
647	Example: One nanogram of Plutonium-239 will have an average of 2.3 radioactive decays per second, and the number of decays will follow a Poisson distribution.	What is the real life example of Poisson distribution
166	Systematic sampling is a type of probability sampling method in which sample members from a larger population are selected according to a random starting point but with a fixed, periodic interval. This interval, called the sampling interval, is calculated by dividing the population size by the desired sample size.	What is meant by systematic sampling
597	Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron.	Why activation function is used in neural network
6654	Self Learning: Ability to recognize patterns, learn from data, and become more intelligent over time (can be AI or programmatically based).  Machine Learning: AI systems with ability to automatically learn and improve from experience without being explicitly programmed via training.	What is self learning in machine learning
350	Stochastic gradient descent is, well, stochastic. Because you are no longer using your entire training set a once, and instead picking one or more examples at a time in some likely random fashion, each time you tun SGD you will obtain a different optimum and a unique cost vs.	Why is stochastic gradient descent stochastic
1297	Two disjoint events can never be independent, except in the case that one of the events is null.  Events are considered disjoint if they never occur at the same time. For example, being a freshman and being a sophomore would be considered disjoint events.	Can an event be independent and disjoint
4870	Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models.	What is preprocessing in machine learning
745	6 Types of Artificial Neural Networks Currently Being Used in Machine LearningFeedforward Neural Network – Artificial Neuron:  Radial basis function Neural Network:  Kohonen Self Organizing Neural Network:  Recurrent Neural Network(RNN) – Long Short Term Memory:  Convolutional Neural Network:  Modular Neural Network:	What are different types of neural networks
777	(Note that how a support vector machine classifies points that fall on a boundary line is implementation dependent. In our discussions, we have said that points falling on the line will be considered negative examples, so the classification equation is w . u + b ≤ 0.)	What equations are used for classification in a support vector machine
7485	The range is the distance from the highest value to the lowest value. The Inter-Quartile Range is quite literally just the range of the quartiles: the distance from the largest quartile to the smallest quartile, which is IQR=Q3-Q1.	How do you compare the interquartile range and range
834	Hyperparameter optimization in machine learning intends to find the hyperparameters of a given machine learning algorithm that deliver the best performance as measured on a validation set. Hyperparameters, in contrast to model parameters, are set by the machine learning engineer before training.	What is Hyperparameter optimization in deep learning
4167	Weighted accuracy is computed by taking the average, over all the classes, of the fraction of correct predictions in this class (i.e. the number of correctly predicted instances in that class, divided by the total number of instances in that class).	How do you calculate weighted accuracy
2495	According to Andrew Ng, the best methods of dealing with an underfitting model is trying a bigger neural network (adding new layers or increasing the number of neurons in existing layers) or training the model a little bit longer.	How do I fix Underfitting neural network
382	Regression trees are used in Statistics, Data Mining and Machine learning. It is a very important and powerful technique when it comes to predictive analysis [5] . The goal is to predict the value of target variable on the basis of several input attributes that act as nodes of the regression tree.	Which type of data is often Modelled using regression trees
1073	The gamma distribution can be used a range of disciplines including queuing models, climatology, and financial services. Examples of events that may be modeled by gamma distribution include: The amount of rainfall accumulated in a reservoir. The size of loan defaults or aggregate insurance claims.	What is Gamma distribution example
6070	MNIST Data Formats The data is stored in a very simple file format designed for storing vectors and multidimensional matrices. The labels values are 0 to 9. Pixels are organized row-wise.  0 means background (white), 255 means foreground (black).	How is Mnist data stored
220	Relationship between PDF and CDF for a Continuous Random VariableBy definition, the cdf is found by integrating the pdf: F(x)=x∫−∞f(t)dt.By the Fundamental Theorem of Calculus, the pdf can be found by differentiating the cdf: f(x)=ddx[F(x)]	How do you find the CDF when given a PDF
1562	If all of the values in the sample are identical, the sample standard deviation will be zero. When discussing the sample mean, we found that the sample mean for diastolic blood pressure was 71.3.	Can a sample mean be zero
1946	Image recognition is used to perform a large number of machine-based visual tasks, such as labeling the content of images with meta-tags, performing image content search and guiding autonomous robots, self-driving cars and accident avoidance systems.	What is the use of image recognition
5788	A knowledge-based system (KBS) is a computer program that reasons and uses a knowledge base to solve complex problems.  Expert systems are designed to solve complex problems by reasoning about knowledge, represented primarily as if–then rules rather than through conventional procedural code.	What is the difference between knowledge based system and expert system
1261	Graphically, the p value is the area in the tail of a probability distribution. It's calculated when you run hypothesis test and is the area to the right of the test statistic (if you're running a two-tailed test, it's the area to the left and to the right).	What is the p value of the test statistic
6666	"Without replacement, each bootstrap sample would be identical to the original sample, so the sample statistics would all be the same and there would be no confidence ""interval""."	Why does the bootstrap method require sampling with replacement
5324	Perceptron Learning Rule states that the algorithm would automatically learn the optimal weight coefficients. The input features are then multiplied with these weights to determine if a neuron fires or not.	What do you mean by Perceptron and its learning rule
939	The average value becomes more and more precise as the number of measurements increases. Although the uncertainty of any single measurement is always ∆, the uncertainty in the mean ∆ avg becomes smaller (by a factor of 1/√) as more measurements are made.	What happens to uncertainty as the number of measurements increases
2367	Statistics provide a valuable source of evidence to support the initiation of new policy or the alteration of an existing policy or program. Once an issue has been identified, it is then necessary to analyse the extent of the issue, and determine what urgency there is for the issue to be addressed.	What do you think the role of statistics is in making evidence based decisions
3210	Imbalanced data sets are a special case for classification problem where the class distribution is not uniform among the classes. Typically, they are composed by two classes: The majority (negative) class and the minority (positive) class.	What is imbalanced dataset
4412	How to Prevent OverfittingCross-validation. Cross-validation is a powerful preventative measure against overfitting.  Train with more data. It won't work every time, but training with more data can help algorithms detect the signal better.  Remove features.  Early stopping.  Regularization.  Ensembling.	How do you prevent Overfitting and Underfitting in machine learning
226	Multiple regression (an extension of simple linear regression) is used to predict the value of a dependent variable (also known as an outcome variable) based on the value of two or more independent variables (also known as predictor variables).	What is a predictor variable in multiple regression
1130	Linear filters process time-varying input signals to produce output signals, subject to the constraint of linearity.  Since linear time-invariant filters can be completely characterized by their response to sinusoids of different frequencies (their frequency response), they are sometimes known as frequency filters.	What makes a filter linear
3661	"In information theory, the entropy of a random variable is the average level of ""information"", ""surprise"", or ""uncertainty"" inherent in the variable's possible outcomes.  The minimum surprise is when p = 0 or p = 1, when the event is known and the entropy is zero bits."	What is entropy in information theory and coding
769	Artificial intelligence (AI) is a branch of computer science.  Most AI programs are not used to control robots. Even when AI is used to control robots, the AI algorithms are only part of the larger robotic system, which also includes sensors, actuators, and non-AI programming.	How artificial intelligence is related to robotics
4092	A significant result indicates that your data are significantly heteroscedastic, and thus the assumption of homoscedasticity in the regression residuals is violated. In your case the data violate the assumption of homoscedasticity, as your p value is 8.6⋅10−28. The e is standard scientific notation for powers of 10.	What is E in P value
880	The one-sample Z test is used only for tests of the sample mean. Thus, our hypothesis tests whether the average of our sample (M) suggests that our students come from a population with a know mean (m) or whether it comes from a different population.	What does a one sample z test tell you
4276	An inference network is a flexible construction for parameterizing approximating distributions during inference.	What is inference network
4322	The classic approach to the multiple comparison problem is to control the familywise error rate. Instead of setting the critical P level for significance, or alpha, to 0.05, you use a lower critical value.	How do you control multiple comparisons
5689	Machine learning has a limited scope. AI is working to create an intelligent system which can perform various complex tasks. Machine learning is working to create machines that can perform only those specific tasks for which they are trained. AI system is concerned about maximizing the chances of success.	What is the distinction between artificial intelligence AI and machine learning from your perspective what are some of the legitimate concerns about the future of AI
3256	SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types.	What is are true about kernel in SVM
7166	"In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract ""topics"" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body."	What is topic Modelling in NLP
2332	Data Science and Data analytics, the next big thing in Information Science.  It is estimated that by 2020 the proportion of jobs in data analytics, data science, and machine learning will be 3x more than all other technical jobs. This is going to be a real gold mine for future professionals in the data analytics field.	Is Data Analytics the next big thing
2683	Sensitivity is a measure of the proportion of actual positive cases that got predicted as positive (or true positive).  This implies that there will be another proportion of actual positive cases, which would get predicted incorrectly as negative (and, thus, could also be termed as the false negative).	What is sensitivity in machine learning
4165	1 Answer. A probability distribution is the theoretical outcome of an experiment whereas a sampling distribution is the real outcome of an experiment.	What is the difference between probability distribution and sampling distribution
766	Decision tree builds regression or classification models in the form of a tree structure. The topmost decision node in a tree which corresponds to the best predictor called root node.  Decision trees can handle both categorical and numerical data.	Can decision trees be used for regression
2356	In mathematics, a Fourier transform (FT) is a mathematical transform that decomposes a function (often a function of time, or a signal) into its constituent frequencies, such as the expression of a musical chord in terms of the volumes and frequencies of its constituent notes.	What are Fourier transforms used for
2576	Nonresponse bias is the bias that results when respondents differ in meaningful ways from nonrespondents.  Response rate is often low, making mail surveys vulnerable to nonresponse bias. Voluntary response bias. Voluntary response bias occurs when sample members are self-selected volunteers, as in voluntary samples.	Is non response bias a selection bias
3749	Time series is ordered data. So the validation data must be ordered to. Forward chaining ensures this.	Which of the following cross validation techniques is better suited for time series data
945	The Q statistic is used to try to partition the variability we see between studies into variability that is due to random variation, and variability that is due to potential differences between the studies. This is really similar to the way we partition variance when doing an ANOVA.	What is the Q statistic
2168	How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression.	What are the different ways to deal with Multicollinearity
997	AlphaGo and its successors use a Monte Carlo tree search algorithm to find its moves based on knowledge previously acquired by machine learning, specifically by an artificial neural network (a deep learning method) by extensive training, both from human and computer play.	Does AlphaGo use Deep Q Learning
7104	Correlation analysis explores the association between two or more variables and makes inferences about the strength of the relationship.  Technically, association refers to any relationship between two variables, whereas correlation is often used to refer only to a linear relationship between two variables.	What is Association and correlation in data mining
5199	First, after looking around on the web, it seems that there is no way to compute a (discrete) Fourier transform through a neural network. You can hack it by hard-coding the thing to include the Fourier constants for the transform and then get a decent result.	Can neural networks learn Fourier Transform
8160	A saddle point of a matrix is an element which is both the largest element in its column and the smallest element in its row.	What is saddle point in Matrix
1302	So, for data, structured data is easily organizable and follows a rigid format; unstructured is complex and often qualitative information that is impossible to reduce to or organize in a relational database and semi-structured data has elements of both.	What is structured semi structured and unstructured data
3380	Softmax is a non-linear activation function, and is arguably the simplest of the set. In this expression, zi is the current value. The denominator in the expression is the sum across every value passed to a node in the layer.	Is Softmax a linear function
4565	While a P value can inform the reader whether an effect exists, the P value will not reveal the size of the effect. In reporting and interpreting studies, both the substantive significance (effect size) and statistical significance (P value) are essential results to be reported.	Is AP value an effect size
6343	The DQN Agent The DQN (Deep Q-Network) algorithm was developed by DeepMind in 2015. It was able to solve a wide range of Atari games (some to superhuman level) by combining reinforcement learning and deep neural networks at scale.	What is deep Q Network
146	CNNs	Which Optimizer is best for image classification
931	Explanation: The two types of Fourier series are- Trigonometric and exponential.	What are the two types of Fourier series
4885	Yes, using the additional survey information from part​ (b) dramatically reduces the sample size. The required sample size decreases dramatically from 423 to 50​, with the addition of the survey information.	Does the added knowledge in Part B have much of an effect on the sample size
3894	The mean of the sampling distribution of the mean is the mean of the population from which the scores were sampled. Therefore, if a population has a mean μ, then the mean of the sampling distribution of the mean is also μ. The symbol μM is used to refer to the mean of the sampling distribution of the mean.	What does a sampling distribution of means show
698	Consistency of an estimator means that as the sample size gets large the estimate gets closer and closer to the true value of the parameter. Unbiasedness is a finite sample property that is not affected by increasing sample size. An estimate is unbiased if its expected value equals the true parameter value.	What is the difference between an unbiased estimator and a consistent estimator
585	Analysis of variance (ANOVA) can determine whether the means of three or more groups are different. ANOVA uses F-tests to statistically test the equality of means.  As in my posts about understanding t-tests, I'll focus on concepts and graphs rather than equations to explain ANOVA F-tests.	Why do we use ANOVA instead of F distribution
6732	Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables.  Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x).	Is regression an algorithm
7115	"Backpropagation is a short form for ""backward propagation of errors."" It is a standard method of training artificial neural networks. This method helps to calculate the gradient of a loss function with respects to all the weights in the network."	What backpropagation is usually used for in neural networks
1225	A learning curve is a concept that graphically depicts the relationship between the cost and output over a defined period of time, normally to represent the repetitive task of an employee or worker.	What do learning curves show
4191	Negative sampling is a technique used to train machine learning models that generally have several order of magnitudes more negative observations compared to positive ones. And in most cases, these negative observations are not given to us explicitly and instead, must be generated somehow.	What is negative sampling in machine learning
2295	As the names suggest, pre-pruning or early stopping involves stopping the tree before it has completed classifying the training set and post-pruning refers to pruning the tree after it has finished.	Whats the difference between pre pruning and post pruning decision trees
2489	A conditional probability estimate is a probability estimate that we make given or assuming the occurrence of some other event. In this case we might start with an estimate that the probability of rain is 30% and then make a conditional probability estimate that the probability of rain given a cloudy sky is 65%.	Is a conditional probability estimate
6780	Face validity refers to the extent to which a test appears to measure what it is intended to measure. A test in which most people would agree that the test items appear to measure what the test is intended to measure would have strong face validity.	What is face validity of a test
3079	Although random sampling is generally the preferred survey method, few people doing surveys use it because of prohibitive costs; i.e., the method requires numbering each member of the survey population, whereas nonrandom sampling involves taking every nth member.	What is random and non random sampling
1323	The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. The squaring is necessary to remove any negative signs.	What is the mean square error in machine learning
661	Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron.	What does an activation function do
3960	more  The range of each group of data. Example: you measure the length of leaves on a rose bush. Some are less than 1 cm, and the longest is 9 cm.	What is the meaning of class intervals
6868	In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred. Bayes' theorem thus gives the probability of an event based on new information that is, or may be related, to that event.	What is the relevance of Bayes Theorem in posterior probability
848	The Backpropagation algorithm looks for the minimum value of the error function in weight space using a technique called the delta rule or gradient descent. The weights that minimize the error function is then considered to be a solution to the learning problem.	What is back propagation in machine learning
424	Optimal control deals with the problem of finding a control law for a given system such that a certain optimality criterion is achieved. A control problem includes a cost functional that is a function of state and control variables.	What is optimal control problem
5523	Data Preprocessing is a technique that is used to convert the raw data into a clean data set.  In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.	What is data preprocessing in ML
8190	The random forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a “forest”), this model uses two key concepts that gives it the name random: Random sampling of training data points when building trees.	Why is random forest called random
3797	Example question: Find a critical value for a 90% confidence level (Two-Tailed Test). Step 1: Subtract the confidence level from 100% to find the α level: 100% – 90% = 10%. Step 2: Convert Step 1 to a decimal: 10% = 0.10. Step 3: Divide Step 2 by 2 (this is called “α/2”).	How do you find the critical t value for a two tailed test
4022	The covariance matrix provides a useful tool for separating the structured relationships in a matrix of random variables. This can be used to decorrelate variables or applied as a transform to other variables. It is a key element used in the Principal Component Analysis data reduction method, or PCA for short.	Why do we use covariance matrix
77	Descriptive statistics are broken down into measures of central tendency and measures of variability (spread). Measures of central tendency include the mean, median and mode, while measures of variability include standard deviation, variance, minimum and maximum variables, and kurtosis and skewness.	What should be included in descriptive statistics
239	A Bernouilli distribution is a discrete probability distribution for a Bernouilli trial — a random experiment that has only two outcomes (usually called a “Success” or a “Failure”).  The expected value for a random variable, X, from a Bernoulli distribution is: E[X] = p. For example, if p = . 04, then E[X] = 0.4.	What is the mean of a Bernoulli distribution
331	A radial basis function (RBF) is a real-valued function whose value depends only on the distance between the input and some fixed point, either the origin, so that , or some other fixed point , called a center, so that . Any function that satisfies the property is a radial function.	What is radial basis function in machine learning
1267	There is really only one advantage to using a random forest over a decision tree: It reduces overfitting and is therefore more accurate.	What are some advantages of using a random forest over a decision tree given that a decision tree is simpler
6897	Data science is an umbrella term for a group of fields that are used to mine large datasets. Data analytics software is a more focused version of this and can even be considered part of the larger process. Analytics is devoted to realizing actionable insights that can be applied immediately based on existing queries.	What is the difference between working in analytics and data science
6733	The following seven techniques can help you, to train a classifier to detect the abnormal class.Use the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	How do you handle unbalanced binary classification
1358	15:3426:12Suggested clip · 115 secondsConvolutional Neural Network in Matlab - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How Matlab implement convolutional neural network
6416	The standard error of the regression (S), also known as the standard error of the estimate, represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable.	What does standard error of estimate tell you
5302	"The outcome variable is also called the response or dependent variable, and the risk factors and confounders are called the predictors, or explanatory or independent variables. In regression analysis, the dependent variable is denoted ""Y"" and the independent variables are denoted by ""X""."	What is the dependent variable when running a linear regression
8665	A null hypothesis is a type of conjecture used in statistics that proposes that there is no difference between certain characteristics of a population or data-generating process. The alternative hypothesis proposes that there is a difference.	What does the null and alternative hypothesis mean in a hypothesis test mean
1048	The linear Discriminant analysis estimates the probability that a new set of inputs belongs to every class. The output class is the one that has the highest probability. That is how the LDA makes its prediction. LDA uses Bayes' Theorem to estimate the probabilities.	How does Linear Discriminant Analysis work in laymans terms
222	A Generative Model ‌learns the joint probability distribution p(x,y). It predicts the conditional probability with the help of Bayes Theorem. A Discriminative model ‌learns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems.	When should one use generative models and not discriminative models
3650	Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.	Which loss function is used for classification
4714	Decision theory is the science of making optimal decisions in the face of uncertainty. Statistical decision theory is concerned with the making of decisions when in the presence of statistical knowledge (data) which sheds light on some of the uncertainties involved in the decision problem.	What is decision theory in statistics
888	The purpose of cluster analysis is to place objects into groups, or clusters, suggested by the data, not defined a priori, such that objects in a given cluster tend to be similar to each other in some sense, and objects in different clusters tend to be dissimilar.	What is the purpose of cluster analysis
1955	There is no direct evidence that the brain uses a backprop-like algorithm for learning. Past work has shown, however, that backprop-trained models can account for observed neural responses, such as the response properties of neurons in the posterior parietal cortex68 and primary motor cortex69.	Does the brain use backpropagation
7223	From the mathematical point of view, linear regression and ANOVA are identical: both break down the total variance of the data into different “portions” and verify the equality of these “sub-variances” by means of a test (“F” Test).	Is Anova the same as linear regression
3983	When there are two or more independent variables, it is called multiple regression.	How many variables can you have in a regression
4606	conditions—Random, Normal, and Independent—is. important when constructing a confidence interval.	What are the three conditions for constructing a confidence interval for the population mean
5011	Do you know how to choose the right machine learning algorithm among 7 different types?1-Categorize the problem.  2-Understand Your Data.  Analyze the Data.  Process the data.  Transform the data.  3-Find the available algorithms.  4-Implement machine learning algorithms.  5-Optimize hyperparameters.More items	How do you choose a machine learning algorithm
7110	Systematic sampling involves selecting fixed intervals from the larger population to create the sample. Cluster sampling divides the population into groups, then takes a random sample from each cluster.	What are the differences between systematic random sampling and cluster sampling
6053	Moments are a set of statistical parameters to measure a distribution. Four moments are commonly used: 1st, Mean: the average. 2d, Variance: Standard deviation is the square root of the variance: an indication of how closely the values are spread about the mean.	What is moment in statistics
6477	Naive Bayes classifier (Russell, & Norvig, 1995) is another feature-based supervised learning algorithm. It was originally intended to be used for classification tasks, but with some modifications it can be used for regression as well (Frank, Trigg, Holmes, & Witten, 2000) .	Can naive Bayes be used for regression
1308	intra-observer (or within observer) reliability; the degree to which measurements taken by the same observer are consistent, • inter-observer (or between observers) reliability; the degree to which measurements taken by different observers are similar.	What is inter and intra observer reliability
4776	The binomial theorem is valid more generally for any elements x and y of a semiring satisfying xy = yx. The theorem is true even more generally: alternativity suffices in place of associativity. The binomial theorem can be stated by saying that the polynomial sequence {1, x, x2, x3, } is of binomial type.	What is binomial theorem
3219	The Deep Dream Generator is a computer vision platform that allows users to input photos into the program and transform them through an artificial intelligence algorithm.  In simple terms, many levels of neural networks process the images input into the program.	What is deep dream generator
1026	0:5218:40Suggested clip · 82 secondsChain Rule For Finding Derivatives - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the derivative using the chain rule
3713	Decision tree builds classification or regression models in the form of a tree structure.  The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.	Can decision trees be used for classification
6368	The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.  The results of the K-means clustering algorithm are: The centroids of the K clusters, which can be used to label new data. Labels for the training data (each data point is assigned to a single cluster)	How do clustering algorithms work
738	Bayesian network models capture both conditionally dependent and conditionally independent relationships between random variables. Models can be prepared by experts or learned from data, then used for inference to estimate the probabilities for causal or subsequent events.	How does Bayesian network work
3746	mAP (mean Average Precision) for Object DetectionPrecision & recall.Precision measures how accurate is your predictions.  Recall measures how good you find all the positives.  IoU (Intersection over union)Precision is the proportion of TP = 2/3 = 0.67.Recall is the proportion of TP out of the possible positives = 2/5 = 0.4.	How do you calculate the mean average precision object
471	Descriptive statistics uses the data to provide descriptions of the population, either through numerical calculations or graphs or tables. Inferential statistics makes inferences and predictions about a population based on a sample of data taken from the population in question.	What is the difference between descriptive and inferential statistics give examples
1296	In a skewed distribution, the upper half and the lower half of the data have a different amount of spread, so no single number such as the standard deviation could describe the spread very well.	How does skew affect standard deviation
2299	In ordinary least squares, the relevant assumption of the classical linear regression model is that the error term is uncorrelated with the regressors. The presence of omitted-variable bias violates this particular assumption. The violation causes the OLS estimator to be biased and inconsistent.	Which assumption does omitted variable bias violate
6217	The standard normal distribution table provides the probability that a normally distributed random variable Z, with mean equal to 0 and variance equal to 1, is less than or equal to z. It does this for positive values of z only (i.e., z-values on the right-hand side of the mean).	What is the mean and variance of a standard normal distribution
367	The dependent variable must be continuous (interval/ratio). The observations are independent of one another. The dependent variable should be approximately normally distributed. The dependent variable should not contain any outliers.	What are the two requirements of the one sample t test
5427	Choosing a statistical testType of DataCompare one group to a hypothetical valueOne-sample ttestWilcoxon testCompare two unpaired groupsUnpaired t testMann-Whitney testCompare two paired groupsPaired t testWilcoxon testCompare three or more unmatched groupsOne-way ANOVAKruskal-Wallis test6 more rows•	What is the best statistical test to compare two groups
1219	8 Powerful Tricks That Make You Grasp New Concepts Faster1) Use mental associations. Colours, acronyms and word associations can be especially useful tools to help you hold on to thoughts, patterns and concepts.  2) Apply the 80/20 principle.  3) Break it down.  4) Write it down.  5) Connect existing knowledge.  6) Try Brain exercises.  7) Learn your way.  8) Teach other people.	How do you understand a concept deeply
833	After a performing a test, scientists can: Reject the null hypothesis (meaning there is a definite, consequential relationship between the two phenomena), or. Fail to reject the null hypothesis (meaning the test has not identified a consequential relationship between the two phenomena)	What is the meaning of a null hypothesis being rejected
1384	Dimensionality reduction is the process of reducing the number of random variables or attributes under consideration. High-dimensionality data reduction, as part of a data pre-processing-step, is extremely important in many real-world applications.	What is the need of dimensionality reduction in data mining
6841	Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.	What is a gradient machine learning
234	The K-means clustering algorithm is used to find groups which have not been explicitly labeled in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets.	When to use K means clustering
7080	Maximum entropy is the state of a physical system at greatest disorder or a statistical model of least encoded information, these being important theoretical analogs.	What does maximum entropy mean
800	Epsilon is used when we are selecting specific actions base on the Q values we already have. As an example if we select pure greedy method ( epsilon = 0 ) then we are always selecting the highest q value among the all the q values for a specific state.	What is Epsilon in Q learning
978	Correlation is a technique for investigating the relationship between two quantitative, continuous variables, for example, age and blood pressure. Pearson's correlation coefficient (r) is a measure of the strength of the association between the two variables.	What type of data are required if using Pearson's correlation coefficient
6939	"They are sometimes called ""normal"" values. By comparing your test results with reference values, you and your healthcare provider can see if any of your test results fall outside the range of expected values. Values that are outside expected ranges can provide clues to help identify possible conditions or diseases."	What does your value mean on test results
8183	Collective Intelligence. knowledge collected from many people towards a common goal.	What is collective intelligence quizlet
1580	The Central Limit Theorem and Means In other words, add up the means from all of your samples, find the average and that average will be your actual population mean. Similarly, if you find the average of all of the standard deviations in your sample, you'll find the actual standard deviation for your population.	How do you find the sample mean using central limit theorem
7161	Construct validity means the test measures the skills/abilities that should be measured. Content validity means the test measures appropriate content.	What is the difference between content and construct validity
1294	The distribution for z is the standard normal distribution; it has a mean of 0 and a standard deviation of 1. For Ha: p ≠ 26, the P-value would be P(z ≤ -1.83) + P(z ≥ 1.83) = 2 * P(z ≤ -1.83). Regardless of Ha, z = (p̂ - p0) / sqrt(p0 * (1 - p0) / n), where z gives the number of standard deviations p̂ is from p0.	How do you find the p value in a normal distribution
5838	The input gate controls the extent to which a new value flows into the cell, the forget gate controls the extent to which a value remains in the cell and the output gate controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit.	What is forget gate in Lstm
2637	The difference is a matter of design. In the test of independence, observational units are collected at random from a population and two categorical variables are observed for each unit.  In the goodness-of-fit test there is only one observed variable.	What is the difference between the terms goodness of fit and test of independence
3767	1. Which search agent operates by interleaving computation and action? Explanation: In online search, it will first take an action and then observes the environment.	Which search agent operates by interleaving computation and action in artificial Intelligence
6612	Sensitivity aka Recall (true positives / all actual positives) = TP / TP + FN. 45 / (45 + 5) = 45 / 50 = 0.90 or 90% Sensitivity. 5. Specificity (true negatives / all actual negatives) =TN / TN + FP.	What is sensitivity and specificity in confusion matrix
5603	Theoretically, yes. TensorFlow is designed with flexibility in mind, so that should be possible.	Can Scikit Learn model be used in TensorFlow Serving
783	A negative correlation can indicate a strong relationship or a weak relationship. Many people think that a correlation of –1 indicates no relationship. But the opposite is true. A correlation of -1 indicates a near perfect relationship along a straight line, which is the strongest relationship possible.	What does a correlation coefficient of negative 1 mean
1144	The Kappa Architecture was first described by Jay Kreps. It focuses on only processing data as a stream. It is not a replacement for the Lambda Architecture, except for where your use case fits.  The idea is to handle both real-time data processing and continuous reprocessing in a single stream processing engine.	What is the difference between the Kappa and the lambda architecture
6427	Empirical Relationship between Mean, Median and Mode In case of a moderately skewed distribution, the difference between mean and mode is almost equal to three times the difference between the mean and median. Thus, the empirical mean median mode relation is given as: Mean – Mode = 3 (Mean – Median)	What is the relation between mean mode median
1245	Statistical knowledge helps you use the proper methods to collect the data, employ the correct analyses, and effectively present the results. Statistics is a crucial process behind how we make discoveries in science, make decisions based on data, and make predictions.	What are the uses of statistics
7403	The 7 Steps of Machine Learning1 - Data Collection. The quantity & quality of your data dictate how accurate our model is.  2 - Data Preparation. Wrangle data and prepare it for training.  3 - Choose a Model.  4 - Train the Model.  5 - Evaluate the Model.  6 - Parameter Tuning.  7 - Make Predictions.	What are the steps involved in machine learning
610	A feature selection method is proposed to select a subset of variables in principal component analysis (PCA) that preserves as much information present in the complete data as possible. The information is measured by means of the percentage of consensus in generalised Procrustes analysis.	How Principal component analysis is used for feature selection
4645	Active learning engages students in learning, using activities such as reading, writing, discussion, or problem solving, which promote analysis, synthesis, and evaluation of class content. Active in-class learning also provides students with informal opportunities for feedback on how well they understood the material.	What are active learning strategies
7652	Structural equation modeling (SEM) is a multivariate statistical framework that is used to model complex relationships between directly and indirectly observed (latent) variables.  Modeling the aggregate effects of common and rare variants in multiple potentially interesting genes using latent variable SEM.	What is structural equation modeling PDF
3946	slang. a dismissal; discharge. They gave him the boot for coming in late. 17. informal.	What is boot slang for
2641	A random effect model is a model all of whose factors represent random effects. (See Random Effects.) Such models are also called variance component models. Random effect models are often hierarchical models. A model that contains both fixed and random effects is called a mixed model.	What is a random effect in a mixed model
5963	Stratified random sampling is used when your population is divided into strata (characteristics like male and female or education level), and you want to include the stratum when taking your sample.	Where is stratified sampling used
692	Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.	What is gradient descent rule
89	No. Principal components analysis involves breaking down the variance structure of a group of variables. Categorical variables are not numerical at all, and thus have no variance structure.	Can you do principal component analysis on categorical variables
312	The general procedure for using regression to make good predictions is the following:Research the subject-area so you can build on the work of others.  Collect data for the relevant variables.Specify and assess your regression model.If you have a model that adequately fits the data, use it to make predictions.	How do you predict regression
1013	Important Properties Property #1: The total area under a t distribution curve is 1.0: that is 100%. Property #2: A t-curve is symmetric around 0. Property #3: While a t-curve extends infinitely in either direction, it approaches, but never touches the horizontal axis.	What are the properties of a T curve
7739	1a : to divide into parts or shares. b : to divide (a place, such as a country) into two or more territorial units having separate political status. 2 : to separate or divide by a partition (such as a wall) —often used with off. Other Words from partition Synonyms More Example Sentences Learn More about partition.	What does partitioning mean
1191	In convolutional layers the weights are represented as the multiplicative factor of the filters. For example, if we have the input 2D matrix in green. with the convolution filter. Each matrix element in the convolution filter is the weights that are being trained.	What is weights in convolutional neural network
414	That's your sample size--the number of participants needed to achieve valid conclusions or statistical significance in quantitative research.  When sample sizes are too small, you run the risk of not gathering enough data to support your hypotheses or expectations.	Does sample size matter in quantitative research
507	Neural Networks and Deep Reinforcement Learning.  Neural networks are function approximators, which are particularly useful in reinforcement learning when the state space or action space are too large to be completely known. A neural network can be used to approximate a value function, or a policy function.	Does reinforcement learning use neural networks
5055	k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification.	Can Knn be used for clustering
2535	Cross Entropy is definitely a good loss function for Classification Problems, because it minimizes the distance between two probability distributions - predicted and actual.  So cross entropy make sure we are minimizing the difference between the two probability. This is the reason.	Why is cross entropy used for classification
6790	A covariance matrix is a square matrix which gives two types of information. If you are looking at the population covariance matrix then. each diagonal element is the variance of the corresponding random variable. each off-diagonal element is the covariance of the corresponding pair of random variables.	What is the intuitive meaning of a covariance matrix
4254	In the statistical theory of design of experiments, randomization involves randomly allocating the experimental units across the treatment groups.  Randomization reduces bias by equalising other factors that have not been explicitly accounted for in the experimental design (according to the law of large numbers).	What is randomization experiment
6973	In computational mathematics, an iterative method is a mathematical procedure that uses an initial value to generate a sequence of improving approximate solutions for a class of problems, in which the n-th approximation is derived from the previous ones.	What is the condition for iterative method
1308	Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network.	What is deep learning in simple words
845	See the following examples from SciPol:Making driving safer. Though self-driving cars are still a few years away from being fully safe to drive, this area of AI could dramatically decrease the rates of deaths and injuries on the roads.  Transforming how we learn.  Help us become more energy efficient.  Helping wildlife.	What problems can we solve using AI
3963	"Q-learning is a model-free reinforcement learning algorithm to learn quality of actions telling an agent what action to take under what circumstances.  ""Q"" names the function that the algorithm computes with the maximum expected rewards for an action taken in a given state."	What is Q function in machine learning
1373	Lemmatization and stemming are the techniques of keyword normalization, while Levenshtein and Soundex are techniques of string matching.	Which techniques can be used for normalization in text mining
2957	How to reduce False Positive and False Negative in binary classificationfirstly random forest overfits if the training data and testing data are not drawn from same distribution.check the data for linearity,multicollinearity ,outliers,etc.More items	How can you reduce false positives in classification
6479	Systematic random samplingCalculate the sampling interval (the number of households in the population divided by the number of households needed for the sample)Select a random start between 1 and sampling interval.Repeatedly add sampling interval to select subsequent households.	How do you do systematic sampling
2284	Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms. In fact, it is the number of node layers, or depth, of neural networks that distinguishes a single neural network from a deep learning algorithm, which must have more than three.	Is Deep Learning same as neural network
3507	A false positive means that the results say you have the condition you were tested for, but you really don't. With a false negative, the results say you don't have a condition, but you really do.	What is the difference between false positive and false negative
5564	Statistical classification helps in determining the set to which a particular observation belongs. Multiple methods can be used for the classification process, namely, Frequentest procedure and Bayesian procedure among others. It helps in quicker arranging and collection of data,as well as more efficient work rate.	What is statistical classification What is the importance of such a classification
748	Cosine similarity is a metric used to measure how similar the documents are irrespective of their size.  The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.	Why does cosine similarity work in Machine Learning
67	Use of AI in Following Things/Fields/Areas:Virtual Assistant or Chatbots.Agriculture and Farming.Autonomous Flying.Retail, Shopping and Fashion.Security and Surveillance.Sports Analytics and Activities.Manufacturing and Production.Live Stock and Inventory Management.More items•	Where is artificial intelligence used
8117	The most basic approach is to stick to the default value and hope for the best. A better implementation of the first option is to test a broad range of possible values. Depending on how the loss changes, you go for a higher or lower learning rate. The aim is to find the fastest rate that still decreases the loss.	How do you optimize learning rate
1889	An ordinal variable is a categorical variable for which the possible values are ordered. Ordinal variables can be considered “in between” categorical and quantitative variables. Thus it does not make sense to take a mean of the values.	What type of variable is ordinal
7382	Increase Training Dataset Size Leaning on the law of large numbers, perhaps the simplest approach to reduce the model variance is to fit the model on more training data. In those cases where more data is not readily available, perhaps data augmentation methods can be used instead.	How do you reduce variance in machine learning
1233	To run the t-test, arrange your data in columns as seen below. Click on the “Data” menu, and then choose the “Data Analysis” tab. You will now see a window listing the various statistical tests that Excel can perform. Scroll down to find the t-test option and click “OK”.	How do I test statistical significance in Excel
8630	Deep Q-Networks In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as the input and the Q-value of all possible actions is generated as the output.	What is a deep Q Network
1693	The performance of deep learning neural networks often improves with the amount of data available. Data augmentation is a technique to artificially create new training data from existing training data. This means, variations of the training set images that are likely to be seen by the model.	What is augmentation in deep learning
1409	Approximately Normal Distributions with Discrete Data. If a random variable is actually discrete, but is being approximated by a continuous distribution, a continuity correction is needed.	Can discrete random variables be normally distributed
7	Noisy data are data with a large amount of additional meaningless information in it called noise. This includes data corruption and the term is often used as a synonym for corrupt data. It also includes any data that a user system cannot understand and interpret correctly.	What does noise in data mean
1377	Most home pregnancy tests are reliable, for example Clearblue's tests have an accuracy of over 99% from the day you expect your period, and while it's possible a test showing a negative result is wrong, particularly if you're testing early, getting a false positive is extremely rare.	What are the odds of a false positive test
6392	8 Methods to Boost the Accuracy of a ModelAdd more data. Having more data is always a good idea.  Treat missing and Outlier values.  Feature Engineering.  Feature Selection.  Multiple algorithms.  Algorithm Tuning.  Ensemble methods.	How can the accuracy of machine learning model be improved
232	Standard deviation is a number used to tell how measurements for a group are spread out from the average (mean or expected value). A low standard deviation means that most of the numbers are close to the average, while a high standard deviation means that the numbers are more spread out.	What does standard deviation mean in simple terms
2894	A heuristic is admissible if it never overestimates the true cost to a nearest goal. A heuristic is consistent if, when going from neighboring nodes a to b, the heuristic difference/step cost never overestimates the actual step cost. This can also be re-expressed as the triangle inequality men- tioned in Lecture 3.	How can you prove that a heuristic is admissible in artificial intelligence
1410	A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers.	What is the discrete random variable
2483	Smoothing techniques in NLP are used to address scenarios related to determining probability / likelihood estimate of a sequence of words (say, a sentence) occuring together when one or more words individually (unigram) or N-grams such as bigram(wi/wi−1) or trigram (wi/wi−1wi−2) in the given set have never occured in	What is smoothing in NLP
7663	Accuracy reflects how close a measurement is to a known or accepted value, while precision reflects how reproducible measurements are, even if they are far from the accepted value. Measurements that are both precise and accurate are repeatable and very close to true values.	What is difference between precision and accuracy
5325	A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation.	Why is it called neural network
1461	Cluster analysis is a tool for classifying objects into groups and is not concerned with the geometric representation of the objects in a low-dimensional space. To explore the dimensionality of the space, one may use multidimensional scaling.	What is the difference between cluster analysis and multidimensional scaling
6052	Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map. The results are down sampled or pooled feature maps that highlight the most present feature in the patch, not the average presence of the feature in the case of average pooling.	What is Max pooling layer
7204	BFS is slower than DFS. DFS is faster than BFS. Time Complexity of BFS = O(V+E) where V is vertices and E is edges. Time Complexity of DFS is also O(V+E) where V is vertices and E is edges.	What is the time complexity of BFS and DFS
1627	8 Methods to Boost the Accuracy of a ModelAdd more data. Having more data is always a good idea.  Treat missing and Outlier values.  Feature Engineering.  Feature Selection.  Multiple algorithms.  Algorithm Tuning.  Ensemble methods.	How do you increase the accuracy of a decision tree
2039	Multicollinearity might be a handful to pronounce but it's a topic you should be aware of in the machine learning field. Due to multicollinearity, regression coefficients will not be estimated precisely and cause a high standard error.	Is Multicollinearity a problem in machine learning
7525	Selective unsupervised feature learning with Convolutional Neural Network (S-CNN) Abstract: Supervised learning of convolutional neural networks (CNNs) can require very large amounts of labeled data.  This method for unsupervised feature learning is then successfully applied to a challenging object recognition task.	Is CNN supervised or unsupervised
1083	The following methods for validation will be demonstrated:Train/test split.k-Fold Cross-Validation.Leave-one-out Cross-Validation.Leave-one-group-out Cross-Validation.Nested Cross-Validation.Time-series Cross-Validation.Wilcoxon signed-rank test.McNemar's test.More items	How do you validate machine learning models
584	If the P-value is less than (or equal to) , then the null hypothesis is rejected in favor of the alternative hypothesis. And, if the P-value is greater than , then the null hypothesis is not rejected.	How do you know whether to accept or reject the null hypothesis
420	Posterior probability = prior probability + new evidence (called likelihood). For example, historical data suggests that around 60% of students who start college will graduate within 6 years. This is the prior probability. However, you think that figure is actually much lower, so set out to collect new data.	What is posterior probability example
3637	The lognormal distribution is a distribution skewed to the right. The pdf starts at zero, increases to its mode, and decreases thereafter. The degree of skewness increases as increases, for a given . For the same , the pdf's skewness increases as increases.	What properties do a log normal distribution have
1006	The key classification metrics: Accuracy, Recall, Precision, and F1- Score.	Which are evaluation metrics for classification
4809	On-policy methods attempt to evaluate or improve the policy that is used to make decisions. In contrast, off-policy methods evaluate or improve a policy different from that used to generate the data.	What is on policy and off policy
6918	How to choose the size of the convolution filter or Kernel size1x1 kernel size is only used for dimensionality reduction that aims to reduce the number of channels. It captures the interaction of input channels in just one pixel of feature map.  2x2 and 4x4 are generally not preferred because odd-sized filters symmetrically divide the previous layer pixels around the output pixel.	How do I choose my CNN kernel size
8040	4.7 Confusion matrix patterns The “normalized” term means that each of these groupings is represented as having 1.00 samples.  The columns sum the samples assigned to each class, and the diagonal elements divided by these sums are the precision values. The diagonal elements represent the recall values.	What is normalized confusion matrix
4843	High Dimensional means that the number of dimensions are staggeringly high — so high that calculations become extremely difficult. With high dimensional data, the number of features can exceed the number of observations. For example, microarrays, which measure gene expression, can contain tens of hundreds of samples.	What does high dimensional data mean
580	The ROC curve shows the trade-off between sensitivity (or TPR) and specificity (1 – FPR). Classifiers that give curves closer to the top-left corner indicate a better performance. As a baseline, a random classifier is expected to give points lying along the diagonal (FPR = TPR).	How do you read a ROC curve
4052	If you want to process the gradients before applying them you can instead use the optimizer in three steps:Compute the gradients with compute_gradients().Process the gradients as you wish.Apply the processed gradients with apply_gradients().	How does one do gradient clipping in TensorFlow
772	Q-Learning is a value-based reinforcement learning algorithm which is used to find the optimal action-selection policy using a Q function. Our goal is to maximize the value function Q. The Q table helps us to find the best action for each state.	What is Q learning algorithm in machine learning
2321	Weights control the signal (or the strength of the connection) between two neurons. In other words, a weight decides how much influence the input will have on the output. Biases, which are constant, are an additional input into the next layer that will always have the value of 1.	What are weights in machine learning
17	Regression analysis is a reliable method of identifying which variables have impact on a topic of interest. The process of performing a regression allows you to confidently determine which factors matter most, which factors can be ignored, and how these factors influence each other.	What does a regression model tell you
5607	Conditional probability is the probability of one event occurring with some relationship to one or more other events. For example: Event A is that it is raining outside, and it has a 0.3 (30%) chance of raining today. Event B is that you will need to go outside, and that has a probability of 0.5 (50%).	What is conditional probability explain with an example
451	Univariate linear regression focuses on determining relationship between one independent (explanatory variable) variable and one dependent variable. Regression comes handy mainly in situation where the relationship between two features is not obvious to the naked eye.	What is a univariate regression analysis
8137	Thus, the SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe, whereas the Jaccard index only counts mutual presence as matches and compares it to the	What is the main difference between simple matching coefficient SMC similarity and Jaccard similarity
152	While statistical significance relates to whether an effect exists, practical significance refers to the magnitude of the effect. However, no statistical test can tell you whether the effect is large enough to be important in your field of study.  An effect of 4 points or less is too small to care about.	How might a statistical test be statistically significant but not practical
5633	In lay terms, the standard deviation can be thought of as roughly the average distance of scores from the mean. Precisely, the standard deviation is defined to be the square root of the average squared deviation of scores from the mean.	Can standard deviation in statistics be defined as average deviation in laymans terms
90	The Hidden Markov Model (HMM) is a relatively simple way to model sequential data. A hidden Markov model implies that the Markov Model underlying the data is hidden or unknown to you. More specifically, you only know observational data and not information about the states.	What is a simple explanation of the Hidden Markov Model algorithm
3978	A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease.	How do you interpret estimated coefficients
1272	A simple way to fix imbalanced data-sets is simply to balance them, either by oversampling instances of the minority class or undersampling instances of the majority class. This simply allows us to create a balanced data-set that, in theory, should not lead to classifiers biased toward one class or the other.	How do you balance classes in machine learning
4808	The sample space of a random experiment is the collection of all possible outcomes. An event associated with a random experiment is a subset of the sample space. The probability of any outcome is a number between 0 and 1. The probabilities of all the outcomes add up to 1.	What is probability sample space
3843	Whereas AI is preprogrammed to carry out a task that a human can but more efficiently, artificial general intelligence (AGI) expects the machine to be just as smart as a human.	What is the difference between AI and AGI
515	The loss and accuracy of all three models is comparable but the Neocognitron and Coward model have a higher processing time than the Convolutional Neural Network. It is also evident that the Neocognitron requires more training steps than the Convolutional Neural Network to reach the same accuracy and loss.	What is the difference between Neocognitron and Convolutional neural network
5506	Decision tree learning is generally best suited to problems with the following characteristics: Instances are represented by attribute-value pairs. There is a finite list of attributes (e.g. hair colour) and each instance stores a value for that attribute (e.g. blonde).	What type of problems are best suited for decision tree learning
6456	Here are the steps for finding any percentile for a normal distribution X: 1a. If you're given the probability (percent) less than x and you need to find x, you translate this as: Find a where p(X < a) = p (and p is the given probability). That is, find the pth percentile for X.	How do you find the percentile of a normal distribution
3901	A hypothesis is an approximate explanation that relates to the set of facts that can be tested by certain further investigations. There are basically two types, namely, null hypothesis and alternative hypothesis. A research generally starts with a problem.	What are the two types of hypothesis testing
2871	Definition. The explained sum of squares (ESS) is the sum of the squares of the deviations of the predicted values from the mean value of a response variable, in a standard regression model — for example, yi = a + b1x1i + b2x2i +   the value estimated by the regression line .	What is sum of square regression
4840	Average: Theory & Formulas.  We all know that the average is sum of observations divided by the total number of observations. Average Formula = Sum of observations/ Number of observations. This is the simple formula which helps us to calculate the average in math.	What is average formula
6538	Disadvantages of decision trees:They are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree.They are often relatively inaccurate.More items	What are the disadvantages of using a decision tree for classification
4622	their superior predictive power and their theoretical foundation.  their accuracy is poor in many domains compared to neural networks.	Why is sensitivity analysis frequently used for artificial neural networks
571	Learning a new skill is often an extremely rewarding experience. If it's something you like, you'll quickly notice yourself improving, which can give you a great confidence boost. In most cases, trying something new is often about overcoming fear.	How do you feel when you learn something new
1207	Misleading statistics are simply the misusage - purposeful or not - of a numerical data. The results provide a misleading information to the receiver, who then believes something wrong if he or she does not notice the error or the does not have the full data picture.	What are misleading statistics
3529	"The term ""c=0"" I think was coined by Nicholas Squeglia. It is defined as a sampling plan which does not allow for the acceptance if any defects are found."	What does C 0 sampling plan mean
4490	The Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to maximize the chances of A winning, while on the next turn player B is trying to minimize the chances of A winning (i.e., to maximize B's own chances of winning).	How does the Minimax algorithm work
5557	The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30).	What is the difference between T and Z statistic
1418	An eigenvalue is a number, telling you how much variance there is in the data in that direction, in the example above the eigenvalue is a number telling us how spread out the data is on the line. The eigenvector with the highest eigenvalue is therefore the principal component.	What is eigenvalue in principal component analysis
986	"In statistics, a rank correlation is any of several statistics that measure an ordinal association—the relationship between rankings of different ordinal variables or different rankings of the same variable, where a ""ranking"" is the assignment of the ordering labels ""first"", ""second"", ""third"", etc. to different"	What is rank difference correlation
455	While explanation for sudden death in certain infants remains incomplete, the term SIDS was only accepted as an official diagnosis on death certificates in 1971, with the term “sudden infant death” being allocated a separate code (coding number 798.0) in the World Health Organization's International Classification of	When was SIDS recognized
1153	“K-means can't handle non-convex sets”. Convex sets: In Euclidean space, an object is convex if for every pair of points within the object, every point on the straight line segment that joins them is also within the object.  That's two non-convex shapes, and they are not spatially separated.	Can K means detect non convex clusters
2119	Moments help in finding AM, standard deviation and variance of the population directly, and they help in knowing the graphic shapes of the population. We can call moments as the constants used in finding the graphic shape, as the graphic shape of the population also help a lot in characterizing a population.	Why do we use moments in statistics
7542	The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions.	What is the difference between linear model and generalized linear model
3119	A type I error (false-positive) occurs if an investigator rejects a null hypothesis that is actually true in the population; a type II error (false-negative) occurs if the investigator fails to reject a null hypothesis that is actually false in the population.	What are type I and type II errors and significance levels
631	Separating data into training and testing sets is an important part of evaluating data mining models.  By using similar data for training and testing, you can minimize the effects of data discrepancies and better understand the characteristics of the model.	Why do you split data into training and test sets
1080	A kernel is the foundational layer of an operating system (OS). It functions at a basic level, communicating with hardware and managing resources, such as RAM and the CPU.  The kernel performs a system check and recognizes components, such as the processor, GPU, and memory. It also checks for any connected peripherals.	What is kernel and its functions
8379	The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. The area under the normal distribution curve represents probability and the total area under the curve sums to one.	What is a normal distribution in statistics
3849	The sample covariance matrix is a square matrix whose i, j element is the sample covariance (an estimate of the population covariance) between the sets of observed values of two of the variables and whose i, i element is the sample variance of the observed values of one of the variables.	What is the sample covariance
3663	Prior probability shows the likelihood of an outcome in a given dataset. For example, in the mortgage case, P(Y) is the default rate on a home mortgage, which is 2%. P(Y|X) is called the conditional probability, which provides the probability of an outcome given the evidence, that is, when the value of X is known.	What is prior probability with example
656	Singular value decomposition is essentially trying to reduce a rank matrix to a rank K matrix. But what does this mean? It means that we can take a list of unique vectors, and approximate them as a linear combination of unique vectors. Take this example, the image below is an image made of 400 unique row vectors.	What is an intuitive explanation of singular value decomposition SVD
4015	Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks.  With gradient clipping, pre-determined gradient threshold be introduced, and then gradients norms that exceed this threshold are scaled down to match the norm.	What does gradient clipping do
6162	The system of IP address classes was developed for the purpose of Internet IP addresses assignment. The classes created were based on the network size. For example, for the small number of networks with a very large number of hosts, the Class A was created.	Why are there different classes of IP addresses
1396	An indicator random variable is a special kind of random variable associated with the occurence of an event. The indicator random variable IA associated with event A has value 1 if event A occurs and has value 0 otherwise. In other words, IA maps all outcomes in the set A to 1 and all outcomes outside A to 0.	What is an indicator random variable
383	Statistical learning is a framework for understanding data based on statistics, which can be classified as supervised or unsupervised.	What is statistical learning in data science
2800	Empirical definitions.  The definition of empirical is something that is based solely on experiment or experience. An example of empirical is the findings of dna testing.	What is an empirical example
7361	Multiclass classification makes the assumption that each sample is assigned to one and only one label: a fruit can be either an apple or a pear but not both at the same time. - Multilabel classification assigns to each sample a set of target labels.  The set of labels can be different for each output variable.	What is the difference between multiclass and Multilabel classification
1842	A false negative is a test result that indicates a person does not have a disease or condition when the person actually does have it, according to the National Institute of Health (NIH).	What does false negative mean
1906	Support Vector Machine algorithms are supervised learning models that analyse data used for classification and regression analysis. They essentially filter data into categories, which is achieved by providing a set of training examples, each set marked as belonging to one or the other of the two categories.	What is an algorithm in machine learning
1422	1 — Linear Regression.  2 — Logistic Regression.  3 — Linear Discriminant Analysis.  4 — Classification and Regression Trees.  5 — Naive Bayes.  6 — K-Nearest Neighbors.  7 — Learning Vector Quantization.  8 — Support Vector Machines.More items•	Which classification algorithms is easiest to start with for prediction
242	One way of finding a point estimate ˆx=g(y) is to find a function g(Y) that minimizes the mean squared error (MSE). Here, we show that g(y)=E[X|Y=y] has the lowest MSE among all possible estimators. That is why it is called the minimum mean squared error (MMSE) estimate. h(a)=E[(X−a)2]=EX2−2aEX+a2.	How do you find the minimum mean square error
551	A two layer (one input layer, one output layer; no hidden layer) neural network can represent the XOR function. We must compose multiple logical operations by using a hidden layer to represent the XOR function.	Can a 2 layer neural network represent the XOR function
5675	Neurons are organized into bundle fibers called nerves.  Dendrites are structures of neurons that conduct electrical impulses toward the cell body.	What is the difference between a nerve and a neuron quizlet
8644	"Convenience sampling is a type of nonprobability sampling in which people are sampled simply because they are ""convenient"" sources of data for researchers. In probability sampling, each element in the population has a known nonzero chance of being selected through the use of a random selection procedure."	What is the meaning of convenience sampling
3139	A regression line is a straight line that de- scribes how a response variable y changes as an explanatory variable x changes. We often use a regression line to predict the value of y for a given value of x.	What does the line of regression tell you
1353	A sparsity penalty term is included in the loss function to prevent the identity mapping by keeping only a selected set of neurons active at any instance.  The constraint forces the AE to represent each input using only a small number of hidden neurons.	What is sparsity constraint
4935	In statistics, Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference.	Is linear regression Bayesian
419	Binary cross-entropy is for multi-label classifications, whereas categorical cross entropy is for multi-class classification where each example belongs to a single class.	What is the difference between binary cross entropy and categorical cross entropy
6965	A decision tree typically starts with a single node, which branches into possible outcomes. Each of those outcomes leads to additional nodes, which branch off into other possibilities.  A decision node, represented by a square, shows a decision to be made, and an end node shows the final outcome of a decision path.	What does the decision node illustrate in a decision tree
1578	To activate the Multinomial Logit Model dialog box, start XLSTAT, then select the XLSTAT / Modeling data / Logistic regression for binary response data command, or click on the logistic regression button of the Modeling Data toolbar (see below). When you click on the button, the Logistic regression dialog box appears.	How do you do multinomial logistic regression on Excel
5885	Specifically, you learned: Machine learning algorithms are procedures that are implemented in code and are run on data. Machine learning models are output by algorithms and are comprised of model data and a prediction algorithm.	What is the difference between machine learning model and ML algorithm
4368	To calculate: Administer the two tests to the same participants within a short period of time. Correlate the test scores of the two tests. – Inter-Rater Reliability: Determines how consistent are two separate raters of the instrument.	How do you calculate reliability in statistics
7461	The t‐distribution is used as an alternative to the normal distribution when sample sizes are small in order to estimate confidence or determine critical values that an observation is a given distance from the mean.	Why do we use t distribution
1671	To calculate the learnable parameters here, all we have to do is just multiply the by the shape of width m, height n, previous layer's filters d and account for all such filters k in the current layer. Don't forget the bias term for each of the filter.	How are learnable parameters calculated CNN
1210	The loss function used by the perceptron algorithm is called 0-1 loss. 0-1 loss simply means that for each mistaken prediction you incur a penalty of 1 and for each correct prediction incur no penalty. The problem with this loss function is given a linear classifier its hard to move towards a local optimum.	What is the loss function of the standard perceptron algorithm
1780	7 Steps of Machine LearningStep #1: Gathering Data.  Step #2: Preparing that Data.  Step #3: Choosing a Model.  Step #4: Training.  Step #5: Evaluation.  Step #6: Hyperparameter Tuning.  Step #7: Prediction.	What are the steps in machine learning
371	Yes, the vectors from a word2vec model can be used as input in the learning of a new task, and in some (not all) cases, may yield better performance in the new model.	Does word2vec transfer learning
5451	Mutual information is a quantity that measures a relationship between two random variables that are sampled simultaneously. In particular, it measures how much information is communicated, on average, in one random variable about another.	How does mutual information work
983	The decision tree tool is used in real life in many areas, such as engineering, civil planning, law, and business. Decision trees can be divided into two types; categorical variable and continuous variable decision trees.	What are decision trees What are the different types of decision trees
8637	Sampling from a 1D DistributionNormalize the function f(x) if it isn't already normalized.Integrate the normalized PDF f(x) to compute the CDF, F(x).Invert the function F(x).  Substitute the value of the uniformly distributed random number U into the inverse normal CDF.	How do you sample probability distributions
259	Interpolation is making an educated guess with the information within a certain data set. It is a “best guess” using the information you have at hand.	What is interpolation in machine learning
953	The problems that are addressed by AI search algorithms fall into three general classes: single-agent path-finding problems, two-players games, and constraint-satisfaction problems. Search plays a major role in solving many Artificial Intelligence (AI) problems. Search is a universal problem-solving mechanism in AI.	What is the importance of search algorithm in developing AI
1240	Back-propagation is just a way of propagating the total loss back into the neural network to know how much of the loss every node is responsible for, and subsequently updating the weights in such a way that minimizes the loss by giving the nodes with higher error rates lower weights and vice versa.	How does backpropagation in artificial neural networks work
1912	How many parity check bits must be included with the data word to achieve single-bit error correction and double error correction when data words are as follows: 16 bits.	How many parity check bits must be included with the data word to achieve single error correction and double error detection when the data word contains 32 bits
7853	Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.	Why is word2vec used
3334	Step 1: Load Python packages.  Step 2: Pre-Process the data.  Step 3: Subset the data.  Step 4: Split the data into train and test sets.  Step 5: Build a Random Forest Classifier.  Step 6: Predict.  Step 7: Check the Accuracy of the Model.  Step 8: Check Feature Importance.	How do you create a classification model
1003	From Wikipedia, the free encyclopedia. In mathematical optimization, constrained optimization (in some contexts called constraint optimization) is the process of optimizing an objective function with respect to some variables in the presence of constraints on those variables.	What is meant by constrained optimization
958	Cross correlation presents a technique for comparing two time series and finding objectively how they match up with each other, and in particular where the best match occurs. It can also reveal any periodicities in the data.	What does cross correlation tell you
2146	The most common evaluation metric that is used in object recognition tasks is 'mAP', which stands for 'mean average precision'. It is a number from 0 to 100 and higher values are typically better, but it's value is different from the accuracy metric in classification.	What is mAP object detection
845	How to Analyze Survey ResultsUnderstand the four measurement levels.  Select your research question(s).  Analyze quantitative data first.  Use cross-tabulation to better understand your target audience.  Understand the statistical significance.  Take into consideration causation versus correlation.  Compare data with that of past data.	How do you analyze survey data
7574	Different types of deep learning models.Autoencoders. An autoencoder is an artificial neural network that is capable of learning various coding patterns.  Deep Belief Net.  Convolutional Neural Networks.  Recurrent Neural Networks.  Reinforcement Learning to Neural Networks.	What are the types of deep learning
4306	Log-Log linear regression A regression model where the outcome and at least one predictor are log transformed is called a log-log linear model.	What is a log log regression
8631	"Uninformative priors. An uninformative prior or diffuse prior expresses vague or general information about a variable. The term ""uninformative prior"" is somewhat of a misnomer. Such a prior might also be called a not very informative prior, or an objective prior, i.e. one that's not subjectively elicited."	What is an uninformative prior
1156	Univariate and multivariate represent two approaches to statistical analysis. Univariate involves the analysis of a single variable while multivariate analysis examines two or more variables. Most multivariate analysis involves a dependent variable and multiple independent variables.	What is univariate and multivariate analysis
8002	Markov Chain Monte–Carlo (MCMC) is an increasingly popular method for obtaining information about distributions, especially for estimating posterior distributions in Bayesian inference. This article provides a very basic introduction to MCMC sampling.	What is Markov Chain Monte Carlo used for
7598	Python is easy to learn and work with, and provides convenient ways to express how high-level abstractions can be coupled together. Nodes and tensors in TensorFlow are Python objects, and TensorFlow applications are themselves Python applications. The actual math operations, however, are not performed in Python.	How is TensorFlow different from Python
1231	A utility-based agent is an agent that acts based not only on what the goal is, but the best way to reach that goal.  Think about it this way: A goal-based agent (yes, another of the intelligent agents out there) makes decisions based simply on achieving a set goal.	What is utility based agent in artificial intelligence
8365	The three main metrics used to evaluate a classification model are accuracy, precision, and recall. Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.	How do you find the accuracy of a machine learning model
828	Word2vec being log-linear means we calculate the gradient at the output and then directly propagate this back into the embedding parameters (the main computational burden during training). This means faster trainer over bigger datasets yielding more accurate embedding vectors.	Why is word2vec a log linear model
7338	Generative Models. LSTMs can be used as a generative model. Given a large corpus of sequence data, such as text documents, LSTM models can be designed to learn the general structural properties of the corpus, and when given a seed input, can generate new sequences that are representative of the original corpus.	Is Lstm generative or discriminative
1200	"A few common responses to compliments are ""you're welcome"", ""no problem"", ""my pleasure"" or ""glad I could help"". The best of all is "" My Pleasure""."	How do you respond to your best
59	A one-brain AI would still not be a true intelligence, only a better general-purpose AI—Legg's multi-tool. But whether they're shooting for AGI or not, researchers agree that today's systems need to be made more general-purpose, and for those who do have AGI as the goal, a general-purpose AI is a necessary first step.	Is artificial general intelligence possible
304	The chi-square test may be used both as a test of goodness-of-fit (comparing frequencies of one nominal variable to theoretical expectations) and as a test of independence (comparing frequencies of one nominal variable for different values of a second nominal variable).	How the chi square test for independence and the chi square goodness of fit test are related
7691	Gradient descent is a simple optimization procedure that you can use with many machine learning algorithms.  Stochastic gradient descent refers to calculating the derivative from each training data instance and calculating the update immediately.	What is stochastic gradient descent in machine learning
5458	They are often confused with each other. The 'K' in K-Means Clustering has nothing to do with the 'K' in KNN algorithm. k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification.	What is the difference between Knn and K means clustering
786	A random variable, usually written X, is a variable whose possible values are numerical outcomes of a random phenomenon. There are two types of random variables, discrete and continuous.	What are the 2 types of random variables
5757	A one hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical.	What is the use of one hot encoding
3902	Ridge regression is a term used to refer to a linear regression model whose coefficients are not estimated by ordinary least squares (OLS), but by an estimator, called ridge estimator, that is biased but has lower variance than the OLS estimator.	What is the difference between OLS and ridge regression
1287	Weka has a lot of machine learning algorithms. This is great, it is one of the large benefits of using Weka as a platform for machine learning. A down side is that it can be a little overwhelming to know which algorithms to use, and when.	Is Weka good for machine learning
7404	An (ordinary) Poisson process is a special Markov process [ref. to Stadje in this volume], in continuous time, in which the only possible jumps are to the next higher state. A Poisson process may also be viewed as a counting process that has particular, desirable, properties.	Is Poisson process a Markov process
324	In general, logistic regression performs better when the number of noise variables is less than or equal to the number of explanatory variables and random forest has a higher true and false positive rate as the number of explanatory variables increases in a dataset.	Why is logistic regression better than random forest
1287	In the AI lexicon this is known as “inference.” Inference is where capabilities learned during deep learning training are put to work. Inference can't happen without training. Makes sense. That's how we gain and use our own knowledge for the most part.	What is AI inference
4907	Hidden Markov model	Which algorithm is used for solving temporal probabilistic reasoning
4201	You should put it after the non-linearity (eg. relu layer). If you are using dropout remember to use it before.	Where should I insert batch normalization
822	If the outcomes are mutually independent, then yes the method is valid. If the outcomes are mutually exclusive, then no, the method is not valid. It's easy to see why this is the case. If you have three binary models, then the sum of the outcomes do not necessarily sum to one.	Can you split a multinomial logistic regression model into separate binary logistic regression models
5586	Advantages of distributed representations Mapping efficiency: a micro-feature-based distributed representation often allows a simple mapping (that uses few connections or weights) to solve a task. For example, suppose we wish to classify 100 different colored shapes as to whether or not they are yellow.	What are the advantages of distributed representations
7033	Categorical data clustering refers to the case where the data objects are defined over categorical attributes.  That is, there is no single ordering or inherent distance function for the categorical values, and there is no mapping from categorical to numerical values that is semantically sensible.	Can you cluster categorical data
49	A Poisson queue is a queuing model in which the number of arrivals per unit of time and the number of completions of service per unit of time, when there are customers waiting, both have the Poisson distribution. The Poisson distribution is good to use if the arrivals are all random and independent of each other.	What is Poisson distribution in queuing theory
406	Let's Start with NLP and NLG Setting aside NLU for the moment, we can draw a really simple distinction: Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language.	What is NLP NLU NLG
6012	The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative. Using the original matrix (A), NMF will give you two matrices (W and H).	How does NMF topic modeling work
2027	It is called Laplace smoothing because the smoothing proceeds from a logic of slightly correcting the observed proportions (in the case of categorical variables) in the direction of a uniform distribution among the categories (i.e., injecting a bit of equi-probability among them).	Why is additive smoothing also called Laplace smoothing
904	The geometric mean is used in finance to calculate average growth rates and is referred to as the compounded annual growth rate. Consider a stock that grows by 10% in year one, declines by 20% in year two, and then grows by 30% in year three.	When should geometric mean be used
5003	The convolutional neural networks (CNNs) have proven to be a powerful tool for discriminative learning. Recently researchers have also started to show interest in the generative aspects of CNNs in order to gain a deeper understanding of what they have learned and how to further improve them.	Is CNN generative or discriminative
2256	An estimator of a given parameter is said to be unbiased if its expected value is equal to the true value of the parameter. In other words, an estimator is unbiased if it produces parameter estimates that are on average correct.	How do you know if an estimator is unbiased
956	0:3910:15Suggested clip · 118 secondsConducting a Multiple Regression using Microsoft Excel Data YouTubeStart of suggested clipEnd of suggested clip	How do you run a regression with multiple variables
1107	The golden section search is a technique for finding the extremum (minimum or maximum) of a strictly unimodal function by successively narrowing the range of values inside which the extremum is known to exist.	What do you mean by local maxima with respect to search technique
6235	Research involves core signal processing, emphasising its use in a wide range of applications and integrated engineering. The work involves fast transforms and algorithms, multidimensional signal processing, 3-D image and video compression for storage and transmission.	What is signal processing algorithms
494	Mallick: Absolutely. A degree from Stanford in AI is worth a lot more than many other universities because you get to work with top researchers who are at the cutting edge of AI research. The choice of your major also makes a huge difference.	Is a degree in artificial intelligence worth it
5972	Identifying Good Problems for MLClear Use Case. Start with the problem, not the solution.  Know the Problem Before Focusing on the Data. Be prepared to have your assumptions challenged.  Lean on Your Team's Logs. ML requires a lot of relevant data.  Predictive Power. Your features contain predictive power.  Predictions vs. Decisions.	How do you identify machine learning problems
3068	Multi hot encoding is one of such popular encoding technique in order to successfully convert categorical variables into numerical variables.  Now, both independent variables and dependent variable became encoded and converted to numerical values from categorical values.	What is multi hot encoding
646	Disadvantages of randomised control trial study designTrials which test for efficacy may not be widely applicable. Trials which test for effectiveness are larger and more expensive.Results may not always mimic real life treatment situation (e.g. inclusion / exclusion criteria; highly controlled setting)	What are the disadvantages of a randomized controlled trial
5660	To interpret the PCA result, first of all, you must explain the scree plot. From the scree plot, you can get the eigenvalue & %cumulative of your data. The eigenvalue which >1 will be used for rotation due to sometimes, the PCs produced by PCA are not interpreted well.	How do you interpret the principal component analysis
8389	TensorFlow 2.0 is an updated version of TensorFlow that has been designed with a focus on simple execution, ease of use, and developer's productivity. TensorFlow 2.0 makes the development of machine learning applications even easier.	What is the difference between TensorFlow 1 and 2
4383	"A confusion matrix is a table that is often used to describe the performance of a classification model (or ""classifier"") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing."	Why do we need a confusion matrix
5802	2.4. 7 Cosine Similarity Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis.	Where is cosine similarity used
2123	1:0827:37Suggested clip · 115 secondsCreating a Dataset and training an Artificial Neural Network with KerasYouTubeStart of suggested clipEnd of suggested clip	How do I create a neural network dataset
8322	Data smoothing uses an algorithm to remove noise from a data set, allowing important patterns to stand out. It can be used to predict trends, such as those found in securities prices. Different data smoothing models include the random method, random walk, and the moving average.	What is smoothing in data analytics
7920	Both indices take values from zero to one. In a similarity index, a value of 1 means that the two communities you are comparing share all their species, while a value of 0 means they share none. In a dissimilarity index the interpretation is the opposite: 1 means that the communities are totally different.	How do you interpret Sorensen index of similarity
484	Max pooling is a sample-based discretization process. The objective is to down-sample an input representation (image, hidden-layer output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.	What does Max pooling do
751	Covariance is when two variables vary with each other, whereas Correlation is when the change in one variable results in the change in another variable.Differences between Covariance and Correlation.CovarianceCorrelationCovariance can vary between -∞ and +∞Correlation ranges between -1 and +17 more rows•	Should I use correlation or covariance
938	One reason you should consider when using ReLUs is, that they can produce dead neurons. That means that under certain circumstances your network can produce regions in which the network won't update, and the output is always 0.	Why is ReLu used in hidden layers
5210	A continuous sample space is based on the same principles, but it has an infinite number of items in the space.  In other words, you can't write out the space in the same way that you would write out the sample space for a die roll.	What is a continuous sample space
1223	Z score is used when: the data follows a normal distribution, when you know the standard deviation of the population and your sample size is above 30. T-Score - is used when you have a smaller sample <30 and you have an unknown population standard deviation.	What is the difference between z score and T score
7060	The AUC value lies between 0.5 to 1 where 0.5 denotes a bad classifer and 1 denotes an excellent classifier.	What is a good ROC AUC score
830	A core characteristic of non-probability sampling techniques is that samples are selected based on the subjective judgement of the researcher, rather than random selection (i.e., probabilistic methods), which is the cornerstone of probability sampling techniques.	What is non probability sampling method
516	The Two-Sample assuming Equal Variances test is used when you know (either through the question or you have analyzed the variance in the data) that the variances are the same. The Two-Sample assuming UNequal Variances test is used when either: You know the variances are not the same.	Which t test is equal or unequal variance
3917	A common application is to take the standard deviation of the last 20 periods, multiply it by 1.5 and add that amount to the average value. Whenever the value of your time series data crosses above that value then that would indicate an upward trend. Likewise a lower Bollinger band can used to identify a down trend.	How do you find trends in data
5253	Linear time invariant (LTI) filters are linear applications that transform a signal into another signal, as such that the application commutes with time shifts.	What is LTI filter
219	Two types of cross-validation can be distinguished: exhaustive and non-exhaustive cross-validation.Exhaustive cross-validation.Non-exhaustive cross-validation.k*l-fold cross-validation.k-fold cross-validation with validation and test set.	What are the different types of cross validation
5065	Time series analysis can be useful to see how a given asset, security, or economic variable changes over time. It can also be used to examine how the changes associated with the chosen data point compare to shifts in other variables over the same time period.	What is the time series analysis and how is it used
365	In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.	What is a Perceptron in machine learning
6725	If outcomes are equally likely, then the probability of an event occurring is the number in the event divided by the number in the sample space. The probability of rolling a six on a single roll of a die is 1/6 because there is only 1 way to roll a six out of 6 ways it could be rolled.	What is the P A if all outcomes in the sample space are equally likely
1555	Whole Numbers {0, 1, 2, 3, 4…..} These include the natural (counting) numbers, but they also include zero.	Which type of number is 0
5049	1. The mean of the distribution of sample means is called the Expected Value of M and is always equal to the population mean μ. 3.	What is the relationship between the expected value of the sample mean and the population mean
2649	Selectors are the names given to styles in internal and external style sheets. In this CSS Beginner Tutorial we will be concentrating on HTML selectors, which are simply the names of HTML tags and are used to change the style of a specific type of element.	What are selectors
1238	Examples of Predictive AnalyticsRetail. Probably the largest sector to use predictive analytics, retail is always looking to improve its sales position and forge better relations with customers.  Health.  Sports.  Weather.  Insurance/Risk Assessment.  Financial modeling.  Energy.  Social Media Analysis.More items•	What are examples of predictive analytics
6268	There are multiple ways to select a good starting point for the learning rate. A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. We might start with a large value like 0.1, then try exponentially lower values: 0.01, 0.001, etc.	How do you set learning rate
4065	PGMs with undirected edges are known as Markov networks (MNs) or Markov random fields (MRFs).	Are a Markov network and a Markov random field synonymous
6157	In case of mean and median, it is not necessary. However, the accuracy of the mean would be higher if the class intervals are short. Similarly the median would be more accurate if the 'median class', class interval in which median falls, is of short length.	Is it necessary to convert the class interval equal interval while calculating mean median or mode
6598	Spearman Rank Correlation: Worked Example (No Tied Ranks)The formula for the Spearman rank correlation coefficient when there are no tied ranks is:  Step 1: Find the ranks for each individual subject.  Step 2: Add a third column, d, to your data.  Step 5: Insert the values into the formula.More items•	How do you use Spearman's rank correlation coefficient
3026	So, if your significance level is 0.05, the corresponding confidence level is 95%. If the P value is less than your significance (alpha) level, the hypothesis test is statistically significant. If the confidence interval does not contain the null hypothesis value, the results are statistically significant.	What is the relationship between confidence level and P value
1431	Generative adversarial nets can be applied in many fields from generating images to predicting drugs, so don't be afraid of experimenting with them. We believe they help in building a better future for machine learning.	What are generative adversarial networks used for
1412	The V-model is an SDLC model where execution of processes happens in a sequential manner in a V-shape. It is also known as Verification and Validation model. The V-Model is an extension of the waterfall model and is based on the association of a testing phase for each corresponding development stage.	In which model testing is done earlier
298	The pdf and cdf give a complete description of the probability distribution of a random variable.  The pdf represents the relative frequency of failure times as a function of time. The cdf is a function, F(x)\,\!, of a random variable X\,\!, and is defined for a number x\,\! by: F(x)=P(X\le x)=\int_{0}^{x}f(s)ds\ \,\!	What is PDF and CDF in probability
1834	Feature Extraction using Convolution Neural Networks (CNN) and Deep Learning.  It is a process which involves the following tasks of pre-processing the image (normalization), image segmentation, extraction of key features and identification of the class.	What is feature extraction in CNN
3506	The null hypothesis (H0) for a one tailed test is that the mean is greater (or less) than or equal to µ, and the alternative hypothesis is that the mean is < (or >, respectively) µ.	What is the null hypothesis for a one tailed test
4300	Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.  The MSE loss (Y-axis) reaches its minimum value at prediction (X-axis) = 100. The range is 0 to ∞.	What is squared error loss function
4909	In research, an experimenter bias, also known as research bias, occurs when a researcher unconsciously affects results, data, or a participant in an experiment due to subjective influence. It is very important to consider experimenter bias as a possible issue in any research setting.	What is meant by experimenter bias and how can it be controlled
5867	Parametric tests assume underlying statistical distributions in the data. Nonparametric tests do not rely on any distribution.  They can thus be applied even if parametric conditions of validity are not met.	What is the difference between parametric and nonparametric statistics
1255	Use systematic sampling when there's low risk of data manipulation. Systematic sampling is the preferred method over simple random sampling when a study maintains a low risk of data manipulation.	When would you use systematic sampling
771	According to Accenture's Technology Vision 2017, AI has the potential to double annual economic growth rates by 2035.  To avoid missing out on this opportunity, policy makers and business leaders must prepare for, and work toward, a future with artificial intelligence.	Why Artificial intelligence is the future of growth
8216	Hold-out is when you split up your dataset into a 'train' and 'test' set. The training set is what the model is trained on, and the test set is used to see how well that model performs on unseen data.	What is hold out in machine learning
4310	The general algorithm is The Backpropagation algorithm is suitable for the feed forward neural network on fixed sized input-output pairs. The Backpropagation Through Time is the application of Backpropagation training algorithm which is applied to the sequence data like the time series.	What is the difference between back propagation algorithm and back propagation through time Bptt algorithm
3191	1 Answer. The difference between a classification and regression is that a classification outputs a prediction probability for class/classes and regression provides a value. We can make a neural network to output a value by simply changing the activation function in the final layer to output the values.	What is the difference in the output layer between a neural network used for classification and one used for regression
1613	AUC (Area under the ROC Curve). AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.	What does area under the ROC curve mean
2568	The short answer is: Logistic regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.)	Is logistic regression a generalized linear model
1393	"The kurtosis of any univariate normal distribution is 3. It is common to compare the kurtosis of a distribution to this value. Distributions with kurtosis less than 3 are said to be platykurtic, although this does not imply the distribution is ""flat-topped"" as is sometimes stated."	What is the kurtosis of a normal distribution
270	“Statistical significance helps quantify whether a result is likely due to chance or to some factor of interest,” says Redman.	What is the importance of statistical significance in research
5245	Why is an alpha level of . 05 commonly used? Seeing as the alpha level is the probability of making a Type I error, it seems to make sense that we make this area as tiny as possible.  The smaller the alpha level, the smaller the area where you would reject the null hypothesis.	Why is an alpha level of .05 commonly used
518	The mean is more commonly known as the average. The median is the mid-point in a distribution of values among cases, with an equal number of cases above and below the median. The mode is the value that occurs most often in the distribution.	What is the use of mean median and mode in statistics
6842	The Distributional Hypothesis is that words that occur in the same contexts tend to have similar meanings (Harris, 1954).  Although the Distributional Hypothesis originated in Linguistics, it is now receiving attention in Cognitive Science (McDonald and Ramscar, 2001).	What is distributional hypothesis
2640	March 2016	When did AlphaGo beat Lee sedol
2133	In machine learning, the polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models.	What is polynomial kernel in SVM
1811	Try to understand the basic of the data structure first like the basic topics like the Stack, queue, list, tree, graph, etc. Start practicing the algorithm and just try to solve the basic algorithm problems. Google the topics that you are learning and just watch the you tube videos.	In which order should I learn data structures
7183	Having an antibody test too early can lead to false negative results. That's because it takes a week or two after infection for your immune system to produce antibodies. The reported rate of false negatives is 20%.	Can the COVID-19 antibody test give false negatives
4813	The coefficient of a continuous predictor is the estimated change in the natural log of the odds for the reference event for each unit increase in the predictor.	How does a logistic regression model know what the coefficients are
360	"Analysis of Variance (ANOVA) is a statistical method used to test differences between two or more means. It may seem odd that the technique is called ""Analysis of Variance"" rather than ""Analysis of Means."" As you will see, the name is appropriate because inferences about means are made by analyzing variance."	Why is it called analysis of variance
1748	It is the limit of the probability of the interval (x,x+Δ] divided by the length of the interval as the length of the interval goes to 0. Remember that P(x<X≤x+Δ)=FX(x+Δ)−FX(x). =dFX(x)dx=F′X(x),if FX(x) is differentiable at x. is called the probability density function (PDF) of X.	How do you find the probability distribution function
1253	The short answer is: Logistic regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.)	Why is logistic regression considered a linear model
2882	In order to choose the support vectors, we want to maximize the margin m and that implies we reduce the magnitude or norm of the vector that's perpendicular to the hyperplanes(s) and closest to a datapoint. which implies that the lower the norm of vector w, then greater is the margin.	How are support vectors chosen
5147	Best Practices of Data CleaningSetting up a Quality Plan. RELATED BLOG.  Fill-out missing values. One of the first steps of fixing errors in your dataset is to find incomplete values and fill them out.  Removing rows with missing values.  Fixing errors in the structure.  Reducing data for proper data handling.	How does machine learning clean data
6530	"""A Bayesian network is a probabilistic graphical model which represents a set of variables and their conditional dependencies using a directed acyclic graph.""  It is also called a Bayes network, belief network, decision network, or Bayesian model."	What is Bayesian network in artificial intelligence
1080	To find the relative frequency, divide the frequency by the total number of data values. To find the cumulative relative frequency, add all of the previous relative frequencies to the relative frequency for the current row.	How do you find the relative frequency distribution
5	AI would have a low error rate compared to humans, if coded properly. They would have incredible precision, accuracy, and speed. They won't be affected by hostile environments, thus able to complete dangerous tasks, explore in space, and endure problems that would injure or kill us.	What are the positives of artificial intelligence
6136	If a confusion matrix threshold is at disposal, instead, we recommend the usage of the Matthews correlation coefficient over F1 score, and accuracy.  We decided to focus on accuracy and F1 score because they are the most common metrics used for binary classification in machine learning.	Is the Matthews correlation coefficient widely used in binary classifier quality assessment
5786	A chi-square (χ2) statistic is a test that measures how a model compares to actual observed data.  The chi-square statistic compares the size any discrepancies between the expected results and the actual results, given the size of the sample and the number of variables in the relationship.	What is Chi Square in statistics
2040	The idea of mean filtering is simply to replace each pixel value in an image with the mean (`average') value of its neighbors, including itself. This has the effect of eliminating pixel values which are unrepresentative of their surroundings. Mean filtering is usually thought of as a convolution filter.	What is mean filtering
3802	Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'.	Can we use logistic regression for multi class classification
1655	In computer vision, the bag-of-words model (BoW model) sometimes called bag-of-visual-words model can be applied to image classification, by treating image features as words. In document classification, a bag of words is a sparse vector of occurrence counts of words; that is, a sparse histogram over the vocabulary.	What is Bag of Words in image processing
1925	A hierarchical model is a model in which lower levels are sorted under a hierarchy of successively higher-level units. Data is grouped into clusters at one or more levels, and the influence of the clusters on the data points contained in them is taken account in any statistical analysis.	What is hierarchical Modelling
6297	Any good analysis of survey data from a stratified sample includes the same seven steps:Estimate a population parameter.Compute sample variance within each stratum.Compute standard error.Specify a confidence level.Find the critical value (often a z-score or a t-score).Compute margin of error.More items	How do you analyze stratified random sampling
4541	A Markov chain in which every state can be reached from every other state is called an irreducible Markov chain. If a Markov chain is not irreducible, but absorbable, the sequences of microscopic states may be trapped into some independent closed states and never escape from such undesirable states.	What is irreducible Markov chain
350	Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data.	What are unsupervised learning algorithms
4867	Elman neural network (ENN) is one of recurrent neural networks (RNNs). Comparing to traditional neural networks, ENN has additional inputs from the hidden layer, which forms a new layer–the context layer. So the standard back-propagation (BP) algorithm used in ENN is called Elman back-propagation algorithm (EBP).	What is Elman neural network
4435	In the context of CNN, a filter is a set of learnable weights which are learned using the backpropagation algorithm. You can think of each filter as storing a single template/pattern.  Filter is referred to as a set of shared weights on the input.	What is the filter in CNN
1301	Parametric tests assume underlying statistical distributions in the data. Nonparametric tests do not rely on any distribution.  They can thus be applied even if parametric conditions of validity are not met.	What is Parametric vs nonparametric
343	Two random variables X and Y are said to be bivariate normal, or jointly normal, if aX+bY has a normal distribution for all a,b∈R. In the above definition, if we let a=b=0, then aX+bY=0. We agree that the constant zero is a normal random variable with mean and variance 0.	What is bivariate normal distribution with example
8034	11 websites to find free, interesting datasetsFiveThirtyEight.  BuzzFeed News.  Kaggle.  Socrata.  Awesome-Public-Datasets on Github.  Google Public Datasets.  UCI Machine Learning Repository.  Data.gov.More items	How do I find datasets
3016	A method of computing a kind of arithmetic mean of a set of numbers in which some elements of the set carry more importance (weight) than others. Example: Grades are often computed using a weighted average. Suppose that homework counts 10%, quizzes 20%, and tests 70%.	What is weighted average with example
74	Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units.  So, we use Feature Scaling to bring all values to same magnitudes and thus, tackle this issue.	What is scaling in linear regression
537	In the case of multiclass classification, a typically used loss function is the Hard Loss Function [29, 36, 61], which counts the number of misclassifications: ℓ(f, z) = ℓH(f, z) = [f(x)≠y].	What function is used for multiclass classification
2904	In many applications including econometrics and biostatistics a fixed effects model refers to a regression model in which the group means are fixed (non-random) as opposed to a random effects model in which the group means are a random sample from a population.	What are fixed effects in regression
116	Prior: Probability distribution representing knowledge or uncertainty of a data object prior or before observing it. Posterior: Conditional probability distribution representing what parameters are likely after observing the data object. Likelihood: The probability of falling under a specific category or class.	What is prior likelihood and posterior
595	LSI Graph is a free LSI keyword tool designed to help you identify dozens of related terms to use in your copy. Visit the website and enter your target keyword to generate a long list of potential LSI keywords. When you have a long list of LSI keywords, it may be tempting to use as many as possible in your content.	How do I find my LSI keywords
661	Statistical inference involves hypothesis testing (evaluating some idea about a population using a sample) and estimation (estimating the value or potential range of values of some characteristic of the population based on that of a sample).	What does statistical inference take into account
3176	Estimating the disparity field between two stereo images is a common task in computer vision, e.g., to determine a dense depth map. Variational methods currently are among the most accurate techniques for dense disparity map reconstruction.	What is disparity estimation
1646	Convolutional layers are different in that they have a fixed number of weights governed by the choice of filter size and number of filters, but independent of the input size. The filter weights absolutely must be updated in backpropagation, since this is how they learn to recognize features of the input.	What are weights in CNN
1078	The difference between MLE/MAP and Bayesian inference MLE gives you the value which maximises the Likelihood P(D|θ). And MAP gives you the value which maximises the posterior probability P(θ|D). As both methods give you a single fixed value, they're considered as point estimators.	What is the difference between Maximum Likelihood ML and Maximum a Posteriori MAP estimation
5338	Because our sample size is large.  It is called the standard error because it refers to how much the sample mean fluctuates or is in error around the actual population mean.	Why do we use the t distribution instead of the normal distribution as our reference distribution
5680	In practical terms, deep learning is just a subset of machine learning. In fact, deep learning technically is machine learning and functions in a similar way (hence why the terms are sometimes loosely interchanged).	Is deep learning a part of machine learning
737	In the context of market research, a sampling unit is an individual person. The term sampling unit refers to a singular value within a sample database. For example, if you were conducting research using a sample of university students, a single university student would be a sampling unit.	What is a sampling a sampling unit and a sample unit
244	The infinite impulse response (IIR) filter is a recursive filter in that the output from the filter is computed by using the current and previous inputs and previous outputs. Because the filter uses previous values of the output, there is feedback of the output in the filter structure.	What is meant by IIR filter
3137	The Loss Function is one of the important components of Neural Networks. Loss is nothing but a prediction error of Neural Net. And the method to calculate the loss is called Loss Function. In simple words, the Loss is used to calculate the gradients. And gradients are used to update the weights of the Neural Net.	What is loss function in neural network
1142	A nocebo effect is said to occur when negative expectations of the patient regarding a treatment cause the treatment to have a more negative effect than it otherwise would have.	How does the nocebo effect work
4655	Definition. A convenience sample is a type of non-probability sampling method where the sample is taken from a group of people easy to contact or to reach. For example, standing at a mall or a grocery store and asking people to answer questions would be an example of a convenience sample.	How is convenience sampling done
642	Definition: Quota sampling is a sampling methodology wherein data is collected from a homogeneous group. It involves a two-step process where two variables can be used to filter information from the population. It can easily be administered and helps in quick comparison.	What is meant by quota sampling
3097	A random variable is a numerical description of the outcome of a statistical experiment. A random variable that may assume only a finite number or an infinite sequence of values is said to be discrete; one that may assume any value in some interval on the real number line is said to be continuous.	What is a random variable in statistics
957	Alpha sets the standard for how extreme the data must be before we can reject the null hypothesis. The p-value indicates how extreme the data are.  If the p-value is greater than alpha (p > . 05), then we fail to reject the null hypothesis, and we say that the result is statistically nonsignificant (n.s.).	What is the difference between an alpha level and a P value
3580	Data mining relies heavily on programming, and yet there's no conclusion which is the best language for data mining. It all depends on the dataset you deal with.  Most languages can fall somewhere on the map. R and Python are the most popular programming languages for data science, according to research from KD Nuggets.	Does data mining require coding
6096	Bootstrapping is any test or metric that uses random sampling with replacement, and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.	What is bootstrap in statistics
8554	An internal covariate shift occurs when there is a change in the input distribution to our network. When the input distribution changes, hidden layers try to learn to adapt to the new distribution. This slows down the training process.	What is an internal covariate shift
5740	The z-score is positive if the value lies above the mean, and negative if it lies below the mean. It is also known as a standard score, because it allows comparison of scores on different kinds of variables by standardizing the distribution.	Is a z score a standardized score
82	In a positively skewed distribution the outliers will be pulling the mean down the scale a great deal. The median might be slightly lower due to the outlier, but the mode will be unaffected. Thus, with a negatively skewed distribution the mean is numerically lower than the median or mode.	What are the implications of an outlier in a positively or negatively skewed distribution
7547	The most frequently used evaluation metric of survival models is the concordance index (c index, c statistic). It is a measure of rank correlation between predicted risk scores f^ and observed time points y that is closely related to Kendall's τ.	How do you evaluate a survival model
8341	The WordNet is a part of Python's Natural Language Toolkit. It is a large word database of English Nouns, Adjectives, Adverbs and Verbs. These are grouped into some set of cognitive synonyms, which are called synsets.  In the wordnet, there are some groups of words, whose meaning are same.	What is NLTK WordNet
3333	If the correlation between education and unobserved ability is positive, omitted variables bias will occur in an upward direction. Conversely, if the correlation between an explanatory variable and an unobserved relevant variable is negative, omitted variables bias will occur in a downward direction.	How do you determine the direction of omitted variable bias
533	The hypergeometric distribution is discrete. It is similar to the binomial distribution.The hypergeometric distribution is used under these conditions:Total number of items (population) is fixed.Sample size (number of trials) is a portion of the population.Probability of success changes after each trial.	What are the requirements of a hypergeometric distribution
3829	Below are the methods to convert a categorical (string) input to numerical nature:Label Encoder: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables).  Convert numeric bins to number: Let's say, bins of a continuous variable are available in the data set (shown below).	How do you convert a categorical variable to numeric
4572	Latent semantic indexing (LSI) is an indexing and retrieval method that uses a mathematical technique called singular value decomposition (SVD) to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text.	What is LSI model
519	Unsupervised machine learning helps you to finds all kind of unknown patterns in data. Clustering and Association are two types of Unsupervised learning. Four types of clustering methods are 1) Exclusive 2) Agglomerative 3) Overlapping 4) Probabilistic.	What are the types of unsupervised learning
2996	Explanation: It is depth-first search algorithm because its space requirements are linear in the size of the proof.	Which algorithm is in more similar to backward chaining algorithm
2858	SVM Scoring Function A Support Vector Machine is a binary (two class) classifier; if the output of the scoring function is negative then the input is classified as belonging to class y = -1. If the score is positive, the input is classified as belonging to class y = 1.	What is the output of SVM classifier
2844	A GAN is a generative model that is trained using two neural network models. One model is called the “generator” or “generative network” model that learns to generate new plausible samples.  After training, the generative model can then be used to create new plausible samples on demand.	Why is Gan used
7464	A pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling.	Why is the pooling layer used in CNN
2817	The basic principle behind the working of the boosting algorithm is to generate multiple weak learners and combine their predictions to form one strong rule. These weak rules are generated by applying base Machine Learning algorithms on different distributions of the data set.	How does boosting work in machine learning
1078	ROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests.  The term ROC stands for Receiver Operating Characteristic.	Why ROC curve is used
1180	numpy.random. permutation (x) Randomly permute a sequence, or return a permuted range. If x is a multi-dimensional array, it is only shuffled along its first index.	What is NP random permutation
3716	Recall the relevant definitions. Two matrices A and B are similar if there exists a nonsingular (invertible) matrix S such […] If 2 by 2 Matrices Satisfy A=AB−BA, then A2 is Zero Matrix Let A,B be complex 2×2 matrices satisfying the relation A=AB−BA. Prove that A2=O, where O is the 2×2 zero matrix.	How do you find the matrix of a similar matrix
7548	Track a Single Object Using Kalman FilterCreate vision. KalmanFilter by using configureKalmanFilter.Use predict and correct methods in a sequence to eliminate noise present in the tracking system.Use predict method by itself to estimate ball's location when it is occluded by the box.	How use Kalman filter for object tracking
1824	Definition. Inter-rater reliability is the extent to which two or more raters (or observers, coders, examiners) agree. It addresses the issue of consistency of the implementation of a rating system. Inter-rater reliability can be evaluated by using a number of different statistics.	What does Inter rater mean
8298	Logistic regression is a pretty flexible method. It can readily use as independent variables categorical variables. Most software that use Logistic regression should let you use categorical variables.  A single column in your model can handle as many categories as needed for a single categorical variable.	Can you do logistic regression on categorical variables
2012	A feature descriptor is an algorithm which takes an image and outputs feature descriptors/feature vectors. Feature descriptors encode interesting information into a series of numbers and act as a sort of numerical “fingerprint” that can be used to differentiate one feature from another.	What are feature descriptors
4805	Low Pass filtering: It is also known as the smoothing filter. It removes the high-frequency content from the image.  Median Filtering: It is also known as nonlinear filtering. It is used to eliminate salt and pepper noise.	Is median filter a low pass filter
898	In sociology and social psychology, an in-group is a social group to which a person psychologically identifies as being a member. By contrast, an out-group is a social group with which an individual does not identify.	What is ingroup and outgroup psychology
5298	Commonly used Statistical models in Predictive AnalyticsLogistic Regression: Logistic regression models the relation between a dependent and two or more independent variables (explanatory and response variables).  Time Series:  Clustering:  Decision Trees:  Neural Network:	What are the different statistical models
1339	The main use of F-distribution is to test whether two independent samples have been drawn for the normal populations with the same variance, or if two independent estimates of the population variance are homogeneous or not, since it is often desirable to compare two variances rather than two averages.	What is the use of F distribution
2477	Two disjoint events can never be independent, except in the case that one of the events is null.  Events are considered disjoint if they never occur at the same time. For example, being a freshman and being a sophomore would be considered disjoint events. Independent events are unrelated events.	Can two events be independent and disjoint
6066	A vector is an object that has both a magnitude and a direction.  Two examples of vectors are those that represent force and velocity. Both force and velocity are in a particular direction. The magnitude of the vector would indicate the strength of the force or the speed associated with the velocity.	What is vector and examples
1131	a graph marking the similarity or difference between two stimuli versus the similarity or difference in their elicited responses. See also stimulus generalization.	What is a stimulus generalization gradient
4877	Regularization Overview Regularization techniques address the prevention of ill-posed problems; problems where “the solution is highly sensitive to changes in the final data” (Wikipedia). Errors or problems with the data or method of inputting the data can lead to larger errors in the solutions.	What kind problems are solved by regularization
246	You can show that all states in the same communicating class have the same period. A class is said to be periodic if its states are periodic. Similarly, a class is said to be aperiodic if its states are aperiodic. Finally, a Markov chain is said to be aperiodic if all of its states are aperiodic.	What does it mean for a Markov chain to be aperiodic
6978	A typical perceptual map is a two-dimensional graph with a vertical axis (Y) and a horizontal axis (X). Each axis consists of a pair of opposite attributes at each end.	What are the two axis on the perceptual map
7379	0:517:39Suggested clip · 113 secondsUnderstanding the normal distribution - statistics help - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you explain normal distribution
3895	Multivariate Normality–Multiple regression assumes that the residuals are normally distributed. No Multicollinearity—Multiple regression assumes that the independent variables are not highly correlated with each other. This assumption is tested using Variance Inflation Factor (VIF) values.	What is multivariate normality assumption
7191	Adjusted R-squared value can be calculated based on value of r-squared, number of independent variables (predictors), total sample size. Every time you add a independent variable to a model, the R-squared increases, even if the independent variable is insignificant. It never declines.	How is adjusted r2 calculated
1864	Run regression analysisOn the Data tab, in the Analysis group, click the Data Analysis button.Select Regression and click OK.In the Regression dialog box, configure the following settings: Select the Input Y Range, which is your dependent variable.  Click OK and observe the regression analysis output created by Excel.	How do you conduct a regression analysis
4292	A quartile is a statistical term that describes a division of observations into four defined intervals based on the values of the data and how they compare to the entire set of observations.	What is the definition of quartile in statistics
7816	Events A and B are independent if the equation P(A∩B) = P(A) · P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together.	How do you know if an event is independent
1224	"In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input."	What does activation function do in neural network
1574	A dummy variable is a numerical variable used in regression analysis to represent subgroups of the sample in your study. In research design, a dummy variable is often used to distinguish different treatment groups.	What is the purpose of dummy variables
5741	The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side.  The normal distribution is often called the bell curve because the graph of its probability density looks like a bell.	What is a normal curve in statistics
892	When resources are limited, populations exhibit logistic growth. In logistic growth, population expansion decreases as resources become scarce, leveling off when the carrying capacity of the environment is reached, resulting in an S-shaped curve.	What do you mean by logistic growth
5547	You can use test statistics to determine whether to reject the null hypothesis. The test statistic compares your data with what is expected under the null hypothesis. The test statistic is used to calculate the p-value. A test statistic measures the degree of agreement between a sample of data and the null hypothesis.	What is the appropriate test statistic
156	When we refer to values as being “statistically equivalent” or to a “conclusion of statistical equivalence,” we mean the difference between groups is smaller than what is considered meaningful and statistically falls within the interval indicated by the equivalence bounds. In any one-sided test, for an alpha level of .	What does statistically equivalent mean
8478	Here are five ways, but it really all boils down to stretching your brain by learning new things:Become a renaissance man. Or woman.  Play the brain game Dual N-Back. Do this 20 minutes a day.  Do regular high cardio exercise.  Learn an instrument.  Buy the book Boost Your IQ by Carolyn Skitt, and play all the games.	How do you get genius level intelligence
4130	There are several methods through which you can evaluate a Logistic regression model:Goodness of Fit.Likelihood ratio test.Wald's Test.Hosmer-Lemeshov Test.ROC (AUC) curve.Confidence Intervals.Correlation factors and coefficients.Variance Inflation Factor(VIF)More items	How do you evaluate a logit model
630	Cross-entropy can be calculated using the probabilities of the events from P and Q, as follows: H(P, Q) = – sum x in X P(x) * log(Q(x))	How is cross entropy loss calculated
5840	The planning in Artificial Intelligence is about the decision making tasks performed by the robots or computer programs to achieve a specific goal. The execution of planning is about choosing a sequence of actions with a high likelihood to complete the specific task.	How artificial intelligence is used in planning
258	: being, relating to, or involving statistical methods that assign probabilities or distributions to events (such as rain tomorrow) or parameters (such as a population mean) based on experience or best guesses before experimentation and data collection and that apply Bayes' theorem to revise the probabilities and	What does it mean to be Bayesian
503	"To put simply, likelihood is ""the likelihood of θ having generated D"" and posterior is essentially ""the likelihood of θ having generated D"" further multiplied by the prior distribution of θ."	What is the difference between likelihood function and posterior probability
7182	"StepsStep 1: For each (x,y) point calculate x2 and xy.Step 2: Sum all x, y, x2 and xy, which gives us Σx, Σy, Σx2 and Σxy (Σ means ""sum up"")Step 3: Calculate Slope m:m = N Σ(xy) − Σx Σy N Σ(x2) − (Σx)2Step 4: Calculate Intercept b:b = Σy − m Σx N.Step 5: Assemble the equation of a line."	What is the least squares regression formula
6379	Gradient boosted regression and classification is an additive training tree classification method where trees are build in series (iteratively) and compared to each other based on a mathematically derived score of splits. The trees are compared based on weighted leaf scores within each tree.	How does gradient boosting work for classification
665	To format the size of data points in a scatter plot graph, right click any of the data points and select 'format data series' then select marker options and customize for larger or smaller data points.	How do you make the dots on a scatter plot bigger
218	Put more accurately, cross entropy error measures the difference between a correct probability distribution and a predicted distribution.  Binary cross entropy can be calculated as above with no problem. Or suppose you have a different ML problem with correct = (1, 0) and predicted = (0.8, 0.2).	What is the difference of binomial cross entropy and cross entropy
6931	Altman's Z-Score model is a numerical measurement that is used to predict the chances of a business going bankrupt in the next two years. The model was developed by American finance professor Edward Altman in 1968 as a measure of the financial stability of companies.	What is Altman's Z score model
7139	A regression coefficient is the same thing as the slope of the line of the regression equation. The equation for the regression coefficient that you'll find on the AP Statistics test is: B1 = b1 = Σ [ (xi – x)(yi – y) ] / Σ [ (xi – x)2].	How do you find the regression line coefficient
837	Decision Trees in Machine Learning. Decision Tree models are created using 2 steps: Induction and Pruning. Induction is where we actually build the tree i.e set all of the hierarchical decision boundaries based on our data. Because of the nature of training decision trees they can be prone to major overfitting.	How is a decision tree trained
5099	Steps to Making Your Frequency DistributionStep 1: Calculate the range of the data set.  Step 2: Divide the range by the number of groups you want and then round up.  Step 3: Use the class width to create your groups.  Step 4: Find the frequency for each group.	How do you find the frequency distribution
3871	Normality of the residuals is an assumption of running a linear model. So, if your residuals are normal, it means that your assumption is valid and model inference (confidence intervals, model predictions) should also be valid. It's that simple!	What does it mean if the residuals are normally distributed
1194	The Word2Vec Model This model was created by Google in 2013 and is a predictive deep learning based model to compute and generate high quality, distributed and continuous dense vector representations of words, which capture contextual and semantic similarity.	Can word2vec be considered deep learning
7955	XGboost is the most widely used algorithm in machine learning, whether the problem is a classification or a regression problem. It is known for its good performance as compared to all other machine learning algorithms.	Is XGBoost good for regression
42	The difference is that PCA will try to reduce dimensionality by exploring how one feature of the data is expressed in terms of the other features(linear dependecy). Feature selection instead, takes the target into consideration.  PCA is based on extracting the axes on which data shows the highest variability.	What is the difference between principal component analysis PCA and feature selection in machine learning Is PCA a means of feature selection
717	Model calibration is done by adjusting the selected parameters such as growth rates, loss rates in the model to obtain a best fit between the model calculations and the monthly average field data (Set #1) collected during first year (June 18, 2004–June 27, 2005).	How do you calibrate a model
810	Communalities – This is the proportion of each variable's variance that can be explained by the factors (e.g., the underlying latent continua). It is also noted as h2 and can be defined as the sum of squared factor loadings for the variables.  They are the reproduced variances from the factors that you have extracted.	What is the meaning of communality in factor analysis
158	The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map.	What is convolution in convolutional neural network
317	In statistics, the kth order statistic of a statistical sample is equal to its kth-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference.	What is KTH in statistics
6602	Agent, also called softbot (“software robot”), a computer program that performs various actions continuously and autonomously on behalf of an individual or an organization. For example, an agent may archive various computer files or retrieve electronic messages on a regular schedule.	What is an agent in it
6078	Sampling distributions are important for inferential statistics. In practice, one will collect sample data and, from these data, estimate parameters of the population distribution. Thus, knowledge of the sampling distribution can be very useful in making inferences about the overall population.	Why sampling distribution is important
8177	frequency–inverse document frequency	What does TF IDF stand for
305	One way of finding a point estimate ˆx=g(y) is to find a function g(Y) that minimizes the mean squared error (MSE). Here, we show that g(y)=E[X|Y=y] has the lowest MSE among all possible estimators. That is why it is called the minimum mean squared error (MMSE) estimate.	How do you reduce mean squared error
4179	In mathematics, specifically in functional analysis, each bounded linear operator on a complex Hilbert space has a corresponding Hermitian adjoint (or adjoint operator). Adjoints of operators generalize conjugate transposes of square matrices to (possibly) infinite-dimensional situations.	What is the adjoint of a linear operator
7783	The parameters of LDA model have the prior distribution, and are estimated by Bayesian method. LDA model has attracted many scholars' attention since its start, but its mathematical theory is too complex to understand quickly.	Is LDA a Bayesian
827	“Malicious use of AI,” they wrote in their 100-page report, “could threaten digital security (e.g. through criminals training machines to hack or socially engineer victims at human or superhuman levels of performance), physical security (e.g. non-state actors weaponizing consumer drones), and political security (e.g.	What is the threat of artificial intelligence
2156	Today, neural networks are used for solving many business problems such as sales forecasting, customer research, data validation, and risk management. For example, at Statsbot we apply neural networks for time-series predictions, anomaly detection in data, and natural language understanding.	What kind of problems neural nets can solve
6480	In the literature, the distinction between frames and semantic networks is actually rather blurred. However, the more structure a system has, the more likely it is to be termed a frame system rather than a semantic network.	What is more structured between frames and semantic nets
6840	Area under the ROC Curve	What does AUC of ROC mean
1172	When a study's aim is to investigate a correlational relationship, however, we recommend sampling between 500 and 1,000 people. More participants in a study will always be better, but these numbers are a useful rule of thumb for researchers seeking to find out how many participants they need to sample.	How many participants do you need for statistical significance
1438	– Rejection sampling: reject samples disagreeing with evidence. – Markov chain Monte Carlo (MCMC): sample from a stochastic process. whose stationary distribution is the true posterior.	What does rejection sampling mean in Bayesian nets
4169	The scope of regression toward the means is different from gambler's fallacy. Gambler's fallacy is predicting what is the result of the next event, but regression toward the means is talking about the trend of future events. Let's go back to the coin example, having a tail in the next toss is a specific event.	How is regression to mean different from gamblers fallacy
8657	bucketized_column. Represents discretized dense input bucketed by boundaries .	What do you use the TF Feature_column Bucketized_column function for
1119	Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.	What is CNN batch normalization layer
2486	When analysing data, such as the grades earned by 100 students, it is possible to use both descriptive and inferential statistics in your analysis. Typically, in most research conducted on groups of people, you will use both descriptive and inferential statistics to analyse your results and draw conclusions.	Can you use both descriptive and inferential statistics
364	Agglomerative clustering uses a bottom-up approach, wherein each data point starts in its own cluster. These clusters are then joined greedily, by taking the two most similar clusters together and merging them.  For each cluster, you further divide it down to two clusters until you hit the desired number of clusters.	How does agglomerative clustering work
5572	"""A discrete variable is one that can take on finitely many, or countably infinitely many values"", whereas a continuous random variable is one that is not discrete, i.e. ""can take on uncountably infinitely many values"", such as a spectrum of real numbers."	What is the difference between discrete random variable and continuous random variable
56	The general algorithm is The Backpropagation algorithm is suitable for the feed forward neural network on fixed sized input-output pairs. The Backpropagation Through Time is the application of Backpropagation training algorithm which is applied to the sequence data like the time series.	What is the difference between backpropagation algorithm and Backpropagation through time Bptt algorithm
1315	The difference between nonprobability and probability sampling is that nonprobability sampling does not involve random selection and probability sampling does.  At least with a probabilistic sample, we know the odds or probability that we have represented the population well.	What is the difference between probability sampling and non probability sampling
4720	The main aim of a sample size calculation is to determine the number of participants needed to detect a clinically relevant treatment effect. Pre-study calculation of the required sample size is warranted in the majority of quantitative studies.	Why is sample size calculation important
660	Tokenization is the process Stripe uses to collect sensitive card or bank account details, or personally identifiable information (PII), directly from your customers in a secure manner. A token representing this information is returned to your server to use. Tokens cannot be stored or used more than once.	How does Stripes tokenization work
26	Class boundaries are values halfway between the upper class limit of one class and the lower class limit of the next. Class limits specify the span of data values that fall within a class.	What are class limits and class boundaries
1314	Regression is the statistical model that you use to predict a continuous outcome on the basis of one or more continuous predictor variables. In contrast, ANOVA is the statistical model that you use to predict a continuous outcome on the basis of one or more categorical predictor variables.	When do you apply regression analysis and analysis of variance
663	Normal distributions are symmetric, unimodal, and asymptotic, and the mean, median, and mode are all equal. A normal distribution is perfectly symmetrical around its center. That is, the right side of the center is a mirror image of the left side. There is also only one mode, or peak, in a normal distribution.	What is a normal curve and its characteristics
1025	Statistical Methods for Finding the Best Regression ModelAdjusted R-squared and Predicted R-squared: Generally, you choose the models that have higher adjusted and predicted R-squared values.  P-values for the predictors: In regression, low p-values indicate terms that are statistically significant.More items•	How do you choose the best regression model in R
6541	Sampling is a statistical procedure that is concerned with the selection of the individual observation; it helps us to make statistical inferences about the population. In sampling, we assume that samples are drawn from the population and sample means and population means are equal.	What are the importance of sampling in statistics
1077	T = (X – μ) / [ σ/√(n) ]. This makes the equation identical to the one for the z-score; the only difference is you're looking up the result in the T table, not the Z-table. For sample sizes over 30, you'll get the same result.	What is the similarity between a Z score and a T score
351	Transfer learning is useful when you have insufficient data for a new domain you want handled by a neural network and there is a big pre-existing data pool that can be transferred to your problem.	What is transfer learning and how is it useful
7489	According to Daniel Little, University of Michigan-Dearborn, an endogenous variable is defined in the following way: A variable xj is said to be endogenous within the causal model M if its value is determined or influenced by one or more of the independent variables X (excluding itself).	How do you know if a variable is endogenous
5717	In non-probability sampling, the sample is selected based on non-random criteria, and not every member of the population has a chance of being included. Common non-probability sampling methods include convenience sampling, voluntary response sampling, purposive sampling, snowball sampling, and quota sampling.	What is non probability sampling with example
5937	So you are model-free. This is when you apply Q learning.  With value iteration, you learn the expected cost when you are given a state x. With q-learning, you get the expected discounted cost when you are in state x and apply action a.	Is Q learning value iteration
13	The Word error rate (WER) is a metric based on the Levenshtein distance, where the Levenshtein distance works at the character level, WER works at the word level. It was originally used for measuring the performance of speech recognition systems, but is also used in the evaluation of machine translation.	Which one is the evaluation metric used to measure performance of a machine translation system
1253	The XOr, or “exclusive or”, problem is a classic problem in ANN research. It is the problem of using a neural network to predict the outputs of XOr logic gates given two binary inputs. An XOr function should return a true value if the two inputs are not equal and a false value if they are equal.	What is XOR problem in neural network
635	Hadoop is an open source, Java based framework used for storing and processing big data. The data is stored on inexpensive commodity servers that run as clusters.  Cafarella, Hadoop uses the MapReduce programming model for faster storage and retrieval of data from its nodes.	What is Hadoop and Big Data
341	Time series data means that data is in a series of particular time periods or intervals. The data is considered in three types: Time series data: A set of observations on the values that a variable takes at different times. Cross-sectional data: Data of one or more variables, collected at the same point in time.	How do you describe time series data
4398	In its simplest form, binary search is used to quickly find a value in a sorted sequence (consider a sequence an ordinary array for now). We'll call the sought value the target value for clarity. Binary search maintains a contiguous subsequence of the starting sequence where the target value is surely located.	Where is binary search used
202	➢ To determine the critical region for a normal distribution, we use the table for the standard normal distribution. If the level of significance is α = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28.	How do you find the critical region
8600	Accuracy of a measured value refers to how close a measurement is to the correct value. The uncertainty in a measurement is an estimate of the amount by which the measurement result may differ from this value. Precision of measured values refers to how close the agreement is between repeated measurements.	What is the relationship between the accuracy and uncertainty of a measurement
437	The law of averages is sometimes known as “Gambler's Fallacy. ” It evokes the idea that an event is “due” to happen.  The law of averages says it's due to land on black! ” Of course, the wheel has no memory and its probabilities do not change according to past results.	What is wrong with the law of averages
39	The feature map on CNN is the output of one filter applied to the previous layer. A filter that is given is drawn across the entire previous layer, moved one pixel at a time. Each position results in activation of the neuron and the output are collected in the feature map.	What is feature map in convolutional neural network
107	Natural Language Processing (NLP) is the part of AI that studies how machines interact with human language.  Combined with machine learning algorithms, NLP creates systems that learn to perform tasks on their own and get better through experience.	What is NLP in machine learning
198	Regression analysis is a tool for building statistical models that characterize relationships among a dependent variable and one or more independent variables, all of which are numerical. Simple linear regression involves a single independent variable. Multiple regression involves two or more independent variables.	What characterizes a regression analysis
9	One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set.	What is a one class SVM
7264	Classification is a data mining function that assigns items in a collection to target categories or classes. The goal of classification is to accurately predict the target class for each case in the data. For example, a classification model could be used to identify loan applicants as low, medium, or high credit risks.	What is classification model in data mining
5839	TensorFlow is an open source machine learning framework for carrying out high-performance numerical computations. It provides excellent architecture support which allows easy deployment of computations across a variety of platforms ranging from desktops to clusters of servers, mobiles, and edge devices.	What is TensorFlow good for
4973	Decision tree classifier – Decision tree classifier is a systematic approach for multiclass classification. It poses a set of questions to the dataset (related to its attributes/features). The decision tree classification algorithm can be visualized on a binary tree.	Can decision trees be used for multi class classification
1360	Classification Accuracy It is the ratio of number of correct predictions to the total number of input samples. It works well only if there are equal number of samples belonging to each class. For example, consider that there are 98% samples of class A and 2% samples of class B in our training set.	How do you know if a classification model is accurate
5069	These pages demonstrate how to use Moran's I or a Mantel test to check for spatial autocorrelation in your data. Moran's I is a parametric test while Mantel's test is semi-parametric. Both will also indicate if your spatial autocorrelation is positive or negative and provide a p-value for the level of autocorrelation.	How do you test for spatial autocorrelation
7787	Top 10 Data Analytics toolsR Programming. R is the leading analytics tool in the industry and widely used for statistics and data modeling.  Tableau Public:  SAS:  Apache Spark.  Excel.  RapidMiner:KNIME.  QlikView.More items•	What are the tools used in data analysis
563	The key difference between a GRU and an LSTM is that a GRU has two gates (reset and update gates) whereas an LSTM has three gates (namely input, output and forget gates). Why do we make use of GRU when we clearly have more control on the network through the LSTM model (as we have three gates)?	What is the difference between LSTM and GRU
1915	The distribution of sample statistics is called sampling distribution.  Next a new sample of sixteen is taken, and the mean is again computed. If this process were repeated an infinite number of times, the distribution of the now infinite number of sample means would be called the sampling distribution of the mean.	How is a sampling distribution different from the distribution of a sample
3964	Advantages and Disadvantages of Decision Trees in Machine Learning. Decision Tree is used to solve both classification and regression problems. But the main drawback of Decision Tree is that it generally leads to overfitting of the data.	What are advantages and disadvantages of decision tree
1813	Three keys to managing bias when building AIChoose the right learning model for the problem. There's a reason all AI models are unique: Each problem requires a different solution and provides varying data resources.  Choose a representative training data set.  Monitor performance using real data.	How can machine learning overcome bias
533	A major difference is in its shape: the normal distribution is symmetrical, whereas the lognormal distribution is not. Because the values in a lognormal distribution are positive, they create a right-skewed curve.  A further distinction is that the values used to derive a lognormal distribution are normally distributed.	What is the difference between normal and lognormal distribution
1400	This variant of hierarchical clustering is called top-down clustering or divisive clustering . We start at the top with all documents in one cluster. The cluster is split using a flat clustering algorithm. This procedure is applied recursively until each document is in its own singleton cluster.	What is divisive clustering
319	The median absolute deviation and interquartile range are robust measures of statistical dispersion, while the standard deviation and range are not. Trimmed estimators and Winsorised estimators are general methods to make statistics more robust.	What statistics are robust
1343	In probability theory and statistics, the discrete uniform distribution is a symmetric probability distribution wherein a finite number of values are equally likely to be observed; every one of n values has equal probability 1/n.  A simple example of the discrete uniform distribution is throwing a fair die.	What is the meaning of discrete uniform distribution
2985	Calculating Standard Error of the MeanFirst, take the square of the difference between each data point and the sample mean, finding the sum of those values.Then, divide that sum by the sample size minus one, which is the variance.Finally, take the square root of the variance to get the SD.	How do you calculate standard error
5234	Content-based filtering, makes recommendations based on user preferences for product features. Collaborative filtering mimics user-to-user recommendations.  Content-based filtering can recommend a new item, but needs more data of user preference in order to incorporate best match.	Recommendation Systems What is the difference between item to item collaborative filtering and content based filtering
8386	any of various neurons located in extrastriate visual areas, particularly those in the inferotemporal cortex, that respond regardless of the location of a stimulus in the receptive field.	What is location invariance
2823	1 (Gamma-Poisson relationship) There is an interesting relationship between the gamma and Poisson distributions. If X is a gamma(α, β) random variable, where α is an integer, then for any x, P(X ≤ x) = P(Y ≥ α), (1) where Y ∼ Poisson(x/β). There are a number of important special cases of the gamma distribution.	What is the relationship between Poisson processes and the gamma distribution
450	: a group of people or things that make up a complete unit (such as a musical group, a group of actors or dancers, or a set of clothes) See the full definition for ensemble in the English Language Learners Dictionary. ensemble. noun.	What does ensemble mean
1402	In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.	What is Perceptron learning algorithm
5728	Fisher's Exact Test The null hypothesis is that these two classifications are not different. The P values in this test are computed by considering all possible tables that could give the row and column totals observed. A mathematical short cut relates these permutations to factorials; a form shown in many textbooks.	What is the null hypothesis for Fisher's exact test
452	The most common form of pooling is max pooling. Max pooling is done to in part to help over-fitting by providing an abstracted form of the representation. As well, it reduces the computational cost by reducing the number of parameters to learn and provides basic translation invariance to the internal representation.	What is the use of Max pooling
347	Nonparametric regression is a category of regression analysis in which the predictor does not take a predetermined form but is constructed according to information derived from the data. That is, no parametric form is assumed for the relationship between predictors and dependent variable.	What is the point of non parametric regression
0	"In this context, correlation only makes sense if the relationship is indeed linear. Second, the slope of the regression line is proportional to the correlation coefficient: slope = r*(SD of y)/(SD of x) Third: the square of the correlation, called ""R-squared"", measures the ""fit"" of the regression line to the data."	Is R the slope of the regression line
4950	A mean can be determined for grouped data, or data that is placed in intervals.  The sum of the products divided by the total number of values will be the value of the mean.	What is the mean for grouped data
1024	The goal of cluster analysis is to obtain groupings or clusters of similar samples. This is accomplished by using a distance measure derived from the multivariate gene expression data that characterizes the ``distance'' of the patients' expression patterns with each other.	What is the goal of cluster analysis
4336	Some techniques which are used in digital image processing include:Anisotropic diffusion.Hidden Markov models.Image editing.Image restoration.Independent component analysis.Linear filtering.Neural networks.Partial differential equations.More items	What are the algorithms used in image processing
7543	You can use a generative model. You can also use simple tricks. For example, with photograph image data, you can get big gains by randomly shifting and rotating existing images. It improves the generalization of the model to such transforms in the data if they are to be expected in new data.	How can you improve the generalization of the deep learning model
4746	An exogenous variable is a variable that is not affected by other variables in the system. For example, take a simple causal system like farming. Variables like weather, farmer skill, pests, and availability of seed are all exogenous to crop production.	Which variables are exogenous
3548	Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are. Instead, they need to be recoded into a series of variables which can then be entered into the regression model.	Can you do multiple regression with categorical variables
668	Energy is quantized in some systems, meaning that the system can have only certain energies and not a continuum of energies, unlike the classical case. This would be like having only certain speeds at which a car can travel because its kinetic energy can have only certain values.	Why is energy quantized
1150	1:113:06Suggested clip · 115 secondsStatistics - How to make a histogram - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the scale of a histogram
6430	Let's Start with NLP and NLG Setting aside NLU for the moment, we can draw a really simple distinction: Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language.	What is NLP NLU NLG
8635	A discrete distribution is a statistical distribution that shows the probabilities of discrete (countable) outcomes, such as 1, 2, 3  Overall, the concepts of discrete and continuous probability distributions and the random variables they describe are the underpinnings of probability theory and statistical analysis.	What are discrete distributions
1359	A box and whisker plot—also called a box plot—displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum.	What values are in a box plot
952	A common exercise in empirical studies is a “robustness check”, where the researcher examines how certain “core” regression coefficient estimates behave when the regression specification is modified by adding or removing regressors.	What does robustness check mean
2735	Statistically significant means a result is unlikely due to chance. The p-value is the probability of obtaining the difference we saw from a sample (or a larger one) if there really isn't a difference for all users.  Statistical significance doesn't mean practical significance.	What does it mean if a test is not statistically significant
6819	Observational error (or measurement error) is the difference between a measured value of a quantity and its true value.  Random errors are errors in measurement that lead to measurable values being inconsistent when repeated measurements of a constant attribute or quantity are taken.	What is measurement error physics
7262	In 1D CNN, kernel moves in 1 direction. Input and output data of 1D CNN is 2 dimensional. Mostly used on Time-Series data. In 2D CNN, kernel moves in 2 directions. Input and output data of 2D CNN is 3 dimensional.	What is 1d CNN
801	To apply the linear regression t-test to sample data, we require the standard error of the slope, the slope of the regression line, the degrees of freedom, the t statistic test statistic, and the P-value of the test statistic.  Therefore, the P-value is 0.0121 + 0.0121 or 0.0242. Interpret results.	What is the test statistic for linear regression
5612	"The decision rule is: Reject H0 if Z < 1.645. The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960. The complete table of critical values of Z for upper, lower and two-tailed tests can be found in the table of Z values to the right in ""Other Resources."""	How do you find the decision rule
1203	How you do this:Count the total number of items. In this chart the total is 40.Divide the count (the frequency) by the total number. For example, 1/40 = . 025 or 3/40 = . 075.	How do you find the relative frequency in a normal distribution
1139	fastText is a library for efficient learning of word representations and sentence classification. In this document we present how to use fastText in python.	What is fastText in Python
315	5 Answers. N is the population size and n is the sample size. The question asks why the population variance is the mean squared deviation from the mean rather than (N−1)/N=1−(1/N) times it.	What is the difference between n and n 1 in calculating population variance
6975	Convolutional Neural Networks	Which neural network is best for image classification
7250	The Linear Regression Equation The equation has the form Y= a + bX, where Y is the dependent variable (that's the variable that goes on the Y axis), X is the independent variable (i.e. it is plotted on the X axis), b is the slope of the line and a is the y-intercept.	How do you find the regression equation
137	The sigmoid activation function This causes vanishing gradients and poor learning for deep networks. This can occur when the weights of our networks are initialized poorly – with too-large negative and positive values.  It's called a rectified linear unit activation function, or ReLU.	Which of the following activations can cause vanishing gradient problem
4676	There are 5 values above the median (upper half), the middle value is 77 which is the third quartile. The interquartile range is 77 – 64 = 13; the interquartile range is the range of the middle 50% of the data.  When the sample size is odd, the median and quartiles are determined in the same way.	What is the difference between interquartile range and median
878	Image processing techniques use filters to enhance an image. Their main applications are to transform the contrast, brightness, resolution and noise level of an image. Contouring, image sharpening, blurring, embossing and edge detection are typical image processing functions (see Table 4.1).	What can be done with image processing
171	Standard deviation looks at how spread out a group of numbers is from the mean, by looking at the square root of the variance. The variance measures the average degree to which each point differs from the mean—the average of all data points.	Why do we use standard deviation over variance
4327	Markov chains are used in a broad variety of academic fields, ranging from biology to economics. When predicting the value of an asset, Markov chains can be used to model the randomness. The price is set by a random factor which can be determined by a Markov chain.	What is Markov chain used for
4258	A relative frequency distribution lists the data values along with the percent of all observations belonging to each group. These relative frequencies are calculated by dividing the frequencies for each group by the total number of observations.  The horizontal axis represents the range of data values.	What does a relative frequency distribution list
4713	Use Simple Random Sampling One of the most effective methods that can be used by researchers to avoid sampling bias is simple random sampling, in which samples are chosen strictly by chance. This provides equal odds for every member of the population to be chosen as a participant in the study at hand.	How do you avoid bias in sample selection
7808	In case of continous series, a mid point is computed as lower−limit+upper−limit2 and Mean Deviation is computed using following formula.FormulaN = Number of observations.f = Different values of frequency f.x = Different values of mid points for ranges.Me = Median.	How do you find the mean deviation for a continuous series
553	The coefficient of variation represents the ratio of the standard deviation to the mean, and it is a useful statistic for comparing the degree of variation from one data series to another, even if the means are drastically different from one another.	What makes coefficient of variation useful
2267	While a frequency distribution gives the exact frequency or the number of times a data point occurs, a probability distribution gives the probability of occurrence of the given data point.	What is the distinction between a probability distribution and a relative frequency distribution
941	Test error is consistently higher than training error: if this is by a small margin, and both error curves are decreasing with epochs, it should be fine. However if your test set error is not decreasing, while your training error is decreasing alot, it means you are over fitting severely.	Why is test error higher than training error
3888	Communality value is also a deciding factor to include or exclude a variable in the factor analysis. A value of above 0.5 is considered to be ideal. But in a study, it is seen that a variable with low community value (<0.5), is contributing to a well defined factor, though loading is low.	What are acceptable Communalities for factor analysis
957	Adding more training data.Reducing parameters. We have too many neurons in our hidden layers or too many layers. Let's remove some layers, or reduce the number of hidden neurons.Increase regularization. Either by increasing our. for L1/L2 weight regularization. We can also use dropout the technique.	Why and what to do when neural networks perform poorly on the training set
733	Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used to solve complex problems.  Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.	What do you mean by knowledge representation
254	Yes you can. It is also seen that using both of them together increases the accuracy.	Can l combine dropout and l2 regularization
969	Hashing is an algorithm that calculates a fixed-size bit string value from a file. A file basically contains blocks of data. Hashing transforms this data into a far shorter fixed-length value or key which represents the original string.	What is hash of a string
8383	"Stochastic Gradient Descent (SGD) Here, the term ""stochastic"" comes from the fact that the gradient based on a single training sample is a ""stochastic approximation"" of the ""true"" cost gradient."	Why is it called stochastic gradient descent
1314	Key TakeawaysThe union of two or more sets is the set that contains all the elements of the two or more sets.  The general probability addition rule for the union of two events states that P(A∪B)=P(A)+P(B)−P(A∩B) P ( A ∪ B ) = P ( A ) + P ( B ) − P ( A ∩ B ) , where A∩B A ∩ B is the intersection of the two sets.More items	How do you find a union in statistics
6929	Unsupervised learning is the training of machine using information that is neither classified nor labeled and allowing the algorithm to act on that information without guidance.	What is unsupervised learning in data analytics
2777	"In statistics, ordinal regression (also called ""ordinal classification"") is a type of regression analysis used for predicting an ordinal variable, i.e. a variable whose value exists on an arbitrary scale where only the relative ordering between different values is significant."	What is an ordinal regression analysis
4234	Key Takeaways. Standard deviation looks at how spread out a group of numbers is from the mean, by looking at the square root of the variance. The variance measures the average degree to which each point differs from the mean—the average of all data points.	Are variance and mean deviation the same
3394	YES. we calculate height of the class interval by dividing the frequency by that class width. That class which has the maximum height will be the modal class, containing the mode.	How do you find the mode when a class interval is not equal
7854	Frequency is not quantized, and has a continuous spectrum. As such, a photon can have any energy, as E=ℏω. However, quantum mechanically, if a particle is restricted by a potential, i.e. for V≠0, the energy spectrum is discrete.	Is frequency quantized
147	4:066:40Suggested clip · 67 secondsSPSS - Factorial ANOVA, Two Independent Factors - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do a factorial Anova in SPSS
2090	Deep learning itself does feature engineering whereas machine learning requires manual feature engineering. 2) Which of the following is a representation learning algorithm? Neural network converts data in such a form that it would be better to solve the desired problem. This is called representation learning.	Which of the following is representation learning algorithm
5824	When small samples are used to estimate a population mean, in cases where the population standard deviation is unknown: the t-distribution must be used to obtain the critical value. the resulting margin of error for a confidence interval estimate will tend to be fairly small.	When small samples are used to estimate a population mean in cases where the population standard deviation is unknown
6942	How to approach analysing a datasetstep 1: divide data into response and explanatory variables. The first step is to categorise the data you are working with into “response” and “explanatory” variables.  step 2: define your explanatory variables.  step 3: distinguish whether response variables are continuous.  step 4: express your hypotheses.	How do you approach a data set
1282	Neural style transfer is an optimization technique used to take two images—a content image and a style reference image (such as an artwork by a famous painter)—and blend them together so the output image looks like the content image, but “painted” in the style of the style reference image.	How do neural style transfers work
667	Similarity is a machine learning method that uses a nearest neighbor approach to identify the similarity of two or more objects to each other based on algorithmic distance functions.  As a method, similarity is different than: Neural Networks which create vector nodes to predict an outcome.	What is similarity in machine learning
8636	A training dataset is a dataset of examples used during the learning process and is used to fit the parameters (e.g., weights) of, for example, a classifier.	What is training set in machine learning
881	A t-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related in certain features. The t-test is one of many tests used for the purpose of hypothesis testing in statistics. Calculating a t-test requires three key data values.	What is the t statistic used for
970	How to train a Machine Learning model in 5 minutesModel Naming — Give Your Model a Name: Let's start with giving your model a name, describe your model and attach tags to your model.  Data Type Selection — Choose data type(Images/Text/CSV): It's time to tell us about the type of data you want to train your model.More items	How do you train a data model
1253	Feature Scaling is a technique to standardize the independent features present in the data in a fixed range.  If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.	What is scaling in machine learning
6885	The Elbow Method is more of a decision rule, while the Silhouette is a metric used for validation while clustering. Thus, it can be used in combination with the Elbow Method. Therefore, the Elbow Method and the Silhouette Method are not alternatives to each other for finding the optimal K.	Which method is not used for finding the best K in K means technique
1350	SY = the standard deviation of the Y variable. SX = the standard deviation of the X variable. X bar = the mean of the X variable. Y bar = the mean of the Y variable.	What is y bar in regression
7222	As an unsupervised classification technique, clustering identifies some inherent structures present in a set of objects based on a similarity measure. Clustering methods can be based on statistical model identification (McLachlan & Basford, 1988) or competitive learning.	What is clustering in neural networks
2166	The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively.	How is ROC AUC score calculated
4833	Construct a probability distribution: StepsStep 1: Write down the number of widgets (things, items, products or other named thing) given on one horizontal line.  Step 2: Directly underneath the first line, write the probability of the event happening.	How do you create a probability distribution
6553	The test command, when applied to a single hypothesis, produces an F- statistic with one numerator d.f. The t-statistic of which you speak is the square root of that F-statistic. Its p-value is identical to that of the F-statistic. E.g.	What is the test command in Stata
645	As already discussed, SVM aims at maximizing the geometric margin and returns the corresponding hyperplane.  Such points are called as support vectors (fig. - 1). Therefore, the optimization problem as defined above is equivalent to the problem of maximizing the margin value (not geometric/functional margin values).	What does SVM optimize
2463	A semantic network is a graphic notation for representing knowledge in patterns of interconnected nodes. Semantic networks became popular in artificial intelligence and natural language processing only because it represents knowledge or supports reasoning.	What is Semantic Network in artificial Intelligence
126	1:2611:18Suggested clip · 118 secondsMultiple Logistic Regression in SPSS - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do a multiple logistic regression in SPSS
5958	Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns. Two major goals in the study of biological systems are inference and prediction.  Many methods from statistics and machine learning (ML) may, in principle, be used for both prediction and inference.	What is the difference between machine learning and statistical inference
6306	verb (used with object), in·ter·po·lat·ed, in·ter·po·lat·ing. to introduce (something additional or extraneous) between other things or parts; interject; interpose; intercalate. Mathematics. to insert, estimate, or find an intermediate term in (a sequence).	What does it mean to interpolate something
7303	Characteristics of a Relationship. Correlations have three important characterstics. They can tell us about the direction of the relationship, the form (shape) of the relationship, and the degree (strength) of the relationship between two variables.	What are the two attributes of a correlation coefficient
2808	Line of Best Fit	Which line is obtained by method of least square
7668	Image processing is often viewed as arbitrarily manipulating an image to achieve an aesthetic standard or to support a preferred reality. However, image processing is more accurately defined as a means of translation between the human visual system and digital imaging devices.	What is the importance of image processing
1456	Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater.	What is loss in deep learning
283	Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data.  A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable.	What is meant by linear regression
8133	This is answered by examining the meaning of each term in the phrase: modal means the one that occurs most often (averages: mode), a class interval is the width of one of your groups in the frequency table or, the class interval is what you use when grouping data together, e.g., if you counted the number of pencils in	What is the mode class interval
422	It is a combination of the prior distribution and the likelihood function, which tells you what information is contained in your observed data (the “new evidence”). In other words, the posterior distribution summarizes what you know after the data has been observed.	What is prior and posterior distribution
98	In greedy algorithm approach, decisions are made from the given solution domain. As being greedy, the closest solution that seems to provide an optimum solution is chosen. Greedy algorithms try to find a localized optimum solution, which may eventually lead to globally optimized solutions.	What is greedy approach in data structure
2319	As the maps are based on the perception of the buyer they are sometimes called perceptual maps. Positioning maps show where existing products and services are positioned in the market so that the firm can decide where they would like to place (position) their product.	What is the difference between a positioning map and a perceptual map
1009	While PPCA is used to model a probability density of data, PLDA can be used to make probabilistic inferences about the class of data.	What is probabilistic linear discriminant analysis used for
2087	It is generally called POS tagging.  In simple words, we can say that POS tagging is a task of labelling each word in a sentence with its appropriate part of speech. We already know that parts of speech include nouns, verb, adverbs, adjectives, pronouns, conjunction and their sub-categories.	Why is TAG part of speech
1105	The integral operator is a linear operator because it preserves two operations; the addition between functions and the multiplication of a function	Is an integral a linear operator
8159	The idea behind bootstrap is to use the data of a sample study at hand as a “surrogate population”, for the purpose of approximating the sampling distribution of a statistic; i.e. to resample (with replacement) from the sample data at hand and create a large number of “phantom samples” known as bootstrap samples.	What is the purpose of bootstrapping
202	The derivative of sigmoid(x) is defined as sigmoid(x)*(1-sigmoid(x)).  Short answer : The derivative of the sigmoid function at any is implemented as because calculating the derivative this way is computationally effective.	Why is the derivative of sigmoid nonlinearity often implemented as x 1 x The derivative of sigmoid x is defined as sigmoid x * 1 sigmoid x
8677	Excessive dust, spider webs, and loose sensors and detectors can all be the source of false alarms.	What causes false alarms
86	Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data.	What is statistical learning model
7443	Bayesian learning uses Bayes' theorem to determine the conditional probability of a hypotheses given some evidence or observations.	What is Bayesian learning in machine learning
5104	You can think of independent and dependent variables in terms of cause and effect: an independent variable is the variable you think is the cause, while a dependent variable is the effect.	How do you define dependent and independent variables
1497	The gradient is a vector which gives us the direction in which loss function has the steepest ascent. The direction of steepest descent is the direction exactly opposite to the gradient, and that is why we are subtracting the gradient vector from the weights vector.	What is gradient in deep learning
704	A Classification report is used to measure the quality of predictions from a classification algorithm.  The report shows the main classification metrics precision, recall and f1-score on a per-class basis. The metrics are calculated by using true and false positives, true and false negatives.	What is classification report in machine learning
4920	Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. Confusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives.	What is the importance of confusion matrix
129	The Bayes theorem is a basis for discriminant analysis.	Does discriminant analysis uses Bayes theorem
1236	ANSWER. A false positive means that the results say you have the condition you were tested for, but you really don't. With a false negative, the results say you don't have a condition, but you really do.	What is meant by a false positive result
2111	Data visualization helps to tell stories by curating data into a form easier to understand, highlighting the trends and outliers. A good visualization tells a story, removing the noise from data and highlighting the useful information.	What is the use of data visualization
6938	Consider statistics as a problem-solving process and examine its four components: asking questions, collecting appropriate data, analyzing the data, and interpreting the results. This session investigates the nature of data and its potential sources of variation. Variables, bias, and random sampling are introduced.	What is the statistical problem solving process
8482	#8 Kronecker delta is a mixed tensor of rank two and it is invariant|TENSOR ANALYSIS.	Is the Kronecker delta a tensor of rank 2
5667	Step 1: Find the mean.Step 2: Subtract the mean from each score.Step 3: Square each deviation.Step 4: Add the squared deviations.Step 5: Divide the sum by the number of scores.Step 6: Take the square root of the result from Step 5.	How do you estimate the population standard deviation using the sample standard deviation
4953	A continuity correction is the name given to adding or subtracting 0.5 to a discrete x-value.  For example, suppose we would like to find the probability that a coin lands on heads less than or equal to 45 times during 100 flips.	What is continuity correction in statistics
5084	Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data).	Which machine learning is applied only for unlabeled data
760	Softmax Thus sigmoid is widely used for binary classification problems. While building a network for a multiclass problem, the output layer would have as many neurons as the number of classes in the target. For instance if you have three classes, there would be three neurons in the output layer.	Which activation function is used for binary classification
5372	To review, the Forget gate decides what is relevant to keep from prior steps. The input gate decides what information is relevant to add from the current step. The output gate determines what the next hidden state should be.	Which of the following gates in Lstm decides on keeping relevant features from the current input
8103	To predict a continuous value, you need to adjust your model (regardless whether it is Recurrent or Not) to the following conditions:Use a linear activation function for the final layer.Chose an appropriate cost function (square error loss is typically used to measure the error of predicting real values)	How can we make a neural network to predict a continuous variable rather than a classification problem
1295	Starting TensorBoardOpen up the command prompt (Windows) or terminal (Ubuntu/Mac)Go into the project home directory.If you are using Python virtuanenv, activate the virtual environment you have installed TensorFlow in.Make sure that you can see the TensorFlow library through Python.More items•	How do I open a TensorBoard file
499	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is supervised and unsupervised machine learning
4646	Elbow methodCompute clustering algorithm (e.g., k-means clustering) for different values of k.  For each k, calculate the total within-cluster sum of square (wss).Plot the curve of wss according to the number of clusters k.More items	How do you select K in K means algorithm
2265	A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes. When writing a TensorFlow program, the main object you manipulate and pass around is the tf$Tensor .	What is a tensor object
3060	Use temporal data types to store date, time, and time-interval information. Although you can store this data in character strings, it is better to use temporal types for consistency and validation. An hour, minute, and second to six decimal places (microseconds), and the time zone offset from GMT.	What is temporal data type
914	Disparate-Treatment occurs when an employer discriminates against a specific individual or employee because of that persons race, color, national origin, sex, or religion. Disparate-Impact occurs when an employer discriminates against an entire protected class through practices, procedures, or tests.	What is the difference between disparate treatment and disparate impact quizlet
2128	The formula for calculating a z-score is is z = (x-μ)/σ, where x is the raw score, μ is the population mean, and σ is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation.	How do you calculate z score normalization
460	Categorical variables represent types of data which may be divided into groups. Examples of categorical variables are race, sex, age group, and educational level. There are 8 different event categories, with weight given as numeric data.	What does categorical data mean in statistics
6854	An autoregressive integrated moving average, or ARIMA, is a statistical analysis model that uses time series data to either better understand the data set or to predict future trends.	What are some applications of the Autoregressive integrated moving average ARIMA model
809	To convert a logit ( glm output) to probability, follow these 3 steps:Take glm output coefficient (logit)compute e-function on the logit using exp() “de-logarithimize” (you'll get odds then)convert odds to probability using this formula prob = odds / (1 + odds) .	How do you convert logit to probability
887	"Dear researchers, in real world, a ""reasonable"" sample size for a logistic regression model is: at least 10 events (not 10 samples) per independent variable."	How many variables should be in a logistic regression model
5044	Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum.	How does the Adam Optimizer work
693	In context of deep learning the logits layer means the layer that feeds in to softmax (or other such normalization). The output of the softmax are the probabilities for the classification task and its input is logits layer.	What is deep learning Logits
311	In probability theory and statistics, a covariance matrix (also known as auto-covariance matrix, dispersion matrix, variance matrix, or variance–covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector.	What is the covariance of a matrix
5205	We shall look at 5 popular clustering algorithms that every data scientist should be aware of.K-means Clustering Algorithm.  Mean-Shift Clustering Algorithm.  DBSCAN – Density-Based Spatial Clustering of Applications with Noise.  EM using GMM – Expectation-Maximization (EM) Clustering using Gaussian Mixture Models (GMM)More items•	Which clustering algorithm is best
5714	In statistics, we usually say “random sample,” but in probability it's more common to say “IID.” Identically Distributed means that there are no overall trends–the distribution doesn't fluctuate and all items in the sample are taken from the same probability distribution.	What is an IID sample
4173	The first method involves the conditional distribution of a random variable X2 given X1. Therefore, a bivariate normal distribution can be simulated by drawing a random variable from the marginal normal distribution and then drawing a second random variable from the conditional normal distribution.	How do you create a bivariate normal distribution
6301	Latent semantic indexing (LSI) is a concept used by search engines to discover how a term and content work together to mean the same thing, even if they do not share keywords or synonyms.  Basically, though, you often need specific keywords on your pages to boost your website traffic.	What is latent semantic indexing and where can it be applied
174	Correlation measures linearity between X and Y. If ρ(X,Y) = 0 we say that X and Y are “uncorrelated.” If two variables are independent, then their correlation will be 0. However, like with covariance.	What is the correlation between two independent random variables
4477	Noisy data is meaningless data. • It includes any data that cannot be understood and interpreted correctly by machines, such as unstructured text. • Noisy data unnecessarily increases the amount of storage space required and can also adversely affect the results of any data mining analysis.	What is noisy data and how do you handle it
2276	A Gaussian blur effect is typically generated by convolving an image with an FIR kernel of Gaussian values.  In the first pass, a one-dimensional kernel is used to blur the image in only the horizontal or vertical direction. In the second pass, the same one-dimensional kernel is used to blur in the remaining direction.	How does a Gaussian blur work
4186	Explanation: Neural networks learn by example. They are more fault tolerant because they are always able to respond and small changes in input do not normally cause a change in output. Because of their parallel architecture, high computational rates are achieved.	What are the advantages of neural networks ability to learn by example
7704	Autocorrelation is a characteristic of data in which the correlation between the values of the same variables is based on related objects. It violates the assumption of instance independence, which underlies most of the conventional models.	Why is autocorrelation bad
1268	The easiest way to calculate the multiple correlation coefficient (i.e. the correlation between two or more variables on the one hand, and one variable on the other) is to create a multiple linear regression (predicting the values of one variable treated as dependent from the values of two or more variables treated as	How do you calculate multiple correlations in R
4916	To convert a frequency distribution to a probability distribution, divide area of the bar or interval of x by the total area of all the Bars. A simpler formula is: , N is the total Frequency and w is the interval of x. Example (From a frequency distribution table construct a probability plot).	How do you convert probability distributions
375	An autoencoder accepts input, compresses it, and then recreates the original input.  A variational autoencoder assumes that the source data has some sort of underlying probability distribution (such as Gaussian) and then attempts to find the parameters of the distribution.	What is the difference between traditional Autoencoder and variational Autoencoder
1124	The Exponential curve (also known as a J-curve) occurs when there is no limit to population size. The Logistic curve (also known as an S-curve) shows the effect of a limiting factor (in this case the carrying capacity of the environment).	What is J curve and S curve
6804	Kalman filters are used to optimally estimate the variables of interests when they can't be measured directly, but an indirect measurement is available. They are also used to find the best estimate of states by combining measurements from various sensors in the presence of noise.	Why is Kalman filter used
6752	It's a form of machine learning and therefore a branch of artificial intelligence. Depending on the complexity of the problem, reinforcement learning algorithms can keep adapting to the environment over time if necessary in order to maximize the reward in the long-term.	Is reinforcement learning AI
972	Binary classification refers to those classification tasks that have two class labels. Examples include: Email spam detection (spam or not). Churn prediction (churn or not).	What is binary classification give an example
365	In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well.	What does the word embedding mean in the context of Machine Learning
3526	Multiple regression estimates how the changes in each predictor variable relate to changes in the response variable.  What does it mean to control for the variables in the model? It means that when you look at the effect of one variable in the model, you are holding constant all of the other predictors in the model.	What does it mean to control for a variable in multiple regression
966	Linear graphs are scaled so that equal vertical distances represent the same absolute-dollar-value change. The logarithmic scale reveals percentage changes.  A change from 100 to 200, for example, is presented in the same way as a change from 1,000 to 2,000.	What's the difference between linear and logarithmic scale
285	A p-value that is calculated using an approximation to the true distribution is called an asymptotic p-value.  A p-value calculated using the true distribution is called an exact p-value. For large sample sizes, the exact and asymptotic p-values are very similar.	What is an exact p value
6340	In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the	What makes something a probability density function
2250	Dual booting has multiple decision impacting disadvantages, below are some of the notable ones.Restart required to access the other OS. Every time you need to switch between the OS, you will have to restart the PC.  Setup process is rather complicated.  Not very secure.	What are the disadvantages of dual boot
3537	Machine learning uses two types of techniques: supervised learning, which trains a model on known input and output data so that it can predict future outputs, and unsupervised learning, which finds hidden patterns or intrinsic structures in input data.	How many techniques are available in machine learning
3980	"In statistics, a unimodal probability distribution or unimodal distribution is a probability distribution which has a single peak. The term ""mode"" in this context refers to any peak of the distribution, not just to the strict definition of mode which is usual in statistics."	What is unimodal in statistics
5911	k-means is the most widely-used centroid-based clustering algorithm. Centroid-based algorithms are efficient but sensitive to initial conditions and outliers. This course focuses on k-means because it is an efficient, effective, and simple clustering algorithm. Figure 1: Example of centroid-based clustering.	What are the best clustering algorithms used in machine learning
6489	In neural networks, a hidden layer is located between the input and output of the algorithm, in which the function applies weights to the inputs and directs them through an activation function as the output. In short, the hidden layers perform nonlinear transformations of the inputs entered into the network.	What is the purpose of hidden layer in neural network
8210	Imbalanced data sets are a special case for classification problem where the class distribution is not uniform among the classes. Typically, they are composed by two classes: The majority (negative) class and the minority (positive) class.	What is an imbalanced dataset
511	OCR converts images of typed or handwritten text into machine-encoded text. The major steps in image recognition process are gather and organize data, build a predictive model and use it to recognize images.	How does image recognition AI work
5891	Answer: Recursive function is a function which calls itself again and again.  A recursive function in general has an extremely high time complexity while a non-recursive one does not. A recursive function generally has smaller code size whereas a non-recursive one is larger.	What is the difference between recursive and non recursive function
1128	In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to. A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.	What is linear classifier in machine learning
6039	If you have both a response variable and an explanatory variable, the explanatory variable is always plotted on the x-axis (the horizontal axis). The response variable is always plotted on the y-axis (the vertical axis).	How do you know which is the explanatory variable
2134	RECALL is the ratio of the number of relevant records retrieved to the total number of relevant records in the database. It is usually expressed as a percentage. ──────b•d────── Page 2 PRECISION is the ratio of the number of relevant records retrieved to the total number of irrelevant and relevant records retrieved.	What is the relationship between precision and recall
750	Multilayer Perceptron (MLP) MLP is a deep learning method. A multilayer perceptron is a neural network connecting multiple layers in a directed graph, which means that the signal path through the nodes only goes one way. Each node, apart from the input nodes, has a nonlinear activation function.	Is Multilayer Perceptron deep learning
8178	While precision refers to the percentage of your results which are relevant, recall refers to the percentage of total relevant results correctly classified by your algorithm. Unfortunately, it is not possible to maximize both these metrics at the same time, as one comes at the cost of another.	How do you interpret precision and recall
5800	The most efficient algorithm is one that takes the least amount of execution time and memory usage possible while still yielding a correct answer.	Which algorithm is more efficient
7746	The center of mass is the mean position of the mass in an object. Then there's the center of gravity, which is the point where gravity appears to act. For many objects, these two points are in exactly the same place. But they're only the same when the gravitational field is uniform across an object.	Does the Centre of mass and Centre of gravity lie at the same point explain with example
1019	Here eta (learning rate) and n_iter (number of iterations) are the hyperparameters that would have to be adjusted in order to obtain the best values for the model parameters a and b.	Which of the parameters are considered to be hyper parameters
5677	Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.  Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.	What is a neural network optimizer
3584	"The ""least squares"" method is a form of mathematical regression analysis used to determine the line of best fit for a set of data, providing a visual demonstration of the relationship between the data points."	What is the purpose of least squares regression analysis
735	This might sound confusing but here it goes: The p-value is the probability of observing data as extreme as (or more extreme than) your actual observed data, assuming that the Null hypothesis is true. A Type 1 Error is a false positive -- i.e. you falsely reject the (true) null hypothesis.	Is P value the same as Type I error
1564	Underfitting in Neural Networks Underfitting happens when the network is not able to generate accurate predictions on the training set—not to mention the validation set.	What is Underfitting in neural network
7068	A stochastic process is a family of random variables {Xθ}, where the parameter θ is drawn from an index set Θ. For example, let's say the index set is “time”.  One example of a stochastic process that evolves over time is the number of customers (X) in a checkout line.	What is a stochastic process provide an example
5142	There is no need to use LINEAR hidden layer in a neural network. Because two (or three or four) linear layers can't provide more intelligence than a single linear layer.	Why does deep learning architectures only use the non linear activation function in the hidden layers
5743	In probability theory and statistics, the Poisson distribution (/ˈpwɑːsɒn/; French pronunciation: ​[pwasɔ̃]), named after French mathematician Siméon Denis Poisson, is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these	What is the Poisson process in statistics
7639	This means that for each output that the decoder makes, it has access to the entire input sequence and can selectively pick out specific elements from that sequence to produce the output. Therefore, the mechanism allows the model to focus and place more “Attention” on the relevant parts of the input sequence as needed.	How does attention mechanism work
8031	You cannot predict the sequence of random numbers, even with a deep neural network.	Can we predict a random number generator through deep learning
1856	A normal distribution of data is one in which the majority of data points are relatively similar, meaning they occur within a small range of values with fewer outliers on the high and low ends of the data range.	What does it mean if your data is normally distributed
7012	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve.	What things follow a normal distribution
217	sample is obtained by randomly selecting an individual and then selecting every kth individual from the population after the first one.	What must be true for a sample to be considered a simple random sample
1390	In statistics a minimum-variance unbiased estimator (MVUE) or uniformly minimum-variance unbiased estimator (UMVUE) is an unbiased estimator that has lower variance than any other unbiased estimator for all possible values of the parameter.	What is minimum variance of an estimator
4656	Summation of all three networks in single table:ANNSpatial RelationshipNoPerformanceANN is considered to be less powerful than CNN, RNN.ApplicationFacial recognition and Computer vision.Main advantagesHaving fault tolerance, Ability to work with incomplete knowledge.6 more rows•	What is the benefit of CNN instead of Ann computer vision
429	Therefore, the eigenvalues of A are λ = 4,−2. (λ = −2 is a repeated root of the characteristic equation.) Once the eigenvalues of a matrix (A) have been found, we can find the eigenvectors by Gaussian Elimination. to row echelon form, and solve the resulting linear system by back substitution.	How do you find eigenvectors from eigenvalues
3974	Some practical uses of probability distributions are: To calculate confidence intervals for parameters and to calculate critical regions for hypothesis tests. For univariate data, it is often useful to determine a reasonable distributional model for the data.	What are the uses of probability distribution
5360	The two-way linear fixed effects regression (2FE) has become a default method for estimating causal effects from panel data. Many applied researchers use the 2FE estimator to adjust for unob- served unit-specific and time-specific confounders at the same time.	What is a two way fixed effects model
3626	Binomial. The binomial distribution function specifies the number of times (x) that an event occurs in n independent trials where p is the probability of the event occurring in a single trial. It is an exact probability distribution for any number of discrete trials.	What is binomial distribution function
4433	A very special kind of continuous distribution is called a Normal distribution.	Is the normal distribution discrete or continuous
7256	"A confusion matrix is a table that is often used to describe the performance of a classification model (or ""classifier"") on a set of test data for which the true values are known.  The classifier made a total of 165 predictions (e.g., 165 patients were being tested for the presence of that disease)."	What is confusion matrix in classification
6904	Differential calculus is usually taught first. I think most students find it more intuitive because they deal with rates of change in real life. Integral calculus is more abstract, and indefinite integrals are much easier to evaluate if you understand differentiation.	Which comes first differential or integral calculus
407	“Goodness of Fit” of a linear regression model attempts to get at the perhaps sur- prisingly tricky issue of how well a model fits a given set of data, or how well it will predict a future set of observations.	What is the goodness of fit in regression
3822	In word2vec, you train to find word vectors and then run similarity queries between words. In doc2vec, you tag your text and you also get tag vectors.  If two authors generally use the same words then their vector will be closer.	What is the difference between Word2Vec and Doc2Vec
7185	In summary, model parameters are estimated from data automatically and model hyperparameters are set manually and are used in processes to help estimate model parameters. Model hyperparameters are often referred to as parameters because they are the parts of the machine learning that must be set manually and tuned.	What is the difference between a model parameter and a learning algorithms Hyperparameter
184	3:0713:58Suggested clip · 114 secondsSurvival Analysis in R - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you prepare data for survival analysis in R
4595	"A moment of a probability function taken about 0, (1) (2) The raw moments (sometimes also called ""crude moments"") can be expressed as terms of the central moments (i.e., those taken about the mean ) using the inverse binomial transform."	What is raw moments in statistics
454	Precision and recall can be combined into a single score that seeks to balance both concerns, called the F-score or the F-measure. The F-Measure is a popular metric for imbalanced classification.	Which evaluation measure is appropriate to use for a classification model with imbalanced classes
1160	Batch size controls the accuracy of the estimate of the error gradient when training neural networks. Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm. There is a tension between batch size and the speed and stability of the learning process.	How does batch size affect accuracy
1032	The distribution becomes normal when you have several different forces of varying magnitude acting together. Generally, the more forces then the more normal the distribution will become. This occurs a lot in nature which is why the normal distribution is so prevalent.	Why does the normal distribution occur frequently in nature
5927	Look at current performance data to establish a baseline and benchmark for improvement.Form a hypothesis. An A/B hypothesis is an assumption on which to base the test.  Design and run the test. Create the two versions to test, A and B.  Analyze the results.  Implement the results.	How do you do the UX AB test
3446	The three different ways of feature extraction are horizontal direction, vertical direction and diagonal direction. Recognition rate percentage for vertical, horizontal and diagonal based feature extraction using feed forward back propagation neural network as classification phase are 92.69, 93.68, 97.80 respectively.	What are the feature extraction techniques in image processing
6748	Linear regression quantifies the relationship between one or more predictor variable(s) and one outcome variable.  For example, it can be used to quantify the relative impacts of age, gender, and diet (the predictor variables) on height (the outcome variable).	What is regression example
5461	This measure is represented as a value between 0.0 and 1.0, where a value of 1.0 indicates a perfect fit, and is thus a highly reliable model for future forecasts, while a value of 0.0 would indicate that the model fails to accurately model the data at all.	What value of R 2 coefficient of determination indicates your model performs worst
5272	There are two possible objectives in a discriminant analysis: finding a predictive equation for classifying new individuals or interpreting the predictive equation to better understand the relationships that may exist among the variables. In many ways, discriminant analysis parallels multiple regression analysis.	What are the objectives of discriminant analysis
6802	2. HIDDEN MARKOV MODELS. A hidden Markov model (HMM) is a statistical model that can be used to describe the evolution of observable events that depend on internal factors, which are not directly observable. We call the observed event a `symbol' and the invisible factor underlying the observation a `state'.	What is the use of hidden Markov model
8514	In machine learning, a “kernel” is usually used to refer to the kernel trick, a method of using a linear classifier to solve a non-linear problem. It entails transforming linearly inseparable data like (Fig. 3) to linearly separable ones (Fig. 2).	What are kernel methods in machine learning
4653	Handling overfittingReduce the network's capacity by removing layers or reducing the number of elements in the hidden layers.Apply regularization , which comes down to adding a cost to the loss function for large weights.Use Dropout layers, which will randomly remove certain features by setting them to zero.	How do you deal with Overfitting in deep learning
7903	A sampling distribution is a probability distribution of a statistic obtained from a larger number of samples drawn from a specific population. The sampling distribution of a given population is the distribution of frequencies of a range of different outcomes that could possibly occur for a statistic of a population.	What is a normal sample distribution
7108	From our confusion matrix, we can calculate five different metrics measuring the validity of our model.Accuracy (all correct / all) = TP + TN / TP + TN + FP + FN.Misclassification (all incorrect / all) = FP + FN / TP + TN + FP + FN.Precision (true positives / predicted positives) = TP / TP + FP.More items	How do you evaluate a confusion matrix
569	The inverse CDF technique for generating a random sample uses the fact that a continuous CDF, F, is a one-to-one mapping of the domain of the CDF into the interval (0,1). Therefore, if U is a uniform random variable on (0,1), then X = F–1(U) has the distribution F.	How do you use an inverse CDF to simulate random draws
306	They are often confused with each other. The 'K' in K-Means Clustering has nothing to do with the 'K' in KNN algorithm. k-Means Clustering is an unsupervised learning algorithm that is used for clustering whereas KNN is a supervised learning algorithm used for classification.	Is K means the same as K nearest neighbor
235	An algorithm is said to be constant time (also written as O(1) time) if the value of T(n) is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an array takes constant time as only one operation has to be performed to locate it.	What is time complexity of an algorithm explain with example
2778	In neural image captioning systems, a recurrent neural network (RNN) is typically viewed as the primary `generation' component.  This view suggests that the RNN should only be used to encode linguistic features and that only the final representation should be `merged' with the image features at a later stage.	What is the role of recurrent neural networks RNNs in an image caption generator
882	The t distribution is therefore leptokurtic. The t distribution approaches the normal distribution as the degrees of freedom increase.  Since the t distribution is leptokurtic, the percentage of the distribution within 1.96 standard deviations of the mean is less than the 95% for the normal distribution.	Does a t distribution have a normal distribution
7762	Marginal distributions are P(X = x), P(Y = y).	How do you write a marginal distribution
7481	Most statisticians agree that the minimum sample size to get any kind of meaningful result is 100. If your population is less than 100 then you really need to survey all of them.	What is the minimum sample size for statistical significance
5616	The distribution of a variable is a description of the relative numbers of times each possible outcome will occur in a number of trials.  If the measure is a Radon measure (which is usually the case), then the statistical distribution is a generalized function in the sense of a generalized function.	What is the distribution of the variable
1353	k-means clustering	Which algorithm is used in unsupervised machine learning
5013	In a dataset a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set.	What is the difference between training set and test set
69	If the mean more accurately represents the center of the distribution of your data, and your sample size is large enough, use a parametric test. If the median more accurately represents the center of the distribution of your data, use a nonparametric test even if you have a large sample size.	How do you know if something is parametric or nonparametric
3617	Taguchi loss function formulaL is the loss function.y is the value of the characteristic you are measuring (e.g. length of product)m is the value you are aiming for (in our example, perfect length for the product)k is a proportionality constant (i.e. just a number)	How is Taguchi quality loss function calculated
6300	A probability distribution is a statistical function that describes all the possible values and likelihoods that a random variable can take within a given range.  These factors include the distribution's mean (average), standard deviation, skewness, and kurtosis.	What does it mean when you have a probability distribution
7498	Logistic regression is known and used as a linear classifier. It is used to come up with a hyperplane in feature space to separate observations that belong to a class from all the other observations that do not belong to that class. The decision boundary is thus linear.13‏/03‏/2019	Is linear classifier logistic regression
5395	Univariate is a term commonly used in statistics to describe a type of data which consists of observations on only a single characteristic or attribute. A simple example of univariate data would be the salaries of workers in industry.	What is univariate data
7445	Batch processing requires separate programs for input, process and output.  In contrast, real time data processing involves a continual input, process and output of data. Data must be processed in a small time period (or near real time). Radar systems, customer services and bank ATMs are examples.	What is the difference between batch processing and real time processing
3157	In the statistical analysis of time series, autoregressive–moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the autoregression (AR) and the second for the moving average (MA).	What is ARMA model in time series
856	We can use the regression line to predict values of Y given values of X. For any given value of X, we go straight up to the line, and then move horizontally to the left to find the value of Y. The predicted value of Y is called the predicted value of Y, and is denoted Y'.	How do you find the predicted value in a regression equation
169	All of these, in different ways, involve hierarchical representation of data. Lists - linked lists are used to represent hierarchical knowledge. Trees - graphs which represent hierarchical knowledge. LISP, the main programming language of AI, was developed to process lists and trees.	What are knowledge representation techniques
842	Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate).	What correlation means statistics
3602	A time series is a sequence of numerical data points in successive order. In investing, a time series tracks the movement of the chosen data points, such as a security's price, over a specified period of time with data points recorded at regular intervals.	What is meant by time series data
3424	The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer.	How do you determine the size of a hidden layer
4500	In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.	What is the difference between a biased and an unbiased estimator
1085	Group projects, discussions, and writing are examples of active learning, because they involve doing something.	Which of the following are examples of active learning
1254	Examples of such greedy algorithms are Kruskal's algorithm and Prim's algorithm for finding minimum spanning trees, and the algorithm for finding optimum Huffman trees.	What are greedy algorithms give examples
882	Hebb proposed a mechanism to update weights between neurons in a neural network. This method of weight updation enabled neurons to learn and was named as Hebbian Learning.  Information is stored in the connections between neurons in neural networks, in the form of weights.	What is Hebbian learning in neural networks
1047	The sampling distribution of the sample mean is very useful because it can tell us the probability of getting any specific mean from a random sample.	What is the sampling distribution of the means and why is it useful
1986	In statistics, normality tests are used to determine if a data set is well-modeled by a normal distribution and to compute how likely it is for a random variable underlying the data set to be normally distributed.	Why do we need to test the normality of the distribution of data
4036	The power of Hypothesis test is the probability of rejecting null hypothesis . As stated above we may commit Type I and Type II errors while testing a hypothesis.  Accordingly 1 – b value is the measure of how well the test is working or what is technically described as the power of the test.	What do you mean by the power of a hypothesis test how can it be measured
5798	In the presence of heteroskedasticity, there are two main consequences on the least squares estimators: The least squares estimator is still a linear and unbiased estimator, but it is no longer best. That is, there is another estimator with a smaller variance.	What are the consequences of using least squares when heteroskedasticity is present
301	"""A Bayesian network is a probabilistic graphical model which represents a set of variables and their conditional dependencies using a directed acyclic graph.""  It is also called a Bayes network, belief network, decision network, or Bayesian model."	What is Bayesian network in AI
1966	Plot a symbol at the median and draw a box between the lower and upper quartiles. Calculate the interquartile range (the difference between the upper and lower quartile) and call it IQ. The line from the lower quartile to the minimum is now drawn from the lower quartile to the smallest point that is greater than L1.	How do you calculate a box plot
6156	Univariate analysis has the purpose to describe a single variable distribution in one sample. It is the first important step of every clinical trial.	What is the purpose of univariate analysis
7638	The error function and its approximations can be used to estimate results that hold with high probability or with low probability. Given random variable and constant : where A and B are certain numeric constants. If L is sufficiently far from the mean, i.e. , then: so the probability goes to 0 as .	What is the use of error function
7170	High coefficient value means the variable is playing a major role in deciding the boundary (in case of logistic). Odds ratio tells the changes produced in output variable per unit change in that particular input variable. For the relation between both, odd ratio r = exp(coefficient).	What are the relationships between the coefficient in the logistic regression and the odds ratio
29	Experience replay is the fundamental data generating mech- anism in off-policy deep reinforcement learning (Lin, 1992). It has been shown to improve sample efficiency and stability. by storing a fixed number of the most recently collected. transitions for training.	What is experience replay in deep RL
5496	When n * p and n * q are greater than 5, you can use the normal approximation to the binomial to solve a problem.	Under what conditions is it appropriate to use a normal approximation to the binomial
7315	All Answers (6) Indeed a common rule of thumb is 10 outcome events per predictor, but sometimes this rule is too conservative and can be relaxed (see Vittinghoff E, McCulloch CE. 2007. Relaxing the rule of ten events per variable in logistic and Cox regression.	How many variables should be in a regression model
4230	KNN represents a supervised classification algorithm that will give new data points accordingly to the k number or the closest data points, while k-means clustering is an unsupervised clustering algorithm that gathers and groups data into k number of clusters.	How is Knn different from K means clustering
784	Advantages of Mini-Batch Gradient Descent Stable Convergence: Another advantage is the more stable converge towards the global minimum since we calculate an average gradient over n samples that results in less noise.	What are the advantages of mini batch gradient descent over full batch gradient descent
5570	Cost Function It is a function that measures the performance of a Machine Learning model for given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number. Depending on the problem Cost Function can be formed in many different ways.	What is the cost function in machine learning
1039	The t-value measures the size of the difference relative to the variation in your sample data. Put another way, T is simply the calculated difference represented in units of standard error. The greater the magnitude of T, the greater the evidence against the null hypothesis.	What does the T score tell you
4341	Some of my suggestions to you would be:Feature Scaling and/or Normalization - Check the scales of your gre and gpa features.  Class Imbalance - Look for class imbalance in your data.  Optimize other scores - You can optimize on other metrics also such as Log Loss and F1-Score.More items	How can you improve the accuracy of a logistic regression model in python
409	a survey of high school students to measure teenage use of illegal drugs will be a biased sample because it does not include home-schooled students or dropouts. A sample is also biased if certain members are underrepresented or overrepresented relative to others in the population.	What is an example of a biased sample
732	The effect of Gaussian smoothing is to blur an image, in a similar fashion to the mean filter. The degree of smoothing is determined by the standard deviation of the Gaussian. (Larger standard deviation Gaussians, of course, require larger convolution kernels in order to be accurately represented.)	How does Gaussian smoothing work
538	Prediction by partial matching (PPM) is an adaptive statistical data compression technique based on context modeling and prediction. PPM models use a set of previous symbols in the uncompressed symbol stream to predict the next symbol in the stream.	What is ppm in data compression
6549	In probability theory and statistics, two real-valued random variables, , , are said to be uncorrelated if their covariance, , is zero. If two variables are uncorrelated, there is no linear relationship between them.	What does it mean when two variables are uncorrelated
748	If a vector is perpendicular to a basis of a plane, then it is perpendicular to that entire plane. So, the cross product of two (linearly independent) vectors, since it is orthogonal to each, is orthogonal to the plane which they span.	Does cross product give normal vector
5201	After the SBI PO selection process is over the shortlisted candidates will be posted as “Probationary Officers” in SBI partner branches and will be on probation period for two years.	What happens after selection in SBI PO
4844	Multiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. Multiple regression is an extension of linear (OLS) regression that uses just one explanatory variable.	What is multiple linear regression analysis
1014	With supervised learning, you have features and labels. The features are the descriptive attributes, and the label is what you're attempting to predict or forecast.	What are features and labels in machine learning
710	A decision tree is simply a set of cascading questions. When you get a data point (i.e. set of features and values), you use each attribute (i.e. a value of a given feature of the data point) to answer a question. The answer to each question decides the next question.	How do you explain a decision tree
6943	Some use cases for unsupervised learning — more specifically, clustering — include: Customer segmentation, or understanding different customer groups around which to build marketing or other business strategies. Genetics, for example clustering DNA patterns to analyze evolutionary biology.	What is unsupervised learning example
472	Student's t Distribution. The t distribution (aka, Student's t-distribution) is a probability distribution that is used to estimate population parameters when the sample size is small and/or when the population variance is unknown.	What does the T distribution tell us
5965	An estimate of a population parameter may be expressed in two ways: Point estimate. A point estimate of a population parameter is a single value of a statistic. For example, the sample mean x is a point estimate of the population mean μ.	What are the methods of estimation in statistics
3600	Linear regression is used to predict the continuous dependent variable using a given set of independent variables. Logistic Regression is used to predict the categorical dependent variable using a given set of independent variables.  The output for Linear Regression must be a continuous value, such as price, age, etc.	What is the difference between regression and logistic regression
7077	Student's t-test assumes that the two population(being compared) distributions are normally distributed with equal variance. Welch's t-test is designed for unequal sample distribution variance, but the assumption of sample distribution normality is maintained.	What is the difference between Welchs t test and Students t test
4063	String theory has not failed, and there has been progress since 1999. It's just that it's a pretty abstract field of research, so it's hard to describe the recent progress in an accessible and understandable way.	Has string theory been discredited
486	The law of large numbers is a theorem from probability and statistics that suggests that the average result from repeating an experiment multiple times will better approximate the true or expected underlying result. The law of large numbers explains why casinos always make money in the long run.	Why does the law of large numbers work
1192	There are four types of classification. They are Geographical classification, Chronological classification, Qualitative classification, Quantitative classification.	What are the types of classification in statistics
6940	A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data.  The mean (often called the average) is most likely the measure of central tendency that you are most familiar with, but there are others, such as the median and the mode.	Why is average called measures of central tendency and measure of location
4033	A blurring filter where you move over the image with a box filter (all the same values in the window) is an example of a linear filter. A non-linear filter is one that cannot be done with convolution or Fourier multiplication. A sliding median filter is a simple example of a non-linear filter.	What is linear filter in image processing Where do I find an example of a linear filter vs a non linear filter
98	Descriptive, prescriptive, and normative are three main areas of decision theory and each studies a different type of decision making.	What are the different theories of decision making
8483	It is able to do this by using a novel form of reinforcement learning, in which AlphaGo Zero becomes its own teacher. The system starts off with a neural network that knows nothing about the game of Go. It then plays games against itself, by combining this neural network with a powerful search algorithm.	Does AlphaGo use reinforcement learning
5696	If you want a representative sample of a particular population, you need to ensure that:The sample source includes all the target population.The selected data collection method (online, phone, paper, in person) can reach individuals that represent that target population.More items•	How do you know if a sample size is representative
5599	Here are applications of Reinforcement Learning:Robotics for industrial automation.Business strategy planning.Machine learning and data processing.It helps you to create training systems that provide custom instruction and materials according to the requirement of students.Aircraft control and robot motion control.	What are the applications of reinforcement learning
3955	The Artificial Neural Network receives the input signal from the external world in the form of a pattern and image in the form of a vector.  Each of the input is then multiplied by its corresponding weights (these weights are the details used by the artificial neural networks to solve a certain problem).	How do artificial neural networks work
7002	Formal definition: a nonlinear process is any stochastic process that is not linear.  Realizations of time-series processes are called time series but the word is also often applied to the generating processes.	What is non linear time series
5596	There are two types of coefficients that are typically be displayed in a multiple regression table: unstandardized coefficients, and standardized coefficients. To interpret an unstandardized regression coefficient: for every metric unit change in the independent variable, the dependent variable changes by X units.	How do you interpret unstandardized regression coefficients
944	To find the harmonic mean of a set of n numbers, add the reciprocals of the numbers in the set, divide the sum by n, then take the reciprocal of the result.	How do you calculate harmonic mean
8238	A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN).  MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.	What is Multilayer Perceptron in machine learning
7754	Algorithms have been criticized as a method for obscuring racial prejudices in decision-making. Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases. For example, black people are likely to receive longer sentences than white people who committed the same crime.	Why is algorithmic bias bad
7390	You can think of independent and dependent variables in terms of cause and effect: an independent variable is the variable you think is the cause, while a dependent variable is the effect. In an experiment, you manipulate the independent variable and measure the outcome in the dependent variable.	What is the dependent and independent variable in sociology
2233	Lets do this step by step:Step 1: find the mean.Step 2: fin the standard deviation of the mean (using the population SD)Step 3: find the Z score.Step 4: compare to the critical Z score. From the stated hypothesis, we know that we are dealing with a 1-tailed hypothesis test.  Step 4 : compare to the critical Z score.	How do you find the Z in hypothesis testing
2563	Generally, a large learning rate allows the model to learn faster, at the cost of arriving on a sub-optimal final set of weights. A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train.	Why is learning rate important
2425	For example, following a run of 10 heads on a flip of a fair coin (a rare, extreme event), regression to the mean states that the next run of heads will likely be less than 10, while the law of large numbers states that in the long term, this event will likely average out, and the average fraction of heads will tend to	What is an example of regression to the mean
7523	A sampling frame is a list or other device used to define a researcher's population of interest. The sampling frame defines a set of elements from which a researcher can select a sample of the target population.	What is a sampling frame in statistics
3586	The Poisson distribution is a limiting case of the binomial distribution which arises when the number of trials n increases indefinitely whilst the product μ = np, which is the expected value of the number of successes from the trials, remains constant.	Is the Poisson distribution is the limiting case of binomial distribution
6824	AREA UNDER THE ROC CURVE In general, an AUC of 0.5 suggests no discrimination (i.e., ability to diagnose patients with and without the disease or condition based on the test), 0.7 to 0.8 is considered acceptable, 0.8 to 0.9 is considered excellent, and more than 0.9 is considered outstanding.	What is a good area under ROC curve
3807	1960s	When was deep learning invented
5905	In statistics, a two-tailed test is a method in which the critical area of a distribution is two-sided and tests whether a sample is greater than or less than a certain range of values. It is used in null-hypothesis testing and testing for statistical significance.	What is a two tailed test
1013	A simple way to get true randomness is to use Random.org. The randomness comes from atmospheric noise, which for many purposes is better than the pseudo-random number algorithms typically used in computer programs. Since you're going to simulate randomness, you are going to end up using pseudorandom number generator.	How do you simulate randomness
3045	Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.	What is LSA algorithm
1017	A recursive system is a system in which current output depends on previous output(s) and input(s) but in non-recursive system current output does not depend on previous output(s).	What is recursive and nonrecursive system
154	Clustering is considered unsupervised learning, because there's no labeled target variable in clustering. Clustering algorithms try to, well, cluster data points into similar groups (or… clusters) based on different characteristics of the data.	Is clustering supervised or unsupervised How do you classify it
1494	Machine learning, a subset of artificial intelligence (AI), depends on the quality, objectivity and size of training data used to teach it.  Machine learning bias generally stems from problems introduced by the individuals who design and/or train the machine learning systems.	Is Machine Learning Biased
699	Even though it evaluates the upper tail area, the chi-square test is regarded as a two-tailed test (non-directional), since it is basically just asking if the frequencies differ.	Is Chi square a two tailed test
6838	A residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a nonlinear model is more appropriate.	How do you explain a residual plot
4190	Normally distributed data The normal distribution is symmetric, so it has no skew (the mean is equal to the median). On a Q-Q plot normally distributed data appears as roughly a straight line (although the ends of the Q-Q plot often start to deviate from the straight line).	How do you tell if a QQ plot is normally distributed
897	The coefficient of variation (CV) is the ratio of the standard deviation to the mean. The higher the coefficient of variation, the greater the level of dispersion around the mean.  The lower the value of the coefficient of variation, the more precise the estimate.	Is it better to have a higher or lower coefficient of variation
5522	Deep metric learning (DML) is an emerging field in metric learning by introducing deep neural network. Taking advantage of the nonlinear feature representation learning ability of deep learning and discrimination power of metric learning, DML is widely applied in various computer vision tasks.	What is metric learning in deep learning
3021	Bias machine learning can even be applied when interpreting valid or invalid results from an approved data model. Nearly all of the common machine learning biased data types come from our own cognitive biases. Some examples include Anchoring bias, Availability bias, Confirmation bias, and Stability bias.	What is bias in machine learning example
5650	Definitions. The median (middle quartile) marks the mid-point of the data and is shown by the line that divides the box into two parts. Half the scores are greater than or equal to this value and half are less. The middle “box” represents the middle 50% of scores for the group.	How do you analyze a box and whisker plot
6021	As you experiment with your algorithm to try and improve your model, your loss function will tell you if you're getting(or reaching) anywhere. At its core, a loss function is a measure of how good your prediction model does in terms of being able to predict the expected outcome(or value).	Why do we use loss function
6470	"However, experts expect that it won't be until 2060 until AGI has gotten good enough to pass a ""consciousness test"". In other words, we're probably looking at 40 years from now before we see an AI that could pass for a human."	How far away are we from AGI
1217	Assumptions. The assumptions of discriminant analysis are the same as those for MANOVA. The analysis is quite sensitive to outliers and the size of the smallest group must be larger than the number of predictor variables. Multivariate normality: Independent variables are normal for each level of the grouping variable.	What are the assumptions of discriminant analysis
417	A Multi Layer Perceptron (MLP) contains one or more hidden layers (apart from one input and one output layer). While a single layer perceptron can only learn linear functions, a multi layer perceptron can also learn non – linear functions.	How many hidden layers are present in multi layer Perceptron
8552	A relative frequency distribution lists the data values along with the percent of all observations belonging to each group. These relative frequencies are calculated by dividing the frequencies for each group by the total number of observations.	What is a relative frequency distribution
7651	You often measure a continuous variable on a scale. For example, when you measure height, weight, and temperature, you have continuous data. With continuous variables, you can calculate and assess the mean, median, standard deviation, or variance.	How do you measure continuous variables
2948	In robust statistics, robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Regression analysis seeks to find the relationship between one or more independent variables and a dependent variable.	What are robust regressions and robust statistics
210	ASSUMPTIONS. No formal distributional assumptions, random forests are non-parametric and can thus handle skewed and multi-modal data as well as categorical data that are ordinal or non-ordinal.	What are the assumptions in a random forest model
1344	Sample ROC plot: x-axis = (1-specificity), y-axis = sensitivity. The area under the ROC curve represents accuracy of a trial test. ROC curve AUC is determined by multiple cut-points of the trial test, it gives better estimate of accuracy.	How do you find the accuracy of a ROC curve
489	The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale. Another motivation is to measure the progress of computer vision for large scale image indexing for retrieval and annotation.	What is Ilsvrc
807	Unsupervised learning is where you only have input data (X) and no corresponding output variables. The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.	Which is unsupervised machine learning
4620	The two-way linear fixed effects regression (2FE) has become a default method for estimating causal effects from panel data. Many applied researchers use the 2FE estimator to adjust for unobserved unit-specific and time-specific confounders at the same time.	What is two way fixed effects
372	Strongly Connected Components1) Create an empty stack 'S' and do DFS traversal of a graph. In DFS traversal, after calling recursive DFS for adjacent vertices of a vertex, push the vertex to stack.  2) Reverse directions of all arcs to obtain the transpose graph.3) One by one pop a vertex from S while S is not empty. Let the popped vertex be 'v'.	How do you find strongly connected components using DFS
2895	The Bag-of-Words (BoW) framework is well-known in image classification. In the framework, there are two essential steps: 1) coding, which encodes local features by a visual vocabulary, and 2) pooling, which pools over the response of all features into image representation.	How can you use a bag of words model for image classification
215	Perceptron can have only one output , and output at perceptron can be used as inputs to several other perceptrons .  Just like perceptron, sigmoid neuron has weights for each input and an overall bias ( say b ) . BUT the output is not 0 or 1 rather sigma * ( weight * inputs + bias ) , sigma = sigmoid function .	What is the difference between perceptrons and a sigmoid neurons with regard to machine learning
1288	Overfitting is a significant practical difficulty for decision tree models and many other predictive models. Overfitting happens when the learning algorithm continues to develop hypotheses that reduce training set error at the cost of an. increased test set error.	What is overfitting in decision tree
6458	The planning problem in Artificial Intelligence is about the decision making performed by intelligent creatures like robots, humans, or computer programs when trying to achieve some goal.  In the following we discuss a number of ways of formalizing planning, and show how the planning problem can be solved automatically.	What is planning problem in AI
5154	An experimental design where one group of individuals in one treatment condition is compared to another group of individuals in a different treatment condtion is called a between-subjects experimental design.	What is between subjects experimental design
8549	Generally, we use softmax activation instead of sigmoid with the cross-entropy loss because softmax activation distributes the probability throughout each output node. But, since it is a binary classification, using sigmoid is same as softmax. For multi-class classification use sofmax with cross-entropy.	Why is it better to use Softmax function than sigmoid function
1019	Hyperparameter tuning is searching the hyperparameter space for a set of values that will optimize your model architecture. This is different from tuning your model parameters where you search your feature space that will best minimize a cost function.	What are Hyperparameters in decision tree
5070	A false positive means that the results say you have the condition you were tested for, but you really don't. With a false negative, the results say you don't have a condition, but you really do.	What's the difference between false negative and false positive
1408	Decision theory is an interdisciplinary approach to arrive at the decisions that are the most advantageous given an uncertain environment. Decision theory brings together psychology, statistics, philosophy, and mathematics to analyze the decision-making process.	What is decision theory explain its different methods
3014	The Matthews correlation coefficient (MCC) or phi coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975.	What is MCC in machine learning
5188	Here trace of the matrix is the sum of the elements of the main diagonal i.e the diagonal from the upper left to the lower right of a matrix. Normal of the matrix is the square root of the sum of all the elements.  To evaluate trace of the matrix, take sum of the main diagonal elements.	What is trace and norm of a matrix
7257	Weight is the parameter within a neural network that transforms input data within the network's hidden layers.  As an input enters the node, it gets multiplied by a weight value and the resulting output is either observed, or passed to the next layer in the neural network.	How weights are assigned in neural networks
7421	Sampled signal is applied to adaptive transversal filter equalizer. Transversal filters are actually FIR discrete time filters. The object is to adapt the coefficients to minimize the noise and intersymbol interference (depending on the type of equalizer) at the output.	Which filter is used in adaptive equalizer block diagram
8187	K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.  A cluster refers to a collection of data points aggregated together because of certain similarities. You'll define a target number k, which refers to the number of centroids you need in the dataset.	What does K represent in K means clustering
7976	In statistics, Poisson regression is a generalized linear model form of regression analysis used to model count data and contingency tables.  This model is popular because it models the Poisson heterogeneity with a gamma distribution.	What is the Poisson regression model
8196	Abstract. Autoassociative neural networks are feedforward nets trained to produce an approximation of the identity mapping between network inputs and outputs using backpropagation or similar learning procedures. The key feature of an autoassociative network is a dimensional bottleneck between input and output.	What is an Autoassociative network
239	It is used to predict values of a continuous response variable using one or more explanatory variables and can also identify the strength of the relationships between these variables (these two goals of regression are often referred to as prediction and explanation).	Why do we use OLS regression
2695	For the alternative formulation, where X is the number of trials up to and including the first success, the expected value is E(X) = 1/p = 1/0.1 = 10. For example 1 above, with p = 0.6, the mean number of failures before the first success is E(Y) = (1 − p)/p = (1 − 0.6)/0.6 = 0.67.	What is the expected value of a geometric random variable
1481	How to Analyze a PhotographStep 1: Find an Image to Analyze. Find any high quality commercial image (stock photos, advertisement images, documentary stock, etc.).  Step 2: Observe Your Image.  Step 3: Analyzing People.  Step 4: Analyzing Setting.  Step 5: Looking at Generics Vs.  Step 6: Looking at Colour.  Step 7: Looking at Viewer's Positioning.	How do you Analyse a photo
4217	Many problems in AI can be modeled as constraint satisfaction problems (CSPs). Hence the development of effective solution techniques for CSPs is an important research problem.  Each constraint is defined over some subset of the original set of variables and restricts the values these variables can simultaneously take.	Why are Constraint satisfaction Problems CSPs useful in artificial intelligence
526	There are three common types of basic production systems: the batch system, the continuous system, and the project system. In the batch system, general-purpose equipment and methods are used to produce small quantities of output (goods or services) with specifications that vary greatly from one batch to the next.3 days ago	What is production system and its types
3832	If your test statistic is positive, first find the probability that Z is greater than your test statistic (look up your test statistic on the Z-table, find its corresponding probability, and subtract it from one). Then double this result to get the p-value.	How do you convert a test statistic to p value
6955	A hierarchical model is a model in which lower levels are sorted under a hierarchy of successively higher-level units. Data is grouped into clusters at one or more levels, and the influence of the clusters on the data points contained in them is taken account in any statistical analysis.	What is a hierarchical model in statistics
3216	Inferential statistics helps to suggest explanations for a situation or phenomenon. It allows you to draw conclusions based on extrapolations, and is in that way fundamentally different from descriptive statistics that merely summarize the data that has actually been measured.	What is inferential statistics used for
6202	Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.	Why is Adam optimizer used
4489	Step 1: Divide your confidence level by 2: .95/2 = 0.475. Step 2: Look up the value you calculated in Step 1 in the z-table and find the corresponding z-value. The z-value that has an area of .475 is 1.96. Step 3: Divide the number of events by the number of trials to get the “P-hat” value: 24/160 = 0.15.	How do you find confidence intervals
7277	Unlike the previous measures of variability, the variance includes all values in the calculation by comparing each value to the mean. To calculate this statistic, you calculate a set of squared differences between the data points and the mean, sum them, and then divide by the number of observations.	How do you compare variability of two data sets
443	Because there are infinite values that X could assume, the probability of X taking on any one specific value is zero. Therefore we often speak in ranges of values (p(X>0) = . 50). The normal distribution is one example of a continuous distribution.	What is an example of a continuous probability distribution
371	Reliability refers to the extent that the instrument yields the same results over multiple trials. Validity refers to the extent that the instrument measures what it was designed to measure.  Construct validity uses statistical analyses, such as correlations, to verify the relevance of the questions.	What is validity and reliability in statistics
1172	Beta diversity measures the change in diversity of species from one environment to another. In simpler terms, it calculates the number of species that are not the same in two different environments. There are also indices which measure beta diversity on a normalized scale, usually from zero to one.	How is beta diversity measured
5662	This is a form of hypothesis testing and it is used to optimize a particular feature of a business. It is called A/B testing and refers to a way of comparing two versions of something to figure out which performs better.	Is a B testing the same as hypothesis testing
3032	♦ Error rate: proportion of errors made over the. whole set of instances. ● Resubstitution error: error rate obtained. from training data. ● Resubstitution error is usually quite.	What is error rate data mining
1018	In probability theory, convolution is a mathematical operation that allows to derive the distribution of a sum of two random variables from the distributions of the two summands.  In the case of continuous random variables, it is obtained by integrating the product of their probability density functions (pdfs).	Why is the sum of two random variables a convolution
8572	β and γ are themselves learnable parameters that are updated during network training. Batch normalization layers normalize the activations and gradients propagating through a neural network, making network training an easier optimization problem.	What is the use of learnable parameters in batch normalization layer
425	Eigenanalysis is a mathematical operation on a square, symmetric matrix. A square matrix has the same number of rows as columns. A symmetric matrix is the same if you switch rows and columns. Distance and similarity matrices are nearly always square and symmetric.	What is Eigen analysis
1056	Variance is calculated by calculating an expected return and summing a weighted average of the squared deviations from the mean return.	How do you calculate expected return variance
219	Simple random sampling is where individuals are chosen completely by chance from a population. The addition of SRS increases the chance a guilty person will be found.	What are the differences between simple random sampling and restricted random sampling
4264	To visualize the weights, you can use a tf. image_summary() op to transform a convolutional filter (or a slice of a filter) into a summary proto, write them to a log using a tf. train. SummaryWriter , and visualize the log using TensorBoard.	How do you visualize weights in TensorFlow
7510	We have a bias when, rather than being neutral, we have a preference for (or aversion to) a person or group of people. Thus, we use the term “implicit bias” to describe when we have attitudes towards people or associate stereotypes with them without our conscious knowledge.	What is the difference between bias and implicit bias
625	At the point of non-differentiability, you can assign the derivative of the function at the point “right next” to the singularity and the algorithm will work fine. For example, in ReLU we can give the derivative of the function at zero as 0.	How do deep learning algorithms use ReLU if it is not differentiable at 0
1250	A Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.	What is Markov chain in probability
3668	Posterior Distribution = Prior Distribution + Likelihood Function (“new evidence”)Interval estimates for parameters,Point estimates for parameters,Prediction inference for future data,Probabilistic evaluations for your hypothesis.	How do you find the posterior distribution
3777	Multinomial logistic regression is used to predict categorical placement in or the probability of category membership on a dependent variable based on multiple independent variables. The independent variables can be either dichotomous (i.e., binary) or continuous (i.e., interval or ratio in scale).	What is multinomial logistic regression used for
30	Hidden Markov models have been around for a pretty long time (1970s at least). It's a misnomer to call them machine learning algorithms.  It is most useful, IMO, for state sequence estimation, which is not a machine learning problem since it is for a dynamical process, not a static classification task.	Is Markov model machine learning
1833	Discrete distributions have a countable number of outcomes, which means that the potential outcomes can be put into a list. The list may be finite or infinite; the Poisson distribution is a discrete distribution whose list {0, 1, 2, } is infinite.	Which distribution is discrete distribution
3115	The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time. A large F ratio means that the variation among group means is more than you'd expect to see by chance.	What does the F value mean in Anova
5608	SVMs don't output probabilities natively, but probability calibration methods can be used to convert the output to class probabilities.  For many problems, it is convenient to get a probability P(y=1∣x), i.e. a classification that not only gives an answer, but also a degree of certainty about the answer.	Does SVM give any probabilistic output
7960	To minimize or avoid performance bias, investigators can consider cluster stratification of patients, in which all patients having an operation by one surgeon or at one hospital are placed into the same study group, as opposed to placing individual patients into groups.	How can we prevent investigators bias
2529	Before you can figure out if you have a left tailed test or right tailed test, you have to make sure you have a single tail to begin with. A tail in hypothesis testing refers to the tail at either end of a distribution curve. Area under a normal distribution curve. Two tails (both left and right) are shaded.	How do you tell if a hypothesis test is left or right tailed
668	Any kappa below 0.60 indicates inadequate agreement among the raters and little confidence should be placed in the study results.Kappa Coefficient Interpretation.Value of kLevel of agreement% of data that are reliable0.40 - 0.59Weak15 - 35%0.60 - 0.79Moderate35 - 63%0.80 - 0.90Strong64 - 81%Above 0.90Almost Perfect82 - 100%2 more rows	How do you interpret Kappa results
1351	Linear models describe a continuous response variable as a function of one or more predictor variables. They can help you understand and predict the behavior of complex systems or analyze experimental, financial, and biological data.	What is a linear model used for
6361	Logarithmic Loss, or simply Log Loss, is a classification loss function often used as an evaluation metric in Kaggle competitions.  Log Loss quantifies the accuracy of a classifier by penalising false classifications.	What is a log loss function
6993	Moments in mathematical statistics involve a basic calculation. These calculations can be used to find a probability distribution's mean, variance, and skewness. Using this formula requires us to be careful with our order of operations.	Why are moments calculated in statistics
305	In everyday use, AC voltages (and currents) are always given as RMS values because this allows a sensible comparison to be made with steady DC voltages (and currents), such as from a battery. For example, a 6V AC supply means 6V RMS with the peak voltage about 8.6V.	Why do we use rms value instead of average value
3718	Imbalanced data sets are a special case for classification problem where the class distribution is not uniform among the classes. Typically, they are composed by two classes: The majority (negative) class and the minority (positive) class.	What is imbalance data set
3337	Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data.	What is data science and where it is used
5535	Limitations of Hypothesis testing in ResearchThe tests should not be used in a mechanical fashion.  Test do not explain the reasons as to why does the difference exist, say between the means of the two samples.  Results of significance tests are based on probabilities and as such cannot be expressed with full certainty.More items	What are the limitations of hypothesis testing
949	Max Pooling Layer Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map.	What is a max pooling layer
168	The standard normal or z-distribution assumes that you know the population standard deviation. The t-distribution is based on the sample standard deviation.	What is the difference between T distribution and Z distribution
1450	Def: A uniform random permutation is one in which each of the n! possible permutations are equally likely.  Def Given a set of n elements, a k-permutation is a sequence containing k of the n elements.	What is uniform random permutation
1362	A data structure is a collection of data type 'values' which are stored and organized in such a way that it allows for efficient access and modification. When we think of data structures, there are generally four forms:  Linear: arrays, lists. Tree: binary, heaps, space partitioning etc.	What is a data structure and its types
1004	Despite having similar aims and processes, there are two main differences between them: Machine learning works out predictions and recalibrates models in real-time automatically after design. Meanwhile, predictive analytics works strictly on “cause” data and must be refreshed with “change” data.	How would you describe the main differences between predictive analytics and machine learning
2006	Classification is one of the important areas of research in the field of data mining and neural network is one of the widely used techniques for classification.  ANN has many advantages but it has some hindrances like long training time, high computational cost, and adjustment of weight.	Can Ann be used for classification problem
3640	The purpose of statistical inference is to estimate this sample to sample variation or uncertainty.	What is the purpose of statistical inference
5350	Biased but consistent , it approaches the correct value, and so it is consistent. ), these are both negatively biased but consistent estimators.	Can a biased estimator be consistent
5815	"To construct a histogram, the first step is to ""bin"" (or ""bucket"") the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval.  The bins are usually specified as consecutive, non-overlapping intervals of a variable."	What is an interval in a histogram
835	Non-linearity in neural networks simply mean that the output at any unit cannot be reproduced from a linear function of the input.	What is nonlinearity in neural networks
8412	A vector is an element of a vector space. Assuming you're talking about an abstract vector space, which has an addition and scalar multiplication satisfying a number of properties, then a vector space is what we call a set which satisfies those properties.	What is the difference between vector and vector space
3443	Random event/process/variable: an event/process that is not and cannot be made exact and, consequently, whose outcome cannot be predicted, e.g., the sum of the numbers on two rolled dice. 5. Probability: an estimate of the likelihood that a random event will produce a certain outcome.	What is a random event in probability
894	The significance level, also denoted as alpha or α, is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference.	Why do we use 0.05 level of significance
7654	Use Regression to Analyze a Wide Variety of Relationships Include continuous and categorical variables. Use polynomial terms to model curvature. Assess interaction terms to determine whether the effect of one independent variable depends on the value of another variable.	When should regression analysis be performed
157	Data binning is the process of grouping individual data values into specific bins or groups according to defined criteria. For example, census data can be binned into defined age groups.	What does binning data mean
5855	Gradient images are created from the original image (generally by convolving with a filter, one of the simplest being the Sobel filter) for this purpose. Each pixel of a gradient image measures the change in intensity of that same point in the original image, in a given direction.	What is gradient filter in image processing
3495	Stochastic Gradient Descent (SGD) addresses both of these issues by following the negative gradient of the objective after seeing only a single or a few training examples. The use of SGD In the neural network setting is motivated by the high cost of running back propagation over the full training set.	What is SGD in neural network
604	2:537:37Suggested clip · 108 secondsPrepare your dataset for machine learning (Coding TensorFlow YouTubeStart of suggested clipEnd of suggested clip	How do I create a TensorFlow dataset
1072	Structural risk minimization (SRM) is an inductive principle of use in machine learning.  The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data.	What is SRM in machine learning
4366	"Classification is a supervised machine learning approach, in which the algorithm learns from the data input provided to it — and then uses this learning to classify new observations.  The name (""Naive"") derives from the fact that the algorithm assumes that attributes are conditionally independent."	What is classification techniques in machine learning
5684	Conviction compares the probability that X appears without Y if they were dependent with the actual frequency of the appearance of X without Y.	What is conviction in association rule mining
6216	Advantages of Systematic SamplingEasy to Execute and Understand.Control and Sense of Process.Clustered Selection Eliminated.Low Risk Factor.Assumes Size of Population Can Be Determined.Need for Natural Degree of Randomness.Greater Risk of Data Manipulation.	What are the advantages of systematic random sampling
7441	In the context of conventional artificial neural networks convergence describes a progression towards a network state where the network has learned to properly respond to a set of training patterns within some margin of error.	What does neural network convergence mean
3479	“The decision of whether to use a one‐ or a two‐tailed test is important because a test statistic that falls in the region of rejection in a one‐tailed test may not do so in a two‐tailed test, even though both tests use the same probability level.”	Why do you need to specify whether well use one tailed or two tailed test
1258	Entropy, the measure of a system's thermal energy per unit temperature that is unavailable for doing useful work. Because work is obtained from ordered molecular motion, the amount of entropy is also a measure of the molecular disorder, or randomness, of a system.	What is the definition of entropy
2967	"Poisson regression is used to predict a dependent variable that consists of ""count data"" given one or more independent variables. The variable we want to predict is called the dependent variable (or sometimes the response, outcome, target or criterion variable)."	What is Poisson regression used for
5530	It should not be less than 60%. If the variance explained is 35%, it shows the data is not useful, and may need to revisit measures, and even the data collection process. If the variance explained is less than 60%, there are most likely chances of more factors showing up than the expected factors in a model.	What is a good explained variance score
4689	Histograms, 3D Bivariate. Three-dimensional histograms are used to visualize crosstabulations of values in two variables. They can be considered to be a conjunction of two simple (i.e., univariate) histograms, combined such that the frequencies of co-occurrences of values on the two analyzed variables can be examined.	Can histograms be used for bivariate analysis
7165	The subject of this chapter is image key points which we define as a distinctive point in an input image which is invariant to rotation, scale and distortion.	What are key points in image processing
2607	Major: Mathematics and Statistics. Programs called “mathematics and statistics” either combine the study of math and statistics or focus on a specialization that uses both math and statistics. Topics of study include calculus, algebra, differential equations, probability theory, and computing.	What is Mathematics and Statistics major
7560	To recap, we have covered some of the the most important machine learning algorithms for data science: 5 supervised learning techniques- Linear Regression, Logistic Regression, CART, Naïve Bayes, KNN. 3 unsupervised learning techniques- Apriori, K-means, PCA.	Which machine learning algorithm is best
4335	Structural information theory (SIT) is a theory about human perception and in particular about visual perceptual organization, which is the neuro-cognitive process that enables us to perceive scenes as structured wholes consisting of objects arranged in space.	What is structural information theory a theory about visual perception
4110	You tend to take logs of the data when there is a problem with the residuals. For example, if you plot the residuals against a particular covariate and observe an increasing/decreasing pattern (a funnel shape), then a transformation may be appropriate.	In linear regression when is it appropriate to use the log of an independent variable instead of the actual values
714	"This process occurs over and over as the weights are continually tweaked. The set of data which enables the training is called the ""training set."" During the training of a network the same set of data is processed many times as the connection weights are ever refined."	What is training set in neural network
5483	The two major types of bias are:Selection Bias.Information Bias.	What are 2 types of bias
4415	GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.	What is GloVe word Embeddings
843	So that we only have to have one area table, rather than an infinite number of area tables. Of course, technology can find area under any normal curve and so tables of values are a bit archaic.	Why do we Standardise normal distribution
1570	1 Answer. In word2vec, you train to find word vectors and then run similarity queries between words. In doc2vec, you tag your text and you also get tag vectors.  If two authors generally use the same words then their vector will be closer.	What is the difference between word2vec and doc2vec
3789	demean() is intended to create group- and de-meaned variables for panel regression models (fixed effects models), or for complex random-effect-within-between models (see Bell et al. 2015, 2018 ), where group-effects (random effects) and fixed effects correlate (see Bafumi and Gelman 2006 ).	What is the purpose of demeaning variables in a fixed model regression
3596	Definition. Stimulus generalization is the tendency of a new stimulus to evoke responses or behaviors similar to those elicited by another stimulus. For example, Ivan Pavlov conditioned dogs to salivate using the sound of a bell and food powder.	What is an example of stimulus generalization
3560	Bayesian search theory is the application of Bayesian statistics to the search for lost objects. It has been used several times to find lost sea vessels, for example the USS Scorpion, and has played a key role in the recovery of the flight recorders in the Air France Flight 447 disaster of 2009.	What is Bayesian search
6459	A confidence interval, in statistics, refers to the probability that a population parameter will fall between a set of values for a certain proportion of times. Confidence intervals measure the degree of uncertainty or certainty in a sampling method.	What is confidence interval in econometrics
5925	Recursive and Nonrecursive Discrete-Time Systems This is a recursive system which means the output at time n depends on any number of a past output values. So, a recursive system has feed back output of the system into the input.	What is recursive system
434	Disparate impact lawsuits claim that an employer's facially neutral practice had a discriminatory effect. Disparate impact is a way to prove employment discrimination based on the effect of an employment policy or practice rather than the intent behind it.	What is disparate impact discrimination and how is it proved
3811	"Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process – call it – with unobservable (""hidden"") states. HMM assumes that there is another process whose behavior ""depends"" on . The goal is to learn about by observing ."	What is hidden Markov model in machine learning
3362	As explained above, the shape of the t-distribution is affected by sample size.  As the sample size increases, so do degrees of freedom. When degrees of freedom are infinite, the t-distribution is identical to the normal distribution. As sample size increases, the sample more closely approximates the population.	How does sample size affect T distribution
6610	A p-value less than 0.05 (typically ≤ 0.05) is statistically significant. It indicates strong evidence against the null hypothesis, as there is less than a 5% probability the null is correct (and the results are random). Therefore, we reject the null hypothesis, and accept the alternative hypothesis.	Why is the standard p value 0 05
6800	In data mining, anomaly detection is referred to the identification of items or events that do not conform to an expected pattern or to other items present in a dataset.  Machine learning algorithms have the ability to learn from data and make predictions based on that data.	What is anomaly detection in machine learning
4717	Draw a boxplot of your data. If your data comes from a normal distribution, the box will be symmetrical with the mean and median in the center. If the data meets the assumption of normality, there should also be few outliers. A normal probability plot showing data that's approximately normal.	How do you test the assumption of normality
3278	A continuity correction factor is used when you use a continuous probability distribution to approximate a discrete probability distribution. For example, when you want to use the normal to approximate a binomial.  p = probability of an event (e.g. 60%), q = probability the event doesn't happen (100% – p).	Whats an example of when to use a continuity correction factor
1257	In the context of a local search, we call local beam search a specific algorithm that begins selecting randomly generated states and then, for each level of the search tree, it always considers. new states among all the possible successors of the current ones, until it reaches a goal.	What is local beam search
3620	In a hypothesis test, we:Evaluate the null hypothesis, typically denoted with H0.  Always write the alternative hypothesis, typically denoted with Ha or H1, using less than, greater than, or not equals symbols, i.e., (≠, >, or <).More items	How do you write a null and alternative hypothesis
5592	The Dissimilarity matrix is a matrix that expresses the similarity pair to pair between two sets. It's square and symmetric. The diagonal members are defined as zero, meaning that zero is the measure of dissimilarity between an element and itself.	What is a dissimilarity matrix
1801	The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss.  It is often called the bell curve, because the graph of its probability density looks like a bell. Many values follow a normal distribution.	Why is it called a Gaussian distribution
48	Bias is a disproportionate weight in favor of or against an idea or thing, usually in a way that is closed-minded, prejudicial, or unfair. Biases can be innate or learned. People may develop biases for or against an individual, a group, or a belief. In science and engineering, a bias is a systematic error.	What does bias mean
134	Dimensionality reduction refers to techniques for reducing the number of input variables in training data. When dealing with high dimensional data, it is often useful to reduce the dimensionality by projecting the data to a lower dimensional subspace which captures the “essence” of the data.	Why do we need dimensionality reduction techniques
741	An artificial neuron (also referred to as a perceptron) is a mathematical function. It takes one or more inputs that are multiplied by values called “weights” and added together. This value is then passed to a non-linear function, known as an activation function, to become the neuron's output.	What is neuron in deep learning
639	A Markov process is a random process in which the future is independent of the past, given the present. Thus, Markov processes are the natural stochastic analogs of the deterministic processes described by differential and difference equations. They form one of the most important classes of random processes.	What do you mean by Markov process
1079	Vector autoregression (VAR) is a statistical model used to capture the relationship between multiple quantities as they change over time.  This equation includes the variable's lagged (past) values, the lagged values of the other variables in the model, and an error term.	What is vector autoregressive model in time series
7874	Normal distribution describes continuous data which have a symmetric distribution, with a characteristic 'bell' shape. Binomial distribution describes the distribution of binary data from a finite sample. Thus it gives the probability of getting r events out of n trials.	What is the difference between binomial and normal distribution
848	The range can only tell you basic details about the spread of a set of data. By giving the difference between the lowest and highest scores of a set of data it gives a rough idea of how widely spread out the most extreme observations are, but gives no information as to where any of the other data points lie.	What does the range in statistics tell us
6000	The term normal score is used with two different meanings in statistics.  A given data point is assigned a value which is either exactly, or an approximation, to the expectation of the order statistic of the same rank in a sample of standard normal random variables of the same size as the observed data set.	What is a normal score in statistics
8247	Lift is a measure of the effectiveness of a predictive model calculated as the ratio between the results obtained with and without the predictive model. Cumulative gains and lift charts are visual aids for measuring model performance.	What is lift predictive modeling
8127	Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data. Due to this value of denominator in the formula for variance in case of sample data is 'n-1', and it is 'n' for population data.	Is sample variance the same as variance
976	PIT is a state of the art mutation testing system, providing gold standard test coverage for Java and the jvm. It's fast, scalable and integrates with modern test and build tooling.	What is Pit mutation testing
488	Unsupervised learning uses the entire dataset for the supervised training process. In contrast, in self-supervised learning, you withhold part of the data in some form, and you try to predict the rest.  In contrast, in self-supervised learning, you withhold part of the data in some form, and you try to predict the rest.	What is the difference between self supervised and unsupervised learning
2685	Regression Techniques Regression algorithms are machine learning techniques for predicting continuous numerical values.	Which of the algorithm is used to predict continuous values
3360	Multi-agent reinforcement learning is the study of numerous artificial intelligence agents cohabitating in an environment, often collaborating toward some end goal. When focusing on collaboration, it derives inspiration from other social structures in the animal kingdom. It also draws heavily on game theory.	What is multi agent reinforcement learning
1155	The value of the z-score tells you how many standard deviations you are away from the mean.  A positive z-score indicates the raw score is higher than the mean average. For example, if a z-score is equal to +1, it is 1 standard deviation above the mean. A negative z-score reveals the raw score is below the mean average.	What do z scores indicate
4408	(1 p)xp = (1 p)a+1p + ··· + (1 p)bp = (1 p)a+1p (1 p)b+1p 1 (1 p) = (1 p)a+1 (1 p)b+1 We can take a = 0 to find the distribution function for a geometric random variable. The initial d indicates density and p indicates the probability from the distribution function.	How do you find the distribution function of a random variable
4253	Acceptance sampling is a quality control procedure, which uses the inspection of small samples instead of 100 percent inspection in making the decision to accept or reject much larger quantities, called a lot.	What is the relationship between inspection and acceptance sampling
6951	In statistics, the likelihood function (often simply called the likelihood) measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters.  But in both frequentist and Bayesian statistics, the likelihood function plays a fundamental role.	What is Bayesian chance
6069	The main difference between inductive and deductive reasoning is that inductive reasoning aims at developing a theory while deductive reasoning aims at testing an existing theory. Inductive reasoning moves from specific observations to broad generalizations, and deductive reasoning the other way around.	What is the difference between inductive reasoning
6642	Eigenfunctions are those functions that satisfy eigenvalue equations. In quantum physics we say that because an eigenvalue equation is linear, then all linear combinations of its solutions are also solutions.	Is a linear combination of eigenfunctions an eigenfunction
4566	robust is a programmer's command that computes a robust variance estimator based on a varlist of equation-level scores and a covariance matrix.	What does robust do in Stata
6230	From the perspective of algorithm steps, the difference is when computing the center of each cluster, K-center method will take the average(mean) of samples in each cluster. While K-median method choose to take the median of samples instead of mean.  K-medians is robust to outliers and results in compact clusters.	What is key difference two kmeans variance K center clustering and K medians clustering
1567	Moment generating functions are a way to find moments like the mean(μ) and the variance(σ2). They are an alternative way to represent a probability distribution with a simple one-variable function.	What are moment generating functions used for
1662	Definition: The range of a random variable is the smallest interval that contains all the values of the random variable. A variation of the last definition says that the range of a random variable is the smallest interval that contains all the values of the random variable with probability 1.	What is the range of a random variable
6890	A Fourier transform is holographic because all points in the input affect a single point in the output and vice versa. The neural nets in organic brains have been considered holographic because skills and memories seem to be spread out over many different neurons.	How are neural nets related to Fourier transforms
424	1:2013:53Suggested clip · 97 secondsThe Binomial Distribution: Mathematically Deriving the Mean and YouTubeStart of suggested clipEnd of suggested clip	How do you prove the mean of a binomial distribution
7578	Squaring the residuals, averaging the squares, and taking the square root gives us the r.m.s error. You then use the r.m.s. error as a measure of the spread of the y values about the predicted y value.	How do you find the root mean square error
943	The p-value is calculated using the sampling distribution of the test statistic under the null hypothesis, the sample data, and the type of test being done (lower-tailed test, upper-tailed test, or two-sided test).  a lower-tailed test is specified by: p-value = P(TS ts | H 0 is true) = cdf(ts)	How is P value calculated
693	Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'.	Can logistic regression be used for multi class classification
579	0:254:04Suggested clip · 117 secondsProbability density functions - Finding the constant k (example to try YouTubeStart of suggested clipEnd of suggested clip	How do you find the constant in a probability density function
7964	CNN Architectures: LeNet, AlexNet, VGG, GoogLeNet, ResNet and more… A Convolutional Neural Network (CNN, or ConvNet) are a special kind of multi-layer neural networks, designed to recognize visual patterns directly from pixel images with minimal preprocessing..	Is ResNet a CNN
1147	In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression.	What are features in machine learning
8422	Physical scientists often use the term root-mean-square as a synonym for standard deviation when they refer to the square root of the mean squared deviation of a signal from a given baseline or fit.	Is RMS same as standard deviation
1507	sigmoid activation function	Which activation function is more prone to vanishing gradient problem
1157	Cluster sampling is a probability sampling method in which you divide a population into clusters, such as districts or schools, and then randomly select some of these clusters as your sample. The clusters should ideally each be mini-representations of the population as a whole.	What is cluster sampling technique
4494	The Gini coefficient for the entire world has been estimated by various parties to be between 0.61 and 0.68.	What is the average Gini coefficient
2235	An XOR (exclusive OR gate) is a digital logic gate that gives a true output only when both its inputs differ from each other. The truth table for an XOR gate is shown below: Truth Table for XOR. The goal of the neural network is to classify the input patterns according to the above truth table.	What is XOR neural network
8134	Neural networks generally perform supervised learning tasks, building knowledge from data sets where the right answer is provided in advance. The networks then learn by tuning themselves to find the right answer on their own, increasing the accuracy of their predictions.	What is neural network learning
3997	Consequences of Heteroscedasticity The OLS estimators and regression predictions based on them remains unbiased and consistent. The OLS estimators are no longer the BLUE (Best Linear Unbiased Estimators) because they are no longer efficient, so the regression predictions will be inefficient too.	What are the consequences of heteroscedasticity in regression
370	An expert system is an example of a knowledge-based system. Expert systems were the first commercial systems to use a knowledge-based architecture. A knowledge-based system is essentially composed of two sub-systems: the knowledge base and the inference engine. The knowledge base represents facts about the world.	What is expert system example
2652	Using a confidence interval is a better way of conveying this information since it keeps the emphasis on the effect size - which is the important information - rather than the p-value. (Where NE and NC are the numbers in the experimental and control groups, respectively.)	Why is giving a confidence interval with an effect size better than giving an effect size alone
1126	2:194:05Suggested clip · 97 secondsChoosing Intervals for a Histogram - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you determine the intervals for a histogram
7181	Definition. Univariate analyses are used extensively in quality of life research. Univariate analysis is defined as analysis carried out on only one (“uni”) variable (“variate”) to summarize or describe the variable (Babbie, 2007; Trochim, 2006).	What is meant by univariate analysis
3622	Types of machine learning AlgorithmsSupervised learning.Unsupervised Learning.Semi-supervised Learning.Reinforcement Learning.	What are the different types of learning that algorithms use
4208	Statistics is the study of the collection, organization, analysis, and interpretation of data.  Mathematical statistics is the study of statistics from a mathematical standpoint, using probability theory as well as other branches of mathematics such as linear algebra and analysis.	Is mathematics and statistics the same thing
990	To calculate permutations, we use the equation nPr, where n is the total number of choices and r is the amount of items being selected. To solve this equation, use the equation nPr = n! / (n - r)!.	How do you calculate permutations
521	When you conduct a study that looks at a single variable, that study involves univariate data. For example, you might study a group of college students to find out their average SAT scores or you might study a group of diabetic patients to find their weights. Bivariate data is when you are studying two variables.	What is univariate and bivariate analysis with examples
6280	Terms in this set (35) The main difference between a z-score and t-test is that the z-score assumes you do/don't know the actual value for the population standard deviation, whereas the t-test assumes you do/don't know the actual value for the population standard deviation.	What is the main difference between z score and T score quizlet
5473	Probability Density Functions are a statistical measure used to gauge the likely outcome of a discrete value, e.g., the price of a stock or ETF. PDFs are plotted on a graph typically resembling a bell curve, with the probability of the outcomes lying below the curve.	What does the probability density function tell us
2854	The other assumption of one-way anova is that the variation within the groups is equal (homoscedasticity). While Kruskal-Wallis does not assume that the data are normal, it does assume that the different groups have the same distribution, and groups with different standard deviations have different distributions.	What is the difference between one way Anova and Kruskal Wallis test
2874	In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The exponential distribution, Erlang distribution, and chi-squared distribution are special cases of the gamma distribution.	Is gamma distribution discrete or continuous
261	"2 Answers. Simply put because one level of your categorical feature (here location) become the reference group during dummy encoding for regression and is redundant. I am quoting form here ""A categorical variable of K categories, or levels, usually enters a regression as a sequence of K-1 dummy variables."	Why do we omit one dummy variable
4823	Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate.	Which gate is used in gated recurrent unit
6029	For a statistical test to be valid, your sample size needs to be large enough to approximate the true distribution of the population being studied. To determine which statistical test to use, you need to know: whether your data meets certain assumptions. the types of variables that you're dealing with.	How do I know which statistical test to use
3666	Seven Techniques for Data Dimensionality ReductionMissing Values Ratio. Data columns with too many missing values are unlikely to carry much useful information.  Low Variance Filter.  High Correlation Filter.  Random Forests / Ensemble Trees.  Principal Component Analysis (PCA).  Backward Feature Elimination.  Forward Feature Construction.	How can you reduce the size of data
3664	Mentor: Well, if the line is a good fit for the data then the residual plot will be random. However, if the line is a bad fit for the data then the plot of the residuals will have a pattern.	How do you know if a residual plot is good
721	Run regression analysisOn the Data tab, in the Analysis group, click the Data Analysis button.Select Regression and click OK.In the Regression dialog box, configure the following settings: Select the Input Y Range, which is your dependent variable.  Click OK and observe the regression analysis output created by Excel.	How do you create a regression model
6606	: one half of the difference obtained by subtracting the first quartile from the third quartile in a frequency distribution.	What is quartile deviation
5085	The bivariate Pearson correlation indicates the following: Whether a statistically significant linear relationship exists between two continuous variables. The strength of a linear relationship (i.e., how close the relationship is to being a perfectly straight line)5 days ago	What does a bivariate correlation show
8028	Contents. In image processing filters are mainly used to suppress either the high frequencies in the image, i.e. smoothing the image, or the low frequencies, i.e. enhancing or detecting edges in the image. An image can be filtered either in the frequency or in the spatial domain.	What are the filters used in image processing
8207	Other examples of active learning techniques include role-playing, case studies, group projects, think-pair-share, peer teaching, debates, Just-in-Time Teaching, and short demonstrations followed by class discussion. There are two easy ways to promote active learning through the discussion.	What are some examples of active learning
2322	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is the difference between semi supervised learning and hybrid supervised unsupervised learning
4098	Correlation coefficients are used to measure the strength of the relationship between two variables.  This measures the strength and direction of a linear relationship between two variables. Values always range between -1 (strong negative relationship) and +1 (strong positive relationship).	What does a correlation coefficient of indicate about the two variables
6359	A recurrent neural network (RNN) is a type of artificial neural network commonly used in speech recognition and natural language processing (NLP). RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario.	What is the goal of the recurrent neural network
7198	How to Choose a Machine Learning Model – Some GuidelinesCollect data.Check for anomalies, missing data and clean the data.Perform statistical analysis and initial visualization.Build models.Check the accuracy.Present the results.	How do I choose the best model for machine learning
1625	Covariance measures the directional relationship between the returns on two assets. A positive covariance means that asset returns move together while a negative covariance means they move inversely.	How do you explain covariance
368	Convergence almost surely implies convergence in probability, but not vice versa.  That is, convergence to 0 in probability says that the 1's will get rarer and rarer as one looks ahead in the sequence. In contrast, almost surely is equivalent to the statement that, with probability 1, there exists such that for all .	How should I understand the difference between convergence in probability and almost sure convergence
6227	A factorial ANOVA compares means across two or more independent variables. Again, a one-way ANOVA has one independent variable that splits the sample into two or more groups, whereas the factorial ANOVA has two or more independent variables that split the sample in four or more groups.	What is the difference between one way Anova and factorial Anova
5346	The main requirements that a clustering algorithm should satisfy are:scalability;dealing with different types of attributes;discovering clusters with arbitrary shape;minimal requirements for domain knowledge to determine input parameters;ability to deal with noise and outliers;More items	What are the requirements of clustering algorithms
6672	Logarithmic scales reduce wide-ranging quantities to tiny scopes. For example, the decibel (dB) is a unit used to express ratio as logarithms, mostly for signal power and amplitude (of which sound pressure is a common example). In chemistry, pH is a logarithmic measure for the acidity of an aqueous solution.	What are logarithms used for
7507	Non parametric tests are used when your data isn't normal. Therefore the key is to figure out if you have normally distributed data. For example, you could look at the distribution of your data. If your data is approximately normal, then you can use parametric statistical tests.	How do you know which non parametric test to use
5406	FastText uses a simple and efficient baseline for sentence classification( represent sentences as bag of words (BoW) and train a linear classifier). It uses negative sampling , hierarchical softmax and N-gram features to reduce computational cost and improve efficiency. Have to say, all of the terms made my head spin.	How is FastText trained
4041	Discrete Random Variable. Has either a finite or countable number of values. The values of a discrete random variable can be plotted on a number line with space between each point.	What is a discrete random variable quizlet
6640	Any quantity that has both magnitude and direction is called a vector.  The only difference is that tensor is the generalized form of scalars and vectors . Means scalars and vectors are the special cases of tensor quantities. Scalar is a tensor of rank 0 and vector is a tensor of rank 1.	What is difference between tensor and vector
3313	Convolution is the process of adding each element of the image to its local neighbors, weighted by the kernel. This is related to a form of mathematical convolution. The matrix operation being performed—convolution—is not traditional matrix multiplication, despite being similarly denoted by *.	What is convolution in an image
785	The answer to that is the Erlang distribution. The Gamma distribution is a generalization of that distribution using a continuous instead of a discrete parameter for the number of events.	What is an intuitive explanation of Gamma distribution
3973	Train and serve a TensorFlow model with TensorFlow ServingTable of contents.Create your model. Import the Fashion MNIST dataset. Train and evaluate your model.Save your model.Examine your saved model.Serve your model with TensorFlow Serving. Add TensorFlow Serving distribution URI as a package source:  Make a request to your model in TensorFlow Serving. Make REST requests.	How do you serve a TensorFlow model
1434	Overfitting can be identified by checking validation metrics such as accuracy and loss. The validation metrics usually increase until a point where they stagnate or start declining when the model is affected by overfitting.	How do we know whether a model is overfitting
1311	Class interval refers to the numerical width of any class in a particular distribution. It is defined as the difference between the upper-class limit and the lower class limit. Class Interval = Upper-Class limit – Lower class limit.	How do you find the class interval
4284	The SD line goes through the point of averages, and has slope equal to SDY/SDX if the correlation coefficient r is greater than or equal to zero. The SD line has slope −SDY/SDX if r is negative.  The line slopes up to the right, because r is positive (0.5 at first).	Does the regression line go through the point of averages
7344	When to use it Use Spearman rank correlation when you have two ranked variables, and you want to see whether the two variables covary; whether, as one variable increases, the other variable tends to increase or decrease.	Why do you use Spearman rank correlation coefficient
6324	A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior.  Both classes of networks exhibit temporal dynamic behavior.	What do you mean by recurrent neural network
2984	: the mean of the absolute values of the numerical differences between the numbers of a set (such as statistical data) and their mean or median.	What is mean deviation
3152	The exponential smoothing method takes this into account and allows for us to plan inventory more efficiently on a more relevant basis of recent data. Another benefit is that spikes in the data aren't quite as detrimental to the forecast as previous methods.	What is the advantage of exponential smoothing over moving average
1730	To find the harmonic mean of a set of n numbers, add the reciprocals of the numbers in the set, divide the sum by n, then take the reciprocal of the result.	How do you do harmonic mean
6259	Discriminant analysis is a versatile statistical method often used by market researchers to classify observations into two or more groups or categories. In other words, discriminant analysis is used to assign objects to one group among a number of known groups.	What is discriminant analysis used for
4747	Mixed effects models are useful when we have data with more than one source of random variability. For example, an outcome may be measured more than once on the same person (repeated measures taken over time). When we do that we have to account for both within-person and across-person variability.	When would you use a mixed model
3434	1 : something within or from which something else originates, develops, or takes form an atmosphere of understanding and friendliness that is the matrix of peace. 2a : a mold from which a relief (see relief entry 1 sense 6) surface (such as a piece of type) is made. b : die sense 3a(1)	What does matrix mean
62	Likelihood ratios (LR) in medical testing are used to interpret diagnostic tests. Basically, the LR tells you how likely a patient has a disease or condition. The higher the ratio, the more likely they have the disease or condition. Conversely, a low ratio means that they very likely do not.	How do you interpret likelihood ratios
3206	A final LSTM model is one that you use to make predictions on new data. That is, given new examples of input data, you want to use the model to predict the expected output. This may be a classification (assign a label) or a regression (a real value).28‏/08‏/2017	How does Lstm predict
2292	The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution.	How is the t distribution related to the normal distribution
2403	The three axioms are:For any event A, P(A) ≥ 0. In English, that's “For any event A, the probability of A is greater or equal to 0”.When S is the sample space of an experiment; i.e., the set of all possible outcomes, P(S) = 1.  If A and B are mutually exclusive outcomes, P(A ∪ B ) = P(A) + P(B).	What are the 3 axioms of probability
317	Item based collaborative filtering finds similarity patterns between items and recommends them to users based on the computed information, whilst user based finds similar users and gives them recommendations based on what other people with similar consumption patterns appreciated[3].	Which one is correct about user based and item based collaborative filtering
4781	Bootstrapping is any test or metric that uses random sampling with replacement, and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.  It may also be used for constructing hypothesis tests.	What does bootstrapping in statistics mean
4611	In Semantic networks, you can represent your knowledge in the form of graphical networks. This network consists of nodes representing objects and arcs which describe the relationship between those objects. Also, it categorizes the object in different forms and links those objects.	How knowledge is represented
6112	Synset : a set of synonyms that share a common meaning. Each synset contains one or more lemmas, which represent a specific sense of a specific word.	What is the purpose of Synset
2313	Validation set is used for tuning the parameters of a model. Test set is used for performance evaluation. 2.	Which set is used for fine tuning and optimization
4740	Probability sampling is based on the fact that every member of a population has a known and equal chance of being selected.  With non-probability sampling, those odds are not equal. For example, a person might have a better chance of being chosen if they live close to the researcher or have access to a computer.	What is “probability sampling” Why is probability sampling preferred in comparison to non probability sampling
985	The cumulative distribution function, CDF, or cumulant is a function derived from the probability density function for a continuous random variable. It gives the probability of finding the random variable at a value less than or equal to a given cutoff.	What is probability density function and cumulative distributions
1181	A negative binomial distribution is concerned with the number of trials X that must occur until we have r successes. The number r is a whole number that we choose before we start performing our trials. The random variable X is still discrete. However, now the random variable can take on values of X = r, r+1, r+2,	How do you know if a binomial distribution is negative
6651	Latent Profile Analysis (LPA) tries to identify clusters of individuals (i.e., latent profiles) based on responses to a series of continuous variables (i.e., indicators). LPA assumes that there are unobserved latent profiles that generate patterns of responses on indicator items.	What is Latent profile analysis
7367	However there are disadvantages to the use of second order derivatives. (We should note that first derivative operators exaggerate the effects of noise.) Second derivatives will exaggerated noise twice as much. No directional information about the edge is given.	What is the problem of second order derivatives in edge detection
1391	There is a popular method known as elbow method which is used to determine the optimal value of K to perform the K-Means Clustering Algorithm. The basic idea behind this method is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer elements in the cluster.	Is a way of finding the K value for K means clustering
8529	Machine learning algorithms are the engines of machine learning, meaning it is the algorithms that turn a data set into a model. Which kind of algorithm works best (supervised, unsupervised, classification, regression, etc.)	What is learning algorithm in machine learning
7193	Outlier detection is extensively used in a wide variety of applications such as military surveillance for enemy activities to prevent attacks, intrusion detection in cyber security, fraud detection for credit cards, insurance or health care and fault detection in safety critical systems and in various kind of images.	What are the application of the outlier detection method
7200	General steps to calculate the mean squared error from a set of X and Y values:Find the regression line.Insert your X values into the linear regression equation to find the new Y values (Y').Subtract the new Y value from the original to get the error.Square the errors.Add up the errors.Find the mean.	How do you evaluate the mean squared error
838	Spaced Practice. Space out your studying over time.  Retrieval Practice. Practice bringing information to mind without the help of materials.  Elaboration. Explain and describe ideas with many details.  Interleaving. Switch between ideas while you study.  Concrete Examples.  Dual Coding.	What learning strategies do effective learners use
4705	Artificial intelligence is probably the most widely-known for its application in the etail/retail industry. Conversation intelligence software helps companies interact with customers and follow up leads by analyzing and segmenting sales calls using speech recognition and natural language processing.	In which field is artificial intelligence used
7616	In order to use MLE, we have to make two important assumptions, which are typically referred to together as the i.i.d. assumption. These assumptions state that: Data must be independently distributed. Data must be identically distributed.	What assumptions are generally made when doing maximum likelihood estimation
4693	Object tracking is a discipline within computer vision, which aims to track objects as they move across a series of video frames. Objects are often people, but may also be animals, vehicles or other objects of interest, such as the ball in a game of soccer.	What is object tracking in computer vision
1335	AI has a high learning curve, but for motivated students, the rewards of an AI career far outweigh the investment of time and energy. Succeeding in the field usually requires a bachelor's degree in computer science or a related discipline such as mathematics. More senior positions may require a master's or Ph.	What should I major in for artificial intelligence
3264	The rank mean of one group is compared to the overall rank mean to determine a test statistic called a z-score. If the groups are evenly distributed, then the z-score will be closer to 0.	What is Z in Wilcoxon rank sum test
5856	Gradient Boosting Machines vs. XGBoost.  While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation.	What is the difference between gradient boosting and XGBoost
5734	The feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.	How does feedforward neural network work
2383	During training stage the residual network alters the weights until the output is equivalent to the identity function.  In turn the identity function helps in building a deeper network. The residual function then maps the identity, weights and biases to fit the actual value.	Why do residual networks perform better
738	Amazon ML supports three types of ML models: binary classification, multiclass classification, and regression. The type of model you should choose depends on the type of target that you want to predict.	What are different machine learning models
6323	In machine learning, the true positive rate, also referred to sensitivity or recall, is used to measure the percentage of actual positives which are correctly identified.	Is recall true positive rate
7279	"Often, binary data is used to represent one of two conceptually opposed values, e.g: the outcome of an experiment (""success"" or ""failure"") the response to a yes-no question (""yes"" or ""no"") presence or absence of some feature (""is present"" or ""is not present"")"	What is binary data example
576	Accuracy in Machine Learning Accuracy is the number of correctly predicted data points out of all the data points. More formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives.	How is accuracy calculated in machine learning
603	AB testing is essentially an experiment where two or more variants of a page are shown to users at random, and statistical analysis is used to determine which variation performs better for a given conversion goal.	What is AB testing in Analytics
5212	Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are. Instead, they need to be recoded into a series of variables which can then be entered into the regression model.	Can you use categorical variables in linear regression
649	Vector autoregression (VAR) is a statistical model used to capture the relationship between multiple quantities as they change over time. VAR is a type of stochastic process model. VAR models generalize the single-variable (univariate) autoregressive model by allowing for multivariate time series.	What is vector autoregression used for
1108	Distributions with one clear peak are called unimodal, and distributions with two clear peaks are called bimodal. When a symmetric distribution has a single peak at the center, it is referred to as bell-shaped.	Is the distribution uniform unimodal or bimodal
4055	The Finite Population Correction Factor (FPC) is used when you sample without replacement from more than 5% of a finite population. It's needed because under these circumstances, the Central Limit Theorem doesn't hold and the standard error of the estimate (e.g. the mean or proportion) will be too big.	When and how do we use finite population correction factor
2518	"In systematic sampling, the list of elements is ""counted off"". That is, every kth element is taken.  Stratified sampling also divides the population into groups called strata. However, this time it is by some characteristic, not geographically."	What is the difference between stratified sampling and systematic sampling
2744	As we optimize the squared residuals to estimate the regression parameters, so we need commonly known normal situation. In Statistics, normal means that everything has equal probability.  So equal probability for each value of regression residual is only possible through Normal Distribution.	Why do we assume the residue of an ordinary linear regression is from normal distribution
2815	The network is symmetric because the weight wij for the connection between unit i and R. Rojas: Neural Networks, Springer-Verlag, Berlin, 1996 R. Rojas: Neural Networks, Springer-Verlag, Berlin, 1996 Page 8 344 13 The Hopfield Model unit j is equal to the weight wji of the connection from unit j to unit i.	Why hopfield networks are symmetric
522	A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.	How does a convolutional neural network work
564	The generalized delta rule is a mathematically derived formula used to determine how to update a neural network during a (back propagation) training step. A neural network learns a function that maps an input to an output based on given example pairs of inputs and outputs.	What is generalized delta learning rule
6644	The smaller the p-value, the stronger the evidence that you should reject the null hypothesis. A p-value less than 0.05 (typically ≤ 0.05) is statistically significant.  A p-value higher than 0.05 (> 0.05) is not statistically significant and indicates strong evidence for the null hypothesis.	How is the P value interpret
3717	Discriminant analysis is statistical technique used to classify observations into non-overlapping groups, based on scores on one or more quantitative predictor variables. For example, a doctor could perform a discriminant analysis to identify patients at high or low risk for stroke.	What is meant by discriminant analysis
2705	Reinforcement Learning (RL) refers to a kind of Machine Learning method in which the agent receives a delayed reward in the next time step to evaluate its previous action. It was mostly used in games (e.g. Atari, Mario), with performance on par with or even exceeding humans.	What is reinforcement learning algorithms
293	Key Takeaways. Standard deviation defines the line along which a particular data point lies. Z-score indicates how much a given value differs from the standard deviation. The Z-score, or standard score, is the number of standard deviations a given data point lies above or below mean.	What is the difference between AZ score and standard deviation
5882	Association Rule Mining, as the name suggests, association rules are simple If/Then statements that help discover relationships between seemingly independent relational databases or other data repositories. Most machine learning algorithms work with numeric datasets and hence tend to be mathematical.	What are the association rules in data mining
5587	A sample space is the set of all possible outcomes. However, some sample spaces are better than others. Consider the experiment of flipping two coins. It is possible to get 0 heads, 1 head, or 2 heads. Thus, the sample space could be {0, 1, 2}.	What is a sample space in statistics
6862	Simply stated, high risk prevention strategies aim to identify individuals or groups who are likely to have an increased incidence of a disease, based on the presence of modifiable risk factors known to be causal for the disease (e.g., high blood pressure), or characteristics of individuals or groups that are	What is high risk strategy
5804	The cumulative distribution function FX(x) of a random variable X has three important properties: The cumulative distribution function FX(x) is a non-decreasing function. This follows directly from the result we have just derived: For a<b, we have Pr(a<X≤b)≥0 ⟹ FX(b)−FX(a)≥0 ⟹ FX(a)≤FX(b).	What are the properties of cumulative distribution function
1163	Since this derivation of the LDA direction via least squares does not use a Gaussian assumption for the features, its applicability extends beyond the realm of Gaussian data. However the derivation of the particular intercept or cut-point given in (4.11) does require Gaussian data.	Does Linear Discriminant Analysis work for distributions other than Gaussian
1782	33:4235:21Suggested clip · 85 secondsDerivation of the Normal (Gaussian) Distribution - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you derive the Gaussian distribution
4851	In practice, the sample size used in a study is usually determined based on the cost, time, or convenience of collecting the data, and the need for it to offer sufficient statistical power.  In a census, data is sought for an entire population, hence the intended sample size is equal to the population.	How sample size is determined
1122	Naive Bayes Tutorial (in 5 easy steps)Step 1: Separate By Class.Step 2: Summarize Dataset.Step 3: Summarize Data By Class.Step 4: Gaussian Probability Density Function.Step 5: Class Probabilities.	How is naive Bayes classifier implemented
1336	In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.	What are the Hyperparameters in machine learning
907	Gravity tries to keep things together through attraction and thus tends to lower statistical entropy. The universal law of increasing entropy (2nd law of thermodynamics) states that the entropy of an isolated system which is not in equilibrium will tend to increase with time, approaching a maximum value at equilibrium.	How is gravity related to entropy
5793	The Generative Adversarial Network, or GAN, is an architecture that makes effective use of large, unlabeled datasets to train an image generator model via an image discriminator model. The discriminator model can be used as a starting point for developing a classifier model in some cases.	Can Gan be used for classification
53	To summarize, an algorithm is a method or a procedure we follow to get something done or solve a problem. A model is a computation or a formula formed as a result of an algorithm that takes some values as input and produces some value as output.	What is the difference between model and algorithm
1370	A causal model is the most sophisticated kind of forecasting tool. It expresses mathematically the relevant causal relationships, and may include pipeline considerations (i.e., inventories) and market survey information. It may also directly incorporate the results of a time series analysis.	What model is best for forecasting
8145	2 Answers. Boosting is based on weak learners (high bias, low variance).  Boosting reduces error mainly by reducing bias (and also to some extent variance, by aggregating the output from many models). On the other hand, Random Forest uses as you said fully grown decision trees (low bias, high variance).	What is the difference in bias and variance in 1 Random Forest 2 gradient boosting Why is there this difference
4084	How to Calculate a CorrelationFind the mean of all the x-values.Find the standard deviation of all the x-values (call it sx) and the standard deviation of all the y-values (call it sy).  For each of the n pairs (x, y) in the data set, take.Add up the n results from Step 3.Divide the sum by sx ∗ sy.More items	How do you find the correlation coefficient between two sets of data
4523	Artificial intelligence is generally divided into two types – narrow (or weak) AI and general AI, also known as AGI or strong AI.	What are the 2 types of AI
345	Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error.	What is gradient boosting in machine learning
7251	A regression line (LSRL - Least Squares Regression Line) is a straight line that describes how a response variable y changes as an explanatory variable x changes. The line is a mathematical model used to predict the value of y for a given x.	What does the least square regression line tell you
6542	In computer science, a universal Turing machine (UTM) is a Turing machine that simulates an arbitrary Turing machine on arbitrary input.  In terms of computational complexity, a multi-tape universal Turing machine need only be slower by logarithmic factor compared to the machines it simulates.	What is universal Turing machine in TOC
6380	TensorFlow Datasets is a collection of datasets ready to use, with TensorFlow or other Python ML frameworks, such as Jax. All datasets are exposed as tf. data. Datasets , enabling easy-to-use and high-performance input pipelines. To get started see the guide and our list of datasets.	What is dataset in TensorFlow
1471	The fundamental counting principle states that if there are p ways to do one thing, and q ways to do another thing, then there are p×q ways to do both things. possible outcomes of the experiment. The counting principle can be extended to situations where you have more than 2 choices.	When can you apply the fundamental principle of counting
8204	Three basic principles for the design of a sample survey are: 1. Principle of Optimization The principle of optimization takes into account the factors of (a) Efficiency and (b) cost. (a) Efficiency Efficiency is measured by the inverse of sampling variance of the estimator.	What are the basic principles of sample survey
88	Hierarchical regression is a way to show if variables of your interest explain a statistically significant amount of variance in your Dependent Variable (DV) after accounting for all other variables. This is a framework for model comparison rather than a statistical method.	What is hierarchical regression used for
2053	1. A numerical value that defines the learning capability of a neural network during training. Learn more in: Voltage Instability Detection Using Neural Networks.	What is learning coefficient
3598	The Poisson Model (distribution) Assumptions Independence: Events must be independent (e.g. the number of goals scored by a team should not make the number of goals scored by another team more or less likely.) Homogeneity: The mean number of goals scored is assumed to be the same for all teams.	What are the assumptions of Poisson distribution
457	Essentially, cross-sectional analysis shows an investor which company is best given the metrics she cares about. Time series analysis, also known as trend analysis, focuses in on a single company over time. In this case, the company is being judged in the context of its past performance.	What is the difference between cross sectional and time series ratio analysis
619	Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.  F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.	What is precision recall and f1 score
1070	- if R-squared value 0.3 < r < 0.5 this value is generally considered a weak or low effect size, - if R-squared value 0.5 < r < 0.7 this value is generally considered a Moderate effect size, - if R-squared value r > 0.7 this value is generally considered strong effect size, Ref: Source: Moore, D. S., Notz, W.	What does a weak R squared value mean
4910	Cluster sampling is a probability sampling method in which you divide a population into clusters, such as districts or schools, and then randomly select some of these clusters as your sample. The clusters should ideally each be mini-representations of the population as a whole.	What is cluster probability sampling
730	One of the simplest causal analysis methods involves asking yourself “why” five times. You start by identifying the problem. “My house is always disorganized.” Then, you ask yourself why that is the case. You create a chain of inquiry that offers insight about the core of the problem.	How do you perform a causal analysis
2536	To find the interquartile range (IQR), ​first find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1.	How do you find the median and interquartile range
594	Loss function for Logistic Regression The loss function for linear regression is squared loss. The loss function for logistic regression is Log Loss, which is defined as follows: Log Loss = ∑ ( x , y ) ∈ D − y log ⁡ ( y ′ ) − ( 1 − y ) log ⁡ where: ( x , y ) ∈ D.	What is logistic regression loss function
5439	Active learningSet tasks which have purpose and relevance to the students.Encourage students to reflect on the meaning of what they have learnt.Allow students to negotiate goals and methods of learning with the teacher.Encourage students to critically evaluate different ways and means of learning the content.More items	What are the principles of active learning
5037	Multilayer Perceptron (MLP): used to apply in computer vision, now succeeded by Convolutional Neural Network (CNN). MLP is now deemed insufficient for modern advanced computer vision tasks. Has the characteristic of fully connected layers, where each perceptron is connected with every other perceptron.	What is the difference between MLP and CNN
271	How to useDownload MNIST Dataset.Put and Extract it in executable directory.Run Extractor.Enter Images and Labels File (e.g t10k-images.idx3-ubyte and t10k-labels.idx1-ubyte for training and train-images.idx3-ubyte and train-labels.idx1-ubyte for testing)Enter output directory.	How do I extract a Mnist dataset
887	For a regression problem, the outputs of individual models can literally be averaged to obtain the output of the ensemble model.  Bagging consists in fitting several base models on different bootstrap samples and build an ensemble model that “average” the results of these weak learners.	How do ensemble methods work for regression problems
5606	Shared weights basically means that the same weights is used for two layers in the model. This basically means that the same parameters will be used to represent two different transformations in the system.	What shared weights means in CNN
7083	Natural Language Processing (NLP) uses algorithms to understand and manipulate human language. This technology is one of the most broadly applied areas of machine learning.  This Specialization will equip you with the state-of-the-art deep learning techniques needed to build cutting-edge NLP systems.	What is NLP in deep learning
5020	Step 1: Learn the fundamental data structures and algorithms. First, pick a favorite language to focus on and stick with it.  Step 2: Learn advanced concepts, data structures, and algorithms.  Step 1+2: Practice.  Step 3: Lots of reading + writing.  Step 4: Contribute to open-source projects.  Step 5: Take a break.	How do you start an algorithm for studying
348	Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.	How do you choose best features in machine learning
4376	Machine learning uses two types of techniques: supervised learning, which trains a model on known input and output data so that it can predict future outputs, and unsupervised learning, which finds hidden patterns or intrinsic structures in input data.	What are two techniques of machine learning
4970	Normalization basically means bringing all the values to once scale and there is nothing wrong using percentage but there must be a base value for normalizing the data and if you are asking about 100 as a base value and then converting everything as % it will not be equal to normalization as in normalization the base	What is normalized percentage
7914	word2vec itself is a simple bi-layered neural network architecture, it turns text into meaningful vectors form that deeper networks can understand. In other words the out put of simple neural word2vec model is used as input for Deep Networks.	Is Word2Vec a neural network
868	There are four basic sequence learning problems: sequence prediction, sequence generation, sequence recognition, and sequential decision making. These “problems” show how sequences are formulated.	Which of the following is part of the sequence learning problem
4806	"""Describe what works for you.Explain your time management strategies.Demonstrate your level of organization.Give past examples.Be honest."	How do you answer how do you keep yourself organized
620	Definition. Average precision is a measure that combines recall and precision for ranked retrieval results. For one information need, the average precision is the mean of the precision scores after each relevant document is retrieved.	What is average precision
1884	Method comparisonCorrelation coefficient. A correlation coefficient measures the association between two methods.Scatter plot. A scatter plot shows the relationship between two methods.Fit Y on X.  Linearity.  Residual plot.  Average bias.  Difference plot (Bland-Altman plot)  Fit differences.More items•	How do you compare two methods
3747	Every probability pi is a number between 0 and 1, and the sum of all the probabilities is equal to 1. Examples of discrete random variables include: The number of eggs that a hen lays in a given day (it can't be 2.3) The number of people going to a given soccer match.	What is a discrete random variable What are some examples
3877	Spectroscopy in chemistry and physics, a method of analyzing the properties of matter from their electromagnetic interactions. Spectral estimation, in statistics and signal processing, an algorithm that estimates the strength of different frequency components (the power spectrum) of a time-domain signal.	How is spectral analysis used
1642	The null hypothesis is a general statement that states that there is no relationship between two phenomenons under consideration or that there is no association between two groups. An alternative hypothesis is a statement that describes that there is a relationship between two selected variables in a study.	What is the difference between a null hypothesis and an alternative hypothesis
933	"The sampling distribution of the sample mean can be thought of as ""For a sample of size n, the sample mean will behave according to this distribution."" Any random draw from that sampling distribution would be interpreted as the mean of a sample of n observations from the original population."	How do you describe the sampling distribution of the sample mean
4422	The weighted kappa is calculated using a predefined table of weights which measure the degree of disagreement between the two raters, the higher the disagreement the higher the weight.	What is weighted kappa
5373	In a Data Mining sense, the similarity measure is a distance with dimensions describing object features. That means if the distance among two data points is small then there is a high degree of similarity among the objects and vice versa. The similarity is subjective and depends heavily on the context and application.	What is similarity measure in data mining
1453	Gradient descent subtracts the step size from the current value of intercept to get the new value of intercept. This step size is calculated by multiplying the derivative which is -5.7 here to a small number called the learning rate. Usually, we take the value of the learning rate to be 0.1, 0.01 or 0.001.	How do you find the gradient descent
2516	0:5612:03Suggested clip · 85 secondsPart 7 - Absorbing Markov Chains and Absorbing States - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you know if a matrix is absorbing
648	AlphaGo is a computer program that plays the board game Go.  In October 2015, in a match against Fan Hui, the original AlphaGo became the first computer Go program to beat a human professional Go player without handicap on a full-sized 19×19 board.	What is AlphaGo AI
7723	If you need to change the shape of a variable, you can do the following (e.g. for a 32-bit floating point tensor): var = tf. Variable(tf.They are:reshape.squeeze (removes dimensions of size 1 from the shape of a tensor)expand_dims (adds dimensions of size 1)	How do you change the shape of a tensor
690	A test statistic is a standardized value that is calculated from sample data during a hypothesis test.  A t-value of 0 indicates that the sample results exactly equal the null hypothesis.	What does a test statistic of 0 mean
3319	Inductive probability refers to the likelihood that an inductive argument with true premises will give a true conclusion.  An argument with low inductive probability is less likely to have a true conclusion even if its premises are true.	What is a probability argument
3127	Beyond the agent and the environment, there are four main elements of a reinforcement learning system: a policy, a reward, a value function, and, optionally, a model of the environment. A policy defines the way the agent behaves in a given time.	What are the elements of reinforcement learning
5159	Independent and dependent variablesThe independent variable is the cause. Its value is independent of other variables in your study.The dependent variable is the effect. Its value depends on changes in the independent variable.	How do you identify the independent and dependent variables
1423	Artificial intelligence (AI) is wide-ranging branch of computer science concerned with building smart machines capable of performing tasks that typically require human intelligence.  It is the endeavor to replicate or simulate human intelligence in machines.	What is AI in computing
3085	Semi-supervised clustering is a bridge between Supervised Learning and Cluster Analysis. it's about learning with both labeled and unlabeled data: sometimes we have some prior knowledge about clusters, e.g. we could have some label information.	What is semi supervised clustering
3730	How to train your Deep Neural NetworkTraining data.  Choose appropriate activation functions.  Number of Hidden Units and Layers.  Weight Initialization.  Learning Rates.  Hyperparameter Tuning: Shun Grid Search - Embrace Random Search.  Learning Methods.  Keep dimensions of weights in the exponential power of 2.More items•	How do I train deep neural networks
3692	The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed.	What is the use of finding the root mean square error
7455	Problems that require more than two hidden layers were rare prior to deep learning. Two or fewer layers will often suffice with simple data sets. However, with complex datasets involving time-series or computer vision, additional layers can be helpful.	How many hidden layers are there in deep learning
4666	Random errors often have a Gaussian normal distribution (see Fig. 2). In such cases statistical methods may be used to analyze the data. The mean m of a number of measurements of the same quantity is the best estimate of that quantity, and the standard deviation s of the measurements shows the accuracy of the estimate.	Do random errors always have a Gaussian distribution
1399	In short, linear regression is one of the mathematical models to describe the (linear) relationship between input and output. Least squares, on the other hand, is a method to metric and estimate models, in which the optimal parameters have been found.	What is the difference between linear regression and least squares
525	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve.	Why does data need to be normally distributed
6851	Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.	What is the meaning of principal component analysis
6681	Which category?  What group does this fall into?  Is this weird or is something not normal?  What options should we take?	What questions can machine learning answer
628	AlphaGo was initially trained to mimic human play by attempting to match the moves of expert players from recorded historical games, using a database of around 30 million moves.	How was AlphaGo trained
6360	Naive Bayes assumes conditional independence, P(X|Y,Z)=P(X|Z), Whereas more general Bayes Nets (sometimes called Bayesian Belief Networks) will allow the user to specify which attributes are, in fact, conditionally independent.	What is the difference between Bayes and naive Bayes
2592	The disadvantage of the ANOVA F-test is that if we reject the null hypothesis, we do not know which treatments can be said to be significantly different from the others, nor, if the F-test is performed at level α, can we state that the treatment pair with the greatest mean difference is significantly different at level	What is the limitation of the F ratio in Anova
771	The t distributions were discovered by William S. Gosset was a statistician employed by the Guinness brewing company which had stipulated that he not publish under his own name.  He therefore wrote under the pen name ``Student.	Why is it called Student's t distribution
408	2:4125:32Suggested clip · 98 secondsStructural Equation Modeling: what is it and what can we use it for YouTubeStart of suggested clipEnd of suggested clip	How do you create a structural equation model
1025	In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  In supervised feature learning, features are learned using labeled input data.	What is representation learning in deep learning
6362	Rule of Multiplication The probability that Events A and B both occur is equal to the probability that Event A occurs times the probability that Event B occurs, given that A has occurred.	What is the multiplication rule for probability
4711	Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay.  It is often used in signal processing for analyzing functions or series of values, such as time domain signals.	What is autocorrelation in signal and system
6048	There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. For a robot that is learning to walk, the state is the position of its two legs. For a Go program, the state is the positions of all the pieces on the board.	How do you define states in reinforcement learning
1345	The Poisson counting process can be viewed as a continuous-time Markov chain. Suppose that takes values in and is independent of . Define X t = X 0 + N t for t ∈ [ 0 , ∞ ) .	Is a Poisson process a Markov chain
4007	Maximization Bias is a technical way of saying that Q-Learning algorithm overestimates the value function estimates (V) and action-value estimates (Q).  Given the large variance in rewards, it is quite possible that the initial few estimates of the actions might be positive or more negative.	What is maximization bias
658	Simple random sampling is a method used to cull a smaller sample size from a larger population and use it to research and make generalizations about the larger group.  The advantages of a simple random sample include its ease of use and its accurate representation of the larger population.	Why do Statisticians prefer to select samples by a random process
66	A baseball player can have higher batting average than another on each of two years, but lower than the other when the two are combined. In one case, David Justice had a higher batting average than Derek Jeter in 1995 and 1996, but across the two years, Jeter's average was higher.	What is Simpson's Paradox example
3934	In review, beta-endorphins are proteins that are primarily synthesized by the pituitary gland in response to physiologic stressors such as pain. They function through various mechanisms in both the central and peripheral nervous system to relieve pain when bound to their mu-opioid receptors.	What releases beta endorphins
16	4:306:35Suggested clip · 77 secondsMarginal PDF from Joint PDF - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the marginal density function
3122	Abstract: The dimensionality curse phenomenon states that in high dimensional spaces distances between nearest and farthest points from query points become almost equal. Therefore, nearest neighbor calculations cannot discriminate candidate points.	What is curse of dimensionality in Knn
1305	A population is called multinomial if its data is categorical and belongs to a collection of discrete non-overlapping classes. The null hypothesis for goodness of fit test for multinomial distribution is that the observed frequency fi is equal to an expected count ei in each category.	What is multinomial population
2654	A correlation analysis provides information on the strength and direction of the linear relationship between two variables, while a simple linear regression analysis estimates parameters in a linear equation that can be used to predict values of one variable based on the other.	What is the relationship between correlation and linear regression
672	The bootstrap method is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.  It is used in applied machine learning to estimate the skill of machine learning models when making predictions on data not included in the training data.	What is bootstrap in machine learning
1420	The linear regression model describes the dependent variable with a straight line that is defined by the equation Y = a + b × X, where a is the y-intersect of the line, and b is its slope.	How do you describe linear regression
1053	In statistics, a studentized residual is the quotient resulting from the division of a residual by an estimate of its standard deviation. It is a form of a Student's t-statistic, with the estimate of error varying between points. This is an important technique in the detection of outliers.	What is a Studentized residual used for
652	Now we'll check out the proven way to improve the performance(Speed and Accuracy both) of neural network models:Increase hidden Layers.  Change Activation function.  Change Activation function in Output layer.  Increase number of neurons.  Weight initialization.  More data.  Normalizing/Scaling data.More items•	How can I make my neural network more accurate
4983	CNNs are the best image classifier algorithm we know of, and they work particularly well when given lots and lots of data to work with. Progressive resizing is a technique for building CNNs that can be very helpful during the training and optimization phases of a machine learning project.	What works best for image data in deep learning
115	Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset. Model selection is a process that can be applied both across different types of models (e.g. logistic regression, SVM, KNN, etc.)	What is the goal of model selection in machine learning
1941	4:1213:02Suggested clip · 101 secondsThe Transition Matrix - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you read a transition matrix
6637	Cross-sectional data refers to a setoff observations taken at a single point in time. Samples are constructed by collecting the data of interest across a range of observational units – people, objects, firms – at the same time.	What is cross sectional time series data
934	In regression analysis, the dependent variable is denoted Y and the independent variable is denoted X. So, in this case, Y=total cholesterol and X=BMI. When there is a single continuous dependent variable and a single independent variable, the analysis is called a simple linear regression analysis .	How do you find x and y variables in regression
8185	A scatter plot is a special type of graph designed to show the relationship between two variables. With regression analysis, you can use a scatter plot to visually inspect the data to see whether X and Y are linearly related.	What is the importance of a scatter plot to a regression analysis
274	The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.  The learning rate may be the most important hyperparameter when configuring your neural network.	What is learning rate in CNN
4803	In statistics and control theory, Kalman filtering, also known as linear quadratic estimation (LQE), is an algorithm that uses a series of measurements observed over time, containing statistical noise and other inaccuracies, and produces estimates of unknown variables that tend to be more accurate than those based on a	What is a Kalman filter basics
5387	Definition. An estimator is said to be unbiased if its bias is equal to zero for all values of parameter θ, or equivalently, if the expected value of the estimator matches that of the parameter.	How do you find an unbiased estimator
843	Definition. An entropy source is an input device or a measured characteristic of an I/O device on a computer that supplies random bits: specifically, bits that an attacker cannot know.	What is entropy of a source
915	You can improve your pattern recognition skills by practising. Now you know that patterns can appear in numbers, objects, symbols, music and more, you can pay attention to this. Looking and listening while being aware that there are patterns in things most of the time, helps you to eventually find them easier.	How can I improve my pattern recognition
5641	At the lowest, simplest level, a discriminative system (or model) is one to which you present one input and it produces a single result, which is commonly the discrimination (classification) of the input.	Why is a neural network considered a discriminative model
21	A version space is a hierarchial representation of knowledge that enables you to keep track of all the useful information supplied by a sequence of learning examples without remembering any of the examples.	What is Version space in ML
1015	So, For hidden layers the best option to use is ReLU, and the second option you can use as SIGMOID. For output layers the best option depends, so we use LINEAR FUNCTIONS for regression type of output layers and SOFTMAX for multi-class classification.	Can ReLU be used in output layer
1113	In random forest different features are used for each tree while in bagging different subsets of the training data are used. Gradient boosting generates an ensemble of trees too but does so in a different way, motivated by different ideas.	Why does gradient boosting generally outperform random forests
3669	In General, A Discriminative model ‌models the decision boundary between the classes. A Generative Model ‌explicitly models the actual distribution of each class.  A Discriminative model ‌learns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems.	What is the difference between generative and discriminative models
8071	A binomial random variable is the number of successes x in n repeated trials of a binomial experiment.Binomial DistributionThe mean of the distribution (μx) is equal to n * P .The variance (σ2x) is n * P * ( 1 - P ).The standard deviation (σx) is sqrt[ n * P * ( 1 - P ) ].	How do you find the N and P of a binomial distribution
7497	"Answer. When the ROC curve dips prominently into the lower right half of the graph, this is likely a sign that either the wrong State Value has been specified or the wrong Test-State association direction has been specified in the ""Test Direction"" area of the ""ROC Curve:Options"" dialog."	Why is my ROC curve inverted
855	Data are skewed right when most of the data are on the left side of the graph and the long skinny tail extends to the right. Data are skewed left when most of the data are on the right side of the graph and the long skinny tail extends to the left.	How statistics can be skewed
5322	Google's self-driving car, Waymo, is an example of prescriptive analytics in action. The vehicle makes millions of calculations on every trip that helps the car decide when and where to turn, whether to slow down or speed up and when to change lanes — the same decisions a human driver makes behind the wheel.	What is an example of prescriptive analytics
809	One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture.	What is the vanishing gradient problem and how do we overcome that
654	Wilcoxon – The Wilcoxon signed rank test has the null hypothesis that both samples are from the same population.  Sign – The sign test has the null hypothesis that both samples are from the same population. The sign test compares the two dependent observations and counts the number of negative and positive differences.	What is the difference between sign test and Wilcoxon signed rank test
2337	The original LSTM model is comprised of a single hidden LSTM layer followed by a standard feedforward output layer. The Stacked LSTM is an extension to this model that has multiple hidden LSTM layers where each layer contains multiple memory cells.	What is hidden layer in Lstm
3090	"In mathematics and statistics, an asymptotic distribution is a probability distribution that is in a sense the ""limiting"" distribution of a sequence of distributions."	What does asymptotic distribution mean
3180	A type II error is also known as a false negative and occurs when a researcher fails to reject a null hypothesis which is really false.	What is Type 2 error in statistics
3361	Sampling From a Distribution. When we say we sample from a distribution, we mean that we choose some discrete points, with likelihood defined by the distribution's probability density function.	What does it mean to sample from a distribution
7408	Chi-Square DistributionThe mean of the distribution is equal to the number of degrees of freedom: μ = v.The variance is equal to two times the number of degrees of freedom: σ2 = 2 * v.When the degrees of freedom are greater than or equal to 2, the maximum value for Y occurs when Χ2 = v - 2.More items	How do you calculate chi square distribution
6309	Variance (σ2) in statistics is a measurement of the spread between numbers in a data set. That is, it measures how far each number in the set is from the mean and therefore from every other number in the set.	What exactly is variance
7588	All Answers (6) Chi square test requires 2 categorical variables. T test requires 1 categorical and 1 continuous variables. You can't use them interchangeably.	Can't test be used for categorical variables
5787	It uses data with several classes to predict the classification of the new sample point. KNN is non-parametric since it doesn't make any assumptions on the data being studied, i.e., the model is distributed from the data.	What is an advantage of the K nearest neighbor method
587	Cognitive computing tools such as IBM Watson, artificial intelligence tools such as expert systems, and intelligent personal assistant tools such as Amazon Echo, Apple Siri, Google Assistant, and Microsoft Cortana can be used to extend the ability of humans to understand, decide, act, learn, and avoid problems.	What are Cognitive computing intelligent processes knowledge automation knowledge networks
2471	Causal inference is a statistical tool that enables our AI and machine learning algorithms to reason in similar ways.  We're interested in understanding how changes in our network settings affect latency, so we use causal inference to proactively choose our settings based on this knowledge.	What is causal inference machine learning
3638	Applications of Real-time SystemReal Time System is a system that is put through real time which means response is obtained within a specified timing constraint or system meets the specified deadline.  Applications of Real-time System:  Industrial application:  Medical Science application:  Peripheral Equipment applications:More items•	Which of the following are the applications of real time system
4223	The Analysis of covariance (ANCOVA) is done by using linear regression. This means that Analysis of covariance (ANCOVA) assumes that the relationship between the independent variable and the dependent variable must be linear in nature.	How is analysis of covariance done
508	Each approach uses several methods as follows:Clustering. hierarchical clustering, k-means. mixture models.  Anomaly detection. Local Outlier Factor. Isolation Forest.Neural Networks. Autoencoders. Deep Belief Nets.  Approaches for learning latent variable models such as. Expectation–maximization algorithm (EM) Method of moments.	What are some of the common unsupervised learning techniques
5754	This probability is written P(B|A), notation for the probability of B given A. In the case where events A and B are independent (where event A has no effect on the probability of event B), the conditional probability of event B given event A is simply the probability of event B, that is P(B). P(A and B) = P(A)P(B|A).	How do you calculate conditional probability
609	Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain.	What is the difference between Markov chain and Markov process
963	One common method of consolidating two probability distributions is to simply average them - for every set of values A, set If the distributions both have densities, for example, averaging the probabilities results in a probability distribution with density the average of the two input densities (Figure 1).	How do you combine two distributions
2883	Tokenization is the process of dividing text into a set of meaningful pieces. These pieces are called tokens.  Depending on the task at hand, we can define our own conditions to divide the input text into meaningful tokens.	What is tokenization in machine learning
223	11 websites to find free, interesting datasetsFiveThirtyEight.  BuzzFeed News.  Kaggle.  Socrata.  Awesome-Public-Datasets on Github.  Google Public Datasets.  UCI Machine Learning Repository.  Data.gov.More items	Where can I find datasets
8491	How to Find a Sample Size Given a Confidence Interval and Width (unknown population standard deviation)za/2: Divide the confidence interval by two, and look that area up in the z-table: .95 / 2 = 0.475.  E (margin of error): Divide the given width by 2. 6% / 2.  : use the given percentage. 41% = 0.41.  : subtract. from 1.	What is the formula for determining sample size
7838	Cohen came up with a mechanism to calculate a value which represents the level of agreement between judges negating the agreement by chance.  You can see that balls which are agreed on by chance are removed both from agreed and total number of balls. And that is the whole intuition of Kappa value aka Kappa coefficient.	What is an intuitive explanation of Cohens kappa statistic
3750	training set—a subset to train a model. test set—a subset to test the trained model.	What is meant by training set and test set
7038	Parameters are key to machine learning algorithms.  In this case, a parameter is a function argument that could have one of a range of values. In machine learning, the specific model you are using is the function and requires parameters in order to make a prediction on new data.	What are parameters in deep learning
794	The z-test is best used for greater-than-30 samples because, under the central limit theorem, as the number of samples gets larger, the samples are considered to be approximately normally distributed.	In what situation would you use a z test rather than a t test Central Limit Theorem
6492	Correspondence analysis offers a potential means for communication researchers to examine, and better understand, relationships between categorical variables. Though traditionally not commonly used in communication research, potential applications for CA exist.	How is correspondence analysis useful in research
7173	3:258:09Suggested clip · 111 secondsScatterplot - Equation of a Trend Line - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the trend line in a scatter plot
3572	Simple linear regression is commonly used in forecasting and financial analysis—for a company to tell how a change in the GDP could affect sales, for example. Microsoft Excel and other software can do all the calculations, but it's good to know how the mechanics of simple linear regression work.	Is linear regression Good for forecasting
1455	The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models.	What is the purpose of factor analysis
3883	Definition 1. A statistic d is called an unbiased estimator for a function of the parameter g(θ) provided that for every choice of θ, Eθd(X) = g(θ). Any estimator that not unbiased is called biased.  Note that the mean square error for an unbiased estimator is its variance.	What is an unbiased estimator of variance
3407	The sparsity of a matrix can be quantified with a score, which is the number of zero values in the matrix divided by the total number of elements in the matrix. sparsity = count zero elements / total elements. 1. sparsity = count zero elements / total elements. Below is an example of a small 3 x 6 sparse matrix.	What is sparsity in machine learning
5000	To plot the learning curves, we need only a single error score per training set size, not 5.The learning_curve() function from scikit-learnDo the required imports from sklearn .Declare the features and the target.Use learning_curve() to generate the data needed to plot a learning curve.	How do you plot learning curves
8515	A confusion matrix is nothing but a table with two dimensions viz. “ Actual” and “Predicted” and furthermore, both the dimensions have “True Positives (TP)”, “True Negatives (TN)”, “False Positives (FP)”, “False Negatives (FN)” as shown below −	What is a confusion matrix in machine learning
628	A loss function is used to optimize the parameter values in a neural network model. Loss functions map a set of parameter values for the network onto a scalar value that indicates how well those parameter accomplish the task the network is intended to do.	What does loss function do in neural network
2067	Algorithms are always unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function.	What are the uses of algorithm
3243	Thus, the t-statistic measures how many standard errors the coefficient is away from zero. Generally, any t-value greater than +2 or less than – 2 is acceptable. The higher the t-value, the greater the confidence we have in the coefficient as a predictor.	What is a good T stat
4078	A confusion matrix, also known as error matrix is a table layout that is used to visualize the performance of a classification model where the true values are already known.	What is confusion matrix in R
1204	The variables used to explain variations in the level of education are called exogenous. More generally, the variables that show differences we wish to explain are called endogenous, while the variables used to explain the differences are called exogenous. Often this goes along with a causal imagery.	What is exogenous variation
2514	To calculate the standard error, follow these steps:Record the number of measurements (n) and calculate the sample mean (μ).  Calculate how much each measurement deviates from the mean (subtract the sample mean from the measurement).Square all the deviations calculated in step 2 and add these together:More items•	How do you calculate standard error of estimate
4150	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is the difference between supervised and unsupervised learning algorithms
5036	Information provides a way to quantify the amount of surprise for an event measured in bits. Entropy provides a measure of the average amount of information needed to represent an event drawn from a probability distribution for a random variable.	What is the significance of entropy in Information Theory
4402	The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map.	What does convolution mean in CNN
2584	To measure test-retest reliability, you conduct the same test on the same group of people at two different points in time. Then you calculate the correlation between the two sets of results.	How do you test retest reliability
1356	Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training.  Unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy.	Does unlabeled data really help in semi supervised learning
3241	To write a null hypothesis, first start by asking a question. Rephrase that question in a form that assumes no relationship between the variables. In other words, assume a treatment has no effect. Write your hypothesis in a way that reflects this.	How do you state a null hypothesis
1683	Abstract. Network representation learning aims to embed the vertexes in a network into low-dimensional dense representations, in which similar vertices in the network should have “close” representations (usually measured by cosine similarity or Euclidean distance of their representations).	What are network representations
1626	A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific.	Can a sampling frame be seen as a population
1196	False positive rate (FPR) is a measure of accuracy for a test: be it a medical diagnostic test, a machine learning model, or something else. In technical terms, the false positive rate is defined as the probability of falsely rejecting the null hypothesis.	What does false positive rate mean
1030	In signal processing, the Fourier transform can reveal important characteristics of a signal, namely, its frequency components. y k + 1 = ∑ j = 0 n - 1 ω j k x j + 1 . ω = e - 2 π i / n is one of n complex roots of unity where i is the imaginary unit. For x and y , the indices j and k range from 0 to n - 1 .	How do you find the Fourier transform of a signal
6947	Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.	In what real world applications is Naive Bayes classifier used
1612	According to Bezdek (1994), Computational Intelligence is a subset of Artificial Intelligence. There are two types of machine intelligence: the artificial one based on hard computing techniques and the computational one based on soft computing methods, which enable adaptation to many situations.	What is computational intelligence and how is it related to AI
347	The F-distribution arises from inferential statistics concerning population variances. More specifically, we use an F-distribution when we are studying the ratio of the variances of two normally distributed populations.	Where does the F distribution come from
3590	The terms 'multivariate analysis' and 'multivariable analysis' are often used interchangeably in medical and health sciences research. However, multivariate analysis refers to the analysis of multiple outcomes whereas multivariable analysis deals with only one outcome each time [1].	What is the difference between multivariate and multivariable analysis
62	The global facial recognition market size was valued at USD 3.4 billion in 2019 and is anticipated to expand at a CAGR of 14.5% from 2020 to 2027. The technology is improving, evolving, and expanding at an explosive rate. Technologies such as biometrics are extensively used in order to enhance security.	What is the current trend in face recognition
988	To tell briefly, LDA imagines a fixed set of topics. Each topic represents a set of words. And the goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics.	How does LDA model work
937	Big Data is defined as data that is huge in size. Bigdata is a term used to describe a collection of data that is huge in size and yet growing exponentially with time. Examples of Big Data generation includes stock exchanges, social media sites, jet engines, etc.	What is big data with examples
1700	“The distinction between white label and private label are subtle,” he writes. “That's why these terms are so easily confused. Private label is a brand sold exclusively in one retailer, for example, Equate (WalMart). White label is a generic product, which is sold to multiple retailers like generic ibuprofen (Advil).”	Whats the difference between private label and white label
2785	Model calibration is the process of adjustment of the model parameters and forcing within the margins of the uncertainties (in model parameters and / or model forcing) to obtain a model representation of the processes of interest that satisfies pre-agreed criteria (Goodness-of-Fit or Cost Function).	What is a calibrated model
4946	The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.	What is word2vec trained on
365	Entropy is a measure of randomness and disorder; high entropy means high disorder and low energy. As chemical reactions reach a state of equilibrium, entropy increases; and as molecules at a high concentration in one place diffuse and spread out, entropy also increases.	What does it mean to have high entropy
1487	"Accuracy refers to how close measurements are to the ""true"" value, while precision refers to how close measurements are to each other."	What is the difference between precision and accuracy
6739	Differs from a true experiment in that the researchers do not have full experimental control.  A quasi-experimental study that has at least one treatment group and one comparison group, but participants have not been randomly assigned to the 2 groups. You just studied 12 terms!	How does a quasi experiment differ from a true experiment quizlet
1276	An example of a nonlinear classifier is kNN.  The decision boundaries of kNN (the double lines in Figure 14.6 ) are locally linear segments, but in general have a complex shape that is not equivalent to a line in 2D or a hyperplane in higher dimensions.	Is kNN a linear classifier
8179	GRU use less training parameters and therefore use less memory, execute faster and train faster than LSTM's whereas LSTM is more accurate on dataset using longer sequence. In short, if sequence is large or accuracy is very critical, please go for LSTM whereas for less memory consumption and faster operation go for GRU.	Why GRU is better than Lstm
6671	When analyzing unstructured data and integrating the information with its structured counterpart, keep the following in mind:Choose the End Goal.  Select Method of Analytics.  Identify All Data Sources.  Evaluate Your Technology.  Get Real-Time Access.  Use Data Lakes.  Clean Up the Data.  Retrieve, Classify and Segment Data.More items•	How do you handle unstructured data
731	R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  After fitting a linear regression model, you need to determine how well the model fits the data.	What is r squared change in regression
7066	Scikit Learn is a new easy-to-use interface for TensorFlow from Google based on the Scikit-learn fit/predict model.	Does Scikit learn use TensorFlow
6097	Generative adversarial networks (GANs) are an exciting recent innovation in machine learning. GANs are generative models: they create new data instances that resemble your training data. For example, GANs can create images that look like photographs of human faces, even though the faces don't belong to any real person.	What is Gan in deep learning
6487	The idea behind dimensional analysis is that if an equation is to make sense, it must be dimensionally consistent, unlike the one above. However it doesn't work always - particularly when we're dealing with relations between more than 2 terms.	What is the intuition behind dimensional analysis
1236	Eigenvectors are a special set of vectors associated with a linear system of equations (i.e., a matrix equation) that are sometimes also known as characteristic vectors, proper vectors, or latent vectors (Marcus and Minc 1988, p.  Each eigenvector is paired with a corresponding so-called eigenvalue.	What are eigenvectors of a matrix
7861	"It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action. Some academic sources reserve the term ""strong AI"" for machines that can experience consciousness."	Does artificial general intelligence exist
4311	Each observation in a time series can be forecast using all previous observations. We call these fitted values and they are denoted by ^yt|t−1 y ^ t | t − 1 , meaning the forecast of yt based on observations y1,…,yt−1 y 1 , … , y t − 1 .	What are fitted values in time series
5105	"All three went to the same coaching institute, Allen, and were part of an elite Special Rankers Group (SRG) of 18 students.  Belief in the ""positive"" effect of stress seemed almost a religion at Allen Jaipur."	What is SRG in Allen
34	For example, if the researcher wanted a sample of 50,000 graduates using age range, the proportionate stratified random sample will be obtained using this formula: (sample size/population size) x stratum size. The table below assumes a population size of 180,000 MBA graduates per year.	How do you find the sample size for a stratified sample
7071	Federated learning enables multiple actors to build a common, robust machine learning model without sharing data, thus allowing to address critical issues such as data privacy, data security, data access rights and access to heterogeneous data.	Why is federated learning
7195	In Gradient Descent (GD), we perform the forward pass using ALL the train data before starting the backpropagation pass to adjust the weights. This is called (one epoch). In Stochastic Gradient Descent (SGD), we perform the forward pass using a SUBSET of the train set followed by backpropagation to adjust the weights.	What is the difference between stochastic gradient descent SGD and gradient descent Gd
3328	To calculate the variance follow these steps: Work out the Mean (the simple average of the numbers) Then for each number: subtract the Mean and square the result (the squared difference). Then work out the average of those squared differences.	How do I calculate the population variance
5442	High Pass RL Filter An inductor, like a capacitor, is a reactive device.  And this is why this circuit is a high-pass filter circuit. Low frequency signals, however, will go through the inductor, because inductors offer very low resistance to low-frequency, or Dc, signals.	Is inductor high pass filter
4352	Variational Bayesian methods are primarily used for two purposes: To provide an analytical approximation to the posterior probability of the unobserved variables, in order to do statistical inference over these variables.	Why do variational inferences occur
802	Systematic sampling involves selecting fixed intervals from the larger population to create the sample. Cluster sampling divides the population into groups, then takes a random sample from each cluster.	What is systematic and cluster sampling
4409	Offline evaluations test the effectiveness of recommender system algorithms on a certain dataset. Online evaluation attempts to evaluate recommender systems by a method called A/B testing where a part of users are served by recommender system A and the another part of users by recommender system B.	What is offline evaluation
7454	Test for Significance of Regression. The test for significance of regression in the case of multiple linear regression analysis is carried out using the analysis of variance. The test is used to check if a linear statistical relationship exists between the response variable and at least one of the predictor variables.	How do you test the significance of a variable in regression
3748	The standard use of “rollout” (also called a “playout”) is in regard to an execution of a policy from the current state when there is some uncertainty about the next state or outcome - it is one simulation from your current state.	What is rollout in reinforcement learning
7925	If μ is the average number of successes occurring in a given time interval or region in the Poisson distribution. Then the mean and the variance of the Poisson distribution are both equal to μ. Remember that, in a Poisson distribution, only one parameter, μ is needed to determine the probability of any given event.	Why mean and variance are same in Poisson distribution
7309	In an upper-tailed test the decision rule has investigators reject H0 if the test statistic is larger than the critical value. In a lower-tailed test the decision rule has investigators reject H0 if the test statistic is smaller than the critical value.	How do you know if its a lower or upper tailed test
7807	Thus, the eigenvalues of a unitary matrix are unimodular, that is, they have norm 1, and hence can be written as eiα e i α for some α. α . U|v⟩=eiλ|v⟩,U|w⟩=eiμ|w⟩.	What are the eigenvalues of a unitary matrix
5573	While most PCs have a single operating system (OS) built-in, it's also possible to run two operating systems on one computer at the same time. The process is known as dual-booting, and it allows users to switch between operating systems depending on the tasks and programs they're working with.	What does dual booting mean
1169	Two determine if two images are rotated versions of each other, one can either exhaustively rotate them in order to find out if the two match up at some angle, or alternatively extract features from the images that can then be compared to make the same decision.	What is rotation invariant in image processing
5251	Gradient Descent is the most common optimization algorithm in machine learning and deep learning.  On each iteration, we update the parameters in the opposite direction of the gradient of the objective function J(w) w.r.t the parameters where the gradient gives the direction of the steepest ascent.	How are the parameters updated during gradient descent process in machine learning
5073	Linear regression is the most basic and commonly used predictive analysis. Regression estimates are used to describe data and to explain the relationship between one dependent variable and one or more independent variables.	Can regression be used for interpretation
718	Uniform quantization may lead to either slope overload distortion or Granular noise.  Thus we go for Non Uniform quantization because step size varies based on the message signal and it will be tracked with minimal amount of error.	What is the need of non uniform quantization
1479	What they are & why they matter. Neural networks are computing systems with interconnected nodes that work much like neurons in the human brain. Using algorithms, they can recognize hidden patterns and correlations in raw data, cluster and classify it, and – over time – continuously learn and improve.	Why are neural networks important
3035	14:3148:20Suggested clip · 105 secondsMod-13 Lec-46 The Adjoint Operator - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the adjoint of a linear operator
5897	Business Analytics. Schools and Partners: ColumbiaX. MicroMasters ® Program (4 courses)Big Data. Schools and Partners: AdelaideX. MicroMasters ® Program (5 courses)Predictive Analytics using. Python. Schools and Partners: EdinburghX. MicroMasters ® Program (5 courses)	Where can I learn predictive analytics
670	The following are common methods:Mean imputation. Simply calculate the mean of the observed values for that variable for all individuals who are non-missing.  Substitution.  Hot deck imputation.  Cold deck imputation.  Regression imputation.  Stochastic regression imputation.  Interpolation and extrapolation.	How do you impute missing values
4231	Ensemble learning combines the predictions from multiple neural network models to reduce the variance of predictions and reduce generalization error. Techniques for ensemble learning can be grouped by the element that is varied, such as training data, the model, and how predictions are combined.	What is ensemble learning When can I use ensemble learning
7986	Generative Adversarial Networks takes up a game-theoretic approach, unlike a conventional neural network. The network learns to generate from a training distribution through a 2-player game. The two entities are Generator and Discriminator. These two adversaries are in constant battle throughout the training process.	How does the generative network work in generative adversarial nets
712	2. Exponential Moving Average (EMA) The other type of moving average is the exponential moving average (EMA), which gives more weight to the most recent price points to make it more responsive to recent data points.	Which moving average is more responsive
5053	The “trick” is that kernel methods represent the data only through a set of pairwise similarity comparisons between the original data observations x (with the original coordinates in the lower dimensional space), instead of explicitly applying the transformations ϕ(x) and representing the data by these transformed	How does the kernel trick work
33	In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data.	What is map in ML
219	Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function.  You start by defining the initial parameter's values and from there gradient descent uses calculus to iteratively adjust the values so they minimize the given cost-function.	How does gradient descent work
831	Random sampling ensures that results obtained from your sample should approximate what would have been obtained if the entire population had been measured (Shadish et al., 2002). The simplest random sample allows all the units in the population to have an equal chance of being selected.	Why is random sampling used
5635	If at the limit n → ∞ the estimator tend to be always right (or at least arbitrarily close to the target), it is said to be consistent. This notion is equivalent to convergence in probability defined below.	How do you know if an estimator is consistent
4521	Tests of Correlation: The validity of a test is measured by the strength of association, or correlation, between the results obtained by the test and by the criterion measure.	How do you measure validity in statistics
88	A permutation test (also called a randomization test, re-randomization test, or an exact test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under all possible rearrangements of	How does a permutation test construct a distribution
2917	How do you create a decision tree?Start with your overarching objective/“big decision” at the top (root)  Draw your arrows.  Attach leaf nodes at the end of your branches.  Determine the odds of success of each decision point.  Evaluate risk vs reward.	How do you make a decision tree
110	The statistics are presented in a definite form so they also help in condensing the data into important figures. So statistical methods present meaningful information. In other words statistics helps in simplifying complex data to simple-to make them understandable.	What is statistics and its function
634	Neural style transferTable of contents.Setup. Import and configure modules.Visualize the input.Fast Style Transfer using TF-Hub.Define content and style representations.Build the model.Calculate style.Extract style and content.More items	What are the steps in machine learning for a neural style transfer
4136	The k-means clustering algorithm is one of the most widely used, effective, and best understood clustering methods.  In this paper we propose a supervised learning approach to finding a similarity measure so that k-means provides the desired clusterings for the task at hand.	Is K means clustering used for supervised machine learning
1637	Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.	What is gradient in gradient descent
7968	The One-Class SVM A One-Class Support Vector Machine is an unsupervised learning algorithm that is trained only on the 'normal' data, in our case the negative examples. It learns the boundaries of these points and is therefore able to classify any points that lie outside the boundary as, you guessed it, outliers.	Is one class SVM unsupervised
8310	"""A Bayesian network is a probabilistic graphical model which represents a set of variables and their conditional dependencies using a directed acyclic graph.""  It is also called a Bayes network, belief network, decision network, or Bayesian model."	What is a Bayesian network and how does it relate to AI
2513	If you select View/Residual Diagnostics/Correlogram-Q-statistics on the equation toolbar, EViews will display the autocorrelation and partial autocorrelation functions of the residuals, together with the Ljung-Box Q-statistics for high-order serial correlation.	How does EViews detect autocorrelation
187	Nonresponse bias occurs when some respondents included in the sample do not respond. The key difference here is that the error comes from an absence of respondents instead of the collection of erroneous data.  Most often, this form of bias is created by refusals to participate or the inability to reach some respondents.	How does non response cause bias
3625	A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers.	What is a discrete random variable
3112	The most important difference between deep learning and traditional machine learning is its performance as the scale of data increases. When the data is small, deep learning algorithms don't perform that well. This is because deep learning algorithms need a large amount of data to understand it perfectly.	Which of the following is true with regards to classical machine learning vs deep learning
2689	Stratified random sampling is a method of sampling that involves the division of a population into smaller sub-groups known as strata. In stratified random sampling, or stratification, the strata are formed based on members' shared attributes or characteristics such as income or educational attainment.	What are the basic criteria of constructing strata in stratified sampling
1140	Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.  Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.	What is Optimizer neural network
2710	Character N-grams (of at least 3 characters) that are common to words meaning “transport” in the same texts sample in French, Spanish and Greek and their respective frequency.	What is character N grams
1102	“Bias in AI” refers to situations where machine learning-based data analytics systems discriminate against particular groups of people. This discrimination usually follows our own societal biases regarding race, gender, biological sex, nationality, or age (more on this later).	What is bias in AI
2056	The “regular” normal distribution has one random variable; A bivariate normal distribution is made up of two independent random variables. The two variables in a bivariate normal are both are normally distributed, and they have a normal distribution when both are added together.	What does bivariate normal distribution mean
1747	The key difference between time series and panel data is that time series focuses on a single individual at multiple time intervals while panel data (or longitudinal data) focuses on multiple individuals at multiple time intervals.  Fields such as Econometrics and statistics relies on data.	What is the difference between time series data and panel data
7031	The optimal number of clusters can be defined as follow: Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters. For each k, calculate the total within-cluster sum of square (wss).	What method can be used to determine the optimal number of clusters
1367	A method for the off-line recognition of cursive handwriting based on hidden Markov models (HMMs) is described. The features used in the HMMs are based on the arcs of skeleton graphs of the words to be recognized. An algorithm is applied to the skeleton graph of a word that extracts the edges in a particular order.	How can hidden Markov models be used to recognize cursive handwriting
5601	The population median is the value of the 50th percentile of some variable for all the members of the population. When members of the population are sorted by this value, the median is the middle value.	How do you calculate the median of a population
1799	Quantization is the concept that a physical quantity can have only certain discrete values.  For example, matter is quantized because it is composed of individual particles that cannot be subdivided; it is not possible to have half an electron. Also, the energy levels of electrons in atoms are quantized.	What does it mean if something is quantized
5336	Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data.	What is transfer learning in deep learning
2764	Scientists use observation to collect and record data, which enables them to construct and then test hypotheses and theories. Scientists observe in many ways – with their own senses or with tools such as microscopes, scanners or transmitters to extend their vision or hearing.	What is the role of observation
5888	Normalization should have no impact on the performance of a decision tree. It is generally useful, when you are solving a system of equations, least squares, etc, where you can have serious issues due to rounding errors.	Do you need to normalize data for decision tree
636	If you are studying one group, use a paired t-test to compare the group mean over time or after an intervention, or use a one-sample t-test to compare the group mean to a standard value. If you are studying two groups, use a two-sample t-test. If you want to know only whether a difference exists, use a two-tailed test.	How do you know what t test to use
3576	Nonetheless, they are not the same. Standard deviation is used to measure the spread of data around the mean, while RMSE is used to measure distance between some values and prediction for those values.  If you use mean as your prediction for all the cases, then RMSE and SD will be exactly the same.	Is root mean square error the same as standard deviation
6979	Agents can be grouped into four classes based on their degree of perceived intelligence and capability :Simple Reflex Agents.Model-Based Reflex Agents.Goal-Based Agents.Utility-Based Agents.Learning Agent.	What are the different types of agents in artificial intelligence
3063	Image annotation for deep learning is mainly done for object detection with more precision. 3D Cuboid Annotation, Semantic Segmentation, and polygon annotation are used to annotate the images using the right tool to make the objects well-defined in the image for neural network analysis in deep learning.	How do you annotate pictures for deep learning
3465	5:5217:59Suggested clip · 112 secondsHow to Use SPSS-Hierarchical Multiple Regression - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you control for a variable in SPSS regression
8293	Dual-booting enables you to go from a powered-off state to a menu from which you can choose which operating system to load. This menu may have one, two, or even more options, and each choice loads the environment, drivers, and system necessary for the selected option.	What is dual booting and its benefits
867	For omitted variable bias to occur, the following two conditions must exist:The omitted variable must correlate with the dependent variable.The omitted variable must correlate with at least one independent variable that is in the regression model.	How do you address omitted variable bias
2081	Latent class growth analysis (LCGA) is a special type of GMM, whereby the variance and covariance estimates for the growth factors within each class are assumed to be fixed to zero. By this assumption, all individual growth trajectories within a class are homogeneous.	What is Latent class growth analysis
1519	"Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one ""raw"" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics."	What is data wrangling in machine learning
284	Noisy data is a data that has relatively signal-to-noise ratio.  This error is referred to as noise. Noise creates trouble for machine learning algorithms because if not trained properly, algorithms can think of noise to be a pattern and can start generalizing from it, which of course is undesirable.	What is noise in data in machine learning
2365	The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.	How do you interpret precision and recall curve
4337	Key TakeawaysΔ=b2−4ac Δ = b 2 − 4 a c is the formula for a quadratic function 's discriminant.If Δ is greater than zero, the polynomial has two real, distinct roots.If Δ is equal to zero, the polynomial has only one real root.If Δ is less than zero, the polynomial has no real roots, only two distinct complex roots.More items	How do you find the Delta in a quadratic equation
8205	The generalized delta rule is a mathematically derived formula used to determine how to update a neural network during a (back propagation) training step.  A set number of input and output pairs are presented repeatedly, in random order during the training.	What is generalized delta rule
7718	Robust regression is an alternative to least squares regression when data is contaminated with outliers or influential observations and it can also be used for the purpose of detecting influential observations.	Why do we use robust regression
8400	still images efficiently	What does optimize stills mean
2813	Log-likelihood is all your data run through the pdf of the likelihood (logistic function), the logarithm taken for each value, and then they are summed together.	What does log likelihood mean
601	"The correct interpretation of a 95% confidence interval is that ""we are 95% confident that the population parameter is between X and X."""	How do you interpret a confidence interval
3463	Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and	Where does the hidden Markov model is used *
3355	Achieving translation invariance in Convolutional NNs: Then the max pooling layer takes the output from the convolutional layer and reduces its resolution and complexity. It does so by outputting only the max value from a grid.So the information about the exact position of the max value in the grid is discarded.	How exactly does max pooling create translation invariance
942	The subdistribution hazard function, introduced by Fine and Gray, for a given type of event is defined as the instantaneous rate of occurrence of the given type of event in subjects who have not yet experienced an event of that type.	What is a Subdistribution hazard ratio
5949	Then we will propose a generalization to nonlinear models and also multiclass classification. In the case of multiclass classification, a typically used loss function is the Hard Loss Function [29, 36, 61], which counts the number of misclassifications: ℓ(f, z) = ℓH(f, z) = [f(x)≠y].	What function is used for multi class classification
2326	A 78 is pretty good, but the Navy goes by your line scores and not the overall AFQT. Get your line scores from your recruiter and see what you already qualify for.  Just know that it's your most recent ASVAB score that counts, so if you do worse you're stuck with that score.	Is a 78 A good Asvab score
3793	Independent EventsTwo events A and B are said to be independent if the fact that one event has occurred does not affect the probability that the other event will occur.If whether or not one event occurs does affect the probability that the other event will occur, then the two events are said to be dependent.	How do you tell if an event is independent or dependent
499	A dummy variable (binary variable) D is a variable that takes on the value 0 or 1. • Examples: EU member (D = 1 if EU member, 0 otherwise), brand (D = 1 if product has a particular brand, 0 otherwise), gender (D = 1 if male, 0 otherwise)	What is dummy variable give an example
6705	Introduction to Poisson Regression Poisson regression is also a type of GLM model where the random component is specified by the Poisson distribution of the response variable which is a count. When all explanatory variables are discrete, log-linear model is equivalent to poisson regression model.	What is Poisson regression model
5553	Bad Sampling. The data can be misleading due to the sampling method used to obtain data. For instance, the size and the type of sample used in any statistics play a significant role — many polls and questionnaires target certain audiences that provide specific answers, resulting in small and biased sample sizes.	How can statistics be mislead
2374	The Convolutional Recurrent Neural Networks is the combination of two of the most prominent neural networks. The CRNN (convolutional recurrent neural network) involves CNN(convolutional neural network) followed by the RNN(Recurrent neural networks).	What is convolutional recurrent neural network
4203	Mathematically test efficiency is calculated as a percentage of the number of alpha testing (in-house or on-site) defects divided by sum of a number of alpha testing and a number of beta testing (off-site) defects.	How do you calculate the effectiveness of a test case
2160	Mean and Variance of a Binomial Distribution The variance of a Binomial Variable is always less than its mean. ∴ npq<np. For Maximum Variance: p=q=0.5 and σmax = n/4.	What is the maximum value of the variance of binomial distribution
7747	A layer groups a number of neurons together. It is used for holding a collection of neurons. There will always be an input and output layer. We can have zero or more hidden layers in a neural network. The learning process of a neural network is performed with the layers.	What is a layer in a neural network
446	Email messages are a good example. While the actual content is unstructured, it does contain structured data such as name and email address of sender and recipient, time sent, etc. Another example is a digital photograph.	Is email structured or unstructured data
3944	Proof. If X and Y are independent then you need only take g(x) = fX(x) and h(y) = fY (y). Note When fX,Y (x,y) = g(x)h(y) for all x,y you can easily write down the marginal p.d.f.'s. h(y) for a suitable choice of C.	How do you prove that X and Y is independent
636	Unlike the log odds ratio, the odds ratio is always positive. A value of 1 indicates no change. Values between 0 and less than 1 indicate a decrease in the probability of the outcome event. Values greater than 1 indicate an increase in the probability of the outcome event.	How do you interpret odds ratios
8532	datasets Which of the following function is used for loading famous iris dataset from sklearn. datasets? load_iris() Which of the following expressions can access the features of the iris dataset, shown in the below expression? from sklearn import datasets iris = datasets. load_iris() iris.	Which of the following function is used for loading famous Iris dataset from Sklearn datasets
3428	Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease. The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.	How do you interpret AUC ROC curve
6179	One of the key methodologies to improve efficiency in computational intensive tasks is to reduce the dimensions after ensuring most of the key information is maintained. It also eliminates features with strong correlation between them and reduces over-fitting.	When would you reduce dimensions in your data in machine learning
1221	The Sarsa algorithm is an On-Policy algorithm for TD-Learning. The major difference between it and Q-Learning, is that the maximum reward for the next state is not necessarily used for updating the Q-values.	What is the difference between Sarsa and Q learning
2541	Deep belief networks solve this problem by using an extra step called “pre-training”. Pre-training is done before backpropagation and can lead to an error rate not far from optimal. This puts us in the “neighborhood” of the final solution. Then we use backpropagation to slowly reduce the error rate from there.	How does Deep Belief Network solve the vanishing gradient problem
3145	KNN is an algorithm that is useful for matching a point with its closest k neighbors in a multi-dimensional space. It can be used for data that are continuous, discrete, ordinal and categorical which makes it particularly useful for dealing with all kind of missing data.	Can Knn be used for categorical variables
6649	In computer science and engineering, a test vector is a set of inputs provided to a system in order to test that system. In software development, test vectors are a methodology of software testing and software verification and validation.	What is input vector
1565	"""A discrete variable is one that can take on finitely many, or countably infinitely many values"", whereas a continuous random variable is one that is not discrete, i.e. ""can take on uncountably infinitely many values"", such as a spectrum of real numbers."	What is the difference between continuous and discrete random variables
930	A commonly used rule says that a data point is an outlier if it is more than 1.5 ⋅ IQR 1.5\cdot \text{IQR} 1. 5⋅IQR1, point, 5, dot, start text, I, Q, R, end text above the third quartile or below the first quartile. Said differently, low outliers are below Q 1 − 1.5 ⋅ IQR \text{Q}_1-1.5\cdot\text{IQR} Q1−1.	How do you identify outliers
851	This assumption may be checked by looking at a histogram or a Q-Q-Plot. Normality can also be checked with a goodness of fit test (e.g., the Kolmogorov-Smirnov test), though this test must be conducted on the residuals themselves. Third, multiple linear regression assumes that there is no multicollinearity in the data.	How do you check the linearity assumption in multiple regression
8109	When two or more random variables are defined on a probability space, it is useful to describe how they vary together; that is, it is useful to measure the relationship between the variables. A common measure of the relationship between two random variables is the covariance.	Why is joint probability distribution useful
4508	There is no non-parametric form of any regression. Regression means you are assuming that a particular parameterized model generated your data, and trying to find the parameters. Non-parametric tests are test that make no assumptions about the model that generated your data.	What is the non parametric equivalent of the linear regression
8056	An ARMA model, or Autoregressive Moving Average model, is used to describe weakly stationary stochastic time series in terms of two polynomials. The first of these polynomials is for autoregression, the second for the moving average.	Why do we use ARMA model
1803	There are three big-picture methods to understand if a continuous and categorical are significantly correlated — point biserial correlation, logistic regression, and Kruskal Wallis H Test. The point biserial correlation coefficient is a special case of Pearson's correlation coefficient.	How do you check association between categorical and continuous variables
389	Yes, you do need to scale the target variable. I will quote this reference: A target variable with a large spread of values, in turn, may result in large error gradient values causing weight values to change dramatically, making the learning process unstable.	Should I normalize target variable
2338	z = (x – μ) / σ For example, let's say you have a test score of 190. The test has a mean (μ) of 150 and a standard deviation (σ) of 25. Assuming a normal distribution, your z score would be: z = (x – μ) / σ	What is the predefined function to calculate Z statistic value
1121	10 Ways to Improve Transfer of Learning.  Focus on the relevance of what you're learning.  Take time to reflect and self-explain.  Use a variety of learning media.  Change things up as often as possible.  Identify any gaps in your knowledge.  Establish clear learning goals.  Practise generalising.More items•	How can we improve transfer learning
8612	In multi-agent simulation systems the MAS is used as a model to simulate some real-world domain. Typical use is in domains involving many different components, interacting in diverse and complex ways and where the system-level properties are not readily inferred from the properties of the components.	What is multi agent simulation
1462	Linear relationships can be either positive or negative. Positive relationships have points that incline upwards to the right. As x values increase, y values increase. As x values decrease, y values decrease.	What does a positive linear relationship between X and Y in a simple regression imply
8110	The Wilcoxon signed rank test is a nonparametric test that compares the median of a set of numbers against a hypothetical median. The Wilcoxon rank sum test is a nonparametric test to compare two unmatched groups. It is equivalent to the Mann-Whitney test. The Gehan-Wilcoxon test is a method to compare survival curves.	What is the difference between Wilcoxon rank sum test and Wilcoxon signed rank test
7605	Deep learning or hierarchical learning is the part of machine learning which mainly follows the widely used concepts of a neural network.  In this paper, we have used the concept of deep recurrent neural network (Deep-RNN) to train the model for a classification task.	What is deep recurrent neural network
6835	The splitting criteria for CART is MSE(mean squared error). Suppose we are doing a binary tree. the algorithm first will pick a value, and split the data into two subset. For each subset, it will calculate the , and calculate the MSE for each set separately.	What are the splitting criteria for a regression tree
3933	SYNONYMS FOR outlier 2 nonconformist, maverick; original, eccentric, bohemian; dissident, dissenter, iconoclast, heretic; outsider.	What is another word for outlier
1411	In statistics, a confounder (also confounding variable, confounding factor, or lurking variable) is a variable that influences both the dependent variable and independent variable, causing a spurious association. Confounding is a causal concept, and as such, cannot be described in terms of correlations or associations.	What is confounders in statistics
6051	matrix: A rectangular arrangement of numbers or terms having various uses such as transforming coordinates in geometry, solving systems of linear equations in linear algebra and representing graphs in graph theory.	What does a matrix represent in linear algebra
4538	Simple random sampling methods From this population, researchers choose random samples using two ways: random number tables and random number generator software.	What are the two methods of taking simple random samples
8420	In addition, another reason to not initialize everything to zero is so that you get different answers. Some optimization techniques are deterministic, so if you initialize randomly, you'll get different answers each time you run it. This helps you explore the space better and avoid (other) local optima.	Why is zero initialization not a recommended weight initialization technique
1343	Implementing time series ARIMABrief description about ARMA, ARIMA:Step-by-step general approach of implementing ARIMA:Step 1: Load the dataset and plot the source data. (  Step 2: Apply the Augmented Dickey Fuller Test (to confirm the stationarity of data)Step 3: Run ETS Decomposition on data (To check the seasonality in data)More items•	How is Arima model implemented
4842	Formal Definition of Sufficient Statistics More formally, a statistic Y is said to be a sufficient estimator for some parameter θ if the conditional distribution of Y: T(X1, X2,…,Xn) doesn't depend on θ.	How do you calculate sufficient estimator
6557	The concept of an abstract data type might be hard for some people to grasp, but it's really not that difficult. It does present a different way to view and act upon the data elements of your programs but once you learn the basics it's not too bad.  One should not feel superior if they know data structure well.	Is it hard to learn data structures and algorithms
2889	In spite of being linear, the Fourier transform is not shift invariant. In other words, a shift in the time domain does not correspond to a shift in the frequency domain.	Is Fourier transform shift invariant
1229	"So ground truth can help fully identify objects in satellite photos. ""Ground truth"" means a set of measurements that is known to be much more accurate than measurements from the system you are testing. For example, suppose you are testing a stereo vision system to see how well it can estimate 3D positions."	What is ground truth in object detection
1375	Boltzmann learning is statistical in nature, and is derived from the field of thermodynamics. It is similar to error-correction learning and is used during supervised training.  Neural networks that use Boltzmann learning are called Boltzmann machines.	What is Boltzmann learning
1012	The exponential distribution is often used to model the longevity of an electrical or mechanical device. In Example, the lifetime of a certain computer part has the exponential distribution with a mean of ten years (X∼Exp(0.1)).	When would you use an exponential distribution
6522	In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer.	What does it mean if a distribution is positively skewed
7358	3.2 How to test for differences between samplesDecide on a hypothesis to test, often called the “null hypothesis” (H0 ). In our case, the hypothesis is that there is no difference between sets of samples.  Decide on a statistic to test the truth of the null hypothesis.Calculate the statistic.Compare it to a reference value to establish significance, the P-value.	How do you know if two samples are significantly different
6369	This variance represents what the regression line cannot predict. It's equal to the sum of squared deviations of data points around predicted points, divided by N minus two. N is the number of data points in the scatterplot. Regression variance is based on differences between predicted data points and the mean of Y.	How do you find the variance of a regression model
2646	To find the interquartile range (IQR), ​first find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1.	How do you find the interquartile range
1908	The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities .  The above statement is the general representation of the Bayes rule.	What is the difference between the Bayes theorem and conditional probability
931	SVM could be considered as a linear classifier, because it uses one or several hyperplanes as well as nonlinear with a kernel function (Gaussian or radial basis in BCI applications).	Is SVM a non linear classifier
8193	Tensor Processing UnitDesignerGoogleIntroducedMay 2016TypeNeural network Machine learning	Who makes tensor processing units
5262	The remember vector is usually called the forget gate. The output of the forget gate tells the cell state which information to forget by multiplying 0 to a position in the matrix. If the output of the forget gate is 1, the information is kept in the cell state.	What is true about the forget gate in Lstm
1066	How to optimize your meta tags: A checklistCheck whether all your pages and your content have title tags and meta descriptions.Start paying more attention to your headings and how you structure your content.Don't forget to mark up your images with alt text.More items•	How do you optimize meta tags
1177	Measuring the Accuracy of a Test The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives).  The true positive rate (TPR, also called sensitivity) is calculated as TP/TP+FN.	What do you understand by true positive rate and false positive rate
1447	Similarity Measure Numerical measure of how alike two data objects often fall between 0 (no similarity) and 1 (complete similarity) Dissimilarity Measure Numerical measure of how different two data objects are range from 0 (objects are alike) to (objects are different)	What is similarity and dissimilarity
4903	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is difference between supervised and unsupervised machine learning
1883	Probability is the study of random events. It is used in analyzing games of chance, genetics, weather prediction, and a myriad of other everyday events. Statistics is the mathematics we use to collect, organize, and interpret numerical data.	What is statistics and probability
722	In statistics, standardization is the process of putting different variables on the same scale. This process allows you to compare scores between different types of variables. Typically, to standardize variables, you calculate the mean and standard deviation for a variable.	What happens when you standardize a variable
6717	Estimation is a division of statistics and signal processing that determines the values of parameters through measured and observed empirical data. The process of estimation is carried out in order to measure and diagnose the true value of a function or a particular set of populations.	Why is estimation important in statistics
24	In simple terms, a quantile is where a sample is divided into equal-sized, adjacent, subgroups (that's why it's sometimes called a “fractile“).  The median cuts a distribution into two equal areas and so it is sometimes called 2-quantile. Quartiles are also quantiles; they divide the distribution into four equal parts.	How is quantile calculated
1170	Precision and Recall. Precision talks about all the correct predictions out of total positive predictions. Recall means how many individuals were classified correctly out of all the actual positive individuals.	What is precision and recall in logistic regression
7572	Other ways of avoiding experimenter's bias include standardizing methods and procedures to minimize differences in experimenter-subject interactions; using blinded observers or confederates as assistants, further distancing the experimenter from the subjects; and separating the roles of investigator and experimenter.	How do you get rid of experimenter bias
8530	The probability mass function of the negative binomial distribution is. where r is the number of successes, k is the number of failures, and p is the probability of success.	What is K in negative binomial distribution
2177	The number of units is a parameter in the LSTM, referring to the dimensionality of the hidden state and dimensionality of the output state (they must be equal). a LSTM comprises an entire layer.	What is unit in Lstm
984	The traditional problems (or goals) of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals.	What are the goals techniques and progress of AI
2455	Multiply the Grand total by the Pretest probability to get the Total with disease. Compute the Total without disease by subtraction. Multiply the Total with disease by the Sensitivity to get the number of True positives. Multiply the Total without disease by the Specificity to get the number of True Negatives.	How do you find true negatives
1340	A simple linear regression plot for amount of rainfall. Regression analysis is used in stats to find trends in data. For example, you might guess that there's a connection between how much you eat and how much you weigh; regression analysis can help you quantify that.	What is regression analysis example
404	Sampling error is the difference between a population parameter and a sample statistic used to estimate it. For example, the difference between a population mean and a sample mean is sampling error. Sampling error occurs because a portion, and not the entire population, is surveyed.…	What is sampling error with example
6337	In a crossover network, resistors are usually used in combination with other components to control either impedance magnitudes or the relative levels between different drivers in a system.	What does a resistor do in a crossover
2186	In a normal distribution the mean is zero and the standard deviation is 1. It has zero skew and a kurtosis of 3. Normal distributions are symmetrical, but not all symmetrical distributions are normal.	Where is the mean on a normal curve
2114	Definition: Two events, A and B, are independent if the fact that A occurs does not affect the probability of B occurring. Some other examples of independent events are: Landing on heads after tossing a coin AND rolling a 5 on a single 6-sided die. Choosing a marble from a jar AND landing on heads after tossing a coin.	What is a real world example of two independent events
4878	The Q-learning algorithm ProcessStep 1: Initialize Q-values.  Step 2: For life (or until learning is stopped)  Step 3: Choose an action.  Steps 4–5: Evaluate!  Step 1: We init our Q-table.Step 2: Choose an action.  Steps 4–5: Update the Q-function.	How is Q learning implemented
1471	A learning curve is a plot of model learning performance over experience or time. Learning curves are a widely used diagnostic tool in machine learning for algorithms that learn from a training dataset incrementally.  Learning curves are plots that show changes in learning performance over time in terms of experience.	What is a learning curve in machine learning
5617	A permutation test5 is used to determine the statistical significance of a model by computing a test statistic on the dataset and then for many random permutations of that data. If the model is significant, the original test statistic value should lie at one of the tails of the null hypothesis distribution.	What is a permutation test used for
80	4 neurons	How many neurons are in the input layer
6856	The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.  Due to this reason, during the backpropogation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated.	Why is ReLU the best activation function
8311	Discriminant analysis is statistical technique used to classify observations into non-overlapping groups, based on scores on one or more quantitative predictor variables. For example, a doctor could perform a discriminant analysis to identify patients at high or low risk for stroke.	What is a discriminant analysis in statistics
760	Multiple regression formula is used in the analysis of relationship between dependent and multiple independent variables and formula is represented by the equation Y is equal to a plus bX1 plus cX2 plus dX3 plus E where Y is dependent variable, X1, X2, X3 are independent variables, a is intercept, b, c, d are slopes,	What is the formula for multiple regression
1451	The Dirichlet is the multivariate generalization of the beta distribution.  The Dirichlet equals the uniform distribution when all parameters (α1… αk) are equal. The Dirichlet distribution is a conjugate prior to the categorical distribution and multinomial distributions. A compound variant is the Dirichlet-multinomial.	What does Dirichlet mean
4751	Hyperparameter optimization is a big part of deep learning. The reason is that neural networks are notoriously difficult to configure and there are a lot of parameters that need to be set. On top of that, individual models can be very slow to train.	Why do we need to do Hyperparameter tuning in neural networks
5283	Now, let the random variable X represent the number of Heads that result from this experiment. The random variable X can only take on the values 0, 1, or 2, so it is a discrete random variable.	Is the random variable X discrete or continuous
262	SummaryWeighted Mean: A mean where some values contribute more than others.When the weights add to 1: just multiply each weight by the matching value and sum it all up.Otherwise, multiply each weight w by its matching value x, sum that all up, and divide by the sum of weights: Weighted Mean = ΣwxΣw.	How do you work out the weighted mean
1310	"The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  With stratified random sampling, these breaks may not exist*, so you divide your target population into groups (more formally called ""strata"")."	What is the difference between stratified random sampling and cluster sampling
1670	Bootstrap is a potent front-end framework used to create modern websites and web apps. It's open-source and free to use, yet features numerous HTML and CSS templates for UI interface elements such as buttons and forms. Bootstrap also supports JavaScript extensions.	What is bootstrap and why it is used
1983	Canonical Correspondence Analysis (CCA) has been developed to allow ecologists to relate the abundance of species to environmental variables (Ter Braak, 1986). However, this method can be used in other domains. A table Y of descriptive variables that are measured on the same sites.	Why is cannonical correspondence analysis cannonical
685	The random (or precision) error for this data point is defined as the reading minus the average of readings, or -1.20 - (-1.42) = 0.22oC. Thus, the maximum absolute value of random error is 0.22oC. You can verify that the magnitude of the random error for any of the other data points is less than this.	How do you find the maximum random error
6061	The batch size limits the number of samples to be shown to the network before a weight update can be performed. This same limitation is then imposed when making predictions with the fit model. Specifically, the batch size used when fitting your model controls how many predictions you must make at a time.	What is batch size Lstm
552	Due to its mathematical complexity, the theoretical foundations of neural network are not covered. However, the universal approximation theorem (and the tools used in its proof) give a very deep insight into why neural networks are so powerful, and it even lays the groundwork for engineering novel architectures.	Why are neural networks so powerful
91	In layman terms, vectors have magnitude and direction and follow the laws of vector addition.  The only difference is that tensor is the generalized form of scalars and vectors . Means scalars and vectors are the special cases of tensor quantities. Scalar is a tensor of rank 0 and vector is a tensor of rank 1.	What is the difference between a tensor and a vector
226	“Machine learning is essentially a form of applied statistics” “Machine learning is glorified statistics” “Machine learning is statistics scaled up to big data” “The short answer is that there is no difference”	Is machine learning just glorified statistics
622	Cost function(J) of Linear Regression is the Root Mean Squared Error (RMSE) between predicted y value (pred) and true y value (y). Gradient Descent: To update θ1 and θ2 values in order to reduce Cost function (minimizing RMSE value) and achieving the best fit line the model uses Gradient Descent.	What is the cost function in linear regression
4151	Deep learning neural networks are trained using the stochastic gradient descent optimization algorithm. The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.	What is learning rate in deep learning
6424	The two types of growth curves that are most common are logarithmic growth curves and exponential growth curves. Essentially, they are the opposite of each other. I'll start by explaining and exponential growth curve as that is the one people are typically more familiar with.	What are the 2 types of growth curves
441	Nonlinear correlation can be detected by maximal local correlation (M = 0.93, p = 0.007), but not by Pearson correlation (C = –0.08, p = 0.88) between genes Pla2g7 and Pcp2 (i.e., between two columns of the distance matrix). Pla2g7 and Pcp2 are negatively correlated when their transformed levels are both less than 5.	How do you know if a correlation is non linear
5375	Recall that in order for a neural networks to learn, weights associated with neuron connections must be updated after forward passes of data through the network. These weights are adjusted to help reconcile the differences between the actual and predicted outcomes for subsequent forward passes.	How are weights updated in neural network
6316	By simple definition, in classification/clustering analyze a set of data and generate a set of grouping rules which can be used to classify future data.  Classification is a data mining (machine learning) technique used to predict group membership for data instances.	What is classification rule in data mining
1291	The standard score (more commonly referred to as a z-score) is a very useful statistic because it (a) allows us to calculate the probability of a score occurring within our normal distribution and (b) enables us to compare two scores that are from different normal distributions.	In what setting are z scores useful
1257	The main difference between the two types of models is that path analysis assumes that all variables are measured without error. SEM uses latent variables to account for measurement error.	What is the difference between latent variable models and structural equation models
5056	Class limits specify the span of data values that fall within a class. Class boundaries are possible data values. Class boundaries are not possible data values.	What is the difference between class limits and class boundaries in statistics
1420	Let's GO!Step 0 : Pre-requisites. It is recommended that before jumping on to Deep Learning, you should know the basics of Machine Learning.  Step 1 : Setup your Machine.  Step 2 : A Shallow Dive.  Step 3 : Choose your own Adventure!  Step 4 : Deep Dive into Deep Learning.  27 Comments.	How do I start learning deep learning
4680	Here are some important considerations while choosing an algorithm.Size of the training data. It is usually recommended to gather a good amount of data to get reliable predictions.  Accuracy and/or Interpretability of the output.  Speed or Training time.  Linearity.  Number of features.	How do I know which machine learning algorithm to use
1251	The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable.	Why is sigmoid function used
5670	Efficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter. Simplicity: ReLu is simple.	How does ReLU differ from the sigmoid activation function
6155	The main advantage of supervised learning is that it allows you to collect data or produce a data output from the previous experience. The drawback of this model is that decision boundary might be overstrained if your training set doesn't have examples that you want to have in a class.	What are the advantages and disadvantages of a supervised learning machine
5347	The reason dividing by n-1 corrects the bias is because we are using the sample mean, instead of the population mean, to calculate the variance. Since the sample mean is based on the data, it will get drawn toward the center of mass for the data.	Why does dividing by n 1 instead of n remove the bias when calculating sample variance
1279	The alpha state of mind is when you reach a very relaxed state while awake. Your brain begins to emit alpha waves instead of beta, which is what you emit when you're fully awake.	What is the alpha state of mind
3941	Since medical tests can't be absolutely true, false positive and false negative are two problems we have to deal with. A false positive can lead to unnecessary treatment and a false negative can lead to a false diagnostic, which is very serious since a disease has been ignored.	Which is more dangerous false positives or false negatives
6962	Statistical Analysis The root mean square error (RMSE), which is the sample standard deviation of the differences between predicted and observed values, with results in the same unit of measure of observed values.  the correlation coefficient (r) as a measure of the degree of association among data.	Correlation coefficient or RMSE
2120	Neurons can only be seen using a microscope and can be split into three parts: Soma (cell body) — this portion of the neuron receives information. It contains the cell's nucleus.	Can neurons be seen
1416	Z-scores are also known as standardized scores; they are scores (or data values) that have been given a common standard. This standard is a mean of zero and a standard deviation of 1. Contrary to what many people believe, z-scores are not necessarily normally distributed.	Are standardized scores and Z scores the same thing
1071	Cluster sampling is a sampling plan used when mutually homogeneous yet internally heterogeneous groupings are evident in a statistical population.  In this sampling plan, the total population is divided into these groups (known as clusters) and a simple random sample of the groups is selected.	Is cluster sampling a simple random sample
7453	A Markov model is a stochastic model which describes a sequence of possible events (states) in which the probability of each event depends on a subset of previous events [1].  This report will focus on First-Order Markov Chains, in which the probability of a future state depends only on the current state [1].	What is a first order Markov model
802	In data science, association rules are used to find correlations and co-occurrences between data sets. They are ideally used to explain patterns in data from seemingly independent information repositories, such as relational databases and transactional databases.	What is applicability of association rules
8211	It is used to determine the extent to which there is a linear relationship between a dependent variable and one or more independent variables.  In simple linear regression a single independent variable is used to predict the value of a dependent variable.	What does it mean when a simple linear regression model is statistically useful
1301	"Systematic sampling is popular with researchers because of its simplicity. Researchers generally assume the results are representative of most normal populations, unless a random characteristic disproportionately exists with every ""nth"" data sample (which is unlikely)."	Why would you use systematic sampling
189	One of the stages that SIFT uses is to create a pyramid of scales of the image.  The feature detector then works by finding features that have a peak response not only in the image space, but in scale space too. This means that it finds the scale of the image which the feature will produce the highest response.	Why are SIFT descriptors scale invariant
6756	A random variable is a variable whose value is unknown or a function that assigns values to each of an experiment's outcomes.  Random variables are often used in econometric or regression analysis to determine statistical relationships among one another.	What is meant by a random variable
6921	The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.  The “training” data set is the general term for the samples used to create the model, while the “test” or “validation” data set is used to qualify performance.	What is the difference between running cross validation and testing training
1978	Convergence is a term mathematically most common in the study of series and sequences. A model is said to converge when the series s(n)=losswn(ˆy,y) (Where wn is the set of weights after the n'th iteration of back-propagation and s(n) is the n'th term of the series) is a converging series.	What is convergence in the context of Machine Learning
6142	Postprocessing procedures usually include various pruning routines, rule quality processing, rule filtering, rule combination, model combination, or even knowledge integration. All these procedures provide a kind of symbolic filter for noisy, imprecise, or non-user-friendly knowledge derived by an inductive algorithm.	What is post processing in machine learning
4619	The theorem and its generalizations can be used to prove results and solve problems in combinatorics, algebra, calculus, and many other areas of mathematics. The binomial theorem also helps explore probability in an organized way: A friend says that she will flip a coin 5 times.	Why is the binomial theorem useful
201	But in reinforcement learning, we receive sequential samples from interactions with the environment.  Storing all experience in a replay buffer allows us to train on more independent samples. We just draw a batch of transitions from the buffer at random and train on that.	How and why is experience replay important in reinforcement learning How does it make learning faster
1897	Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit.	How do you interpret the root mean square error
1140	NMF stands for non-negative matrix factorization, a technique for obtaining low rank representation of matrices with non-negative or positive elements.  In information retrieval and text mining, we rely on term-document matrices for representing document collections.	What is NMF machine learning
5918	You can convert measures from discrete to continuous or from continuous to discrete. Click the field and choose Discrete or Continuous. The field is green when it is continuous, and blue when it is discrete. For measures in the Data pane, right-click the field and choose Convert to Discrete or Convert to Continuous.	How do you convert discrete data to continuous data
826	"A t-test tells you whether the difference between two sample means is ""statistically significant"" - not whether the two means are statistically different. A t-score with a p-value larger than 0.05 just states that the difference found is not ""statistically significant""."	How do you know if two samples are statistically different
4936	We can compare the quality of two estimators by looking at the ratio of their MSE. If the two estimators are unbiased this is equivalent to the ratio of the variances which is defined as the relative efficiency. rndr = n + 1 n · n n + 1 θ.	How do you interpret relative efficiency
1151	Explanation: Asynchronous update ensures that the next state is at most unit hamming distance from current state. 5. If pattern is to be stored, then what does stable state should have updated value of?	What is asynchronous update in Hopfield model
158	Due to AI, recommendation engines make quick and to-the-point recommendations tailored to each customer's needs and preferences. With the usage of artificial intelligence, online searching is improving as well, since it makes recommendations related to the user's visual preferences rather than product descriptions.	How is AI used in different recommender systems
815	Random errors in experimental measurements are caused by unknown and unpredictable changes in the experiment. These changes may occur in the measuring instruments or in the environmental conditions.	What is random error
5525	Cluster analysis is a statistical method used to group similar objects into respective categories. It can also be referred to as segmentation analysis, taxonomy analysis, or clustering.  For example, when cluster analysis is performed as part of market research, specific groups can be identified within a population.	What is cluster analysis research
1863	It maximizes the margin of the hyperplane. This is the best hyperplane because it reduces the generalization error the most. If we add new data, the Maximum Margin Classifier is the best hyperplane to correctly classify the new data. The Maximum Margin Classifier is our first SVM.	Why SVM is a max margin classifier
3028	How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression.	How do you deal with highly correlated variables
7019	A multi-layered perceptron consists of interconnected neurons transferring information to each other, much like the human brain. Each neuron is assigned a value. The network can be divided into three main layers.	How does multi layer Perceptron work
1600	Reinforcement learning is an area of Machine Learning.  In the absence of a training dataset, it is bound to learn from its experience. Example: The problem is as follows: We have an agent and a reward, with many hurdles in between. The agent is supposed to find the best possible path to reach the reward.	How does reinforcement learning work explain with an example
8189	Insufficient Data can cause a normal distribution to look completely scattered.  An extreme example: if you choose three random students and plot the results on a graph, you won't get a normal distribution. You might get a uniform distribution (i.e. 62 62 63) or you might get a skewed distribution (80 92 99).	Are there any reasons you wouldn't use the normal distribution in statistical analysis
1855	“In some footage, using Optical Flow for creating smoother motion may not produce the desired results.  Frame Blending repeats frames, but it also blends between them as needed to help smooth the motion.” If you aren't changing the frame rate during your export, leave this setting at “Frame Sampling.”	What is the difference between frame sampling frame blending and optical flow
1054	Correct answer: Independent variables are generally graphed on the x-axis, while dependent variables are generally graphed on the y-axis. In this question, time is the independent variable and displacement is the dependent variable.	Can time be graphed as a dependent variable
859	Supervised Learning deals with two main tasks Regression and Classification. Unsupervised Learning deals with clustering and associative rule mining problems. Whereas Reinforcement Learning deals with exploitation or exploration, Markov's decision processes, Policy Learning, Deep Learning and value learning.	What is the difference between supervised unsupervised and reinforcement learning
7896	The number of neurons in the input layer equals the number of input variables in the data being processed. The number of neurons in the output layer equals the number of outputs associated with each input.	How do you determine the number of neurons in the input layer
3427	"In probability, an outcome is in event ""A and B"" only when the outcome is in both event A and event B. (Intersection) In a Venn Diagram, an element is in the intersection of ""A and B"" only when the element is in BOTH sets. Rule (for AND):"	What is the and rule in probability
3786	sudo rm -rf / means to remove the contents of the root folder in a recursive manner. rm = remove, -r = recursive. This basically wipes out the contents of the root folder (the directories, sub-directories and all the files in them).	What happens when you sudo rm rf
1297	It has been found that multilingualism affects the structure, and essentially, the cytoarchitecture of the brain.  Language learning boosts brain plasticity and the brain's ability to code new information. Early language learning plays a significant role in the formation of memory circuits for learning new information.	How does being multilingual change brain structure and function
7936	As binning methods consult the neighborhood of values, they perform local smoothing.Approach:Sort the array of given data set.Divides the range into N intervals, each containing the approximately same number of samples(Equal-depth partitioning).Store mean/ median/ boundaries in each row.	How do you do binning
8315	How do I run a Z Test?State the null hypothesis and alternate hypothesis.Choose an alpha level.Find the critical value of z in a z table.Calculate the z test statistic (see below).Compare the test statistic to the critical z value and decide if you should support or reject the null hypothesis.	How do you use Z test
1039	GANs have plenty of real-world use cases like image generation, artwork generation, music generation, and video generation. Also, they can enhance the quality of your images, stylize or colorize your images, generate faces and can perform many more interesting tasks.	Why is Gan so popular
697	Predictive analytics are used to determine customer responses or purchases, as well as promote cross-sell opportunities. Predictive models help businesses attract, retain and grow their most profitable customers. Improving operations. Many companies use predictive models to forecast inventory and manage resources.	Why is predictive analytics important
5659	Linear regression is used for predicting the continuous dependent variable using a given set of independent features whereas Logistic Regression is used to predict the categorical. Linear regression is used to solve regression problems whereas logistic regression is used to solve classification problems.	What is the difference between linear classification and logistic regression
2884	The One-Sample z-test is used when we want to know whether the difference between the mean of a sample mean and the mean of a population is large enough to be statistically significant, that is, if it is unlikely to have occurred by chance.	When would you use a one sample z test
3187	Deep learning is a subset of machine learning in artificial intelligence that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network.	What is deep learning in machine learning
480	The Kruskal–Wallis test by ranks, Kruskal–Wallis H test (named after William Kruskal and W. Allen Wallis), or one-way ANOVA on ranks is a non-parametric method for testing whether samples originate from the same distribution. It is used for comparing two or more independent samples of equal or different sample sizes.	What is the nonparametric equivalent of Anova
1285	In-group bias is notoriously difficult to avoid completely, but research shows it can be reduced through interaction with other groups, and by giving people an incentive to act in an unbiased manner.	How can you avoid in group bias
8354	An artificial neuron (also referred to as a perceptron) is a mathematical function. It takes one or more inputs that are multiplied by values called “weights” and added together. This value is then passed to a non-linear function, known as an activation function, to become the neuron's output.	What are neurons in deep learning
308	While many people use the terms interchangeably, data science and big data analytics are unique fields, with the major difference being the scope.  Data science produces broader insights that concentrate on which questions should be asked, while big data analytics emphasizes discovering answers to questions being asked.	Is Analytics part of data science
3558	There are two main ways to access subsets of the elements in a tensor, either of which should work for your example.Use the indexing operator (based on tf. slice() ) to extract a contiguous slice from the tensor. input = tf.  Use the tf. gather() op to select a non-contiguous slice from the tensor. input = tf.	How do I find the value of Tensor
2105	Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components. Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting.	Why are STL decomposition models useful
1386	Both LDA and PCA are linear transformation techniques: LDA is a supervised whereas PCA is unsupervised – PCA ignores class labels.  Remember that LDA makes assumptions about normally distributed classes and equal class covariances.	What is the difference between LDA and PCA
1916	If your data contains both numeric and categorical variables, the best way to carry out clustering on the dataset is to create principal components of the dataset and use the principal component scores as input into the clustering.	Can you use categorical variables in clustering
4618	From Wikipedia, the free encyclopedia. Quantization is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set (such as the integers).	What is meant by quantization
1189	Principal components analysis (PCA) is a statistical technique that allows identifying underlying linear patterns in a data set so it can be expressed in terms of other data set of a significatively lower dimension without much loss of information.	What is PCA in neural network
195	The prior distribution is a distribution for the parameters whereas the prior predictive distribution is a distribution for the observations.  The last line is based on the assumption that the upcoming observation is independent of X given θ.	Differences between prior distribution and prior predictive distribution
3439	In mathematics, low-rank approximation is a minimization problem, in which the cost function measures the fit between a given matrix (the data) and an approximating matrix (the optimization variable), subject to a constraint that the approximating matrix has reduced rank.	What is a low rank matrix
3	law of large numbers. A principle stating that the larger the number of similar exposure units considered, the more closely the losses reported will equal the underlying probability of loss.	What is the law of large numbers quizlet
4076	The essential difference between these two is that Logistic regression is used when the dependent variable is binary in nature. In contrast, Linear regression is used when the dependent variable is continuous and nature of the regression line is linear.	What are the two main differences between logistic regression and linear regression
3631	matplotlib. pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc.	What is Matplotlib Pyplot in Python
635	Here are some ideas to help you learn:Read. Books by authors from other countries can expand your cultural understanding.  Watch movies. World cinema has a lot to offer.  Listen to radio shows and podcasts.  Talk with individuals from different cultures.  Travelling to other countries.	How do I show my appreciation of other cultures
2224	Explanation: States of units be updated synchronously and asynchronously in hopfield model.  Explanation: Asynchronous update ensures that the next state is at most unit hamming distance from current state. 5. If pattern is to be stored, then what does stable state should have updated value of?	How can states of units be updated in Hopfield model
665	If they are independent and identically distributed (IID), then they must meet the first two criteria (since differing variances constitute non-identical distributions). However, IID data need not be normally distributed.  Thus, whether or not a set of data is IID is unrelated to whether they are normal.	Is IID normal distribution
3441	For example, we can use a version of k-fold cross-validation that preserves the imbalanced class distribution in each fold. It is called stratified k-fold cross-validation and will enforce the class distribution in each split of the data to match the distribution in the complete training dataset.	Which validation technique is best suited for an imbalanced dataset
1556	A low response rate can give rise to sampling bias if the nonresponse is unequal among the participants regarding exposure and/or outcome.  For many years, a survey's response rate was viewed as an important indicator of survey quality.	Why is a low response rate of concern in survey research
1741	From the table we see that the probability of the observed data is maximized for θ=2. This means that the observed data is most likely to occur for θ=2. For this reason, we may choose ˆθ=2 as our estimate of θ. This is called the maximum likelihood estimate (MLE) of θ.	What is the maximum likelihood estimate of θ
2706	Gradient Descent is the process of minimizing a function by following the gradients of the cost function. This involves knowing the form of the cost as well as the derivative so that from a given point you know the gradient and can move in that direction, e.g. downhill towards the minimum value.	What is use of gradient descent algorithm in linear regression
2893	We often see patterns or relationships in scatterplots. When the y variable tends to increase as the x variable increases, we say there is a positive correlation between the variables. When the y variable tends to decrease as the x variable increases, we say there is a negative correlation between the variables.	How do you determine if there is a correlation in a scatter plot
5149	Pearson's correlation is utilized when you have two quantitative variables and you wish to see if there is a linear relationship between those variables. Your research hypothesis would represent that by stating that one score affects the other in a certain way. The correlation is affected by the size and sign of the r.	How do you know when to use a Pearson correlation
8218	ANN (Artificial Neural Networks) and SVM (Support Vector Machines) are two popular strategies for supervised machine learning and classification.  SVMs don't suffer from either of these two problems. However, it's not readily apparent that SVMs are meant to be a total replacement for ANNs.	Is support vector machine a neural network
5185	The Gamma distribution is widely used in engineering, science, and business, to model continuous variables that are always positive and have skewed distributions. In SWedge, the Gamma distribution can be useful for any variable which is always positive, such as cohesion or shear strength for example.	What is a gamma distribution used for
5685	An activation function is a node that you add to the output layer or between two layers of any neural network. It is also known as the transfer function. It is used to determine the output of neural network layer in between 0 to 1 or -1 to 1 etc.	How does neural network choose activation function
782	Some of the most popular methods for outlier detection are: Z-Score or Extreme Value Analysis (parametric) Probabilistic and Statistical Modeling (parametric) Linear Regression Models (PCA, LMS)	Which algorithm is better for outlier detection
4826	You can use an unsupervised learning algorithm (like clustering) to create your training data for the supervised learning algorithm but you cannot simply convert an unsupervised learning algorithm into a supervised one.	How do I convert unsupervised to supervised learning
6130	An independent random variable is a random variable that doesn't have an effect on the other random variables in your experiment. In other words, it doesn't affect the probability of another event happening.	What does it mean for random variables to be independent
178	Supervised learning algorithms are trained using labeled data. Unsupervised learning algorithms are trained using unlabeled data.  In unsupervised learning, only input data is provided to the model. The goal of supervised learning is to train the model so that it can predict the output when it is given new data.	What are supervised and unsupervised learning algorithms
8570	You can diagnose the calibration of a classifier by creating a reliability diagram of the actual probabilities versus the predicted probabilities on a test set. In scikit-learn, this is called a calibration curve.  The function returns the true probabilities for each bin and the predicted probabilities for each bin.	What is calibration curve in machine learning
837	There are seven significant steps in data preprocessing in Machine Learning:Acquire the dataset.  Import all the crucial libraries.  Import the dataset.  Identifying and handling the missing values.  Encoding the categorical data.  Splitting the dataset.  Feature scaling.	How do you preprocess data for machine learning
4006	No no need to standardize. Because by definition the correlation coefficient is independent of change of origin and scale. As such standardization will not alter the value of correlation.	Does standardizing variables change the correlation
1161	In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the	What does density mean in a probability density function
504	Sobel Filter. The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image. It finds the direction of the largest increase from light to dark and the rate of change in that direction.	What is Sobel edge detection in image processing
1330	Whereas GLS is more efficient than OLS under heteroscedasticity or autocorrelation, this is not true for FGLS. The feasible estimator is, provided the errors covariance matrix is consistently estimated, asymptotically more efficient, but for a small or medium size sample, it can be actually less efficient than OLS.	Why is GLS better than OLS
1547	Structured data is clearly defined and searchable types of data, while unstructured data is usually stored in its native format.  Structured data is often stored in data warehouses, while unstructured data is stored in data lakes.	What is structured and unstructured data with example
6180	The most common exponential and logarithm functions in a calculus course are the natural exponential function, ex , and the natural logarithm function, ln(x) ⁡ . We will take a more general approach however and look at the general exponential and logarithm function.	Are logarithms used in calculus
980	Word2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, word2vec trains words against other words that neighbor them in the input corpus.	Is word2vec basically an autoencoder
4326	Conjoint analysis is a popular method of product and pricing research that uncovers consumers' preferences and uses that information to help select product features, assess sensitivity to price, forecast market shares, and predict adoption of new products or services.	What is conjoint analysis used for
2747	An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way (typically by weighted or unweighted voting) to classify new examples. One of the most active areas of research in supervised learning has been to study methods for constructing good ensembles of classifiers.	What is an ensemble classifier
7817	The shape of any distribution can be described by its various 'moments'. The first four are: 1) The mean, which indicates the central tendency of a distribution. 2) The second moment is the variance, which indicates the width or deviation.	What is the second moment in statistics
765	AI means getting a computer to mimic human behavior in some way.  Deep learning, meanwhile, is a subset of machine learning that enables computers to solve more complex problems.	How are the terms artificial intelligence machine learning and deep learning related
1403	Perceptron Learning Rule The Perceptron receives multiple input signals, and if the sum of the input signals exceeds a certain threshold, it either outputs a signal or does not return an output. In the context of supervised learning and classification, this can then be used to predict the class of a sample.	How does the Perceptron algorithm work
4942	In many applications including econometrics and biostatistics a fixed effects model refers to a regression model in which the group means are fixed (non-random) as opposed to a random effects model in which the group means are a random sample from a population.	What is fixed effect in panel data regression
8568	A t-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related in certain features.  A t-test is used as a hypothesis testing tool, which allows testing of an assumption applicable to a population.	What is a t test and when is it used
6047	Filtering is a technique for modifying or enhancing an image.  Linear filtering is filtering in which the value of an output pixel is a linear combination of the values of the pixels in the input pixel's neighborhood. This section discusses linear filtering in MATLAB and the Image Processing Toolbox.	What is linear filtering in image processing
1444	Building an NLP Pipeline, Step-by-StepStep 1: Sentence Segmentation.  Step 2: Word Tokenization.  Step 3: Predicting Parts of Speech for Each Token.  Step 4: Text Lemmatization.  Step 5: Identifying Stop Words.  Step 6: Dependency Parsing.  Step 6b: Finding Noun Phrases.  Step 7: Named Entity Recognition (NER)More items	What is the order of steps in natural language understanding in AI
3968	The key to interpreting a hierarchical cluster analysis is to look at the point at which any given pair of cards “join together” in the tree diagram. Cards that join together sooner are more similar to each other than those that join together later.	How do you interpret a hierarchical cluster analysis
3098	In statistics, bivariate data is data on each of two variables, where each value of one of the variables is paired with a value of the other variable.  For example, bivariate data on a scatter plot could be used to study the relationship between stride length and length of legs.	What is bivariate variable
387	The Cauchy–Schwarz inequality gives the reason that the numerator is always less than or equal to the denominator. For other definitions of correlation (Spearman, Kendall, Kruskal & Goodman) it's because they're defined in such a manner to always fall between -1 and 1.	Why is correlation less than 1
5859	Bayesian model comparison is a method of model selection based on Bayes factors. The models under consideration are statistical models. The aim of the Bayes factor is to quantify the support for a model over another, regardless of whether these models are correct.	What is Bayesian model selection
6532	Code Generation for Image ProcessingWrite your MATLAB function or application as you would normally, using functions from the Image Processing Toolbox.Add the %#codegen compiler directive at the end of the function signature.  Open the MATLAB Coder (MATLAB Coder) app, create a project, and add your file to the project.More items	How do you code an image in Matlab processing
7049	After getting to know your data through data summaries and visualizations, you might want to transform your variables further to make them more meaningful. This is known as feature processing. For example, say you have a variable that captures the date and time at which an event occurred.	What is feature processing
5837	Regression Techniques Regression algorithms are machine learning techniques for predicting continuous numerical values.	Which algorithm is used to predict continuous values
2907	In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value.	What is vanishing gradient problem in neural networks
6586	0:168:55Suggested clip 108 secondsMode for a Continuous Random Variable | ExamSolutions - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the mode of a continuous distribution
290	The geometric distribution would represent the number of people who you had to poll before you found someone who voted independent. You would need to get a certain number of failures before you got your first success. If you had to ask 3 people, then X=3; if you had to ask 4 people, then X=4 and so on.	What does a geometric distribution look like
2143	A scatterplot displays data about two variables as a set of points in the x y xy xy -plane and is a useful tool for determining if there is a correlation between the variables. Causation means that one event causes another event to occur. Causation can only be determined from an appropriately designed experiment.	Why is it important to distinguish between correlation and causation
543	SummaryWeighted Mean: A mean where some values contribute more than others.When the weights add to 1: just multiply each weight by the matching value and sum it all up.Otherwise, multiply each weight w by its matching value x, sum that all up, and divide by the sum of weights: Weighted Mean = ΣwxΣw.	How do you find the weighted mean
767	only has one IP address. can only refer to one reusable-IP host at any given time, with one IP address, NAT can only provide general in-bound connectivity to one responder in the entire reusable-IP network at a time.	What are the limitations of NAT
465	A data set can also be presented by means of a data frequency table, a table in which each distinct value is listed in the first row and its frequency, which is the number of times the value appears in the data set, is listed below it in the second row.	What is data frequency table
8405	1.2 Semantic or Associative Networks Concepts are represented as nodes with labeled links (e.g., IS-A or Part-of) as relationships among the nodes.  Based on the idea that activation can spread from one node to another, semantic networks have been quite influential in the development of models of memory.	Do you believe that semantic network is the association of one node to another node
92	Differences Between Population Variance and Sample Variance When calculating sample variance, n is the number of sample points (vs N for population size in the formula above). Unlike the population variance, the sample variance is simply a statistic of the sample.	What is the difference between variance and sample variance
7500	The movie and its events are happening about a hundred years after this war. The entire Human race is used for power supply. Their bodies are asleep and their minds are plugged into the Matrix. The Matrix is a virtual world that has been pulled over their minds to hide them from the truth – they are slaves now.	What happens in the Matrix
199	Regression: This is a tool used to evaluate the relationship of a dependent variable in relation to multiple independent variables. A regression will analyze the mean of the dependent variable in relation to changes in the independent variables. Time Series: A time series measures data over a specific period of time.	What is the difference between time series and regression
1815	When I calculate population variance, I then divide the sum of squared deviations from the mean by the number of items in the population (in example 1 I was dividing by 12). When I calculate sample variance, I divide it by the number of items in the sample less one. In our example 2, I divide by 99 (100 less 1).	How do you find population variance from sample variance
6848	In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory.	What is meant by the term instance based learning
4406	If there are c or less defective items in the sample, the lot is accepted. If there are more than c defective items in the sample, the lot is rejected. In other words, the acceptance or rejection of the lot depends on the inspection results of a single sample.	Under what conditions is lot for lot acceptance sampling not accepted
1218	A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN).  MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.	What is the Multilayer Perceptron neural network algorithm
4677	In conclusion neural nets can learn the min function easily if either constrained within the positive or negative interval, and less easily if the interval includes both.	Can deep neural networks learn the minimum function
2011	Two-sample t-test is used when the data of two samples are statistically independent, while the paired t-test is used when data is in the form of matched pairs.  To use the two-sample t-test, we need to assume that the data from both samples are normally distributed and they have the same variances.	What is the difference between at test and a paired t test
1721	Classification algorithms are supervised learning methods to split data into classes. They can work on Linear Data as well as Nonlinear Data. Logistic Regression can classify data based on weighted parameters and sigmoid conversion to calculate the probability of classes.	How is classification used in machine learning
6519	Prewitt operator is similar to the Sobel operator and is used for detecting vertical and horizontal edges in images. However, unlike the Sobel, this operator does not place any emphasis on the pixels that are closer to the center of the mask.	What is the difference between Sobel and Prewitt
541	The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used to compare two related samples, matched samples, or repeated measurements on a single sample to assess whether their population mean ranks differ (i.e. it is a paired difference test).	Why use a Wilcoxon signed rank test
2366	Predictive analytics requires a data-driven culture: 5 steps to startDefine the business result you want to achieve.  Collect relevant data from all available sources.  Improve the quality of data using data cleaning techniques.  Choose predictive analytics solutions or build your own models to test the data.More items•	How do you do predictive analytics
2333	The number of outcomes in non-overlapping intervals are independent.   The probability of two or more outcomes in a sufficiently short interval is virtually zero.   The probability of exactly one outcome in a sufficiently short interval or small region is proportional to the length of the interval or region.	How do I know if my data is Poisson distributed
1092	Purpose of a Model. Models are representations that can aid in defining, analyzing, and communicating a set of concepts. System models are specifically developed to support analysis, specification, design, verification, and validation of a system, as well as to communicate certain information.	What is the use of a model
899	Absolute Positioning You can use two values top and left along with the position property to move an HTML element anywhere in the HTML document. Move Left - Use a negative value for left. Move Right - Use a positive value for left. Move Up - Use a negative value for top.	How do I change the position of an image in CSS
558	The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent.	What is Fourier transform and why do we use it
6864	One of the most important aspects of convenience sampling is its cost effectiveness. This method allows for funds to be distributed to other aspects of the project. Oftentimes this method of sampling is used to gain funding for a larger, more thorough research project.	When should Convenience sampling be used
4575	Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.	What is meant by principal component analysis
3312	Logistic regression is one of the statistical techniques in machine learning used to form prediction models.  In short, Logistic Regression is used when the dependent variable(target) is categorical.	What will be the modeling technique used to predict a categorical variable
8488	In probability theory and statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d. or iid or IID.	What does independent and identically distributed mean
1853	How to Deal with MulticollinearityRedesign the study to avoid multicollinearity.  Increase sample size.  Remove one or more of the highly-correlated independent variables.  Define a new variable equal to a linear combination of the highly-correlated variables.	How do you handle multicollinearity in regression modeling
3724	Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple's Siri and and Google's voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data.	What is RNN algorithm
6385	In the development of the probability function for a discrete random variable, two conditions must be satisfied: (1) f(x) must be nonnegative for each value of the random variable, and (2) the sum of the probabilities for each value of the random variable must equal one.	What are the requirements for probability distribution
992	Fundamentally, classification is about predicting a label and regression is about predicting a quantity.  That classification is the problem of predicting a discrete class label output for an example. That regression is the problem of predicting a continuous quantity output for an example.	What is regression and classification
883	Non-probability sampling is a sampling technique where the odds of any member being selected for a sample cannot be calculated.  In addition, probability sampling involves random selection, while non-probability sampling does not—it relies on the subjective judgement of the researcher.	What is probability and Nonprobability sampling in research
4483	Nevertheless, the same has been delineated briefly below:Step 1: Visualize the Time Series. It is essential to analyze the trends prior to building any kind of time series model.  Step 2: Stationarize the Series.  Step 3: Find Optimal Parameters.  Step 4: Build ARIMA Model.  Step 5: Make Predictions.	How do you conduct a time series analysis
4514	In general, prediction is the process of determining the magnitude of statistical variates at some future point of time.	What does prediction mean in statistics
7973	The normal distribution is a probability function that describes how the values of a variable are distributed. It is a symmetric distribution where most of the observations cluster around the central peak and the probabilities for values further away from the mean taper off equally in both directions.	What makes a distribution a normal distribution
1363	K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.  To achieve this objective, K-means looks for a fixed number (k) of clusters in a dataset.” A cluster refers to a collection of data points aggregated together because of certain similarities.	What does K means clustering tell you
5100	Type I and type II errors are instrumental for the understanding of hypothesis testing in a clinical research scenario.  A type II error can be thought of as the opposite of a type I error and is when a researcher fails to reject the null hypothesis that is actually false in reality.	Why is it important for researchers to understand type I and type II errors
2244	The distinction between probability and likelihood is fundamentally important: Probability attaches to possible results; likelihood attaches to hypotheses. Explaining this distinction is the purpose of this first column. Possible results are mutually exclusive and exhaustive.	What is difference between probability and likelihood
4530	Adaptive artificial neural networks are a class of networks used in dynamic environments. They are characterized by online learning. A number of techniques are used to provide adaptability to neural networks: adaptation by weight modification, by neuronal property modification, and by network structure modification.	What is adaptive neural network
4183	"The Kalman filter produces an estimate of the state of the system as an average of the system's predicted state and of the new measurement using a weighted average. The purpose of the weights is that values with better (i.e., smaller) estimated uncertainty are ""trusted"" more."	What is the use of Kalman filter
6033	The sample variance is an estimator for the population variance. When applied to sample data, the population variance formula is a biased estimator of the population variance: it tends to underestimate the amount of variability.  We are using one fitted value (sample mean) in our estimate of the variance.	Why is the formula for sample variance different from the formula for population variance
8294	A hypergeometric experiment is a statistical experiment with the following properties: You take samples from two groups. You are concerned with a group of interest, called the first group. You sample without replacement from the combined groups. Each pick is not independent, since sampling is without replacement.	What is a hypergeometric experiment
919	In scikit-learn we can use the CalibratedClassifierCV class to create well calibrated predicted probabilities using k-fold cross-validation.  In CalibratedClassifierCV the training sets are used to train the model and the test sets is used to calibrate the predicted probabilities.	What is CalibratedClassifierCV
769	The distribution defined by the density function in (1) is known as the negative binomial distribution ; it has two parameters, the stopping parameter k and the success probability p. In the negative binomial experiment, vary k and p with the scroll bars and note the shape of the density function.	What are the parameters of negative binomial distribution
462	0:133:01Suggested clip · 119 secondsLearn everything about probability in 3 mins! - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How can I learn my probability quickly
442	Once you have calculated the decimal values of each percentage for each given sample size, you then add these decimal values together and divide the total number by the total sum of both sample sizes. You then need to multiply this value by 100 to get the average percentage.5 days ago	How do you calculate the median percentage
3062	AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision.	What is artificial intelligence and its applications
4625	The target variable of a dataset is the feature of a dataset about which you want to gain a deeper understanding. A supervised machine learning algorithm uses historical data to learn patterns and uncover relationships between other features of your dataset and the target.	What is a target in machine learning
7377	A couple of ways of trying to decrease zig-zagging is to use larger batches during training, as well as trying to make the train/val split more consistent.	What could be explanations for validation loss zig zagging when training a deep neural network
2364	Convolution is a general purpose filter effect for images. □ Is a matrix applied to an image and a mathematical operation. comprised of integers. □ It works by determining the value of a central pixel by adding the. weighted values of all its neighbors together.	What are convolution filters
248	Truncated Backpropagation Through Time (truncated BPTT) is a widespread method for learning recurrent computational graphs. Truncated BPTT keeps the computational benefits of Backpropagation Through Time (BPTT) while relieving the need for a complete backtrack through the whole data sequence at every step.	What is truncated Bptt
1070	The regression (or regressive) fallacy is an informal fallacy. It assumes that something has returned to normal because of corrective actions taken while it was abnormal. This fails to account for natural fluctuations. It is frequently a special kind of the post hoc fallacy.	How are regression to the mean and the post hoc fallacy related
721	Again, feature selection keeps a subset of the original features while feature extraction creates new ones. As with feature selection, some algorithms already have built-in feature extraction.  As a stand-alone task, feature extraction can be unsupervised (i.e. PCA) or supervised (i.e. LDA).	Is PCA feature selection or feature extraction
7179	A feature vector is just a vector that contains information describing an object's important characteristics. In image processing, features can take many forms. A simple feature representation of an image is the raw intensity value of each pixel. However, more complicated feature representations are also possible.	What is CNN feature vector
5407	The correlation coefficient is a statistical measure of the strength of the relationship between the relative movements of two variables. The values range between -1.0 and 1.0.  A correlation of -1.0 shows a perfect negative correlation, while a correlation of 1.0 shows a perfect positive correlation.	What is the concept of correlation coefficient
3378	Events A and B are independent if: knowing whether A occured does not change the probability of B. Mathematically, can say in tw. Page 1. Events A and B are independent if: knowing whether A occured does not change the probability of B.	What does it mean for two events A and B to be statistically independent
3815	When there is lack of domain understanding for feature introspection , Deep Learning techniques outshines others as you have to worry less about feature engineering . Deep Learning really shines when it comes to complex problems such as image classification, natural language processing, and speech recognition.	Why should I learn deep learning
6349	A continuous random variable is not defined at specific values.  1: The curve has no negative values (p(x) > 0 for all x) 2: The total area under the curve is equal to 1. A curve meeting these requirements is known as a density curve.	Can a continuous random variable be negative
552	Predictive analytics is the use of data, statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data. The goal is to go beyond knowing what has happened to providing a best assessment of what will happen in the future.	What is meant by predictive analytics
852	A random variable is a variable whose value is unknown or a function that assigns values to each of an experiment's outcomes.  Random variables are often used in econometric or regression analysis to determine statistical relationships among one another.	What is meant by random variable
1950	Unlike classical (sparse, denoising, etc.) autoencoders, Variational autoencoders (VAEs) are generative models, like Generative Adversarial Networks.	Can autoencoders be considered as generative models
4965	PDF according to input X being discrete or continuous generates probability mass functions and CDF does the same but generates cumulative mass function. That means, PDF is derivative of CDF and CDF can be applied at any point where PDF has been applied.  The cumulative function is the integral of the density function.	What is the difference between a probability distribution function and a cumulative
1704	Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.	What is the difference between RMSE linear regression and best fit
1109	To assess which word2vec model is best, simply calculate the distance for each pair, do it 200 times, sum up the total distance, and the smallest total distance will be your best model.	How do you evaluate word2vec
456	Restricted Boltzmann Machines are shallow, two-layer neural nets that constitute the building blocks of deep-belief networks. The first layer of the RBM is called the visible, or input layer, and the second is the hidden layer. Each circle represents a neuron-like unit called a node.	What are the two layers of a restricted Boltzmann machine called
4111	IQ, short for intelligence quotient, is a measure of a person's reasoning ability. In short, it is supposed to gauge how well someone can use information and logic to answer questions or make predictions. IQ tests begin to assess this by measuring short- and long-term memory.	What is the point of IQ
1327	A Bayesian network is a compact, flexible and interpretable representation of a joint probability distribution. It is also an useful tool in knowledge discovery as directed acyclic graphs allow representing causal relations between variables. Typically, a Bayesian network is learned from data.	What is Bayesian network in machine learning
6238	A Fourier transform is holographic because all points in the input affect a single point in the output and vice versa. The neural nets in organic brains have been considered holographic because skills and memories seem to be spread out over many different neurons.	How are neural networks related to Fourier transforms
3265	Unimodal data has a distribution that is single-peaked (one mode). Bimodal data has two peaks (2 modes) and multimodal data refer to distributions with more than two clear peaks.	What is the difference between unimodal Bimodal and multimodal data
8301	17. Deep Convolutional Network (DCN): Convolutional Neural Networks are neural networks used primarily for classification of images, clustering of images and object recognition.	What is neural network classification
73	Steps: In tensorflow one steps is considered as number of epochs multiplied by examples divided by batch size. steps = (epoch * examples)/batch size For instance epoch = 100, examples = 1000 and batch_size = 1000 steps = 100.	What is step in TensorBoard
6621	Logistic regression, also called a logit model, is used to model dichotomous outcome variables. In the logit model the log odds of the outcome is modeled as a linear combination of the predictor variables.	What is logit regression used for
5192	Since both drifts involve a statistical change in the data, the best approach to detect them is by monitoring its statistical properties, the model's predictions, and their correlation with other factors.	How do you detect data Drifting
935	Specifically, synchronous learning is a type of group learning where everyone learns at the same time.  On the contrary, asynchronous learning is more self-directed, and the student decides the times that he will learn. TeachThought explains that, historically, online learning was asynchronous.	What is the difference between synchronous and asynchronous learning
3910	In simple terms: Training a Neural Network means finding the appropriate Weights of the Neural Connections thanks to a feedback loop called Gradient Backward propagation … and that's it folks.	What is training a neural network
911	Exponential Smoothing is one of the more popular smoothing techniques due to its flexibility, ease in calculation, and good performance. Exponential Smoothing uses a simple average calculation to assign exponentially decreasing weights starting with the most recent observations.	Which method is best for smoothing of data
477	Cowell says that the Gini coefficient is useful, particularly because it allows negative values for income and wealth, unlike some other measures of inequality. (If some amount of the population has negative wealth (owes money), the Lorenz curve will dip below the x-axis.) But the Gini coefficient also has limitations.	How reliable is the Gini coefficient
6367	The margin of error is a statistic expressing the amount of random sampling error in the results of a survey. The larger the margin of error, the less confidence one should have that a poll result would reflect the result of a survey of the entire population.	How do polls determine the margin of error
3308	The coefficient of variation (COV) is a measure of relative event dispersion that's equal to the ratio between the standard deviation and the mean. While it is most commonly used to compare relative risk, the COV may be applied to any type of quantitative likelihood or probability distribution.	Where is coefficient variation used
419	Approach –Load dataset from source.Split the dataset into “training” and “test” data.Train Decision tree, SVM, and KNN classifiers on the training data.Use the above classifiers to predict labels for the test data.Measure accuracy and visualise classification.	How do you do the multiclass classification
1708	First multiply the critical value by the standard deviation. Then divide this result by the error from Step 1. Now square this result. This result is the sample size.	How do you find the sample size when given the mean and standard deviation
4886	Ordinary linear squares (OLS) regression compares the response of a dependent variable given a change in some explanatory variables.  Multiple regressions are based on the assumption that there is a linear relationship between both the dependent and independent variables.	What is the difference between OLS and multiple regression
5702	Rectified linear units, compared to sigmoid function or similar activation functions, allow faster and effective training of deep neural architectures on large and complex datasets.	What are the benefits of using rectified linear units vs the typical sigmoid activation function
6437	Investment risk is the idea that an investment will not perform as expected, that its actual return will deviate from the expected return. Risk is measured by the amount of volatility, that is, the difference between actual returns and average (expected) returns.	How do you measure risk and return
1017	Knowledge is the information about a domain that can be used to solve problems in that domain.  As part of designing a program to solve problems, we must define how the knowledge will be represented. A representation scheme is the form of the knowledge that is used in an agent.	What do you mean by knowledge in artificial intelligence
1351	Random binary pattern clustering employing the ART1 net. Different vigilance values cause different numbers of categories (clusters) to form: (a) = 0.5 and (b) = 0.7. For each case, the top row shows prototype vectors extracted by the ART1 network. An example of ART2 clustering is shown in Figure 6.4.	What is the typical value of vigilance parameter in art1 network
348	Data visualization is a technique that uses an array of static and interactive visuals within a specific context to help people understand and make sense of large amounts of data. The data is often displayed in a story format that visualizes patterns, trends and correlations that may otherwise go unnoticed.	What is visualization in machine learning
7155	The cross product is a calculation used in order to define the correlation coefficient between two variables. SP is the sum of all cross products between two variables.	What does cross product mean in statistics
6447	Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .	What is Lemmatization in NLP
5929	While a Machine Learning model makes decisions according to what it has learned from the data, a Neural Network arranges algorithms in a fashion that it can make accurate decisions by itself. Thus, although Machine Learning models can learn from data, in the initial stages, they may require some human intervention.	How are machine learning and neural networks related
2290	Effect of Learning Rate A neural network learns or approximates a function to best map inputs to outputs from examples in the training dataset.  A smaller learning rate may allow the model to learn a more optimal or even globally optimal set of weights but may take significantly longer to train.	How does learning rate affect neural network
3890	The receiver operating characteristic (ROC) curve is a two dimensional graph in which the false positive rate is plotted on the X axis and the true positive rate is plotted on the Y axis. The ROC curves are useful to visualize and compare the performance of classifier methods (see Figure 1).	How ROC curve is plotted
258	The null hypothesis is a general statement that states that there is no relationship between two phenomenons under consideration or that there is no association between two groups. An alternative hypothesis is a statement that describes that there is a relationship between two selected variables in a study.	What is null hypothesis and alternative hypothesis in statistics
4924	In research, there is a convention that the hypothesis is written in two forms, the null hypothesis, and the alternative hypothesis (called the experimental hypothesis when the method of investigation is an experiment).	What are the two forms of hypothesis testing
5877	Perceptron Learning Rule states that the algorithm would automatically learn the optimal weight coefficients. The input features are then multiplied with these weights to determine if a neuron fires or not.	What is the perceptron learning rule
5532	0:395:36Suggested clip · 78 secondsSPSS: Hierarchical Clustering - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do a hierarchical cluster analysis in SPSS
6060	We just need a metric (i.e. loss) to optimize our model. Entropy uses logarithms, computer likes logarithms. We use it.  Instead of cross entropy and per-word perplexity of language models lets take a die roll.	In NLP why do we use perplexity instead of the loss
263	Do not confuse statistical significance with practical importance.  However, a weak correlation can be statistically significant, if the sample size is large enough.	Can a weak correlation be significant
336	Multi-Armed Bandit Problem This is an Artificial Intelligence (AI) technique in which an agent has to interact with an environment, choosing one of the available actions the environment provides in each possible state, to try and collect as many rewards as possible as a result of those actions.	What is bandit in reinforcement learning
7017	For example, the first moment is the expected value E[X]. The second central moment is the variance of X. Similar to mean and variance, other moments give useful information about random variables. The moment generating function (MGF) of a random variable X is a function MX(s) defined as MX(s)=E[esX].	How do you find the moment generating function
7821	0:083:15Suggested clip · 83 secondsTime Series Forecasting in Minutes - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you use time series forecasting
363	The term is often called as corrupt data.  We can't avoid the Noise data, but we can reduce it by using noise filters.	What's Noise How can noise be reduced in a dataset
6426	A chi-square is only a nonparametric criterion. You can make comparisons for each characteristic. You can also use Factorial ANOVA. In Factorial ANOVA, you can investigate the dependence of a quantitative characteristic (dependent variable) on one or more qualitative characteristics (category predictors).	What is the difference between chi square and Anova
5886	Whereas a variable denotes a placeholder for values taken from a given set, a random variable is the same thing but with the additional datum of a probability measure on the set of values.  So, nonrandom variables are precisely those variables which cannot take any values at all.	What is a non random variable
6390	Implement them into your life and you'll see results quickly.Spruce up your appearance. Take time for proper grooming and dressing.  Set goals and meet them. Confident men make goals and keep them.  Exercise. Nothing can boost manly confidence like exercise.  Learn a new skill.  Take stock of past success.	As a dude how do I increase my confidence level
7286	Advantages of Naive Bayes ClassifierIt is simple and easy to implement.It doesn't require as much training data.It handles both continuous and discrete data.It is highly scalable with the number of predictors and data points.It is fast and can be used to make real-time predictions.More items•	What are the advantages of using a naive Bayes classifier as opposed to other methods
2181	Neural networks offer a number of advantages, including requiring less formal statistical training, ability to implicitly detect complex nonlinear relationships between dependent and independent variables, ability to detect all possible interactions between predictor variables, and the availability of multiple training	What are the advantages of neural networks
4743	Nonparametric tests are also called distribution-free tests because they don't assume that your data follow a specific distribution. You may have heard that you should use nonparametric tests when your data don't meet the assumptions of the parametric test, especially the assumption about normally distributed data.	In the field of statistics when are nonparametric tests preferred over parametric tests
7798	Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater.	What is Test loss in machine learning
5463	Establish face validity.Conduct a pilot test.Enter the pilot test in a spreadsheet.Use principal component analysis (PCA)Check the internal consistency of questions loading onto the same factors.Revise the questionnaire based on information from your PCA and CA.	How do you check for the validity and reliability of a questionnaire
87	8 Examples of Artificial IntelligenceGoogle Maps and Ride-Hailing Applications. One doesn't have to put much thought into traveling to a new destination anymore.  Face Detection and Recognition.  Text Editors or Autocorrect.  Search and Recommendation Algorithms.  Chatbots.  Digital Assistants.  Social Media.  E-Payments.	What are some examples of artificial intelligence
2923	The constraints for the maximization problems all involved inequalities, and the constraints for the minimization problems all involved inequalities. Linear programming problems for which the constraints involve both types of inequali- ties are called mixed-constraint problems.	What is a mixed constrained optimization problem
2500	In order to be considered a normal distribution, a data set (when graphed) must follow a bell-shaped symmetrical curve centered around the mean. It must also adhere to the empirical rule that indicates the percentage of the data set that falls within (plus or minus) 1, 2 and 3 standard deviations of the mean.	How do we know a distribution is normal
7907	To carry out a Z-test, find a Z-score for your test or study and convert it to a P-value. If your P-value is lower than the significance level, you can conclude that your observation is statistically significant.	How do you determine if a variable is statistically significant
588	Face validity: Does the content of the test appear to be suitable to its aims? Criterion validity: Do the results correspond to a different test of the same thing?	What is the difference between content and criterion validity
5969	Multi-Arm Bandit is a classic reinforcement learning problem, in which a player is facing with k slot machines or bandits, each with a different reward distribution, and the player is trying to maximise his cumulative reward based on trials.	Is multi armed bandit reinforcement learning
8584	A sampling error is a statistical error that occurs when an analyst does not select a sample that represents the entire population of data and the results found in the sample do not represent the results that would be obtained from the entire population.	What is mean by sampling error
7724	"1: The number of observations n is fixed. 2: Each observation is independent. 3: Each observation represents one of two outcomes (""success"" or ""failure""). 4: The probability of ""success"" p is the same for each outcome."	What are the 4 characteristics of a binomial distribution
1051	Cluster sampling is best used when the clusters occur naturally in a population, when you don't have access to the entire population, and when the clusters are geographically convenient. However, cluster sampling is not as precise as simple random sampling or stratified random sampling.	When should cluster sampling be used
8398	In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one fewer quantile than the number of groups created.	How is quantile calculation
7231	3:2117:13Suggested clip · 116 secondsStepwise regression procedures in SPSS (new, 2018) - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do a reverse stepwise regression in SPSS
3773	In some fields they may be synonyms but in evolutionary computing it can be an important distinction. The objective function is the function being optimised while the fitness function is what is used to guide the optimisation.  The fitness function is traditionally positive values with higher being better.	What is difference between objective and fitness function
1106	The formula of population variance is sigma squared equals the sum of x minus the mean squared divided by n.	What is the formula for population variance
2959	Some Final Advantages of Continuous Over Discrete Data The table below lays out the reasons why. Inferences can be made with few data points—valid analysis can be performed with small samples. More data points (a larger sample) needed to make an equivalent inference. Larger samples are usually more expensive to gather.	Why is continuous data better than discrete
7026	In plain words, AIC is a single number score that can be used to determine which of multiple models is most likely to be the best model for a given dataset. It estimates models relatively, meaning that AIC scores are only useful in comparison with other AIC scores for the same dataset.	What is AIC and how can it be useful
68	In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased.  When a biased estimator is used, bounds of the bias are calculated.	What is bias function
6123	The sample proportion, P is an unbiased estimator of the population proportion, . Unbiased estimators determines the tendency , on the average, for the statistics to assume values closed to the parameter of interest.	Is proportion a biased estimator
3430	It is usually more straightforward to start from the CDF and then to find the PDF by taking the derivative of the CDF. Note that before differentiating the CDF, we should check that the CDF is continuous. As we will see later, the function of a continuous random variable might be a non-continuous random variable.	How do I convert CDF to PDF
888	We can interpret the negative binomial regression coefficient as follows: for a one unit change in the predictor variable, the difference in the logs of expected counts of the response variable is expected to change by the respective regression coefficient, given the other predictor variables in the model are held	How do you interpret a negative binomial regression
3503	There are three types on how batches are defined, one with extremely high scholarships (70–90%), second (10–70%) and third no Scholarships.	What are the batches in Allen
483	If a variable can take on any value between two specified values, it is called a continuous variable; otherwise, it is called a discrete variable. Some examples will clarify the difference between discrete and continuous variables.  The number of heads could be any integer value between 0 and plus infinity.	What is difference between discrete and continuous variable
6104	The type of quantization in which the quantization levels are uniformly spaced is termed as a Uniform Quantization. The type of quantization in which the quantization levels are unequal and mostly the relation between them is logarithmic, is termed as a Non-uniform Quantization.	What is uniform and non uniform quantization
7092	Probability sampling leads to higher quality findings because it provides an unbiased representation of the population. 2. When the population is usually diverse: Researchers use this method extensively as it helps them create samples that fully represent the population.	When do we use probability sampling
225	Binary classification is one of the most common and frequently tackled problems in the machine learning domain. In it's simplest form the user tries to classify an entity into one of the two possible categories. For example, give the attributes of the fruits like weight, color, peel texture, etc.	What is binary classification in neural network
1191	Inferential statistics helps to suggest explanations for a situation or phenomenon. It allows you to draw conclusions based on extrapolations, and is in that way fundamentally different from descriptive statistics that merely summarize the data that has actually been measured.	What is the main purpose of inferential statistics
2557	The difference is that traditional vision systems involve a human telling a machine what should be there versus a deep learning algorithm automatically extracting the features of what is there.	What is the difference between computer vision and deep learning
3368	Yes.  A new computational theory of learning is beginning to shed light on fundamental issues, such as the trade-off among the number of training examples available, the number of hypotheses considered, and the likely accuracy of the learned hypothesis.	Does machine learning really work
817	Just so, the Poisson distribution deals with the number of occurrences in a fixed period of time, and the exponential distribution deals with the time between occurrences of successive events as time flows by continuously.	What is the relationship between Poisson and exponential distribution
617	A weak classifier is simply a classifier that performs poorly, but performs better than random guessing.  AdaBoost can be applied to any classification algorithm, so it's really a technique that builds on top of other classifiers as opposed to being a classifier itself.	What is weak classifier in AdaBoost
1414	weight = weight + learning_rate * (expected - predicted) * x In the Multilayer perceptron, there can more than one linear layer (combinations of neurons).	How does Multilayer Perceptron calculate weight
7121	Neural networks and fuzzy logic systems are parameterised computational nonlinear algorithms for numerical processing of data (signals, images, stimuli). • These algorithms can be either implemented of a general-purpose computer or built into a dedicated hardware.	What is neural network and fuzzy logic
5478	Deep Learning is extensively used for Predictive Analytics, NLP, Computer Vision, and Object Recognition.	Does NLP use deep learning
175	A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables.  Because measurement error is by definition unique variance, it is not captured in the latent variable.	What is the meaning of latent variable
3925	"The ""interquartile range"", abbreviated ""IQR"", is just the width of the box in the box-and-whisker plot. That is, IQR = Q3 – Q1 .  The IQR tells how spread out the ""middle"" values are; it can also be used to tell when some of the other values are ""too far"" from the central value."	What does the interquartile range tell you
2994	Feature extraction is a type of dimensionality reduction where a large number of pixels of the image are efficiently represented in such a way that interesting parts of the image are captured effectively. From: Sensors for Health Monitoring, 2019.	Which is a feature extraction technique
1147	In terms of general theory, random forests can work with both numeric and categorical data. The function randomForest (documentation here) supports categorical data coded as factors, so that would be your type.	What should be the type of categorical variable when using the function randomForest
574	Demeaning data means subtracting the sample mean from each observation so that they are mean zero. Given a simple linear regression Y = alpha + beta X + u, OLS estimation yields Y^ = .	What is demeaning data
3986	The Wasserstein loss function seeks to increase the gap between the scores for real and generated images. We can summarize the function as it is described in the paper as follows: Critic Loss = [average critic score on real images] – [average critic score on fake images]	What is Wasserstein loss
2672	Alpha sets the standard for how extreme the data must be before we can reject the null hypothesis. The p-value indicates how extreme the data are.  If the p-value is greater than alpha (p > . 05), then we fail to reject the null hypothesis, and we say that the result is statistically nonsignificant (n.s.).	Are alpha level and P value the same
1175	d is used for a perfect differentiation of a function w.r.t a function . delta is used for demonstrating a large and finite change . the partial derivative symbol is used when a multi-variable function is to be differentiated w.r.t only a particular variable , while treating the other variables as constants .	What is the difference between the usage of d delta small delta and the partial derivative symbol
1433	Variability and Sample Sizes Increasing or decreasing sample sizes leads to changes in the variability of samples. For example, a sample size of 10 people taken from the same population of 1,000 will very likely give you a very different result than a sample size of 100.  Next: Sampling Distributions.	What causes variation in a sampling distribution
6391	The process of adjusting the weights and threshold of the ADALINE network is based on a learning algorithm named the Delta rule (Widrow and Hoff 1960) or Widrow-Hoff learning rule, also known as LMS (Least Mean Square ) algorithm or Gradient Descent method.	What is the delta rule of Adaline network
1178	Just having them in your face each and every day will subconsciously help you learn to recognize them in live trading.Pennant.Cup And Handle.Ascending Triangle.Triple Bottom.Descending Triangle.Inverse Head And Shoulders.Bullish Symmetric Triangle.Rounding Bottom.More items•	How do you identify stock patterns
4143	their joint probability distribution at (x,y), the functions given by: g(x) = Σy f (x,y) and h(y) = Σx f (x,y) are the marginal distributions of X and Y , respectively. If you're great with equations, that's probably all you need to know. It tells you how to find a marginal distribution.	How do you find the marginal distribution of X and Y
3403	Advertisements. Interpolation search is an improved variant of binary search. This search algorithm works on the probing position of the required value. For this algorithm to work properly, the data collection should be in a sorted form and equally distributed.	What is interpolation search in data structures
178	Theoretically, convolution are linear operations on the signal or signal modifiers, whereas correlation is a measure of similarity between two signals. As you rightly mentioned, the basic difference between convolution and correlation is that the convolution process rotates the matrix by 180 degrees.	What is the relation between convolution and correlation
8240	How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression.	How does logistic regression deal with Multicollinearity
1323	Steps for Using ANOVAStep 1: Compute the Variance Between. First, the sum of squares (SS) between is computed:  Step 2: Compute the Variance Within. Again, first compute the sum of squares within.  Step 3: Compute the Ratio of Variance Between and Variance Within. This is called the F-ratio.	How is analysis of variance calculated
919	Greedy is an algorithm taking the best possible at the current stage without violating constraints. Often it does not produce optimal solution, but it always produces a feasible solution by definition. It is still used quite often in many areas because of its simplicity and speed.	What is greedy algorithm Quora
6574	s2 (sample variance) is the best point estimate for population variance o2. s (sample standard deviation) is the best point estimate for the population standard deviation o.	What is a good point estimator for the population variance
8243	No because in simple tabular q learning you don't need to use neural networks. It's sufficient to keep your q values estimates in a lookup table. Experience replay was designed to alleviate problem arising from using deep neural networks on high dimensional state space such as atari screen doing Q learning (DQN).	Does experience replay useful in tabular forms of Q learning
526	Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner.	What are stochastic processes used for
1774	The distribution of a categorical variable lists all of the values the variable takes and how often it takes each of these values.	What is the distribution of a categorical variable
2792	Computer vision is the process of understanding digital images and videos using computers. It seeks to automate tasks that human vision can achieve. This involves methods of acquiring, processing, analyzing, and understanding digital images, and extraction of data from the real world to produce information.	What is Computer Vision in machine learning
4211	Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron's input is relevant for the model's prediction.	What are activation functions in machine learning
198	William Sealy Gosset	Who invented AB testing
1362	Propositional Logic converts a complete sentence into a symbol and makes it logical whereas in First-Order Logic relation of a particular sentence will be made that involves relations, constants, functions, and constants.	What is the difference between first order logic and propositional logic
257	The normal approximation gives us a very poor result without the continuity correction. We make a continuity correction when p is > 0.5.	When we use a normal distribution to approximate a binomial distribution why do we make a continuity correction
1851	In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the	What is a continuous probability density function
2127	Dictionary learning is learning a set of atoms so that a given image can be well approximated by a sparse linear combination of these learned atoms, while deep learning methods aim at extracting deep semantic feature representations via a deep network.	What is the difference and connection between deep learning and dictionary learning sparse representation
4598	Cluster sampling is a probability sampling method in which you divide a population into clusters, such as districts or schools, and then randomly select some of these clusters as your sample.  In double-stage sampling, you select a random sample of units from within the clusters.	Is cluster sampling random
2113	Instance-Based Learning: The raw training instances are used to make predictions. As such KNN is often referred to as instance-based learning or a case-based learning (where each training instance is a case from the problem domain).  As such KNN is referred to as a non-parametric machine learning algorithm.	Why KNN is called instance based learning
3177	Digital Signal Processors (DSP) take real-world signals like voice, audio, video, temperature, pressure, or position that have been digitized and then mathematically manipulate them.  In the real-world, analog products detect signals such as sound, light, temperature or pressure and manipulate them.	How does signal processing work
1116	Log-likelihood is a measure of model fit. The higher the number, the better the fit. This is usually obtained from statistical output.AICc = -2(log-likelihood) + 2K + (2K(K+1)/(n-K-1))n = sample size,K= number of model parameters,Log-likelihood is a measure of model fit.	How is Akaike information criterion calculated
758	Exclusive Class Interval: When the lower limit is included, but the upper limit is excluded, then it is an exclusive class interval.	What is exclusive class interval in statistics
1652	Convergence in probability implies convergence in distribution. In the opposite direction, convergence in distribution implies convergence in probability when the limiting random variable X is a constant. Convergence in probability does not imply almost sure convergence.	Does convergence in probability always imply convergence in distribution
4057	If you are working on a classification problem, the best score is 100% accuracy. If you are working on a regression problem, the best score is 0.0 error. These scores are an impossible to achieve upper/lower bound.	What is a good accuracy score in machine learning
272	he confidence interval tells you more than just the possible range around the estimate. It also tells you about how stable the estimate is. A stable estimate is one that would be close to the same value if the survey were repeated.	What does the confidence interval tell you
681	The Markov blanket of a node contains the node's parents, children and children's parents (see figure 4). When predicting the behavior of a specific node in the network, the nodes that have to be considered for this prediction are the nodes belonging to the Markov blanket of the chosen node ( Yap et al., 2008).	What is the markov blanket of this node
5460	The null hypothesis is a general statement that states that there is no relationship between two phenomenons under consideration or that there is no association between two groups. An alternative hypothesis is a statement that describes that there is a relationship between two selected variables in a study.	How do you define null and alternative hypothesis
4997	Linear means something related to a line.  A non-linear equation is such which does not form a straight line. It looks like a curve in a graph and has a variable slope value. The major difference between linear and nonlinear equations is given here for the students to understand it in a more natural way.	What is the difference between linear and nonlinear association
5936	Fitting a neural network involves using a training dataset to update the model weights to create a good mapping of inputs to outputs.  Training a neural network involves using an optimization algorithm to find a set of weights to best map inputs to outputs.	How does a neural network train
159	Confirmation bias is the tendency of people to favor information that confirms their existing beliefs or hypotheses.  People display this bias when they gather or recall information selectively, or when they interpret it in a biased way.	How do you explain confirmation bias
5830	Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.	What is hierarchical method for clustering
3189	Step 1: Prepare a table containing less than type cumulative frequency with the help of given frequencies. belongs. Class-interval of this cumulative frequency is the median class-interval. Step 3 : Find out the frequency f and lower limit l of this median class.	How do you find the median of a frequency table using class intervals
5369	Random forests is a robust algorithm that can be used for remotely sensed data classification and regression. Performance of random forests is on par with other machine learning algorithms but it is much easier to use and more forgiving with regard to over fitting and outliers than other algorithms.	Is Random Forest a viable model for image classification
5111	3. OneWay ANOVA – Similar to a ttest, except that this test can be used to compare the means from THREE OR MORE groups (ttests can only compare TWO groups at a time, and for statistical reasons it is generally considered “illegal” to use ttests over and over again on different groups from a single experiment).	What statistical test should I use to compare three groups
2441	The sampling rate determines the spatial resolution of the digitized image, while the quantization level determines the number of grey levels in the digitized image.  The transition between continuous values of the image function and its digital equivalent is called quantization.	What is quantization and sampling
453	Bias can creep into algorithms in several ways. AI systems learn to make decisions based on training data, which can include biased human decisions or reflect historical or social inequities, even if sensitive variables such as gender, race, or sexual orientation are removed.	What causes bias in AI
7975	1 Answer. Reinforcement learning is a collection of different approaches/solutions to problems framed as Markov Decision Processes.  The Policy results from the RL model, so it is not input data.	Does reinforcement learning need data
7143	Spurious states are patterns , where is the set of patterns to be memorized. In other words, they correspond to local minima in the energy function that shouldn't be there. They can be composed of various combinations of the original patterns or simply the negation of any pattern in the original pattern set.	What are spurious states in Hopfield networks
749	To solve the problem using logistic regression we take two parameters w, which is n dimensional vector and b which is a real number. The logistic regression model to solve this is : Equation for Logistic Regression. We apply sigmoid function so that we contain the result of ŷ between 0 and 1 (probability value).	What is W in logistic regression
7824	Similarly, the probability density function of a continuous random variable can be obtained by differentiating the cumulative distribution. The c.d.f. can be used to find out the probability of a random variable being between two values: P(s ≤ X ≤ t) = the probability that X is between s and t.	How do you find the continuous random variable
5096	In statistics, bootstrapping is any test or metric that relies on random sampling with replacement. Bootstrapping allows assigning measures of accuracy (defined in terms of bias, variance, confidence intervals, prediction error or some other such measure) to sample estimates. 231 views.	In laymans terms what is bootstrapping in statistics
561	Cross Entropy is definitely a good loss function for Classification Problems, because it minimizes the distance between two probability distributions - predicted and actual.  So cross entropy make sure we are minimizing the difference between the two probability. This is the reason.	Why do we use cross entropy loss
8382	Inverted dropout is a variant of the original dropout technique developed by Hinton et al. Just like traditional dropout, inverted dropout randomly keeps some weights and sets others to zero. In contrast, traditional dropout requires scaling to be implemented during the test phase.	What is inverted dropout
1281	The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems.	Is artificial intelligence a technology
8304	Informally: a hat is an estimate that is sometimes calculated by the arithmetic mean, but can be some other type of estimate (median, mode, some kind of maximum likelihood estimate, etc.). Bar is an estimate that (usually) happens to be an arithmetic mean.	What is the difference between Y hat and Y Bar
1609	Principal Component Analysis (PCA) is an unsupervised, non-parametric statistical technique primarily used for dimensionality reduction in machine learning. High dimensionality means that the dataset has a large number of features.  PCA can also be used to filter noisy datasets, such as image compression.	What is PCA algorithm in machine learning
1061	Automatic thresholding Select initial threshold value, typically the mean 8-bit value of the original image. Divide the original image into two portions; Pixel values that are less than or equal to the threshold; background. Pixel values greater than the threshold; foreground.	How is the threshold value calculated in image processing
869	Signal Detection Theory assumes that, given this situation, we make our judgment of whether the signal is present, or not, by setting up a Criterion value, β (beta).  When a value is picked up that exceeds β, we respond that the signal is present.	What is beta in signal detection theory
391	Bootstrapping is any test or metric that uses random sampling with replacement, and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.	What does bootstrapping mean in statistics
4437	Chi Square distributions are positively skewed, with the degree of skew decreasing with increasing degrees of freedom. As the degrees of freedom increases, the Chi Square distribution approaches a normal distribution.	What is the shape of the chi square distribution
3182	As far as I know, in Bag Of Words method, features are a set of words and their frequency counts in a document. In another hand, N-grams, for example unigrams does exactly the same, but it does not take into consideration the frequency of occurance of a word.	In Text Classification What is the difference between Bag of Words BOW and N Grams
6573	It is one of the more common descriptive statistics functions used to calculate uncertainty.How to CalculateSubtract each value from the mean.Square each value in step 1.Add all of the values from step 2.Count the number of values and Subtract it by 1.Divide step 3 by step 4.Calculate the Square Root of step 5.	How do you calculate uncertainty in statistics
1290	2:426:07Suggested clip · 113 secondsUnivariate analysis SPSS - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret univariate analysis in SPSS
889	3 nodes	How many nodes are in the output layer
4917	Depth is the number of filters. Depth column (or fibre) is the set of neurons that are all pointing to the same receptive field. Stride has the objective of producing smaller output volumes spatially. For example, if a stride=2, the filter will shift by the amount of 2 pixels as it convolves around the input volume.	What is depth in CNN
3833	In statistical theory, the field of high-dimensional statistics studies data whose dimension is larger than dimensions considered in classical multivariate analysis.  In many applications, the dimension of the data vectors may be larger than the sample size.	What is high dimensional data analysis
3867	In Data Science, bias is a deviation from expectation in the data. More fundamentally, bias refers to an error in the data. But, the error is often subtle or goes unnoticed.	What is bias in data science
4465	The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate.  GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM.	Are GRU Gated Recurrent Unit a special case of LSTM
2324	Researchers can take a number of steps to account for regression to the mean and avoid making incorrect conclusions. The best way is to remove the effect of regression to the mean during the design stage by conducting a randomized controlled trial (RCT).	How do you prevent regression to the mean
6513	Introduction to Association Rules Association rule is unsupervised learning where algorithm tries to learn without a teacher as data are not labelled. Association rule is descriptive not the predictive method, generally used to discover interesting relationship hidden in large datasets.	Are association rules unsupervised learning
3670	The terms cost and loss functions almost refer to the same meaning. But, loss function mainly applies for a single training set as compared to the cost function which deals with a penalty for a number of training sets or the complete batch.  The cost function is calculated as an average of loss functions.	What is the difference between a cost function and a loss function in machine learning
8441	An embedding is a mapping of a discrete — categorical — variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables.	What are embedding vectors
932	The cumulative distribution function, CDF, or cumulant is a function derived from the probability density function for a continuous random variable. It gives the probability of finding the random variable at a value less than or equal to a given cutoff.	What is the difference between a probability distribution function and a cumulative distribution function
8492	The final precision-recall curve metric is average precision (AP) and of most interest to us here. It is calculated as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight.	How is average precision calculated
2940	Two random variables X and Y are said to be bivariate normal, or jointly normal, if aX+bY has a normal distribution for all a,b∈R. In the above definition, if we let a=b=0, then aX+bY=0. We agree that the constant zero is a normal random variable with mean and variance 0.	How do you find the bivariate normal distribution
7273	Hierarchical Clustering Algorithm Also called Hierarchical cluster analysis or HCA is an unsupervised clustering algorithm which involves creating clusters that have predominant ordering from top to bottom.	Is hierarchical clustering unsupervised
2537	The least squares method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve. Least squares regression is used to predict the behavior of dependent variables.	What is the logic in the least squares method of linear regression analysis
1204	The basic procedure is:State the null hypothesis H0 and the alternative hypothesis HA.Set the level of significance .Calculate the test statistic: z = p ^ − p o p 0 ( 1 − p 0 ) n.Calculate the p-value.Make a decision. Check whether to reject the null hypothesis by comparing p-value to .	How do you test proportions in statistics
364	In statistics, a sequence (or a vector) of random variables is homoscedastic /ˌhoʊmoʊskəˈdæstɪk/ if all its random variables have the same finite variance. This is also known as homogeneity of variance. The complementary notion is called heteroscedasticity.	What is Homoscedasticity in statistics
5865	Strictly speaking, a neural network (also called an “artificial neural network”) is a type of machine learning model that is usually used in supervised learning.  A perceptron is a simplified model of a human neuron that accepts an input and performs a computation on that input.	Does machine learning use neural networks
5467	The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image. It finds the direction of the largest increase from light to dark and the rate of change in that direction.	What does the Sobel filter do
3081	Analysis of covariance (ANCOVA) is a general linear model which blends ANOVA and regression.  Mathematically, ANCOVA decomposes the variance in the DV into variance explained by the CV(s), variance explained by the categorical IV, and residual variance.	What does analysis of covariance mean
7994	Data is the currency of applied machine learning.  Resampling is a methodology of economically using a data sample to improve the accuracy and quantify the uncertainty of a population parameter. Resampling methods, in fact, make use of a nested resampling method.	What is resampling in machine learning
7601	"Bayesian networks and neural networks are not exclusive of each other. In fact, Bayesian networks are just another term for ""directed graphical model"".  A neural networks is used to implemented p(x|z) and an approximation to its inverse: q(z|x)≈p(z|x)."	Is Bayesian network a neural network
917	The solution involves four steps.Make sure the samples from each population are big enough to model differences with a normal distribution.  Find the mean of the difference in sample proportions: E(p1 - p2) = P1 - P2 = 0.52 - 0.47 = 0.05.Find the standard deviation of the difference.  Find the probability.	How do you find the difference between sample proportions
7684	Autocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations as a function of the time lag between them.	What is correlation and autocorrelation
1199	DeepMind	Who has beaten AlphaGo
6373	How to Use K-means Cluster Algorithms in Predictive AnalysisPick k random items from the dataset and label them as cluster representatives.Associate each remaining item in the dataset with the nearest cluster representative, using a Euclidean distance calculated by a similarity function.Recalculate the new clusters' representatives.More items	How is clustering used in prediction
4423	Entropy, the measure of a system's thermal energy per unit temperature that is unavailable for doing useful work. Because work is obtained from ordered molecular motion, the amount of entropy is also a measure of the molecular disorder, or randomness, of a system.	What is the physical concept of entropy
8225	When resources are limited, populations exhibit logistic growth. In logistic growth, population expansion decreases as resources become scarce, leveling off when the carrying capacity of the environment is reached, resulting in an S-shaped curve.	What is the logistic population growth model
8120	Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA. The eigenvectors of ATA make up the columns of V , the eigenvectors of AAT make up the columns of U. Also, the singular values in S are square roots of eigenvalues from AAT or ATA.	How do you find the decomposition of a SVD
2717	A posteriori comes from Latin and literally translates as “from the latter” or “from the one behind.” It's often applied to things involving inductive reasoning, which uses specific instances to arrive at a general principle or law (from effect to cause).	What does a posteriori mean
3836	There are two types of factor analyses, exploratory and confirmatory. Exploratory factor analysis (EFA) is method to explore the underlying structure of a set of observed variables, and is a crucial step in the scale development process. The first step in EFA is factor extraction.	What are the types of factor analysis
5479	The Spearman correlation is the same as the Pearson correlation, but it is used on data from an ordinal scale. Which situation would be appropriate for obtaining a phi-coefficient with a Pearson test?	What is the difference between the Pearson correlation and the Spearman correlation quizlet
6843	The larger the sample size, the greater the likelihood that sample statistics will accurately reflect population parameters. The larger the sample size, the smaller the sampling error.	What is the relationship between sample size and sampling error quizlet
2949	In Semantic networks, we can represent our knowledge in the form of graphical networks. This network consists of nodes representing objects and arcs which describe the relationship between those objects. Semantic networks can categorize the object in different forms and can also link those objects.	How knowledge is represented using semantic network
8602	So, a highly significant intercept in your model is generally not a problem. By the same token, if the intercept is not significant you usually would not want to remove it from the model because by doing this you are creating a model that says that the response function must be zero when the predictors are all zero.	What if intercept is not significant in regression
1478	You simply measure the number of correct decisions your classifier makes, divide by the total number of test examples, and the result is the accuracy of your classifier. It's that simple.	How do you evaluate the accuracy of a classifier
1302	Histograms are sometimes called Frequency Plots while boxplots are referred to as Box-and-Whisker Plots. Histograms and boxplots can be drawn either vertically or horizontally.  A histogram is normally used for continuous data while a bar chart is a plot of count data.	What is the difference between a Boxplot and histogram
6499	Empirical and priori probabilities generally do not vary from person to person, and they are often grouped as objective probabilities. Subjective probability is a probability based on personal or subjective judgment.	What are the differences between a priori probability empirical probability and subjective probability
907	Organizations that capitalize on big data stand apart from traditional data analysis environments in three key ways: They pay attention to data flows as opposed to stocks. They rely on data scientists and product and process developers rather than data analysts.	How is big data different
4467	Statisticians use variance to see how individual numbers relate to each other within a data set, rather than using broader mathematical techniques such as arranging numbers into quartiles. One drawback to variance is that it gives added weight to outliers, the numbers that are far from the mean.	Why is variance calculated
8221	The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities .	When can Bayes theorem be used
6746	0:3213:58Suggested clip · 112 secondsSurvival Analysis in R - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do a survival analysis in R
3329	Model averaging refers to the practice of using several models at once for making predictions (the focus of our review), or for inferring parameters (the focus of other papers, and some recent controversy, see, e.g. Banner & Higgs, 2017).	What is model averaging
8505	R is a very dynamic and versatile programming language for data science. This article deals with classification in R. Generally classifiers in R are used to predict specific category related information like reviews or ratings such as good, best or worst. Various Classifiers are: Decision Trees.	What is R classification
1406	2.4. 7 Cosine Similarity Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis.	How cosine similarity is calculated
739	3 layers	How many layers is deep learning
534	Entropy measures the expected (i.e., average) amount of information conveyed by identifying the outcome of a random trial. This implies that casting a die has higher entropy than tossing a coin because each outcome of a die toss has smaller probability (about ) than each outcome of a coin toss ( ).	What is entropy in probability
7945	2:528:15Suggested clip · 90 secondsUnit Conversion Using Dimensional Analysis Tutorial (Factor Label YouTubeStart of suggested clipEnd of suggested clip	How do you set up a dimensional analysis
8286	Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.	How do classification trees work
6680	Summary: Chaos theory is a mathematical theory that can be used to explain complex systems such as weather, astronomy, politics, and economics. Although many complex systems appear to behave in a random manner, chaos theory shows that, in reality, there is an underlying order that is difficult to see.	What is chaos theory used for
2380	Statistical Methods for Finding the Best Regression ModelAdjusted R-squared and Predicted R-squared: Generally, you choose the models that have higher adjusted and predicted R-squared values.  P-values for the predictors: In regression, low p-values indicate terms that are statistically significant.More items•	Which regression model is best
3116	Standard error is used in inferential stats to see whether the sample stat that we get from one sample is larger or smaller than the average differences of the stat (variance or error) of certain stat due to chance.	Why is the standard error important quizlet
148	Similarly, if a matrix has two entries in each column, then it must have two rows. So, it follows that in order for matrix multiplication to be defined, the number of columns in the first matrix must be equal to the number of rows in the second matrix.	How do you tell if the product of a matrix is defined
2411	"ARIMA models are applied in some cases where data show evidence of non-stationarity, where an initial differencing step (corresponding to the ""integrated"" part of the model) can be applied one or more times to eliminate the non-stationarity."	Where is the Arima model used
4449	False-negative test results can happen for many reasons. One older study that tested 27 different kinds of at-home pregnancy tests found that they gave false negatives almost 48 percent of the time.	How likely is a false negative pregnancy test
515	If a z-score is equal to 0, it is on the mean. If a Z-Score is equal to +1, it is 1 Standard Deviation above the mean. If a z-score is equal to +2, it is 2 Standard Deviations above the mean.  This means that raw score of 98% is pretty darn good relative to the rest of the students in your class.	What is a good Z score
8496	A survey is an investigation about the characteristics of a given population by means of collecting data from a sample of that population and estimating their characteristics through the systematic use of statistical methodology.	What is a survey in statistics
795	Post-pruning (or just pruning) is the most common way of simplifying trees. Here, nodes and subtrees are replaced with leaves to improve complexity. Pruning can not only significantly reduce the size but also improve the classification accuracy of unseen objects.	What is pruning in decision trees Why is it important
5989	Types of Recurrent Neural NetworksBinary.Linear.Continuous-Nonlinear.Additive STM equation.Shunting STM equation.Generalized STM equation.MTM: Habituative Transmitter Gates and Depressing Synapses.LTM: Gated steepest descent learning: Not Hebbian learning.More items•	Which ones are types of recurrent neural networks
4378	We use the following formula to compute variance.Var(X) = Σ ( Xi - X )2 / N = Σ xi2 / N.N is the number of scores in a set of scores. X is the mean of the N scores.  Cov(X, Y) = Σ ( Xi - X ) ( Yi - Y ) / N = Σ xiyi / N.N is the number of scores in each set of data. X is the mean of the N scores in the first data set.	How do you calculate covariance matrix from data
1585	"Eigenvalues and eigenvectors allow us to ""reduce"" a linear operation to separate, simpler, problems. For example, if a stress is applied to a ""plastic"" solid, the deformation can be dissected into ""principle directions""- those directions in which the deformation is greatest."	What is the application of eigenvalues and eigenvectors
5551	Theano is deep learning library developed by the Université de Montréal in 2007. It offers fast computation and can be run on both CPU and GPU. Theano has been developed to train deep neural network algorithms.	Is theano a deep learning framework
136	The term PCA Color Augmentation refers to a type of data augmentation technique first mentioned in the paper titled ImageNet Classification with Deep Convolutional Neural Networks.  Specifically, PCA Color Augmentation is designed to shift those values based on which values are the most present in the image.	What is PCA color augmentation Can you give a detailed explanation
6686	Image backup is ideal for enterprises because IT departments don't have to make back ups of all employees' computers, which definitely saves time and money. Also, in comparison with file backup, you backup everything - not only files, but also drivers, settings, in a word, everything.	What are the advantages of using image backup in comparison with other methods of backups
2034	A Power Spectral Density (PSD) is the measure of signal's power content versus frequency.  Therefore, while the power spectrum calculates the area under the signal plot using the discrete Fourier Transform, the power spectrum density assigns units of power to each unit of frequency and thus, enhances periodicities.	What is the difference between power spectrum and power spectral density
7902	A p-value is a measure of the probability that an observed difference could have occurred just by random chance. The lower the p-value, the greater the statistical significance of the observed difference. P-value can be used as an alternative to or in a addition to pre-selected confidence levels for hypothesis testing.	What is a P value and how is it used to make a decision about statistical significance
5562	The Apriori algorithm is used for mining frequent itemsets and devising association rules from a transactional database. The parameters “support” and “confidence” are used. Support refers to items' frequency of occurrence; confidence is a conditional probability. Items in a transaction form an item set.	What is assertion rule mining write Apriori algorithm for finding frequency item set discuss it with suitable examples
2061	Ordinary least-square regression has no normality requirement.	What is the normality requirement for ordinary least squares regression
1273	A logarithmic scale (or log scale) is a way of displaying numerical data over a very wide range of values in a compact way—typically the largest numbers in the data are hundreds or even thousands of times larger than the smallest numbers.	What does a logarithmic scale look like
1911	Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for—tasks that involve creativity and empathy among others.	What is the impact of AI
1552	Class boundaries are the data values which separate classes. They are not part of the classes or the dataset. The lower class boundary of a class is defined as the average of the lower limit of the class in question and the upper limit of the previous class.	What are class boundaries in stats
570	An -dimensional vector, i.e., a vector ( , , , ) with components. In dimensions greater than or equal to two, vectors are sometimes considered synonymous with points and so n-tuples ( , , , ) are sometimes called points in n-space.	What is an N dimensional vector
2588	In this context, a factor is still a variable, but it refers to a categorical independent variable. So you may have heard of fixed factors and random factors.  Like covariates, factors in a linear model can be either control variables or important independent variables. The model uses them the same way in either case.	Are factors and variables the same
8615	K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.  KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.	What is K Nearest Neighbor algorithm in machine learning
626	One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture.	How do you overcome vanishing gradient problems
3920	Stratified random sampling is used when the researcher wants to highlight a specific subgroup within the population. This technique is useful in such researches because it ensures the presence of the key subgroup within the sample.  This allows the researcher to sample the rare extremes of the given population.	Why would you use stratified random sampling
59	To take your first steps down the artificial intelligence career path, hiring managers will likely require that you hold at least a bachelor's degree in mathematics and basic computer technology. However, for the most part, bachelor's degrees will only get you into entry-level positions.	What degree do I need to work with artificial intelligence
601	Ensemble methods	What is ensemble in machine learning
7887	compressing a finite sequence produced by an unknown information source,  telling whether a given finite sequence could have reliably been produced by a given source.	What is entropy in coding theory
7206	The FISs have been successfully applied in several fields such as automatic control, data classification, decision analysis, expert systems, and computer vision. Fuzzy inference is the real process of mapping from a given set of input variables to an output relied upon a set of fuzzy rules.	What are the applications of fuzzy inference system
516	systems development life cycle	What do you mean by the SDLC
8018	something that may or does vary or change; a variable feature or factor. Mathematics, Computers. a quantity or function that may assume any given value or set of values.	What does it mean if something is variable
54	Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data.	What is population variance and sample variance
323	Scatterplots with a linear pattern have points that seem to generally fall along a line while nonlinear patterns seem to follow along some curve.  If there is no clear pattern, then it means there is no clear association or relationship between the variables that we are studying.	What is the difference between nonlinear association and no association
1349	Below are 5 data mining techniques that can help you create optimal results.Classification Analysis. This analysis is used to retrieve important and relevant information about data, and metadata.  Association Rule Learning.  Anomaly or Outlier Detection.  Clustering Analysis.  Regression Analysis.	What are the data mining techniques
6883	A Seq2Seq model is a model that takes a sequence of items (words, letters, time series, etc) and outputs another sequence of items.  The encoder captures the context of the input sequence in the form of a hidden state vector and sends it to the decoder, which then produces the output sequence.	What is SEQ 2 SEQ model
348	The Linear Regression Equation The equation has the form Y= a + bX, where Y is the dependent variable (that's the variable that goes on the Y axis), X is the independent variable (i.e. it is plotted on the X axis), b is the slope of the line and a is the y-intercept.	How do you write a regression model
102	a place where a concentration of a particular phenomenon is found.	What is the meaning of cluster area
1269	"A parameter is any summary number, like an average or percentage, that describes the entire population. The population mean (the greek letter ""mu"") and the population proportion p are two different population parameters. For example:  The population comprises all likely American voters, and the parameter is p."	What is parameter with example
710	An intuitive idea of the general shape of the distribution can also be obtained by considering this sum of squares. Since χ2 is the sum of a set of squared values, it can never be negative. The minimum chi squared value would be obtained if each Z = 0 so that χ2 would also be 0. There is no upper limit to the χ2 value.	Can you get a negative chi square statistic
4380	1:0037:30Suggested clip · 92 secondsBuild Sentiment Analysis Model from Scratch using GBM - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you develop a sentiment analysis model
188	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve.	What is the importance of Gaussian distribution
1769	PCA is designed to model linear variabilities in high-dimensional data. However, many high dimensional data sets have a nonlinear nature. In these cases the high-dimensional data lie on or near a nonlinear manifold (not a linear subspace) and therefore PCA can not model the variability of the data correctly.	Can PCA be used to reduce the dimensionality of a highly nonlinear dataset
108	Batch normalization makes the mean and variance of the activations of each layer independent from the values themselves. This means that the magnitude of the higher order interactions are going to be suppressed, allowing larger learning rates to be used.	Why does batch normalization enable higher learning rate
3310	According to Cohen's original article, values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.	What is considered good inter rater reliability
123	The null hypothesis is a general statement that states that there is no relationship between two phenomenons under consideration or that there is no association between two groups. An alternative hypothesis is a statement that describes that there is a relationship between two selected variables in a study.	What is null and alternative hypothesis
6684	The target variable of a dataset is the feature of a dataset about which you want to gain a deeper understanding. A supervised machine learning algorithm uses historical data to learn patterns and uncover relationships between other features of your dataset and the target.	What is target variable in machine learning
670	How to Use GA for Optimization Problems?Generate the initial population randomly.Select the initial solution with the best fitness values.Recombine the selected solutions using mutation and crossover operators.Insert offspring into the population.More items	How is genetic algorithm used in neural networks
89	Linear models, generalized linear models, and nonlinear models are examples of parametric regression models because we know the function that describes the relationship between the response and explanatory variables.  If the relationship is unknown and nonlinear, nonparametric regression models should be used.	Can you use linear regression for non parametric data
111	Decision tree learning is a supervised machine learning technique for inducing a decision tree from training data. A decision tree (also referred to as a classification tree or a reduction tree) is a predictive model which is a mapping from observations about an item to conclusions about its target value.	What is decision tree technique
762	Strengths and weaknesses of correlationStrengths:WeaknessesCalculating the strength of a relationship between variables.Cannot assume cause and effect, strong correlation between variables may be misleading.1 more row	What are the strengths and weaknesses of the correlational method
4795	A statistical project is the process of answering a research question using statistical techniques and presenting the work in a written report. The research question may arise from any field of scientific endeavor, such as athletics, advertising, aerodynamics, or nutrition.	What is a statistical project
8168	The formula for the Conditional Probability of an event can be derived from Multiplication Rule 2 as follows:Start with Multiplication Rule 2.Divide both sides of equation by P(A).Cancel P(A)s on right-hand side of equation.Commute the equation.We have derived the formula for conditional probability.	How do you solve conditional probability problems
942	Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients.  On the other hand, L2 regularization (e.g. Ridge regression) doesn't result in elimination of coefficients or sparse models. This makes the Lasso far easier to interpret than the Ridge.	How are Lasso and ridge regressions used for regularization
7589	Information provides a way to quantify the amount of surprise for an event measured in bits. Entropy provides a measure of the average amount of information needed to represent an event drawn from a probability distribution for a random variable.	Why is information entropy
5187	Currently AI is Used is Following Things/Fields: Autonomous Flying. Retail, Shopping and Fashion. Security and Surveillance. Sports Analytics and Activities.	What is artificial intelligence used for today
977	Pick a value for the learning rate α. The learning rate determines how big the step would be on each iteration. If α is very small, it would take long time to converge and become computationally expensive. If α is large, it may fail to converge and overshoot the minimum.	What is the effect of choosing a small α in gradient descent
6175	Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression.	Is stochastic gradient descent linear
1368	"today announced the development of ""Wide Learning,"" a machine learning technology capable of accurate judgements even when operators cannot obtain the volume of data necessary for training."	What is wide learning
4861	In the design of experiments and analysis of variance, a main effect is the effect of an independent variable on a dependent variable averaged across the levels of any other independent variables.  Main effects are essentially the overall effect of a factor.	What do main effects mean in Anova
1115	Overview. Feature columns are used to specify how Tensors received from the input function should be combined and transformed before entering the model.	What is feature column
935	The main difference between probability and likelihood is that the former is normalized.  Probability refers to the occurrence of future events, while a likelihood refers to past events with known outcomes. Probability is used when describing a function of the outcome given a fixed parameter value.	What is the difference between probability and likelihood
7531	Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable.	Can you run a regression with ordinal data
4285	Bootstrap Aggregating is an ensemble method. First, we create random samples of the training data set with replacment (sub sets of training data set). Then, we build a model (classifier or Decision tree) for each sample. Finally, results of these multiple models are combined using average or majority voting.	How do you ensemble different models
1190	Here are some tips for connecting the shape of a histogram with the mean and median:If the histogram is skewed right, the mean is greater than the median.  If the histogram is close to symmetric, then the mean and median are close to each other.  If the histogram is skewed left, the mean is less than the median.	How do you find the mean and median of a histogram
883	Logistic regression is a classification algorithm, used when the value of the target variable is categorical in nature. Logistic regression is most commonly used when the data in question has binary output, so when it belongs to one class or another, or is either a 0 or 1.	How logistic regression can be used as a classifier
5354	Multinomial logistic regression (often just called 'multinomial regression') is used to predict a nominal dependent variable given one or more independent variables. It is sometimes considered an extension of binomial logistic regression to allow for a dependent variable with more than two categories.	When would you use multinomial regression
3839	Linear regression is supervised. You start with a dataset with a known dependent variable (label), train your model, then apply it later. You are trying to predict a real number, like the price of a house. Logistic regression is also supervised.	Is regression supervised or unsupervised
2988	Whereas IRR is sensitive to the ordering of ratings, IRA is sensitive to the variation in ratings or differences in rating levels. High IRR can exist with low IRA, and thus the level of reliability does not provide an indication of the level of agreement between raters.	What is the difference between Inter rater reliability and interrater agreement
2045	The common application of indicators is the detection of end points of titrations. The colour of an indicator alters when the acidity or the oxidizing strength of the solution, or the concentration of a certain chemical species, reaches a critical range of values.	What is the function of the indicator
746	The random forest combines hundreds or thousands of decision trees, trains each one on a slightly different set of the observations, splitting nodes in each tree considering a limited number of the features. The final predictions of the random forest are made by averaging the predictions of each individual tree.	How does random forest split
806	“Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems.  Support Vectors are simply the co-ordinates of individual observation.	What is SVM Can you explain the algorithm in detail and its application
3155	The Discrete Fourier Transform is always periodic, but we usually focus only on what's inside the Nyquist window and ignore the periodic copies outside. The regular Fourier Transform (function of continuous frequency) cannot be periodic, to my knowledge.	Is Fourier transform periodic
761	The random forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a “forest”), this model uses two key concepts that gives it the name random: Random sampling of training data points when building trees.	Why random forest is called random
6358	Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).	What is a seq2seq model
2009	5 ways to deal with outliers in dataSet up a filter in your testing tool. Even though this has a little cost, filtering out outliers is worth it.  Remove or change outliers during post-test analysis.  Change the value of outliers.  Consider the underlying distribution.  Consider the value of mild outliers.	How do you deal with outliers in your data
5029	Bias is stated as a penchant that prevents objective consideration of an issue or situation; basically the formation of opinion beforehand without any examination. Selection is stated as the act of choosing or selecting a preference; resulting in a carefully chosen and representative choice.	What is the difference between bias and selection
831	Best Data Visualization Techniques for small and large dataBar Chart. Bar charts are used for comparing the quantities of different categories or groups.  Pie and Donut Charts.  Histogram Plot.  Scatter Plot.  Visualizing Big Data.  Box and Whisker Plot for Large Data.  Word Clouds and Network Diagrams for Unstructured Data.  Correlation Matrices.	How do you visualize a large amount of data
7314	Filtering is a technique for modifying or enhancing an image.  Linear filtering is filtering in which the value of an output pixel is a linear combination of the values of the pixels in the input pixel's neighborhood. This section discusses linear filtering in MATLAB and the Image Processing Toolbox.	What is linear filter in image processing
3607	Approach –Load dataset from source.Split the dataset into “training” and “test” data.Train Decision tree, SVM, and KNN classifiers on the training data.Use the above classifiers to predict labels for the test data.Measure accuracy and visualise classification.	How do you deal with multi class classification
5615	The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.	What is Bag of Words in NLP
8088	The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. The squaring is necessary to remove any negative signs. It also gives more weight to larger differences.	How do you interpret mean square error
5598	Some of the most popular methods for outlier detection are:Z-Score or Extreme Value Analysis (parametric)Probabilistic and Statistical Modeling (parametric)Linear Regression Models (PCA, LMS)Proximity Based Models (non-parametric)Information Theory Models.More items	What are the outlier detection methods
1546	The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image. It finds the direction of the largest increase from light to dark and the rate of change in that direction.	How do Sobel filters work
4987	Discussion ForumQue.Which search implements stack operation for searching the states?a.Depth-limited searchb.Depth-first searchc.Breadth-first searchd.None of the mentioned1 more row	Which search implements stack operation for searching the States
582	Univariate statistics summarize only one variable at a time. Bivariate statistics compare two variables. Multivariate statistics compare more than two variables.	What is the difference between univariate and bivariate analysis
1345	Logistic regression works like ordinary least squares regression but on the logit of the dependent variable. Discriminant analysis is really used only for categorization. Logistic regression is often used when we aren't even interested in categorization but in getting the odds ratios for each variable.	What is the difference between logistic regression and discriminant analysis
5756	The logistic function was discovered anew in 1920 by Pearl and Reed in a study of the population growth of the United States. They were unaware of Verhulst's work (though not of the curves for autocatalytic reactions dis0 cussed presently), and they arrived independently at the logistic curve of (10).	Who developed logistic regression
7426	The significance level is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference.	What is the intuitive explanation of the significance level in statistics
1968	Simply put, your model becomes more complex, and less explainable.	What happens when we introduce more variables to a linear regression model
643	In short, it ensures each subgroup within the population receives proper representation within the sample. As a result, stratified random sampling provides better coverage of the population since the researchers have control over the subgroups to ensure all of them are represented in the sampling.	Why is stratified sampling used
8396	The matrix of features is a term used in machine learning to describe the list of columns that contain independent variables to be processed, including all lines in the dataset. These lines in the dataset are called lines of observation.	What is a feature matrix in machine learning
8374	We maximize the likelihood because we maximize fit of our model to data under an implicit assumption that the observed data are at the same time most likely data.	Why do we maximize the likelihood
1488	An eigenfunction of an operator is a function such that the application of on gives. again, times a constant. (49) where k is a constant called the eigenvalue. It is easy to show that if is a linear operator with an eigenfunction , then any multiple of is also an eigenfunction of .	What is an eigenfunction of an operator
289	The population distribution gives the values of the variable for all the individuals in the population.  The sampling distribution shows the statistic values from all the possible samples of the same size from the population. It is a distribution of the statistic.	What is the difference between a population distribution and a sampling distribution
1445	The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts occurrences of gradient orientation in localized portions of an image.	What is Histogram of Oriented Gradients and how does it work
745	In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution.	What information can the chi square goodness of fit test provide
27	Convolution is the process of adding each element of the image to its local neighbors, weighted by the kernel. This is related to a form of mathematical convolution. The matrix operation being performed—convolution—is not traditional matrix multiplication, despite being similarly denoted by *.	What is convolution in image processing
848	The ability to detect certain types of stimuli, like movements, shape, and angles, requires specialized cells in the brain called feature detectors. Without these, it would be difficult, if not impossible, to detect a round object, like a baseball, hurdling toward you at 90 miles per hour.	What do feature detectors detect
4773	Bayes' Theorem has many applications in areas such as mathematics, medicine, finance, marketing, engineering and many other. This paper covers Bayes' Theorem at a basic level and explores how the formula was derived. We also, look at some extended forms of the formula and give an explicit example.	What are some interesting applications of Bayes theorem
7998	Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.	What do you do in exploratory data analysis
4096	Dummy variables (sometimes called indicator variables) are used in regression analysis and Latent Class Analysis. As implied by the name, these variables are artificial attributes, and they are used with two or more categories or levels.	Why is it called a dummy variable
2129	Backward elimination (or backward deletion) is the reverse process. All the independent variables are entered into the equation first and each one is deleted one at a time if they do not contribute to the regression equation. Stepwise selection is considered a variation of the previous two methods.	What is backward elimination
593	Below are the methods to convert a categorical (string) input to numerical nature:Label Encoder: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables).  Convert numeric bins to number: Let's say, bins of a continuous variable are available in the data set (shown below).	How do you deal with categorical variables in machine learning
5764	The quality loss function as defined by Taguchi is the loss imparted to the society by the product from the time the product is designed to the time it is shipped to the customer. In fact, he defined quality as the conformity around a target value with a lower standard deviation in the outputs.	What is Taguchi quality loss function
5178	Distance Learning Off-line is a mode of delivery that does not require online participation. You do not have to come to campus. Course materials may be available through the internet, but they can also be mailed to you if you prefer.	What is meant by offline classes
1452	"A set of values or elements that is statistically random, but it is derived from a known starting point and is typically repeated over and over.  It is called ""pseudo"" random, because the algorithm can repeat the sequence, and the numbers are thus not entirely random."	What is meant by pseudo random number
682	5 Real-World Problems Big Data Can SolveHelp Overcome Fertility Issues. According to the Centers for Disease Control (CDC) about 10 percent of American women have trouble getting or staying pregnant.  Provide Small-Dollar Loans to People in Need.  Put Students Out of Their Misery.  Read Our Minds.  Catch Terrorists.	What problems can big data solve
1183	(non-RAN-duh-mized KLIH-nih-kul TRY-ul) A clinical trial in which the participants are not assigned by chance to different treatment groups. Participants may choose which group they want to be in, or they may be assigned to the groups by the researchers.	What does non randomized mean
402	Time series data is data that is collected at different points in time. This is opposed to cross-sectional data which observes individuals, companies, etc. at a single point in time. Because data points in time series are collected at adjacent time periods there is potential for correlation between observations.	How do you collect time series data
1251	Exponential Moving Average (EMA) and Simple Moving Average (SMA) are similar in that they each measure trends.  More specifically, the exponential moving average gives a higher weighting to recent prices, while the simple moving average assigns equal weighting to all values.	What is the difference between moving average and exponential smoothing
3942	"The Z value for 95% confidence is Z=1.96. [Note: Both the table of Z-scores and the table of t-scores can also be accessed from the ""Other Resources"" on the right side of the page.] What is the 90% confidence interval for BMI? (Note that Z=1.645 to reflect the 90% confidence level.)"	What is the value of Z for a 95 confidence interval
6833	resample Function One resampling application is the conversion of digitized audio signals from one sample rate to another, such as from 48 kHz (the digital audio tape standard) to 44.1 kHz (the compact disc standard).  resample applies a lowpass filter to the input sequence to prevent aliasing during resampling.	What is resampling in signal processing
5209	In probability and statistics, the quantile function, associated with a probability distribution of a random variable, specifies the value of the random variable such that the probability of the variable being less than or equal to that value equals the given probability.	What does it mean by quantile of a function
1189	In (and after) TensorFlow version 0.11. 0RC1, you can save and restore your model directly by calling tf. train. export_meta_graph and tf.	How do I save and restore model in Tensorflow
459	When working with box plots, the IQR is computed by subtracting the first quartile from the third quartile. In a standard normal distribution (with mean 0 and standard deviation 1), the first and third quartiles are located at -0.67448 and +0.67448 respectively. Thus the interquartile range (IQR) is 1.34896.	How do you find the Iqr with the mean and standard deviation
1055	The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.	What is AUC in machine learning
2170	Top Algorithms/Data Structures/Concepts every computer science student should knowInsertion sort, Selection sort,Merge Sort, Quicksort.Binary Search.Breadth First Search (BFS)Depth First Search (DFS)Lee algorithm | Shortest path in a Maze.Flood fill Algorithm.Floyd's Cycle Detection Algorithm.More items•	What are common algorithms
1042	Factor Analysis in SPSS To conduct a Factor Analysis, start from the “Analyze” menu.  This dialog allows you to choose a “rotation method” for your factor analysis.  This table shows you the actual factors that were extracted.  E.  Finally, the Rotated Component Matrix shows you the factor loadings for each variable.More items	How do you do factor analysis in SPSS
4682	The coefficient of determination can also be found with the following formula: R2 = MSS/TSS = (TSS − RSS)/TSS, where MSS is the model sum of squares (also known as ESS, or explained sum of squares), which is the sum of the squares of the prediction from the linear regression minus the mean for that variable; TSS is the	What is the formula for calculating the coefficient of determination
7655	An overt integrity test is a self-report paper and pencil test that asks a subject directly about their honesty, criminal history, attitudes towards drug use, thefts by other people, and general questions that show integrity.	How do you test integrity
279	As we saw above, KNN algorithm can be used for both classification and regression problems. The KNN algorithm uses 'feature similarity' to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set.	Can Knn be used for regression
6002	Mean DeviationFind the mean of all values.Find the distance of each value from that mean (subtract the mean from each value, ignore minus signs)Then find the mean of those distances.	How do you calculate mean deviation
612	Choosing the right Activation FunctionSigmoid functions and their combinations generally work better in the case of classifiers.Sigmoids and tanh functions are sometimes avoided due to the vanishing gradient problem.ReLU function is a general activation function and is used in most cases these days.More items•	What is the activation function used for
3371	The Basics of a One-Tailed Test Hypothesis testing is run to determine whether a claim is true or not, given a population parameter. A test that is conducted to show whether the mean of the sample is significantly greater than and significantly less than the mean of a population is considered a two-tailed test.	What is a one sided vs a two sided hypothesis test
213	0:003:17Suggested clip · 116 secondsMaximum Likelihood estimation: Poisson distribution - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find Poisson's maximum likelihood estimation
5303	Linear regression attempts to model the relationship between two variables by fitting a linear equation (= a straight line) to the observed data. One variable is considered to be an explanatory variable (e.g. your income), and the other is considered to be a dependent variable (e.g. your expenses).	How would linear regression be described and explained in laymans terms
3250	SOLUTION: sample siae =400; sample mean = 44; sample standard deviation =16. what is the margin of error? I have: 44-400/16=356/16=22.30 E=2.16/400=32/400=. 2E-4 margin or error.	If a sample size 400 sample mean 44 sample standard deviation 16 then what is the margin of error
4589	The residual plot shows a fairly random pattern - the first residual is positive, the next two are negative, the fourth is positive, and the last residual is negative. This random pattern indicates that a linear model provides a decent fit to the data.	What does it mean if the residual plot has a pattern
876	The hazard function is not a density or a probability. However, we can think of it as the probability of failure in an infinitesimally small time period between y and y + ∂y given that the subject has survived up till time y.	Is the hazard function a probability
203	The joint probability is symmetrical, meaning that P(A and B) is the same as P(B and A). The calculation using the conditional probability is also symmetrical, for example: P(A and B) = P(A given B)	How do you find conditional probability from joint probability
1197	11 websites to find free, interesting datasetsFiveThirtyEight.  BuzzFeed News.  Kaggle.  Socrata.  Awesome-Public-Datasets on Github.  Google Public Datasets.  UCI Machine Learning Repository.  Data.gov.More items	Where can I find large data sets
5275	Example of Law of Large Numbers Let's say you rolled the dice three times and the outcomes were 6, 6, 3. The average of the results is 5. According to the law of the large numbers, if we roll the dice a large number of times, the average result will be closer to the expected value of 3.5.	How do you use the law of large numbers
10	2:4518:31Suggested clip · 116 secondsCorrespondence Analysis using SPSS - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret correspondence analysis in SPSS
1280	That is, a studentized residual is just a deleted residual divided by its estimated standard deviation (first formula). In general, studentized residuals are going to be more effective for detecting outlying Y observations than standardized residuals.	What is the difference between standardized and Studentized residuals
1187	Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.	What is rule based learning in AI
7221	SYNONYMS. idea, notion, conception, abstraction, conceptualization. theory, hypothesis, postulation. belief, conviction, opinion, view, image, impression, picture.	What is the other word for concept
478	The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum.	What is the difference between stratified sampling and cluster sampling
7670	A statistic d is called an unbiased estimator for a function of the parameter g(θ) provided that for every choice of θ, Eθd(X) = g(θ). Any estimator that not unbiased is called biased. The bias is the difference bd(θ) = Eθd(X) − g(θ). We can assess the quality of an estimator by computing its mean square error.	How do you calculate an unbiased estimator
229	Sigmoid. Sigmoid takes a real value as input and outputs another value between 0 and 1. It's easy to work with and has all the nice properties of activation functions: it's non-linear, continuously differentiable, monotonic, and has a fixed output range. It is nonlinear in nature.	Is sigmoid a linear function
506	In many applications including econometrics and biostatistics a fixed effects model refers to a regression model in which the group means are fixed (non-random) as opposed to a random effects model in which the group means are a random sample from a population.	What is the difference between fixed and random effects models
3987	Common LDA limitations: Fixed K (the number of topics is fixed and must be known ahead of time) Uncorrelated topics (Dirichlet topic distribution cannot capture correlations) Non-hierarchical (in data-limited regimes hierarchical models allow sharing of data)	What are the limitations of Latent Dirichlet Allocation
5618	The geometric mean differs from the arithmetic average, or arithmetic mean, in how it is calculated because it takes into account the compounding that occurs from period to period. Because of this, investors usually consider the geometric mean a more accurate measure of returns than the arithmetic mean.	Why is geometric mean more accurate than arithmetic mean
697	In order to measure the difference between two colors, the difference is assigned to a distance within the color space.	What does color distance mean
331	The method of least squares is about estimating parameters by minimizing the squared discrepancies between observed data, on the one hand, and their expected values on the other (see Optimization Methods).	What is the least square estimate
1993	The main difference between quota and stratified sampling can be explained in a way that in quota sampling researchers use non-random sampling methods to gather data from one stratum until the required quota fixed by the researcher is fulfilled.	What is the difference between quota sampling and qualitative research
77	K-Means clustering algorithm fails to give good results when the data contains outliers, the density spread of data points across the data space is different and the data points follow non-convex shapes.	In which of the following cases will k means clustering will fail to give good results
380	Poisson approximation to the Binomial The approximation works very well for n values as low as n = 100, and p values as high as 0.02.	In what case would the Poisson distribution be a good approximation of the binomial
4576	Probability sampling gives you the best chance to create a sample that is truly representative of the population. Using probability sampling for finding sample sizes means that you can employ statistical techniques like confidence intervals and margins of error to validate your results.	Why is probability sampling important
6582	We can calculate the mean and standard deviation of a given sample, then calculate the cut-off for identifying outliers as more than 3 standard deviations from the mean. We can then identify outliers as those examples that fall outside of the defined lower and upper limits.	When we detect an outlier we should
449	Apart from overfitting, Decision Trees also suffer from following disadvantages: 1. Tree structure prone to sampling – While Decision Trees are generally robust to outliers, due to their tendency to overfit, they are prone to sampling errors.	Which of the following is a disadvantage of decision tree
1175	1. If having conditional independence will highly negative affect classification, you'll want to choose K-NN over Naive Bayes. Naive Bayes can suffer from the zero probability problem; when a particular attribute's conditional probability equals zero, Naive Bayes will completely fail to produce a valid prediction.	Classification machine learning When should I use a K NN classifier over a Naive Bayes classifier
3814	from keras. datasets import mnist. from keras. models import Sequential. from keras.  from keras. utils import np_utils. # load data. (X_train, y_train), (X_test, y_test) = mnist. load_data()# flatten 28*28 images to a 784 vector for each image. num_pixels = X_train. shape[1] * X_train. shape[2] X_train = X_train.	How do I load Mnist dataset in keras
5790	A Dataset is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations. Each Dataset also has an untyped view called a DataFrame , which is a Dataset of Row . Operations available on Datasets are divided into transformations and actions.	What is dataset row
4862	Grid-searching is the process of scanning the data to configure optimal parameters for a given model. Depending on the type of model utilized, certain parameters are necessary.  Grid-searching can be applied across machine learning to calculate the best parameters to use for any given model.	What is a grid search in machine learning
2036	Feature identification is a well-known technique to identify subsets of a program source code activated when exercising a functionality.  We present an approach to feature identification and comparison for large object-oriented multi-threaded programs using both static and dynamic data.	What is feature identification
1148	In a true experiment, participants are randomly assigned to either the treatment or the control group, whereas they are not assigned randomly in a quasi-experiment.  Thus, the researcher must try to statistically control for as many of these differences as possible.	What is the difference between field experiment and quasi experiment
4225	Unlike the batch gradient descent which computes the gradient using the whole dataset, because the SGD, also known as incremental gradient descent, tries to find minimums or maximums by iteration from a single randomly picked training example, the error is typically noisier than in gradient descent.	What is SGD stochastic gradient descent What's the difference with the usual gradient descent
1763	The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986. Canny also produced a computational theory of edge detection explaining why the technique works.	What is canny edge detection in image processing
256	Downside deviation measures to what extent an investment falls short of your minimum acceptable return by calculating the negative differences from the MAR, squaring the sums, and dividing by the number of periods, and taking the square root.	How do you calculate downside standard deviation
1179	"Inductive Learning is a powerful strategy for helping students deepen their understanding of content and develop their inference and evidence-gathering skills. In an Inductive Learning lesson, students examine, group, and label specific ""bits"" of information to find patterns."	What is inductive learning
3192	T - test is used to if the means of two populations are equal (assuming similar variance) whereas F-test is used to test if the variances of two populations are equal. F - test can also be extended to check whether the means of three or more groups are different or not (ANOVA F-test).	Whats the difference between an F Test and T Test
1945	Classification and regression tree (CART) analysis recursively partitions observations in a matched data set, consisting of a categorical (for classification trees) or continuous (for regression trees) dependent (response) variable and one or more independent (explanatory) variables, into progressively smaller groups (	What is classification and regression tree analysis
694	Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.	What are neural machine translation system
37	TensorBoard is a suite of web applications for inspecting and understanding your TensorFlow runs and graphs. TensorBoard currently supports five visualizations: scalars, images, audio, histograms, and graphs.	What is tensor board
443	Qualitative data and quantitative data There are two types of data in statistics: qualitative and quantitative.	What are the two classification of statistics
5751	A generative adversarial network (GAN) is a machine learning (ML) model in which two neural networks compete with each other to become more accurate in their predictions. GANs typically run unsupervised and use a cooperative zero-sum game framework to learn.  Essentially, GANs create their own training data.	What is Gan ML
7345	The correlation coefficient is determined by dividing the covariance by the product of the two variables' standard deviations. Standard deviation is a measure of the dispersion of data from its average.	How do you find the correlation coefficient with covariance
120	1 Answer. They are essentially the same; usually, we use the term log loss for binary classification problems, and the more general cross-entropy (loss) for the general case of multi-class classification, but even this distinction is not consistent, and you'll often find the terms used interchangeably as synonyms.	Is Log loss the same as cross entropy
4350	The chi-square distribution curve is skewed to the right, and its shape depends on the degrees of freedom df. For df > 90, the curve approximates the normal distribution. Test statistics based on the chi-square distribution are always greater than or equal to zero.	What is the basic shape of the chi square distribution
6946	SGD randomly picks one data point from the whole data set at each iteration to reduce the computations enormously. It is also common to sample a small number of data points instead of just one point at each step and that is called “mini-batch” gradient descent.	Which is also known as stochastic gradient descent
497	Computer Vision. Image processing is mainly focused on processing the raw input images to enhance them or preparing them to do other tasks. Computer vision is focused on extracting information from the input images or videos to have a proper understanding of them to predict the visual input like human brain.	What is computer vision and image processing
992	Supervised clustering is applied on classified examples with the objective of identifying clusters that have high probability density to a single class.  Semi-supervised clustering is to enhance a clustering algorithm by using side information in clustering process.	Can we use clustering for supervised learning
6	The following seven techniques can help you, to train a classifier to detect the abnormal class.Use the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	How does machine learning deal with imbalanced datasets
6363	1 : a branch of mathematics dealing with the collection, analysis, interpretation, and presentation of masses of numerical data. 2 : a collection of quantitative data.	What is the simple definition of statistics
664	Face validity: Does the content of the test appear to be suitable to its aims? Criterion validity: Do the results correspond to a different test of the same thing?	What is the difference between content validity and criterion validity
1560	A time series is a stochastic process that operates in continuous state space and discrete time set. A stochastic process is nothing but a set of random variables. It is a time dependent random phenomenon. Same is time series.	What is the difference between time series and stochastic process
1384	Basically it is a way to describe important visual features in such a way that they are found again even if the size and orientation of them changes in the future.	In intuitive explanation what is SIFT Scale Invariant Feature Transform
393	Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity.	What is the target variable in a decision tree
3469	A discrete variable is a variable whose value is obtained by counting. A continuous variable is a variable whose value is obtained by measuring.  A discrete random variable X has a countable number of possible values.	What are the differences between continuous and discrete variables
7566	Page 1. RANDOM VARIABLES. Random Processes: A random process may be thought of as a process where the outcome is probabilistic (also called stochastic) rather than deterministic in nature; that is, where there is uncertainty as to the result. Examples: 1. Tossing a die – we don't know in advance what number will come	What is random variable and random process
6442	Regression coefficients represent the mean change in the response variable for one unit of change in the predictor variable while holding other predictors in the model constant.  The coefficient indicates that for every additional meter in height you can expect weight to increase by an average of 106.5 kilograms.	What is a coefficient in a regression model
282	Artificial Intelligence enhances the speed, precision and effectiveness of human efforts. In financial institutions, AI techniques can be used to identify which transactions are likely to be fraudulent, adopt fast and accurate credit scoring, as well as automate manually intense data management tasks.	Why do we use AI
1821	Relationship extraction is the task of extracting semantic relationships from a text. Extracted relationships usually occur between two or more entities of a certain type (e.g. Person, Organisation, Location) and fall into a number of semantic categories (e.g. married to, employed by, lives in).	What is relation extraction in NLP
7389	Quartiles let us quickly divide a set of data into four groups, making it easy to see which of the four groups a particular data point is in. For example, a professor has graded an exam from 0-100 points.	How are quartiles used in real life
8044	The main hyperparameter of the SVM is the kernel. It maps the observations into some feature space.  The choice of the kernel and their hyperparameters affect greatly the separability of the classes (in classification) and the performance of the algorithm.	What are Hyperparameters in kernel SVM
3813	"With ""infinite"" numbers of successive random samples, the mean of the sampling distribution is equal to the population mean (µ). As the sample sizes increase, the variability of each sampling distribution decreases so that they become increasingly more leptokurtic."	Is Mean affected by sample size
7474	The coefficients used in simple linear regression can be found using stochastic gradient descent.  Linear regression does provide a useful exercise for learning stochastic gradient descent which is an important algorithm used for minimizing cost functions by machine learning algorithms.	Is gradient descent used in linear regression
4872	A convolutional layer acts as a fully connected layer between a 3D input and output. The input is the “window” of pixels with the channels as depth. This is the same with the output considered as a 1 by 1 pixel “window”. The kernel size of a convolutional layer is k_w * k_h * c_in * c_out.	What is the output of a convolutional layer
3432	Top 10 real-life examples of Machine LearningImage Recognition. Image recognition is one of the most common uses of machine learning.  Speech Recognition. Speech recognition is the translation of spoken words into the text.  Medical diagnosis.  Statistical Arbitrage.  Learning associations.  Classification.  Prediction.  Extraction.More items•	What are some real world applications of specific machine learning algorithms
821	Always remember ReLu should be only used in hidden layers. For classification, Sigmoid functions(Logistic, tanh, Softmax) and their combinations work well. But at the same time, it may suffer from vanishing gradient problem.	What activation function is better for the hidden layers
5080	To assess which word2vec model is best, simply calculate the distance for each pair, do it 200 times, sum up the total distance, and the smallest total distance will be your best model.	How do you evaluate a word2vec model
4511	Here are some of the top reasons why you should multitask.Keeps You Active. When doing a simple task, like maybe texting an important message on your phone, you can easily get distracted by various thoughts.  Tonic for the Brain.  Need of the Hour in this Fast-Changing world.  It is a Personality Trait.	What are the benefits of multitasking
120	So the probability that the sample mean will be >22 is the probability that Z is > 1.6 We use the Z table to determine this: P( > 22) = P(Z > 1.6) = 0.0548.	How do you find the probability of a sample mean
3742	Q-learning is called off-policy because the updated policy is different from the behavior policy, so Q-Learning is off-policy. In other words, it estimates the reward for future actions and appends a value to the new state without actually following any greedy policy.	Why is Q learning off policy
955	AI taking into account many levels of abstraction, embodied AI and multimodal interaction is also DAI. Distributed AI means AI solved by multiple smart or reasoning agents (communicant object, physical or software) where size of agents can be a simple rule or can be a human or more ambient or pervasive structure.	What is the difference between artificial intelligence AI and distributed AI
517	Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis.	What is the difference between a prior and a posterior probability
209	One of the primary foundations of machine learning is data mining. Data mining can be used to extract more accurate data. This ultimately helps refine your machine learning to achieve better results.	Is data mining important for machine learning
2182	In short, tokenization uses a token to protect the data, whereas encryption uses a key.  To access the original data, a tokenization solution exchanges the token for the sensitive data, and an encryption solution decodes the encrypted data to reveal its sensitive form.	How is tokenization different from encryption
3287	Regression Analysis > Forward selection is a type of stepwise regression which begins with an empty model and adds in variables one by one. In each forward step, you add the one variable that gives the single best improvement to your model.	What is forward selection in regression
7395	Inferential statistics lets you draw conclusions about populations by using small samples. Consequently, inferential statistics provide enormous benefits because typically you can't measure an entire population.	What are the advantages of inferential statistics
4537	The decomposition of time series is a statistical task that deconstructs a time series into several components, each representing one of the underlying categories of patterns. There are two principal types of decomposition, which are outlined below.	What is decomposition in statistics
475	Big data analysis caters to a large amount of data set which is also known as data mining, but data science makes use of the machine learning algorithms to design and develop statistical models to generate knowledge from the pile of big data.	What is difference between Data Science and Big Data
4482	"Here are 25 phases that you can use to increase confidence and self-esteem in your children.“You are capable.""  “That was brave.""  “You've got this.""  “I believe in you.""  “You can do hard things.""  “No matter what happens, I love you.""  “Let's try it together.""  “How'd you do that?""More items"	What words improve your confidence levels
5316	Hypothesis Testing — 2-tailed testSpecify the Null(H0) and Alternate(H1) hypothesis.Choose the level of Significance(α)Find Critical Values.Find the test statistic.Draw your conclusion.	How do you do a two sided hypothesis test
6786	t-test is used to test if two sample have the same mean. The assumptions are that they are samples from normal distribution. f-test is used to test if two sample have the same variance. Same assumptions hold.	What is the difference between a T statistic and an F statistic
17	Independent and Identically Distributed	What does IID stand for
970	Methods to Avoid Underfitting in Neural Networks—Adding Parameters, Reducing Regularization ParameterAdding neuron layers or input parameters.  Adding more training samples, or improving their quality.  Dropout.  Decreasing regularization parameter.	How do you reduce high bias in a neural net classifier
8531	Inverse transform sampling is a method for generating random numbers from any probability distribution by using its inverse cumulative distribution F−1(x). Recall that the cumulative distribution for a random variable X is FX(x)=P(X≤x).	How do you do inverse transformation
8166	Discriminative models, also referred to as conditional models or backward models, are a class of supervised machine learning used for classification or regression. These distinguish decision boundaries by inferring knowledge from observed data.	What is discriminative model in machine learning
481	Essentially, multivariate analysis is a tool to find patterns and relationships between several variables simultaneously. It lets us predict the effect a change in one variable will have on other variables.	What is the purpose of multivariate analysis
3458	SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.	Why is SVM used for image classification
6124	The kernel parameter σ is sensitive to the one-class classification model with the Gaussian RBF Kernel. This sigma selection method uses a line search with an state-of-the-art objective function to find the optimal value. The kernel matrix is the bridge between σ and the model.	What is Sigma in RBF kernel
1086	Local features refer to a pattern or distinct structure found in an image, such as a point, edge, or small image patch. They are usually associated with an image patch that differs from its immediate surroundings by texture, color, or intensity.	What are local features of an image
8259	A recurrent neural network, however, is able to remember those characters because of its internal memory. It produces output, copies that output and loops it back into the network. Simply put: recurrent neural networks add the immediate past to the present.	How do recurrent neural networks work
3731	With this method people need to remember their target blood sugar level. Subtract the target blood sugar from the current sugar to calculate the gap. Then divide by the Correction (sensitivity) Factor to calculate the correction dose. Discuss your target levels with your health care team (see Question 1).	How do you use correction factor
2248	Scalable Machine Learning occurs when Statistics, Systems, Machine Learning and Data Mining are combined into flexible, often nonparametric, and scalable techniques for analyzing large amounts of data at internet scale.	What is scalable machine learning
8524	Recurrent Neural Networks are best suited for Text Processing.	Which neural networks are best suited for text processing
4131	If the level of significance is α = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28. For a two tailed test, use α/2 = 0.05 and the critical region is below z = -1.645 and above z = 1.645.	How do you find the critical region of Z test
224	Answer : Algorithm is a noun meaning some special process of solving a certain type of problem.  Whereas logarithm, again a noun, is the exponent of that power of a fixed number, called the base, which equals a given number, called the antilogarithm.	What is the difference between a logarithm and an algorithm
839	Regression is mainly used in order to make estimates or predictions for the dependent variable with the help of single or multiple independent variables, and ANOVA is used to find a common mean between variables of different groups.	Why use multiple regression instead of Anova
5079	Probability and the Normal Curve The normal distribution is a continuous probability distribution. This has several implications for probability. The total area under the normal curve is equal to 1. The probability that a normal random variable X equals any particular value is 0.	Is a normal distribution a discrete probability distribution
295	"The power of a hypothesis test is affected by three factors. Sample size (n). Other things being equal, the greater the sample size, the greater the power of the test.  The greater the difference between the ""true"" value of a parameter and the value specified in the null hypothesis, the greater the power of the test."	How does power affect sample size
1198	"The bag-of-words feature vector is the sum of all one-hot vectors of the words, and therefore has a non-zero value for every word that occurred.  Continuous bag-of-words (CBOW) is exactly the same, but instead of using sparse vectors to represent words, it uses dense vectors (continuous distributional ""embeddings"")."	What is the difference between the Bag of Words model and the Continuous Bag of Words model
868	In statistics, the generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution.	What are generalized linear models used for
913	Markov analysis is a method used to forecast the value of a variable whose predicted value is influenced only by its current state, and not by any prior activity.  Markov analysis is often used for predicting behaviors and decisions within large groups of people.	What is Markov analysis
7316	DEFINITION: Primary sampling unit refers to Sampling units that are selected in the first (primary) stage of a multi-stage sample ultimately aimed at selecting individual elements.	What is a primary sampling unit
6273	The main difference is that ABM typically implement low numbers of highly complex agents, and the main feature they consider are their individual capabilities to face the task. On the opposite, MAS consider (very) large numbers of simpler agents, focusing on the emergence of new phenomena from social interactions.	What is the difference between agent based simulation ABS and multi agent system MAS )
6837	A local vector has integer-typed and 0-based indices and double-typed values, stored on a single machine. MLlib supports two types of local vectors: dense and sparse. A dense vector is backed by a double array representing its entry values, while a sparse vector is backed by two parallel arrays: indices and values.	In machine learning what is the difference between sparse vector and dense vector
266	In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated.  Consistent estimators converge in probability to the true value of the parameter, but may be biased or unbiased; see bias versus consistency for more.	Is an estimator biased
488	In the future, artificial intelligence (AI) is likely to substantially change both marketing strategies and customer behaviors.  Finally, the authors suggest AI will be more effective if it augments (rather than replaces) human managers. AI is going to make our lives better in the future.	How AI will change the future
2759	Distributed deep learning is a sub-area of general distributed machine learning that has recently become very prominent because of its effectiveness in various applications.	What is distributed deep learning
1657	The probability of Type 1 error is alpha -- the criterion that we set as the level at which we will reject the null hypothesis. The p value is something else -- it tells you how UNUSUAL the data are, given the assumption that the null hypothesis is true.	Is the p value the same as type I error alpha
1492	SummaryWeighted Mean: A mean where some values contribute more than others.When the weights add to 1: just multiply each weight by the matching value and sum it all up.Otherwise, multiply each weight w by its matching value x, sum that all up, and divide by the sum of weights: Weighted Mean = ΣwxΣw.	How do you calculate weighted mean
5436	8:3417:13Suggested clip · 72 secondsStepwise regression procedures in SPSS (new, 2018) - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret a stepwise regression analysis
1788	The Best Tools for Machine Learning Model VisualizationLook at evaluation metrics (also you should know how to choose an evaluation metric for your problem)Look at performance charts like ROC, Lift Curve, Confusion Matrix and others.Look at learning curves to estimate overfitting.Look at model predictions on best/worst cases.More items•	How do you visualize machine learning models
4873	Simply put, a random sample is a subset of individuals randomly selected by researchers to represent an entire group as a whole. The goal is to get a sample of people that is representative of the larger population.	What is the main purpose of random sampling
7436	If you want a representative sample of a particular population, you need to ensure that:The sample source includes all the target population.The selected data collection method (online, phone, paper, in person) can reach individuals that represent that target population.More items•	How do you ensure sample is representative of population
1902	Recall and True Positive Rate (TPR) are exactly the same. So the difference is in the precision and the false positive rate.  While precision measures the probability of a sample classified as positive to actually be positive, the false positive rate measures the ratio of false positives within the negative samples.	Is recall the same as true positive rate
4403	Computers analyze, understand and derive meaning by processing human languages using NLP.  By analysing text, computers infer how humans speak, and this computerized understanding of human languages can be exploited for numerous use-cases.	What is state of art in NLP
2280	Existing multi-task learning explores the relatedness with other tasks, but disre- gards the consistency among different views of a single task; whereas existing multi-view learning ignores the label information from other related tasks.	What is the difference between multi view learning and multi task learning
1132	H is the measurement matrix. This matrix influences the Kalman Gain.  R is the sensor noise matrix. This matrix implies the measurement error covariance, based on the amount of sensor noise. In this simulation, Q and R are constants, but some implementations of the Kalman Filter may adjust them throughout execution.	What is H in Kalman filter
866	Boosting. Another general machine learning ensemble method is known as boosting. Boosting differs somewhat from bagging as it does not involve bootstrap sampling.	Does boosting use bootstrap
4663	Genetic algorithms are important in machine learning for three reasons. First, they act on discrete spaces, where gradient-based methods cannot be used. They can be used to search rule sets, neural network architectures, cellular automata computers, and so forth.	Are genetic algorithms machine learning
3205	Kmeans clustering algorithm is applied to reduced datasets which is done by principal component analysis dimension reduction method. Cluster analysis is one of the major data analysis methods widely used for many practical applications in emerging areas[12].	Can clustering be used for reducing dimension
3870	It is used in multinomial logistic regression and is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes. , and the components will add up to 1, so that they can be interpreted as probabilities.	Artificial Neural Networks Why do we use softmax function for output layer
4274	In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed.  A log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive.	What does a lognormal distribution mean
3	Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task.  Transfer learning is an optimization that allows rapid progress or improved performance when modeling the second task.	How does transfer learning work
3641	Blob detectors can detect areas in an image which are too smooth to be detected by a corner detector. Consider shrinking an image and then performing corner detection. The detector will respond to points which are sharp in the shrunk image, but may be smooth in the original image.	How can I identify the features of an image
1143	The standard error is a statistical term that measures the accuracy with which a sample distribution represents a population by using standard deviation. In statistics, a sample mean deviates from the actual mean of a population—this deviation is the standard error of the mean.	How do you interpret standard error
8146	There are several different common loss functions to choose from: the cross-entropy loss, the mean-squared error, the huber loss, and the hinge loss – just to name a few.”	What are different loss functions in machine learning
5307	Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models.	What is data preprocessing in machine learning
2653	Parametric tests assume underlying statistical distributions in the data.  For example, Student's t-test for two independent samples is reliable only if each sample follows a normal distribution and if sample variances are homogeneous. Nonparametric tests do not rely on any distribution.	What are parametric and non parametric statistics
8249	E(Y | Xi) = f (Xi) is known as conditional expectation function(CEF) or population regression function (PRF) or population regression (PR) for short. In simple terms, it tells how the mean or average of response of Y varies with X.	What is the conditional expectation function or the population regression function
405	Covariance can be positive, zero, or negative.  If X and Y are independent variables, then their covariance is 0: Cov(X, Y ) = E(XY ) − µXµY = E(X)E(Y ) − µXµY = 0 The converse, however, is not always true.	How do we prove that the co variance of two independent random variables is equal to 0
1760	Once you find a correlation, you can test for causation by running experiments that “control the other variables and measure the difference.” Two such experiments or analyses you can use to identify causation with your product are: Hypothesis testing. A/B/n experiments.	How do you test for causation
3908	The population standard deviation is a parameter, which is a fixed value calculated from every individual in the population. A sample standard deviation is a statistic. This means that it is calculated from only some of the individuals in a population.	What is the difference between sample standard deviation and population standard deviation
2288	Fei-Fei Li, computer vision is defined as “a subset of mainstream artificial intelligence that deals with the science of making computers or machines visually enabled, i.e., they can analyze and understand an image.” Human vision starts at the biological camera's “eyes,” which takes one picture about every 200	What is vision in artificial intelligence
195	Fit the regression model by unweighted least squares and analyze the residuals.  Estimate the variance function or the standard deviation function.  Use the fitted values from the estimated variance or standard deviation function to obtain the weights.  Estimate the regression coefficients using these weights.	How do you do weighted regression
2887	Prior probability, in Bayesian statistical inference, is the probability of an event before new data is collected. This is the best rational assessment of the probability of an outcome based on the current knowledge before an experiment is performed.	What is meant by prior probability
1248	Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example.	What is the most frequent metric to assess model accuracy for classification problems
4786	) because the integral controller also reduces the rise time and increases the overshoot as the proportional controller does (double effect). The above response shows that the integral controller eliminated the steady-state error in this case.	What is the effect of PI controller on the system performance
4392	The CAC ratio is calculated by looking at the quarter over quarter increase in gross margin divided by the total sales and marketing expenses for that quarter. Gross margin is the total revenue minus cost of goods sold.	How is CAC ratio calculated
3659	Advantages of Neural Networks:Neural Networks have the ability to learn by themselves and produce the output that is not limited to the input provided to them.The input is stored in its own networks instead of a database, hence the loss of data does not affect its working.More items•	What are the advantages of neural network
7491	In computational linguistics, second-order co-occurrence pointwise mutual information is a semantic similarity measure. To assess the degree of association between two given words, it uses pointwise mutual information (PMI) to sort lists of important neighbor words of the two target words from a large corpus.	How does second order pointwise mutual information information retrieval work
2908	There are NO assumptions in any linear model about the distribution of the independent variables. Yes, you only get meaningful parameter estimates from nominal (unordered categories) or numerical (continuous or discrete) independent variables.  They do not need to be normally distributed or continuous.	Should independent and dependent variables be normally distributed for linear regression
7850	Because OLS regression is based on the equation for a straight line: y=a+bx. If you have data you know are non-linear, and you attemp to use OLS to regress it, you're guaranteed a large amount of error (which is essentially an error in choosing the model, rather than the data's conformity to it.	Why cant we use the ordinary least squares method to fit the data in non linear regression
1306	Intuitively, two random variables X and Y are independent if knowing the value of one of them does not change the probabilities for the other one. In other words, if X and Y are independent, we can write P(Y=y|X=x)=P(Y=y), for all x,y.	If X and Y are independent random variables then are X and X+Y independent random variables
1407	Applications of Principal Component Analysis PCA is predominantly used as a dimensionality reduction technique in domains like facial recognition, computer vision and image compression. It is also used for finding patterns in data of high dimension in the field of finance, data mining, bioinformatics, psychology, etc.	What is the application of principal component analysis
6983	A quantile is the value below which a fraction of observations in a group falls. For example, a prediction for quantile 0.9 should over-predict 90% of the times. Given a prediction yi^p and outcome yi, the mean regression loss for a quantile q is. For a set of predictions, the loss will be its average.	What is Average Quantile Loss
388	These are three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.	What are the different types of machine learning
3017	A canonical variate is a new variable (variate) formed by making a linear combination of two or more variates (variables) from a data set. A linear combination of variables is the same as a weighted sum of variables.	What is a canonical variate
1286	Simulated Annealing (SA) is a global optimization algorithm. It belongs to the stochastic optimization algorithms.  By analogy with this physical process, each step of the SA algorithm attempts to replace the current solution by a random solution till the desired output is obtained.	Is simulated annealing machine learning
2990	Disadvantages of decision trees: They are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree. They are often relatively inaccurate. Many other predictors perform better with similar data.	What is the problem with decision tree
5423	A latent variable is a random variable which you can't observe neither in training nor in test phase . It is derived from the latin word latēre which means hidden. Intuitionally, some phenomenons like incidences,altruism one can't measure while others like speed or height one can.	What are latent variables in machine learning
3658	Big data analytics is the use of advanced analytic techniques against very large, diverse data sets that include structured, semi-structured and unstructured data, from different sources, and in different sizes from terabytes to zettabytes.	What is big data and analytics
4561	Neural networks are often compared to decision trees because both methods can model data that has nonlinear relationships between variables, and both can handle interactions between variables.	What are some advantages of using neural networks over decision trees
624	In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.	What does regularization mean
2175	The fitness function simply defined is a function which takes a candidate solution to the problem as input and produces as output how “fit” our how “good” the solution is with respect to the problem in consideration. Calculation of fitness value is done repeatedly in a GA and therefore it should be sufficiently fast.	What does fitness function represent to describe optimization problem
1427	The cross-entropy compares the model's prediction with the label which is the true probability distribution. The cross-entropy goes down as the prediction gets more and more accurate. It becomes zero if the prediction is perfect. As such, the cross-entropy can be a loss function to train a classification model.	How does cross entropy work
4304	Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).	What is sequence learning used for
3484	The squared error has some nice properties: It is symmetrical. That means, if the actual value is and you predict or , you get the same error measure.	Why we take SSE sum of square error and RMSE root mean square error
1489	Currently AI is Used is Following Things/Fields: Retail, Shopping and Fashion. Security and Surveillance. Sports Analytics and Activities. Manufacturing and Production.	How is AI currently being used
3286	Di Excel, klik Power Pivot > Kelola untuk membuka jendela Power Pivot . Menampilkan tab di jendela Power Pivot . Setiap tab berisi tabel dalam model Anda. Kolom dalam setiap tabel muncul sebagai bidang dalam daftar bidang PivotTable.	Bagaimana cara membuka pivot
810	"In statistical hypothesis testing, a type I error is the rejection of a true null hypothesis (also known as a ""false positive"" finding or conclusion; example: ""an innocent person is convicted""), while a type II error is the non-rejection of a false null hypothesis (also known as a ""false negative"" finding or conclusion"	How does a Type I error differ from a Type II error
1434	The general guideline is to use linear regression first to determine whether it can fit the particular type of curve in your data. If you can't obtain an adequate fit using linear regression, that's when you might need to choose nonlinear regression.	How do you know when to use linear or nonlinear regression
2357	Homogeneous sampling is a purposive sampling technique that aims to achieve a homogeneous sample; that is, a sample whose units (e.g., people, cases, etc.) share the same (or very similar) characteristics or traits (e.g., a group of people that are similar in terms of age, gender, background, occupation, etc.).	What is a homogeneous sample
3240	The cumulative frequency is calculated by adding each frequency from a frequency distribution table to the sum of its predecessors. The last value will always be equal to the total for all observations, since all frequencies will already have been added to the previous total.	How do you construct a cumulative frequency distribution
581	The three main metrics used to evaluate a classification model are accuracy, precision, and recall. Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.	How do you evaluate machine learning algorithms
4956	A/B testing is one of the components of the overarching process of Conversion Rate Optimization (CRO) using which you can gather both qualitative and quantitative user insights and use them to understand your potential customers and to optimize your conversion funnel based on that data.	What is A B testing 1
7006	Sample moments are those that are utilized to approximate the unknown population moments. Sample moments are calculated from the sample data. Such moments include mean, variance, skewness, and kurtosis.	What is a sample moment
3636	The hclust function in R uses the complete linkage method for hierarchical clustering by default. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components.	What is the R function to apply hierarchical clustering
278	Yes, although 'linear regression' refers to any approach to model the relationship between one or more variables, OLS is the method used to find the simple linear regression of a set of data.	Is OLS the same as linear regression
8598	Java, Python, Lisp, Prolog, and C++ are major AI programming language used for artificial intelligence capable of satisfying different needs in the development and designing of different software.	Which program is used for artificial intelligence
7124	Answer. To calculate the class interval, first step is to rewrite the table by including the values of mid-interval in place of the values given in range. Then the sum of all the mid- interval values is calculated.	How do you convert Class interval to mid value
3253	Classification is a type of supervised learning. It specifies the class to which data elements belong to and is best used when the output has finite and discrete values. It predicts a class for an input variable as well.	What is ML classification
572	Definition: Bagging is used when the goal is to reduce the variance of a decision tree classifier. Here the objective is to create several subsets of data from training sample chosen randomly with replacement. Each collection of subset data is used to train their decision trees.	Why do we use bagging
4020	AI works by combining large amounts of data with fast, iterative processing and intelligent algorithms, allowing the software to learn automatically from patterns or features in the data.  The process requires multiple passes at the data to find connections and derive meaning from undefined data.	What is the work of artificial intelligence
2449	Trajectory clustering aims at finding out trajectories that are of the same (or similar) pattern, or distinguishing some undesired behaviors (such as outliers). The activities of moving objects are often recorded as their trajectories.	What is trajectory clustering
7841	If skewness is positive, the data are positively skewed or skewed right, meaning that the right tail of the distribution is longer than the left. If skewness is negative, the data are negatively skewed or skewed left, meaning that the left tail is longer. If skewness = 0, the data are perfectly symmetrical.	How do you comment on skewness in statistics
20	A squashing function is essentially defined as a function that squashes the input to one of the ends of a small interval. In Neural Networks, these can be used at nodes in a hidden layer to squash the input.  Popular ones that have been used include the sigmoid function, hyperbolic tangent function, etc.	What is squashing in machine learning
424	You can use regression equations to make predictions. Regression equations are a crucial part of the statistical output after you fit a model.  However, you can also enter values for the independent variables into the equation to predict the mean value of the dependent variable.	Can regression be used for prediction
5803	You can use a bivariate Pearson Correlation to test whether there is a statistically significant linear relationship between height and weight, and to determine the strength and direction of the association.	When would you use a bivariate correlation
1391	Kosaraju's algorithm finds the strongly connected components of a graph.  - For each vertex u of the graph do Visit(u), where Visit(u) is the recursive subroutine: - If u is unvisited then: - Mark u as visited. - For each out-neighbour v of u, do Visit(v).	How does kosaraju algorithm work
60	Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.	How do you use logistic regression for multi class classification
3520	How to Find a Sample Size Given a Confidence Interval and Width (unknown population standard deviation)za/2: Divide the confidence interval by two, and look that area up in the z-table: .95 / 2 = 0.475.  E (margin of error): Divide the given width by 2. 6% / 2.  : use the given percentage. 41% = 0.41.  : subtract. from 1.	How do you find the sample size given a confidence interval and width known as population standard deviation
8449	In mathematics, a random walk is a mathematical object, known as a stochastic or random process, that describes a path that consists of a succession of random steps on some mathematical space such as the integers.	What is random walk in stochastic process
4317	Also known as implicit social cognition, implicit bias refers to the attitudes or stereotypes that affect our understanding, actions, and decisions in an unconscious manner.	What does Implicit Bias mean
6263	Rabin (1987) that uses hashing to find an exact match of a pattern string in a text. It uses a rolling hash to quickly filter out positions of the text that cannot match the pattern, and then checks for a match at the remaining positions.	How does the Rolling Hash function used in Rabin Karp algorithm work
1618	0:042:26:08Suggested clip · 98 secondsStructural Equation Modeling Full Course | Structural Equation YouTubeStart of suggested clipEnd of suggested clip	How do you learn structural equation modeling
859	Naive Bayes is a kind of classifier which uses the Bayes Theorem. It predicts membership probabilities for each class such as the probability that given record or data point belongs to a particular class. The class with the highest probability is considered as the most likely class.	How does naive Bayes classification work
399	80% accurate. Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.  Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes.	What is the difference between accuracy precision recall
312	Box plots are useful as they show outliers within a data set. An outlier is an observation that is numerically distant from the rest of the data. When reviewing a box plot, an outlier is defined as a data point that is located outside the whiskers of the box plot.	Can a Boxplot be used to detect outliers
620	The rate of occurrence for Type I errors equals the significance level of the hypothesis test, which is also known as alpha (α).  Hypothesis tests define that standard using the probability of rejecting a null hypothesis that is actually true. You set this value based on your willingness to risk a false positive.	How does the probability of a Type I error relate to the significance level in a hypothesis test
1271	Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.	What is feature engineering in ML
1837	Two events are mutually exclusive if the probability of them both occurring is zero, that is if Pr(A∩B)=0. With that definition, disjoint sets are necessarily mutually exclusive, but mutually exclusive events aren't necessarily disjoint.	Can two sets be disjoint but not mutually exclusive
6886	A one hot encoding is a representation of categorical variables as binary vectors.  Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.	What is one hot encoded vector
892	Improve your model accuracy by Transfer Learning.Loading data using python libraries.Preprocess of data which includes reshaping, one-hot encoding and splitting.Constructing the model layers of CNN followed by model compiling, model training.Evaluating the model on test data.Finally, predicting the correct and incorrect labels.	How can transfer learning improve accuracy
3034	SECOND RANK TENSOR PROPERTIES. Many properties are tensors that relate one vector to another or relate a scalar to a tensor. If the driving force and the response are collinear the property can be expressed as a scalar, but when that are not, the property must be expressed as a second rank tensor.	What is a second rank tensor
2969	Spectral analysis allows transforming a time series into its coordinates in the space of frequencies, and then to analyze its characteristics in this space. The magnitude and phase can be extracted from the coordinates. Spectral analysis is a very general method used in a variety of domains.	Why is spectral analysis useful
5821	Binomial distributions must also meet the following three criteria:The number of observations or trials is fixed.  Each observation or trial is independent.  The probability of success (tails, heads, fail or pass) is exactly the same from one trial to another.	How do you find the binomial distribution
2703	In Reinforcement Learning, this type of decision is called exploitation when you keep doing what you were doing, and exploration when you try something new. Naturally this raises a question about how much to exploit and how much to explore.	What is exploitation in reinforcement learning
3491	If the biggest problem with supervised learning is the expense of labeling the training data, the biggest problem with unsupervised learning (where the data is not labeled) is that it often doesn't work very well.	What is the problem with unsupervised learning
4464	2.3 Taguchi Taguchi. The Taguchi method optimizes design parameters to minimize variation before optimizing design to hit mean target values for output parameters. The Taguchi method uses special orthogonal arrays to study all the design factors with minimum of experiments.	How does Taguchi method work
1032	Event B is said to be independent of event A if P(A & B)  The joint probability equals the sum of the marginal probabilities minus the probability that either event will occur; that is, P(A & B) = P(A) + P(B) - P(A or B).	What does it mean for event B to be independent of event A
8438	Hidden layers allow for the function of a neural network to be broken down into specific transformations of the data. Each hidden layer function is specialized to produce a defined output.	What is the purpose of hidden layers
3846	The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution.	How does the T distribution differ from the standard normal distribution
7810	1 : a range of medium length. 2 : the midpoint of a range (as of distance or time) 3 : a middle portion (as of a range of musical pitch)	What does mid range mean
3533	The null hypothesis of the Kruskal–Wallis test is that the mean ranks of the groups are the same.	What is the null hypothesis for Kruskal Wallis test
5172	A decision tree is one of the supervised machine learning algorithms. This algorithm can be used for regression and classification problems — yet, is mostly used for classification problems. A decision tree follows a set of if-else conditions to visualize the data and classify it according to the conditions.	What is decision tree and example
1032	Mathematically speaking, the regret is expressed as the difference between the payoff (reward or return) of a possible action and the payoff of the action that has been actually taken. If we denote the payoff function as u the formula becomes: regret = u(possible action) - u(action taken)	What is regret in reinforcement learning
3605	Regression Techniques Regression algorithms are machine learning techniques for predicting continuous numerical values. They are supervised learning tasks which means they require labelled training examples.	What is the best machine learning algorithm to predict numerical data
699	Use regression analysis to describe the relationships between a set of independent variables and the dependent variable. Regression analysis produces a regression equation where the coefficients represent the relationship between each independent variable and the dependent variable.	What does regression analysis tell you
3653	Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities.	What is modality in machine learning
6171	The Dirichlet distribution is a conjugate prior for the multinomial distribution. This means that if the prior distribution of the multinomial parameters is Dirichlet then the posterior distribution is also a Dirichlet distribution (with parameters different from those of the prior).	What is a conjugate prior and why is Dirichlet distribution a conjugate prior for multinomial distribution
115	More specifically the multiple linear regression fits a line through a multi-dimensional space of data points. The simplest form has one dependent and two independent variables. The dependent variable may also be referred to as the outcome variable or regressand.	How many variables do you need to have multiple linear regression
6894	A box plot (also known as box and whisker plot) is a type of chart often used in explanatory data analysis to visually show the distribution of numerical data and skewness through displaying the data quartiles (or percentiles) and averages.	What is Boxplot used for
7009	Descriptive Analytics tells you what happened in the past.  Predictive Analytics predicts what is most likely to happen in the future. Prescriptive Analytics recommends actions you can take to affect those outcomes.	What is the difference between predictive analytics and descriptive analytics
2169	In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification).	What is multi class classification in machine learning
8557	For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes). Note that these predictions are specific to the user, but use information gleaned from many users.	What are some examples of user information required by recommendation engines that use collaborative filtering
1259	Since is more degree 4 will be more complex(overfit the data) than the degree 3 model so it will again perfectly fit the data. In such case training error will be zero but test error may not be zero.	What will happen when you fit degree 4 polynomial in linear regression
5875	An expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.	What inference engine module does in expert system
5847	One common method of consolidating two probability distributions is to simply average them - for every set of values A, set If the distributions both have densities, for example, averaging the probabilities results in a probability distribution with density the average of the two input densities (Figure 1).	How do you combine probability distributions
3656	Traditional algorithms involving face recognition work by identifying facial features by extracting features, or landmarks, from the image of the face. For example, to extract facial features, an algorithm may analyse the shape and size of the eyes, the size of nose, and its relative position with the eyes.	How does face recognition algorithm work
571	The hazard rate measures the propensity of an item to fail or die depending on the age it has reached. It is part of a wider branch of statistics called survival analysis, a set of methods for predicting the amount of time until a certain event occurs, such as the death or failure of an engineering system or component.	What is hazard rate in survival analysis
6485	Real time processing requires a continual input, constant processing, and steady output of data. A great example of real-time processing is data streaming, radar systems, customer service systems, and bank ATMs, where immediate processing is crucial to make the system work properly.	What are the examples of real time processing
87	Events are independent if the outcome of one event does not affect the outcome of another. For example, if you throw a die and a coin, the number on the die does not affect whether the result you get on the coin.	Which is the example of independent probability
1848	The blur, or smoothing, of an image removes “outlier” pixels that may be noise in the image. Blurring is an example of applying a low-pass filter to an image. In computer vision, the term “low-pass filter” applies to removing noise from an image while leaving the majority of the image intact.	What is blurring in image processing
570	Decision trees are prone to overfitting, especially when a tree is particularly deep. This is due to the amount of specificity we look at leading to smaller sample of events that meet the previous assumptions. This small sample could lead to unsound conclusions.	What causes overfitting in decision tree
1822	How to calculate the absolute error and relative errorTo find out the absolute error, subtract the approximated value from the real one: |1.41421356237 - 1.41| = 0.00421356237.Divide this value by the real value to obtain the relative error: |0.00421356237 / 1.41421356237| = 0.298%	How do you find the absolute and relative error
2281	This means a fewer neurons are firing ( sparse activation ) and the network is lighter. This may never be the case in Tanh or Sigmoid.	What does it mean that activation functions like ReLUs in NNs induce sparsity in the hidden units
6685	Systematic random sampling: Systematic random sampling is a method to select samples at a particular preset interval. As a researcher, select a random starting point between 1 and the sampling interval.  (The number of elements in the population divided by the number of elements needed for the sample.)	What is systematic random sampling
7397	One drawback of boxplots is that they tend to emphasize the tails of a distribution, which are the least certain points in the data set. They also hide many of the details of the distribution.	What are the disadvantages of a box plot
211	FNV-1 is rumoured to be a good hash function for strings. For long strings (longer than, say, about 200 characters), you can get good performance out of the MD4 hash function.	What is a good hash function for strings
3309	The heuristic function is a way to inform the search about the direction to a goal.  It provides an informed way to guess which neighbor of a node will lead to a goal. There is nothing magical about a heuristic function. It must use only information that can be readily obtained about a node.	Artificial intelligence what is a heuristic function
4013	Classification is a machine learning concept. It is used for categorical dependent variables, where we need to classify into required groups. Logistic regression is a algorithm within classification.	What is the difference between logistic regression and classification
4658	Graham's law states that the rate of diffusion or of effusion of a gas is inversely proportional to the square root of its molecular weight.  In the same conditions of temperature and pressure, the molar mass is proportional to the mass density.	What is Graham's law of effusion and diffusion
621	Exploring the popular deep learning approach. Transfer learning is the reuse of a pre-trained model on a new problem. It's currently very popular in deep learning because it can train deep neural networks with comparatively little data.	Is transfer learning part of deep learning
6101	in statistics. Regression gives you the linear trend of the outcomes; residuals are the randomness that's “left over” from fitting a regression model. The correlation between the explanatory variable(s) and the residuals is/are zero because there's no linear trend left - it's been removed by the regression.	In laymans terms why cant the residual be correlated with the explanatory variable
3338	The cumulative distribution function (cdf) is the probability that the variable takes a value less than or equal to x. That is. F(x) = Pr[X \le x] = \alpha. For a continuous distribution, this can be expressed mathematically as. F(x) = \int_{-\infty}^{x} {f(\mu) d\mu}	What does cumulative distribution function mean
393	From Wikipedia, the free encyclopedia. An odds ratio (OR) is a statistic that quantifies the strength of the association between two events, A and B.	What is or in statistics
608	The sample proportion is the proportion of individuals in a sample sharing a certain trait, denoted ˆp.	What is the definition of a sample proportion in statistics
7486	If the impulse is at a non-zero frequency (at ω = ω0 ) in the frequency domain (i.e. the time domain. In other words, the Fourier Transform of an everlasting exponential ejω0t is an impulse in the frequency spectrum at ω = ω0 . An everlasting exponential ejωt is a mathematical model.	What is the Fourier transform of impulse function
1401	Fuzzy logic is used in Natural language processing and various intensive applications in Artificial Intelligence. It is extensively used in modern control systems such as expert systems. Fuzzy Logic mimics how a person would make decisions, only much faster. Thus, you can use it with Neural Networks.	How is fuzzy logic used in artificial intelligence
5498	An estimate is unbiased if its expected value equals the true parameter value. This will be true for all sample sizes and is exact whereas consistency is asymptotic and only is approximately equal and not exact.  The sample estimate of standard deviation is biased but consistent.	Can a consistent estimator be biased
7388	Key differences Regression attempts to establish how X causes Y to change and the results of the analysis will change if X and Y are swapped. With correlation, the X and Y variables are interchangeable. Regression assumes X is fixed with no error, such as a dose amount or temperature setting.	What is the difference between multiple correlation and multiple regression
234	Blocks and strata are different. Blocking refers to classifying experimental units into blocks whereas stratification refers to classifying individuals of a population into strata. The samples from the strata in a stratified random sample can be the blocks in an experiment.	In Experimental Design what is the difference between blocking and stratified sampling
6535	William S. Gosset	Who invented t distribution
6260	The p-value is a matter of convenience for us. STATA automatically takes into account the number of degrees of freedom and tells us at what level our coefficient is significant. If it is significant at the 95% level, then we have P < 0.05.	Where is the p value in Stata
5504	Direct link to this answer The rule of thumb for Gaussian filter design is to choose the filter size to be about 3 times the standard deviation (sigma value) in each direction, for a total filter size of approximately 6*sigma rounded to an odd integer value.	How do you choose the sigma for a Gaussian filter
5620	The output, y is generated from a normal (Gaussian) Distribution characterized by a mean and variance. In contrast to OLS, we have a posterior distribution for the model parameters that is proportional to the likelihood of the data multiplied by the prior probability of the parameters.	How does Bayesian regression work
2347	No, moment of inertia is a tensor quantity. Sometimes it behaves as scalar & sometimes as a vector. Sometimes it depends on the directions and sometimes depends on distribution of mass of the particles in the object.	Is moment of inertia a tensor quantity
1197	Knowledge gaps can be identified by means of questionnaires or review of test scores from in training or board examinations. Correcting gaps in knowledge is important, but usually has the least impact on improving competence or performance and outcomes for patients.	What techniques would you like to use for identifying learning gaps of the students
1758	The mean of the negative binomial distribution with parameters r and p is rq / p, where q = 1 – p. The variance is rq / p2. The simplest motivation for the negative binomial is the case of successive random trials, each having a constant probability P of success.	What is the mean and variance of negative binomial distribution
3181	The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable.	Why is sigmoid a good activation function
1019	The repetitive nearest-neighbor algorithm. The nearest-neighbor algorithm depends on what vertex you choose to start from. The repetitive nearest-neighbor algorithm says to try each vertex as starting point, and then choose the best answer.	What is repetitive Nearest Neighbor algorithm
5039	The reason n-1 is used is because that is the number of degrees of freedom in the sample. The sum of each value in a sample minus the mean must equal 0, so if you know what all the values except one are, you can calculate the value of the final one.	Why is there a degree of freedom of n 1 for sample standard deviation
7027	Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks. Standardization assumes that your data has a Gaussian (bell curve) distribution.	When should you Normalise data
5449	In medical testing, false negatives may provide a falsely reassuring message to patients and physicians that disease is absent, when it is actually present. This sometimes leads to inappropriate or inadequate treatment of both the patient and their disease. So, it is desired to have too many false positive.	Is it better to have too many false positives or too many false negatives explain
414	The optimal number of clusters can be defined as follow:Compute clustering algorithm (e.g., k-means clustering) for different values of k.  For each k, calculate the total within-cluster sum of square (wss).Plot the curve of wss according to the number of clusters k.More items	How do you find the optimal number of clusters
1871	Tests of hypotheses that can be made from a single sample of data were discussed on the foregoing page. As with null hypotheses, confidence intervals can be two-sided or one-sided, depending on the question at hand.	Can confidence intervals be one tailed
4023	A probability frequency distribution is a way to show how often an event will happen. It also shows what the probability of each event happening is. A frequency distribution table can be created by hand, or you can make a frequency distribution table in Excel.	What is probability frequency distribution
4347	A t-test tests a null hypothesis about two means; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero.  A chi-square test tests a null hypothesis about the relationship between two variables.	What is the difference between t test and chi square
827	Neural style transfer is trained as a supervised learning task in which the goal is to input two images (x), and train a network to output a new, synthesized image (y).	Is neural style transfer supervised learning
6551	Negative binomial regression is for modeling count variables, usually for over-dispersed count outcome variables. Please note: The purpose of this page is to show how to use various data analysis commands. It does not cover all aspects of the research process which researchers are expected to do.	What is a negative binomial regression model
3725	A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature.	How do you explain a neural network
2656	Data Mining Techniques: Algorithm, Methods & Top Data Mining#1) Frequent Pattern Mining/Association Analysis.#2) Correlation Analysis.#3) Classification.#4) Decision Tree Induction.#5) Bayes Classification.#6) Clustering Analysis.#7) Outlier Detection.#8) Sequential Patterns.More items•	What are some major data mining methods and algorithms
2344	Similar to the t-test/correlation equivalence, the relationship between two dichotomous variables is the same as the difference between two groups when the dependent variable is dichotmous. The appropriate test to compare group differences with a dichotmous outcome is the chi-square statistic.	Which statistical technique is appropriate for find out the correlation between two dichotomous variables
5583	In convolutional networks, multiple filters are taken to slice through the image and map them one by one and learn different portions of an input image. Imagine a small filter sliding left to right across the image from top to bottom and that moving filter is looking for, say, a dark edge.	What is convolutional filter
811	Weights and biases (commonly referred to as w and b) are the learnable parameters of a machine learning model.  When the inputs are transmitted between neurons, the weights are applied to the inputs along with the bias. A neuron. Weights control the signal (or the strength of the connection) between two neurons.	What is a weight in machine learning
577	Sampling Frame vs. A sampling frame is a list of things that you draw a sample from. A sample space is a list of all possible outcomes for an experiment. For example, you might have a sampling frame of names of people in a certain town for a survey you're going to be conducting on family size.	What is the difference between a sample and a sampling frame
4993	The Society for Imprecise Probability: Theories and Applications (SIPTA) was created in February 2002, with the aim of promoting the research on Imprecise probability.	What is the Society for Imprecise Probability Theory and Applications SIPTA
6075	Clustering methods are used to identify groups of similar objects in a multivariate data sets collected from fields such as marketing, bio-medical and geo-spatial. They are different types of clustering methods, including: Partitioning methods. Hierarchical clustering.	What is clustering and its types
806	The gradient vector is scaled uniformly by a scalar learning rate . The learning rate remains constant throughout the learning process.	What is an intuitive explanation of the AdaDelta Deep Learning optimizer
647	TensorFlow can train and run deep neural networks for handwritten digit classification, image recognition, word embeddings, recurrent neural networks, sequence-to-sequence models for machine translation, natural language processing, and PDE (partial differential equation) based simulations.	What can TensorFlow do
4501	How to Interpret ProbabilityIf P(A) equals zero, event A will almost definitely not occur.If P(A) is close to zero, there is only a small chance that event A will occur.If P(A) equals 0.5, there is a 50-50 chance that event A will occur.If P(A) is close to one, there is a strong chance that event A will occur.More items	How do you interpret probabilities
7734	Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. Optimizers help to get results faster.	What is an optimizer in machine learning
2665	Batch normalization enables the use of higher learning rates, greatly accelerating the learning process. It also enabled the training of deep neural networks with sigmoid activations that were previously deemed too difficult to train due to the vanishing gradient problem.	Why do we need batch normalization
2916	Partitioning data into training, validation, and holdout sets allows you to develop highly accurate models that are relevant to data that you collect in the future, not just the data the model was trained on.	Why should data be partitioned into training and validation sets
8309	The law of averages is the commonly held belief that a particular outcome or event will over certain periods of time occur at a frequency that is similar to its probability. Depending on context or application it can be considered a valid common-sense observation or a misunderstanding of probability.	Is there a law of averages
1442	First, let me point out that there is nothing wrong with a positive log likelihood. The likelihood is the product of the density evaluated at the observations. Usually, the density takes values that are smaller than one, so its logarithm will be negative.  Naturally, the logarithm of this value will be positive.	Can the log likelihood be positive
50	AdaBoost is short for Adaptive Boosting and is a very popular boosting technique which combines multiple “weak classifiers” into a single “strong classifier”. It was formulated by Yoav Freund and Robert Schapire. They also won the 2003 Gödel Prize for their work.	What is AdaBoost in machine learning
1395	Disparate impact is often referred to as unintentional discrimination, whereas disparate treatment is intentional.  Disparate treatment is intentional employment discrimination. For example, testing a particular skill of only certain minority applicants is disparate treatment.	What is an example of disparate treatment
5015	In neural networks, a hidden layer is located between the input and output of the algorithm, in which the function applies weights to the inputs and directs them through an activation function as the output. In short, the hidden layers perform nonlinear transformations of the inputs entered into the network.	What is a hidden layer in a neural network
7456	DEFINITION 1. Given a set of active nodes and an ordering on active nodes, amorphous data-parallelism is the parallelism that arises from simultaneously processing active nodes, subject to neighborhood and ordering constraints.	What is amorphous data parallelism
3417	Inference over a Bayesian network can come in two forms. The first is simply evaluating the joint probability of a particular assignment of values for each variable (or a subset) in the network.  We would calculate P(¬x | e) in the same fashion, just setting the value of the variables in x to false instead of true.	What is inference in Bayesian networks
296	"The Random Variable is X = ""The sum of the scores on the two dice"". Let's count how often each value occurs, and work out the probabilities: 2 occurs just once, so P(X = 2) = 1/36. 3 occurs twice, so P(X = 3) = 2/36 = 1/18."	How do you find the probability of a random variable
610	Popular algorithms that can be used for binary classification include:Logistic Regression.k-Nearest Neighbors.Decision Trees.Support Vector Machine.Naive Bayes.	Which algorithms are used to do a binary classification
7900	The mean is the arithmetic average of a set of numbers, or distribution.  A mean is computed by adding up all the values and dividing that score by the number of values. The Median is the number found at the exact middle of the set of values.	How do the mean and median compare in a normal distribution
790	"A confusion matrix is a table that is often used to describe the performance of a classification model (or ""classifier"") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing."	What is a confusion matrix used for
4660	The exponential distribution is in continuous time what the geometric distribution is in discrete time. A positive integer random variable X has the geometric distribution with parameter p ∈ (0, 1] if: P(X = n) = p(1 − p)n−1, ∀n ≥ 1, or, equivalently, if: P(X>n) = (1 − p)n, ∀n ∈ N.	What is the relationship between the exponential and geometric distributions
6847	Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.	What does data augmentation mean
6852	The Upsampling layer is a simple layer with no weights that will double the dimensions of input and can be used in a generative model when followed by a traditional convolutional layer.	What is upsampling in neural networks
262	Decision trees is a non-linear classifier like the neural networks, etc. It is generally used for classifying non-linearly separable data. Even when you consider the regression example, decision tree is non-linear.	Is decision tree linear classifier
412	"K-S should be a high value (Max =1.0) when the fit is good and a low value (Min = 0.0) when the fit is not good. When the K-S value goes below 0.05, you will be informed that the Lack of fit is significant."" I'm trying to get a limit value, but it's not very easy."	What is a good KS statistic value
3743	A chi-square goodness-of-fit test can be conducted when there is one categorical variable with more than two levels. If there are exactly two categories, then a one proportion z test may be conducted. The levels of that categorical variable must be mutually exclusive.	How many variables can you test with the goodness of fit test
58	RELU activation solves this by having a gradient slope of 1, so during backpropagation, there isn't gradients passed back that are progressively getting smaller and smaller. but instead they are staying the same, which is how RELU solves the vanishing gradient problem.	How does the ReLu solve the vanishing gradient problem
100	As a simple definition, linear function is a function which has same derivative for the inputs in its domain. ReLU is not linear. The simple answer is that ReLU 's output is not a straight line, it bends at the x-axis.	Is ReLU a linear activation function
4002	A model is considered to be robust if its output and forecasts are consistently accurate even if one or more of the input variables or assumptions are drastically changed due to unforeseen circumstances.	What is model robustness
8467	According to the central limit theorem, the mean of a sample of data will be closer to the mean of the overall population in question, as the sample size increases, notwithstanding the actual distribution of the data. In other words, the data is accurate whether the distribution is normal or aberrant.	Does the Central Limit Theorem imply that the sample mean is a consistent estimator of the population mean
976	Simple logistic regression analysis refers to the regression application with one dichotomous outcome and one independent variable; multiple logistic regression analysis applies when there is a single dichotomous outcome and more than one independent variable.	What is the difference between logistic and multiple regression
7690	If a build-in function can be applied to a complete array, a vectorization is much faster than a loop appraoch. When large temporary arrays are required, the benefits of the vectorization can be dominated by the expensive allocation of the memory, when it does not match into the processor cache.	Why is vectorization faster in Matlab
4119	In one shot learning, you get only 1 or a few training examples in some categories. In zero shot learning, you are not presented with every class label in training. So in some categories, you get 0 training examples.	Whats the difference between one shot learning and zero shot learning
8558	The Mann Whitney U test, sometimes called the Mann Whitney Wilcoxon Test or the Wilcoxon Rank Sum Test, is used to test whether two samples are likely to derive from the same population (i.e., that the two populations have the same shape).	When would you use a Wilcoxon rank sum test
1230	In our implementation of gradient descent, we have used a function compute_gradient(loss) that computes the gradient of a loss operation in our computational graph with respect to the output of every other node n (i.e. the direction of change for n along which the loss increases the most).	Is backpropagation gradient descent
2929	"By the way, in experimental research, random assignment is much more important than random selection; that's because the purpose of an experiment to establish cause and effect relationships.  Random assignment ""equates the groups"" on all known and unknown extraneous variables at the start of the experiment."	Why is it important to use random assignment when determining which research participants will comprise the different treatment groups in the study
173	In statistics, econometrics, and related fields, multidimensional analysis (MDA) is a data analysis process that groups data into two categories: data dimensions and measurements.  A data set consisting of the number of wins for several football teams over several years is a two-dimensional data set.	What is multidimensional reporting
7248	Collaborative filtering (CF) is a technique used by recommender systems.  In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating).	What is meant by collaborative filtering
2349	In active learning teachers are facilitators rather than one way providers of information.  Other examples of active learning techniques include role-playing, case studies, group projects, think-pair-share, peer teaching, debates, Just-in-Time Teaching, and short demonstrations followed by class discussion.	What is an example of active learning
3058	So by the definition of discrete and continuous random variables, a random variable cannot be both discrete and continuous. No. For a random variable to be discrete, there must a countable sequence such that .	Can a random variable be both discrete and continuous
8498	Unsupervised learning works by analyzing the data without its labels for the hidden structures within it, and through determining the correlations, and for features that actually correlate two data items. It is being used for clustering, dimensionality reduction, feature learning, density estimation, etc.	How does unsupervised learning work
4011	The base rate fallacy occurs when prototypical or stereotypical factors are used for analysis rather than actual data. Because the student is volunteering in a hospital with a stroke center, he sees more patients who have experienced a stroke than would be expected in a hospital without a stroke center.	What is base rate fallacy MCAT
84	The sample mean is a consistent estimator for the population mean. A consistent estimate has insignificant errors (variations) as sample sizes grow larger. More specifically, the probability that those errors will vary by more than a given amount approaches zero as the sample size increases.	Is the sample mean a consistent estimator
3728	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is the difference between supervised and unsupervised learning
1201	0:012:32Suggested clip · 101 secondsMultiple Logistic Regression - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do multiple logistic regression
414	Disjoint events cannot happen at the same time. In other words, they are mutually exclusive. Put in formal terms, events A and B are disjoint if their intersection is zero:  Disjoint events are disjointed, or not connected. Another way of looking at disjoint events are that they have no outcomes in common.	What Does It Mean If A and B are disjoint
5943	The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience.	What is curse of dimensionality explain with an example
7550	The theorem and its generalizations can be used to prove results and solve problems in combinatorics, algebra, calculus, and many other areas of mathematics. The binomial theorem also helps explore probability in an organized way: A friend says that she will flip a coin 5 times.	Why do we use binomial theorem
4037	In computer science, an inverted index (also referred to as a postings file or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content).	What is inversion in indexing process
330	The goodness of fit test is a statistical hypothesis test to see how well sample data fit a distribution from a population with a normal distribution. Put differently, this test shows if your sample data represents the data you would expect to find in the actual population or if it is somehow skewed.	What is the purpose of a goodness of fit test
4832	The function fX(x) gives us the probability density at point x. It is the limit of the probability of the interval (x,x+Δ] divided by the length of the interval as the length of the interval goes to 0. Remember that P(x<X≤x+Δ)=FX(x+Δ)−FX(x). =dFX(x)dx=F′X(x),if FX(x) is differentiable at x.	How do you find the probability of a density function
1667	In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path.	What is admissible heuristic in AI
5906	Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect.	What is correlation statistics
6406	An allocation is Pareto efficient if there is no other allocation in which some other individual is better off and no individual is worse off. Notes: There is no connection between Pareto efficiency and equity! In particular, a Pareto efficient outcome may be very inequitable.	How do you tell if an allocation is Pareto efficient
982	Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.	What is machine learning and its applications
992	This type of index is called an inverted index, namely because it is an inversion of the forward index.  In some search engines the index includes additional information such as frequency of the terms, e.g. how often a term occurs in each document, or the position of the term in each document.	Why is it called an inverted index
2259	In class limit, the upper extreme value of the first class interval and the lower extreme value of the next class interval will not be equal. In class boundary, the upper extreme value of the first class interval and the lower extreme value of the next class interval will be equal.	What is the difference between continuous class intervals and class boundaries
2558	Two classes of digital filters are Finite Impulse Response (FIR) and Infinite Impulse Response (IIR). The term 'Impulse Response' refers to the appearance of the filter in the time domain.  The mathematical difference between the IIR and FIR implementation is that the IIR filter uses some of the filter output as input.	What is FIR and IIR filter
4838	Abstract: The k-Nearest Neighbors (kNN) classifier is one of the most effective methods in supervised learning problems. It classifies unseen cases comparing their similarity with the training data.  Fuzzy-kNN computes a fuzzy degree of membership of each instance to the classes of the problem.	What is fuzzy KNN
914	Data Augmentation in NLPSynonym Replacement: Randomly choose n words from the sentence that are not stop words.  Random Insertion: Find a random synonym of a random word in the sentence that is not a stop word.  Random Swap: Randomly choose two words in the sentence and swap their positions.More items	What data augmentation techniques are available for deep learning on text
6137	Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics.	What is Bayesian inference in statistics When and how is Bayesian inference used
657	There are no acceptable limits for MSE except that the lower the MSE the higher the accuracy of prediction as there would be excellent match between the actual and predicted data set. This is as exemplified by improvement in correlation as MSE approaches zero.	What is an acceptable mean square error
7791	Programming, of course, means allocation in each case. In the linear programming model limited resources are Page 6 P-885 6-22-56 allocated to various activities. In dynamic programming resources are allocated at each of several time periods.	How dynamic programming is different from linear programming
664	Popular ML algorithms include: linear regression, logistic regression, SVMs, nearest neighbor, decision trees, PCA, naive Bayes classifier, and k-means clustering. Classical machine learning algorithms are used for a wide range of applications.	What are the models in machine learning
5765	The Wilcoxon rank sum test is a nonparametric test that may be used to assess whether the distributions of observations obtained between two separate groups on a dependent variable are systematically different from one another.	What is Wilcoxon rank sum test used for
850	Parametric statistics are based on assumptions about the distribution of population from which the sample was taken. Nonparametric statistics are not based on assumptions, that is, the data can be collected from a sample that does not follow a specific distribution.	What is parametric statistics and nonparametric statistics
8266	First of all, a starting pixel called as the seed is considered. The algorithm checks boundary pixel or adjacent pixels are colored or not. If the adjacent pixel is already filled or colored then leave it, otherwise fill it. The filling is done using four connected or eight connected approaches.	How the pixels are addressed in seed fill algorithm
6198	P value. The Kruskal-Wallis test is a nonparametric test that compares three or more unmatched groups.  If your samples are large, it approximates the P value from a Gaussian approximation (based on the fact that the Kruskal-Wallis statistic H approximates a chi-square distribution.	What is p value in Kruskal Wallis test
2955	An unbiased estimator is a statistics that has an expected value equal to the population parameter being estimated. Examples: The sample mean, is an unbiased estimator of the population mean, . The sample variance, is an unbiased estimator of the population variance, .	Which statistics are unbiased estimators
5845	Humans are error-prone and biased, but that doesn't mean that algorithms are necessarily better.  But these systems can be biased based on who builds them, how they're developed, and how they're ultimately used. This is commonly known as algorithmic bias.	Can algorithms be biased
1808	Statistical inference comprises the application of methods to analyze the sample data in order to estimate the population parameters.  The concept of normal (also called gaussian) sampling distribution has an important role in statistical inference, even when the population values are not normally distributed.	What is statistical inference and why is it important
975	If the function is a probability distribution, then the zeroth moment is the total probability (i.e. one), the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis.	What is a zeroth moment in statistics
1300	Kernel method is used by SVM to perform a non-linear classification. They take low dimensional input space and convert them into high dimensional input space. It converts non-separable classes into the separable one, it finds out a way to separate the data on the basis of the data labels defined by us.	Which model helps SVM to implement the algorithm in high dimensional space
5871	The agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It's also known as AGNES (Agglomerative Nesting). The algorithm starts by treating each object as a singleton cluster.	What is agglomerative clustering algorithm
1500	In mathematics, a tensor is an algebraic object that describes a (multilinear) relationship between sets of algebraic objects related to a vector space. Objects that tensors may map between include vectors and scalars, and even other tensors.	What is a tensor in simple terms
565	Accuracy is well defined for any number of classes, so if you use this, a single plot should suffice. Precision and recall, however, are defined only for binary problems.	How many learning curves should I plot for a multi class logistic regression classifier
4397	Convolution is used in the mathematics of many fields, such as probability and statistics. In linear systems, convolution is used to describe the relationship between three signals of interest: the input signal, the impulse response, and the output signal.	What is the use of convolution
6661	The results of the convenience sampling cannot be generalized to the target population because of the potential bias of the sampling technique due to under-representation of subgroups in the sample in comparison to the population of interest. The bias of the sample cannot be measured.	Does convenience sampling generalize to the population
675	Robust regression is an alternative to least squares regression when data is contaminated with outliers or influential observations and it can also be used for the purpose of detecting influential observations. Please note: The purpose of this page is to show how to use various data analysis commands.	When should I use robust regression
2872	The joint behavior of two random variables X and Y is determined by the. joint cumulative distribution function (cdf):(1.1) FXY (x, y) = P(X ≤ x, Y ≤ y),where X and Y are continuous or discrete. For example, the probability.  P(x1 ≤ X ≤ x2,y1 ≤ Y ≤ y2) = F(x2,y2) − F(x2,y1) − F(x1,y2) + F(x1,y1).	How do you find the joint distribution function
1438	The Shape of a Histogram A histogram is unimodal if there is one hump, bimodal if there are two humps and multimodal if there are many humps. A nonsymmetric histogram is called skewed if it is not symmetric. If the upper tail is longer than the lower tail then it is positively skewed.	Is the shape of this histogram unimodal bimodal or multimodal
144	Dimensional Analysis (also called Factor-Label Method or the Unit Factor Method) is a problem-solving method that uses the fact that any number or expression can be multiplied by one without changing its value. It is a useful technique.	What is dimensional analysis and how do we use it
5379	Generally a cosine similarity between two documents is used as a similarity measure of documents. In Java, you can use Lucene (if your collection is pretty large) or LingPipe to do this. The basic concept would be to count the terms in every document and calculate the dot product of the term vectors.	How do you find the similarity between two documents
1274	A feature detector is also referred to as a kernel or a filter. Intuitively, the matrix representation of the input image is multiplied element-wise with the feature detector to produce a feature map, also known as a convolved feature or an activation map.	What is feature detector in CNN
5983	If A and B are two events in a sample space S, then the conditional probability of A given B is defined as P(A|B)=P(A∩B)P(B), when P(B)>0.	How can I derivate conditional probability formula
191	In a statistical study, sampling methods refer to how we select members from the population to be in the study. If a sample isn't randomly selected, it will probably be biased in some way and the data may not be representative of the population.	What is sampling method in statistics
7911	Difference Between Temporal and Spatial Databases A spatial database stores and allows queries of data defined by geometric space. Many spatial databases can represent simple coordinates, points, lines and polygons.  A temporal database stores data relating to time whether past, present or future.	What is the difference between spatial temporal data with other type of data
2998	1- Find the remainder of n by moduling it with 4. 2- If rem = 0, then xor will be same as n. 3- If rem = 1, then xor will be 1. 4- If rem = 2, then xor will be n+1.	How is XOR calculated
1947	In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed.	What is a Gaussian process model
1114	While the multivariable model is used for the analysis with one outcome (dependent) and multiple independent (a.k.a., predictor or explanatory) variables,2,3 multivariate is used for the analysis with more than 1 outcomes (eg, repeated measures) and multiple independent variables.	What is the difference between multivariate and multivariable
7693	The higher the threshold, or closer to (0, 0), the higher the specificity and the lower the sensitivity. The lower the threshold, or closer to (1,1), the higher the sensitivity and lower the specificity. So which threshold value one should pick?	How do you choose the threshold in logistic regression
1235	3.1 . Each bootstrap distribution is centered at the statistic from the corresponding sample rather than at the population mean μ.	Where is a bootstrap distribution centered
3473	A data set is homogeneous if it is made up of things (i.e. people, cells or traits) that are similar to each other. For example a data set made up of 20-year-old college students enrolled in Physics 101 is a homogeneous sample.	How do you know if data is homogeneous
3651	It's more of an approach than a process. Predictive analytics and machine learning go hand-in-hand, as predictive models typically include a machine learning algorithm.  These models are then made up of algorithms. The algorithms perform the data mining and statistical analysis, determining trends and patterns in data.	What is predictive machine learning
356	Linear models, generalized linear models, and nonlinear models are examples of parametric regression models because we know the function that describes the relationship between the response and explanatory variables.  If the relationship is unknown and nonlinear, nonparametric regression models should be used.	Is regression analysis parametric or nonparametric
1041	An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.	What is K in the K nearest neighbors algorithm
125	The cutoff frequency for a high-pass filter is that frequency at which the output (load) voltage equals 70.7% of the input (source) voltage. Above the cutoff frequency, the output voltage is greater than 70.7% of the input, and vice versa.	What is the cutoff frequency of a high pass filter
3683	"Disadvantage: Sigmoid: tend to vanish gradient (cause there is a mechanism to reduce the gradient as ""a"" increases, where ""a"" is the input of a sigmoid function."	What is drawback of sigmoid function
2934	Treatment groups are the sets of participants in a research study that are exposed to some manipulation or intentional change in the independent variable of interest. They are an integral part of experimental research design that helps to measure effects as well as establish causality.	What is a treatment group in statistics
343	Therefore, the maximum likelihood estimator is an unbiased estimator of .	Is a maximum likelihood estimator unbiased
606	The One-sample t-test is used to compare a sample mean to a specific value. An “F Test” uses the F-distribution.Comparison Table Between T-test and F-test (in Tabular Form)Parameter of ComparisonT-testF-testTest statisticT = (mean - comparison value)/ Standard Error ~t(n-1)F = s21 / s22 ~ F(n1-1,n2-1)4 more rows	How do you know if its an F test or a t test
6874	The easiest way of estimating the semantic similarity between a pair of sentences is by taking the average of the word embeddings of all words in the two sentences, and calculating the cosine between the resulting embeddings.	How do you calculate similarity in a sentence
7628	First, Cross-entropy (or softmax loss, but cross-entropy works better) is a better measure than MSE for classification, because the decision boundary in a classification task is large (in comparison with regression).  For regression problems, you would almost always use the MSE.	Why is cross entropy better than MSE
237	Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'.	Can we use logistic regression for multi class classification
797	Rather than using the past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model.  While, the autoregressive model(AR) uses the past forecasts to predict future values.	What is the difference between autoregressive model and moving average model
7846	Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series.  The autocorrelation coefficients are plotted to show the autocorrelation function or ACF. The plot is also known as a correlogram.	What is autocorrelation in forecasting
711	The number of bootstrap samples can be indicated with B (e.g. if you resample 10 times then B = 10). A star next to a statistic, like s* or x̄* indicates the statistic was calculated by resampling. A bootstrap statistic is sometimes denoted with a T, where T*b would be the Bth bootstrap sample statistic T.	How does bootstrap determine sample size
752	Direct Application of AM-GM to an Inequality The simplest way to apply AM-GM is to apply it immediately on all of the terms. For example, we know that for non-negative values, x + y 2 ≥ x y , x + y + z 3 ≥ x y z 3 , w + x + y + z 4 ≥ w x y z 4 .	How do you use am GM inequality
8418	There is a thin line of demarcation amidst t-test and ANOVA, i.e. when the population means of only two groups is to be compared, the t-test is used, but when means of more than two groups are to be compared, ANOVA is preferred.	Which is better Anova or t test
3345	0:559:25Suggested clip · 121 secondsHow To Calculate Pearson's Correlation Coefficient (r) by Hand YouTubeStart of suggested clipEnd of suggested clip	How do you calculate Pearson correlation coefficient
7683	"The dissimilarity matrix, using the euclidean metric, can be calculated with the command: daisy(agriculture, metric = ""euclidean""). The result the of calculation will be displayed directly in the screen, and if you wanna reuse it you can simply assign it to an object: x <- daisy(agriculture, metric = ""euclidean"")."	How do you find the dissimilarity of a matrix
862	This means that the sum of two independent normally distributed random variables is normal, with its mean being the sum of the two means, and its variance being the sum of the two variances (i.e., the square of the standard deviation is the sum of the squares of the standard deviations).	Is the sum of two normal distributions normal
1127	The sum of squared errors is a 'total' and is, therefore, affected by the number of data points. The variance is the 'average' variability but in units squared. The standard deviation is the average variation but converted back to the original units of measurement.	What do the sum of squares variance and standard deviation represent how do they differ
3569	The F-statistic is the test statistic for F-tests. In general, an F-statistic is a ratio of two quantities that are expected to be roughly equal under the null hypothesis, which produces an F-statistic of approximately 1.  In order to reject the null hypothesis that the group means are equal, we need a high F-value.	What does the F statistic mean
6765	Then the marginal pdf's (or pmf's = probability mass functions, if you prefer this terminology for discrete random variables) are defined by fY(y) = P(Y = y) and fX(x) = P(X = x). The joint pdf is, similarly, fX,Y(x,y) = P(X = x and Y = y).	What is a marginal PDF
2371	The intercept (often labeled the constant) is the expected mean value of Y when all X=0. Start with a regression equation with one predictor, X. If X sometimes equals 0, the intercept is simply the expected mean value of Y at that value.	What is the Y intercept in a regression equation
7885	Repeating patterns often show serial correlation when the level of a variable affects its future level. In finance, this correlation is used by technical analysts to determine how well the past price of a security predicts the future price. Serial correlation is also known as autocorrelation or lagged correlation.	What does autocorrelation or serial correlation imply
8686	Firstly, the basic difference between the two is that Market basket analysis is a representation for the whole population (to understand the fact that what products are purchased together as a bunch by the users) whereas the collaborative filtering on the other side restricts itself only to a particular user to	How is association rule compared with collaborative filtering in recommender systems
603	ANOVA is available for both parametric (score data) and non-parametric (ranking/ordering) data.	Can I use Anova for nonparametric data
5542	"A bell curve is a common type of distribution for a variable, also known as the normal distribution. The term ""bell curve"" originates from the fact that the graph used to depict a normal distribution consists of a symmetrical bell-shaped curve."	What is the distribution of a bell curve
1152	"In the binomial distribution, the number of trials is fixed, and we count the number of ""successes"". Whereas, in the geometric and negative binomial distributions, the number of ""successes"" is fixed, and we count the number of trials needed to obtain the desired number of ""successes""."	What is the difference between binomial and geometric distribution
7972	The significance level is the probability of rejecting the null hypothesis when it is true.  For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference.	Why is the significance level of 0 05 so commonly used
6916	The Z Score Formula: One Sample Assuming a normal distribution, your z score would be: z = (x – μ) / σ = (190 – 150) / 25 = 1.6.	What is the z score of a normal distribution
7493	2:2131:26Suggested clip · 109 secondsContinuous Probability Uniform Distribution Problems - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you solve uniform distribution problems
301	Bagging (Bootstrap Aggregation) is used when our goal is to reduce the variance of a decision tree. Here idea is to create several subsets of data from training sample chosen randomly with replacement. Now, each collection of subset data is used to train their decision trees.	What is bagging in decision tree
1204	Standardization isn't required for logistic regression. The main goal of standardizing features is to help convergence of the technique used for optimization.  Otherwise, you can run your logistic regression without any standardization treatment on the features.	Does logistic regression require feature scaling
3069	Any information that is processed by and sent out from a computer or other electronic device is considered output.  An example of output is anything viewed on your computer monitor screen, such as the words you type on your keyboard.	What is output short answer
8551	Lasso regression stands for Least Absolute Shrinkage and Selection Operator. It adds penalty term to the cost function.  The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero.	What is the difference between Ridge and lasso regression
3412	AI can efficiently analyze user behaviors, deduce a pattern, and identify all sorts of abnormalities or irregularities in the network. With such data, it's much easier to identify cyber vulnerabilities quickly.	How is AI being used in cyber security
105	Natural Language processing is considered a difficult problem in computer science. It's the nature of the human language that makes NLP difficult.  While humans can easily master a language, the ambiguity and imprecise characteristics of the natural languages are what make NLP difficult for machines to implement.	Why is NLP difficult
4393	A statistical hypothesis is a hypothesis concerning the parameters or from of the probability distribution for a designated population or populations, or, more generally, of a probabilistic mechanism which is supposed to generate the observations.	What is the meaning of statistical hypothesis
816	A variance of zero indicates that all of the data values are identical. All non-zero variances are positive. A small variance indicates that the data points tend to be very close to the mean, and to each other. A high variance indicates that the data points are very spread out from the mean, and from one another.	How do you interpret the variance in statistics
1200	"Simple random sampling uses a table of random numbers or an electronic random number generator to select items for its sample.  Meanwhile, systematic sampling involves selecting items from an ordered population using a skip or sampling interval. That means that every ""nth"" data sample is chosen in a large data set."	What is distinguish between random sampling and systematic sampling
4715	Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.	How do you explain logistic regression model
1437	Expectation maximization is applicable whenever the data are missing completely at random or missing at random-but unsuitable when the data are not missing at random.	What is Expectation Maximization for missing data
1316	In general, focus on these specific tips throughout your interview:Think out loud. Showcasing your communication skills is critical in any phone interview.  Ask Questions. If anything is unclear about the problem when you first read it over, ask your interviewer.  Start simple, then optimize.  If hinted, use them.	How do you prepare for an algorithm interview
235	Multivariate ANOVA (MANOVA) extends the capabilities of analysis of variance (ANOVA) by assessing multiple dependent variables simultaneously. ANOVA statistically tests the differences between three or more group means.  This statistical procedure tests multiple dependent variables at the same time.	Is Anova Multivariate analysis
3589	The Pearson correlation evaluates the linear relationship between two continuous variables.  The Spearman correlation coefficient is based on the ranked values for each variable rather than the raw data. Spearman correlation is often used to evaluate relationships involving ordinal variables.	Why do we choose Spearman’s rank correlation coefficient rather than Pearsons
4455	The 7 Types of Artificial Neural Networks ML Engineers Need to KnowModular Neural Networks.Feedforward Neural Network – Artificial Neuron.Radial basis function Neural Network.Kohonen Self Organizing Neural Network.Recurrent Neural Network(RNN)Convolutional Neural Network.Long / Short Term Memory.	What are the types of artificial neural network
946	Convolution has applications that include probability, statistics, computer vision, natural language processing, image and signal processing, engineering, and differential equations.	What are the applications of convolution
2213	Training a neural network involves using an optimization algorithm to find a set of weights to best map inputs to outputs. The problem is hard, not least because the error surface is non-convex and contains local minima, flat spots, and is highly multidimensional.	How does training a neural network work
8390	Let's look at five approaches that you may use on your machine learning project to compare classifiers.Independent Data Samples.  Accept the Problems of 10-fold CV.  Use McNemar's Test or 5×2 CV.  Use a Nonparametric Paired Test.  Use Estimation Statistics Instead.	How do you compare two machine learning algorithms
3608	Cosine similarity takes the angle between two non-zero vectors and calculates the cosine of that angle, and this value is known as the similarity between the two vectors. This similarity score ranges from 0 to 1, with 0 being the lowest (the least similar) and 1 being the highest (the most similar).	What is a good cosine similarity
2216	A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas a learning rate that is too small can cause the process to get stuck.  The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate.	What if we use a learning rate that's too large
3392	Before getting features, various image preprocessing techniques like binarization, thresholding, resizing, normalization etc. are applied on the sampled image. After that, feature extraction techniques are applied to get features that will be useful in classifying and recognition of images.	How features are extracted from an image
4118	Preparing Your Dataset for Machine Learning: 8 Basic Techniques That Make Your Data BetterArticulate the problem early.Establish data collection mechanisms.Format data to make it consistent.Reduce data.Complete data cleaning.Decompose data.Rescale data.Discretize data.	How do you organize data for machine learning
153	8 Methods to Boost the Accuracy of a ModelAdd more data. Having more data is always a good idea.  Treat missing and Outlier values.  Feature Engineering.  Feature Selection.  Multiple algorithms.  Algorithm Tuning.  Ensemble methods.	How can machine learning improve accuracy
4943	The deep convolutional generative adversarial network, or DCGAN for short, is an extension of the GAN architecture for using deep convolutional neural networks for both the generator and discriminator models and configurations for the models and training that result in the stable training of a generator model.	What is a Dcgan
323	Deconvolution layer is a very unfortunate name and should rather be called a transposed convolutional layer. Visually, for a transposed convolution with stride one and no padding, we just pad the original input (blue entries) with zeroes (white entries) (Figure 1).	What is a deconvolution layer
197	Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1.  So predicting a probability of . 012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.	What is a good cross entropy loss
950	In computer science, a universal Turing machine (UTM) is a Turing machine that simulates an arbitrary Turing machine on arbitrary input.  Alan Turing introduced the idea of such a machine in 1936–1937.	What is universal Turing machine in automata
1404	Fixed effects are variables that are constant across individuals; these variables, like age, sex, or ethnicity, don't change or change at a constant rate over time. They have fixed effects; in other words, any change they cause to an individual is the same.	What does fixed effect mean in statistics
1272	Two-Tailed Test The rejection region is the region where, if our test statistic falls, then we have enough evidence to reject the null hypothesis. If we consider the right-tailed test, for example, the rejection region is any value greater than c 1 − α , where c 1 − α is the critical value.	How do you find the rejection region of a two tailed test
6139	The average score is 100 - that is what the process of standardisation is all about. The top 30% are those that the Bucks system is designed to select, and therefore they will score 121+.	What is the average 11+ score
3418	Loss function for Logistic Regression is the data set containing many labeled examples, which are pairs. is the label in a labeled example. Since this is logistic regression, every value of must either be 0 or 1. is the predicted value (somewhere between 0 and 1), given the set of features in .	What is the loss function of logistic regression
2989	Another common example of univariate analysis is the mean of a population distribution. Tables, charts, polygons, and histograms are all popular methods for displaying univariate analysis of a specific variable (e.g. mean, median, mode, standard variation, range, etc).	What is univariate analysis example
1369	Some Disadvantages of KNNAccuracy depends on the quality of the data.With large data, the prediction stage might be slow.Sensitive to the scale of the data and irrelevant features.Require high memory – need to store all of the training data.Given that it stores all of the training, it can be computationally expensive.	Which of the following are the disadvantages of using Knn
3447	Example of Stratified Random Sampling Suppose a research team wants to determine the GPA of college students across the U.S. The research team has difficulty collecting data from all 21 million college students; it decides to take a random sample of the population by using 4,000 students.	What are clear examples for stratified random sampling
1690	Descriptive statistics. The expected value and variance of a Poisson-distributed random variable are both equal to λ. , while the index of dispersion is 1.	What is the expected value of a Poisson distribution
503	The step that Agglomerative Clustering take are:Each data point is assigned as a single cluster.Determine the distance measurement and calculate the distance matrix.Determine the linkage criteria to merge the clusters.Update the distance matrix.Repeat the process until every data point become one cluster.	How is agglomerative clustering used
8460	Probability Density Functions are a statistical measure used to gauge the likely outcome of a discrete value, e.g., the price of a stock or ETF. PDFs are plotted on a graph typically resembling a bell curve, with the probability of the outcomes lying below the curve.	What is probability density function with example
5725	“The difference is that, in the Bayesian approach, the parameters that we are trying to estimate are treated as random variables. In the frequentist approach, they are fixed. Random variables are governed by their parameters (mean, variance, etc.) and distributions (Gaussian, Poisson, binomial, etc).	What is the difference between Bayesian and frequentist statistics
149	(i) The value of dimensionless constants cannot be determined by this method. (ii) This method cannot be applied to equations involving exponential and trigonometric functions. (iii) It cannot be applied to an equation involving more than three physical quantities.	What are the advantages and disadvantages of Dimensional Analysis
2750	To convert a frequency distribution to a probability distribution, divide area of the bar or interval of x by the total area of all the Bars. A simpler formula is: , N is the total Frequency and w is the interval of x.	How do you convert a frequency distribution to a probability distribution
6704	In the case of a pair of random variables (X, Y), when random variable X (or Y) is considered by itself, its density function is called the marginal density function.	What is marginal probability density function
607	Probability is about a finite set of possible outcomes, given a probability. Likelihood is about an infinite set of possible probabilities, given an outcome.	What is probability and likelihood
7509	LBP algorithm	Which algorithm is used in face recognition
8224	There are numerous applications of integrals. Using technology such as computer software, internet sources, graphing calculators and smartphone apps can make solving integral problems easier. Some applications of integrals are: Displacement, which is the integral of velocity with respect to time.	What are the applications of integral calculus in different fields
7192	A univariate distribution refers to the distribution of a single random variable.  On the other hand, a multivariate distribution refers to the probability distribution of a group of random variables. For example, a multivariate normal distribution is used to specify the probabilities of returns of a group of n stocks.	What is univariate and multivariate normality
2499	K-Fold CV is where a given data set is split into a K number of sections/folds where each fold is used as a testing set at some point. Lets take the scenario of 5-Fold cross validation(K=5).  This process is repeated until each fold of the 5 folds have been used as the testing set.	What is K fold
3444	The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.	How do you explain adjusted R squared
4980	A convolution is the simple application of a filter to an input that results in an activation. Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in an input, such as an image.	What is the purpose of the convolutional filter
1324	A fundamental problem with stepwise regression is that some real explanatory variables that have causal effects on the dependent variable may happen to not be statistically significant, while nuisance variables may be coincidentally significant.	Why is stepwise regression bad
6181	A random variable is a variable whose value is unknown or a function that assigns values to each of an experiment's outcomes.  Random variables are often used in econometric or regression analysis to determine statistical relationships among one another.	How do you define a random variable
455	Qualities of a Good Sampling FrameInclude all individuals in the target population.Exclude all individuals not in the target population.Includes accurate information that can be used to contact selected individuals.	How do you determine a sampling frame
190	7:3021:58Suggested clip · 120 secondsStatQuest: Principal Component Analysis (PCA), Step-by-Step YouTubeStart of suggested clipEnd of suggested clip	How do you do principal component analysis
897	Stochastic gradient descent (SGD) computes the gradient for each update using a single training data point x_i (chosen at random). The idea is that the gradient calculated this way is a stochastic approximation to the gradient calculated using the entire training data.	What is an intuitive explanation of stochastic gradient descent
39	A two-tailed test will test both if the mean is significantly greater than x and if the mean significantly less than x. The mean is considered significantly different from x if the test statistic is in the top 2.5% or bottom 2.5% of its probability distribution, resulting in a p-value less than 0.05.	How do you know if it is a two tailed test
6027	Text generation with an RNNTable of contents.Setup. Import TensorFlow and other libraries. Download the Shakespeare dataset.  Process the text. Vectorize the text. The prediction task.  Build The Model.Try the model.Train the model. Attach an optimizer, and a loss function.  Generate text. Restore the latest checkpoint.  Advanced: Customized Training.	How RNN is implemented in TensorFlow
1595	The maximum entropy principle is defined as modeling a given set of data by finding the highest entropy to satisfy the constraints of our prior knowledge.  The maximum entropy model is a conditional probability model p(y|x) that allows us to predict class labels given a set of features for a given data point.	What is maximum entropy in NLP
391	4:1410:53Suggested clip · 113 secondsStochastic Gradient Descent, Clearly Explained!!! - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you use stochastic gradient descent
7171	Boolean searching allows the user to combine or limit words and phrases in an online search in order to retrieve relevant results. Using the Boolean terms: AND, OR, NOT, the searcher is able to define relationships among concepts. Use OR to broaden search results.	How are Boolean search strategies useful
359	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is supervised and unsupervised data
2032	How to Avoid Confirmation Bias. Look for ways to challenge what you think you see. Seek out information from a range of sources, and use an approach such as the Six Thinking Hats technique to consider situations from multiple perspectives. Alternatively, discuss your thoughts with others.	How do you avoid confirmation bias
8666	Statistical stationarity: A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time.  Such statistics are useful as descriptors of future behavior only if the series is stationary.	What is stationary in statistics
5595	Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  Factor analysis is related to principal component analysis (PCA), but the two are not identical.	What does factor analysis mean
8445	Sampling error is affected by a number of factors including sample size, sample design, the sampling fraction and the variability within the population.	What are the factors causing sampling error
3939	Multinomial Naïve Bayes uses term frequency i.e. the number of times a given term appears in a document.  After normalization, term frequency can be used to compute maximum likelihood estimates based on the training data to estimate the conditional probability.	How does multinomial naive Bayes work
7962	metric system. A system of measurement in which the basic units are the meter, the second, and the kilogram. In this system, the ratios between units of measurement are multiples of ten. For example, a kilogram is a thousand grams, and a centimeter is one-hundredth of a meter.	What is the definition of metric system
6710	In statistics and probability analysis, the expected value is calculated by multiplying each of the possible outcomes by the likelihood each outcome will occur and then summing all of those values. By calculating expected values, investors can choose the scenario most likely to give the desired outcome.	How do I find the expected value
1106	In machine learning, multiclass or multinomial classification is the problem of classifying instances into one of three or more classes (classifying instances into one of two classes is called binary classification).	What is multiclass classification in machine learning
2461	The marketplace for predictive analytics software has ballooned: G2Crowd records 92 results in the category. Pricing varies substantially based on the number of users and, in some cases, amount of data, but generally starts around $1,000 per year, though it can easily scale into six figures.	How much does Predictive Analytics cost
1159	When error terms from different (usually adjacent) time periods (or cross-section observations) are correlated, we say that the error term is serially correlated. Serial correlation occurs in time-series studies when the errors associated with a given time period carry over into future time periods.	Why serial correlation is a problem
723	Without Further Ado, The Top 10 Machine Learning Algorithms for Beginners:Linear Regression. In machine learning, we have a set of input variables (x) that are used to determine an output variable (y).  Logistic Regression.  CART.  Naïve Bayes.  KNN.	Which is the best machine learning algorithm
7103	Our picks for the best statistics and probability courses for data scientists are…Foundations of Data Analysis — Part 1: Statistics Using R by the University of Texas at Austin (edX)Foundations of Data Analysis — Part 2: Inferential Statistics by the University of Texas at Austin (edX)	Where can I learn statistics for data science
7359	SVD gives you the whole nine-yard of diagonalizing a matrix into special matrices that are easy to manipulate and to analyze. It lay down the foundation to untangle data into independent components. PCA skips less significant components.	What is the difference between SVD and PCA
7981	An environment is everything in the world which surrounds the agent, but it is not a part of an agent itself. An environment can be described as a situation in which an agent is present. The environment is where agent lives, operate and provide the agent with something to sense and act upon it.	What is agent and environment in artificial intelligence
2142	Mean, variance, and standard deviation The mean of the sampling distribution of the sample mean will always be the same as the mean of the original non-normal distribution. In other words, the sample mean is equal to the population mean.	Is the sample mean equal to the population mean
1237	As a general rule, GPUs are a safer bet for fast machine learning because, at its heart, data science model training is composed of simple matrix math calculations, the speed of which can be greatly enhanced if the computations can be carried out in parallel.	Why are GPUs good for machine learning
2579	Response bias can be defined as the difference between the true values of variables in a study's net sample group and the values of variables obtained in the results of the same study.  Nonresponse bias occurs when some respondents included in the sample do not respond.	What is the difference between response bias and nonresponse bias
404	Probability Role of probability in statistics:  Use probability to predict results of experiment under assumptions. Compute probability of error larger than given amount. Compute probability of given departure between prediction and results under assumption.	What is the role of probability to statistic
7530	In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution. It may also be called a center or location of the distribution.  The most common measures of central tendency are the arithmetic mean, the median, and the mode.	What is central location statistics
164	A layer is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves computation, defined in the call() method, and a state (weight variables), defined either in the constructor __init__() or in the build() method.	What are TensorFlow layers
6159	0:001:55Suggested clip · 86 secondsLogistic Functions - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you graph a logistic function
174	This is calculated as the outer product between the actual rating's histogram vector of ratings and the predicted rating's histogram vector of ratings, normalized such that E and O have the same sum. From these three matrices, the quadratic weighted kappa is calculated.	How do you calculate quadratic weighted kappa
8624	A SIFT descriptor is a 3-D spatial histogram of the image gradients in characterizing the appearance of a keypoint. The gradient at each pixel is regarded as a sample of a three-dimensional elementary feature vector, formed by the pixel location and the gradient orientation.	What are sift descriptors
4192	In a simple case with two possible categories or the binary classification problem you have one boundary.  Negative means you want the output to be off/low when the classifier “sees” that particular class. Positive means you want the output to be on/high when the classifier “sees” that class.	What does it mean by positive and negative class in machine learning
926	"Noun. 1. XY - (genetics) normal complement of sex hormones in a male. sex chromosome - (genetics) a chromosome that determines the sex of an individual; ""mammals normally have two sex chromosomes"""	What is the mean of XY
5823	Recall is the number of relevant documents retrieved by a search divided by the total number of existing relevant documents, while precision is the number of relevant documents retrieved by a search divided by the total number of documents retrieved by that search.	What is the difference between recall and precision
1023	9:3918:24Suggested clip · 119 secondsR - Regression Trees - CART - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret regression tree results in R
1103	Static Rules Approach. The most simple, and maybe the best approach to start with, is using static rules. The Idea is to identify a list of known anomalies and then write rules to detect those anomalies. Rules identification is done by a domain expert, by using pattern mining techniques, or a by combination of both.	How do you do anomaly detection
603	Cross-sectional data, or a cross section of a study population, in statistics and econometrics is a type of data collected by observing many subjects (such as individuals, firms, countries, or regions) at the one point or period of time. The analysis might also have no regard to differences in time.	What is cross sectional data with example
2474	One of the most common assumptions in many machine learning and data analysis tasks is that the given data points are realizations of independent and identically distributed (IID) random variables.	What is IID in machine learning
8013	Spectral analysis is a technique that allows us to discover underlying periodicities. To perform spectral analysis, we first must transform data from time domain to frequency domain.	What is spectral analysis used for
885	Common examples of algorithms with coefficients that can be optimized using gradient descent are Linear Regression and Logistic Regression.	What algorithms use gradient descent
7520	Therefore, a number of alternative ways of handling the missing data has been developed.Listwise or case deletion.  Pairwise deletion.  Mean substitution.  Regression imputation.  Last observation carried forward.  Maximum likelihood.  Expectation-Maximization.  Multiple imputation.More items•	How do you handle missing data in regression analysis
5139	Type I error is equivalent to a False positive. Type II error is equivalent to a False negative. Type I error refers to non-acceptance of hypothesis which ought to be accepted. Type II error is the acceptance of hypothesis which ought to be rejected.	What are type I and type II errors in machine learning
8340	Uses. Quota sampling is useful when time is limited, a sampling frame is not available, the research budget is very tight or detailed accuracy is not important. Subsets are chosen and then either convenience or judgment sampling is used to choose people from each subset.	Where is quota sampling used
934	It's O(V+E) because each visit to v of V must visit each e of E where |e| <= V-1. Since there are V visits to v of V then that is O(V).  So total time complexity is O(V + E).	Why is the complexity of DFS o v e
2881	Having more data is always a good idea. It allows the “data to tell for itself,” instead of relying on assumptions and weak correlations. Presence of more data results in better and accurate models.  For example: we do not get a choice to increase the size of training data in data science competitions.	Why does having more data increase accuracy
683	Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.	What does normal distribution mean in statistics
292	If the data set follows the bias then Naive Bayes will be a better classifier. Both Naive Bayes and Logistic regression are linear classifiers, Logistic Regression makes a prediction for the probability using a direct functional form where as Naive Bayes figures out how the data was generated given the results.	Why is logistic regression better than naive Bayes
1135	A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.	What does a correlation matrix show
7151	Findings. A fundamental problem with stepwise regression is that some real explanatory variables that have causal effects on the dependent variable may happen to not be statistically significant, while nuisance variables may be coincidentally significant.	What is wrong with stepwise regression
4836	Below are the different regression techniques: Ridge Regression. Lasso Regression. Polynomial Regression. Bayesian Linear Regression.	What are the types of regression analysis
1110	A type III error is where you correctly reject the null hypothesis, but it's rejected for the wrong reason. This compares to a Type I error (incorrectly rejecting the null hypothesis) and a Type II error (not rejecting the null when you should).	What is a Type 3 error in statistics
2412	Dimensional Analysis (also called Factor-Label Method or the Unit Factor Method) is a problem-solving method that uses the fact that any number or expression can be multiplied by one without changing its value. It is a useful technique.	What is the point of dimensional analysis
6428	Figure 14.11: A nonlinear problem. An example of a nonlinear classifier is kNN.  Linear classifiers misclassify the enclave, whereas a nonlinear classifier like kNN will be highly accurate for this type of problem if the training set is large enough.	How are linear classifiers different from non linear classifiers
4120	A test statistic is a statistic (a quantity derived from the sample) used in statistical hypothesis testing.  An important property of a test statistic is that its sampling distribution under the null hypothesis must be calculable, either exactly or approximately, which allows p-values to be calculated.	How are test statistics in statistical testings derived
702	The probability of observing any single value is equal to 0, since the number of values which may be assumed by the random variable is infinite.	What is the probability of observing a single value in a continuous distribution
761	The BIR is based on Boolean logic and classical set theory in that both the documents to be searched and the user's query are conceived as sets of terms (a bag-of-words model).  Retrieval is based on whether or not the documents contain the query terms.	How does a Boolean model help in information retrieval
528	Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).	What are ensemble methods in statistics
1342	The Chi-square test is a non-parametric statistic, also called a distribution free test. Non-parametric tests should be used when any one of the following conditions pertains to the data: The level of measurement of all the variables is nominal or ordinal.	Is the Pearson chi square a nonparametric test
7982	Share: Filters are systems or elements used to remove substances such as dust or dirt, or electronic signals, etc., as they pass through filtering media or devices. Filters are available for filtering air or gases, fluids, as well as electrical and optical phenomena. Air filters are used for cleaning air.	What is the use of filters
1097	There are four steps in training and using the sentiment classifier:Load a pretrained word embedding.Load an opinion lexicon listing positive and negative words.Train a sentiment classifier using the word vectors of the positive and negative words.Calculate the mean sentiment scores of the words in a piece of text.	How do you train a sentiment analysis
1305	Random forests consist of multiple single trees each based on a random sample of the training data. They are typically more accurate than single decision trees. The following figure shows the decision boundary becomes more accurate and stable as more trees are added.	Why is the decision forest better than the random forest
634	A matrix with rows and columns over a field is a function from the set of all ordered pairs of integers in range to .  A linear operator is a linear function from a Vector space to itself. In notations, given a vector space , a linear operator is a function which satisfies for all in the underlying Field and vectors .	What is the difference between a matrix and a linear operator
1306	Correlated vs. A correlated subquery can be thought of as a filter on the table that it refers to, as if the subquery were evaluated on each row of the table in the outer query. An uncorrelated subquery has no such external column references.	What is the difference between correlated and uncorrelated subquery
1470	By taking advantage of naturally occurring structure, we can design learning algorithms that exhaustively search even infinite hypothesis spaces without explicitly enumerating every hypothesis. For instance, general-to-specific ordering.	What is general to specific ordering in machine learning
7301	a. it allows us to disregard the size of the sample selected when the population is not normal.  it allows us the disregard the shape of the population when n is large.	Why is the central limit theorem so important to the study of sampling distributions
458	Regression is primarily used to build models/equations to predict a key response, Y, from a set of predictor (X) variables. Correlation is primarily used to quickly and concisely summarize the direction and strength of the relationships between a set of 2 or more numeric variables.	Should I use regression or correlation
2493	Face detection is a broader term than face recognition. Face detection just means that a system is able to identify that there is a human face present in an image or video.  Face recognition can confirm identity. It is therefore used to control access to sensitive areas.	What is the difference between face detection and face recognition
3587	ReLU is an activation function, that nullifies negative neurons, and in its simplicity, it also aids computation speed. However, unlike ELU, it doesn't have a normalizing effect, so BatchNorm helps even better.	Does batch normalization help with ReLU activation functions
1000	AI programs can provide automation for low-value tasks freeing up engineers to perform higher-value tasks. By using machine learning to discover patterns in the data, machines will be incredibly important to help with engineering judgment.	How is AI used in engineering
672	Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.	Can you do multiclass classification with logistic regression
7442	SGD is a variant of gradient descent. Instead of performing computations on the whole dataset — which is redundant and inefficient — SGD only computes on a small subset or random selection of data examples.  Essentially Adam is an algorithm for gradient-based optimization of stochastic objective functions.	What is the difference between SGD and Adam
1974	Decision trees are mainly used to perform classification tasks. Samples are submitted to a test in each node of the tree and guided through the tree based on the result. Decision trees can also be used to perform clustering, with a few adjustments.	Can decision trees be used for clustering
669	A linear threshold unit is a simple artificial neuron whose output is its thresholded total net input. That is, an LTU with threshold T calculates the weighted sum of its inputs, and then outputs 0 if this sum is less than T, and 1 if the sum is greater than T.	What is linear threshold unit
8590	1.96	What is the z value in Wilcoxon signed rank test
2279	1) Your model performs better on the training data than on the unknown validation data.  It can also happen when your training loss is calculated as a moving average over 1 epoch, whereas the validation loss is calculated after the learning phase of the same epoch.	What is training and validation loss
1369	Target variable, in the machine learning context is the variable that is or should be the output. For example it could be binary 0 or 1 if you are classifying or it could be a continuous variable if you are doing a regression. In statistics you also refer to it as the response variable.	What is the target variable
5933	Bootstrap aggregating (bagging) In order to promote model variance, bagging trains each model in the ensemble using a randomly drawn subset of the training set. As an example, the random forest algorithm combines random decision trees with bagging to achieve very high classification accuracy.	Which algorithm uses ensemble learning
558	Image recognition is classifying data into one bucket out of many.Steps in the processgather and organize data to work with (85% of the effort)build and test a predictive model (10% of the effort)use the model to recognize images (5% of the effort)	How do you do image recognition
6578	Clustering starts by computing a distance between every pair of units that you want to cluster. A distance matrix will be symmetric (because the distance between x and y is the same as the distance between y and x) and will have zeroes on the diagonal (because every item is distance zero from itself).	What is distance matrix in clustering
962	With inferential statistics, you take data from samples and make generalizations about a population. For example, you might stand in a mall and ask a sample of 100 people if they like shopping at Sears.  This is where you can use sample data to answer research questions.	What is an example of inferential statistics
1981	At a very basic level, deep learning is a machine learning technique. It teaches a computer to filter inputs through layers to learn how to predict and classify information. Observations can be in the form of images, text, or sound. The inspiration for deep learning is the way that the human brain filters information.	What is deep learning and how it works
8462	Tokenization is a common task in Natural Language Processing (NLP).  Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords.	What is Tokenizer in NLP
8062	95%	What percentage of observations fall within 2 standard deviations of the mean
6164	A common problem in machine learning is sparse data, which alters the performance of machine learning algorithms and their ability to calculate accurate predictions. Data is considered sparse when certain expected values in a dataset are missing, which is a common phenomenon in general large scaled data analysis.	What is sparse data in machine learning
6784	Bootstrap Confidence IntervalsCalculate a Population of Statistics. The first step is to use the bootstrap procedure to resample the original data a number of times and calculate the statistic of interest.  Calculate Confidence Interval. Now that we have a population of the statistics of interest, we can calculate the confidence intervals.	How does bootstrap calculate confidence interval
155	Multicollinearity exists whenever an independent variable is highly correlated with one or more of the other independent variables in a multiple regression equation. Multicollinearity is a problem because it undermines the statistical significance of an independent variable.	What is Multicollinearity and why is it a problem
8332	Fortunately, hinge loss, logistic loss and square loss are all convex functions. Convexity ensures global minimum and it's computationally appleaing.	Is squared loss convex
6583	The median is a simple measure of central tendency. To find the median, we arrange the observations in order from smallest to largest value. If there is an odd number of observations, the median is the middle value. If there is an even number of observations, the median is the average of the two middle values.	How do you describe median in statistics
846	In a hypothesis test, we:Evaluate the null hypothesis, typically denoted with H0.  Always write the alternative hypothesis, typically denoted with Ha or H1, using less than, greater than, or not equals symbols, i.e., (≠, >, or <).More items	How do you write a null and alternative hypothesis
1299	Topic modelling refers to the task of identifying topics that best describes a set of documents.  And the goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics.	How does LDA topic modeling work
437	The advantage with Wilcoxon Signed Rank Test is that it neither depends on the form of the parent distribution nor on its parameters. It does not require any assumptions about the shape of the distribution.	What are the advantage of wilcoxon sign rank test over sign test
1398	A baseline is a method that uses heuristics, simple summary statistics, randomness, or machine learning to create predictions for a dataset.  A machine learning algorithm tries to learn a function that models the relationship between the input (feature) data and the target variable (or label).	What is baseline in machine learning
177	Discrete data involves round, concrete numbers that are determined by counting. Continuous data involves complex numbers that are measured across a specific time interval.	How can you tell the difference between continuous and discrete data
5014	Imitation Learning (IL) and Reinforcement Learning (RL) are often introduced as similar, but separate problems. Imitation learning involves a supervisor that provides data to the learner. Reinforcement learning means the agent has to explore in the environment to get feedback signals.	Is imitation learning reinforcement learning
5024	The larger the sample size is the smaller the effect size that can be detected. The reverse is also true; small sample sizes can detect large effect sizes.  Thus an appropriate determination of the sample size used in a study is a crucial step in the design of a study.	Is it important to determine the sample size
1084	Friedman Test Therefore, we have a non-parametric equivalent of the two way ANOVA that can be used for data sets which do not fulfill the assumptions of the parametric method. The method, which is sometimes known as Friedman's two way analysis of variance, is purely a hypothesis test.	Is there a non parametric equivalent of a 2 way Anova
1230	Deep nets are hard to train because of the presence of multiple local minimas. The optimization problem for a deep net is non-convex, hence, has multiple local minimas. If any of the chosen hyper-parameters are not appropriate, you will end up at a bad local minima which will lead to poor performance.	Why are deep neural networks hard to train
2037	There are often only a handful of possible classes or results. For a given classification, one tries to measure the probability of getting different evidence or patterns.  Using Bayes rule, we use this to get what is desired, the conditional probability of the classification given the evidence.	Why do you think you need conditional probability
391	The technique of Monte Carlo Simulation (MCS) was originally developed for use in nuclear weapons design. It provides an efficient way to simulate processes involving chance and uncertainty and can be applied in areas as diverse as market sizing, customer lifetime value measurement and customer service management.	What are some interesting applications of Monte Carlo method
4281	A feedforward neural network is an artificial neural network wherein connections between the units do not form a cycle.  The simplest kind of neural network is a single-layer perceptron network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights.	What is single layer feedforward neural network
2488	Use regression analysis to describe the relationships between a set of independent variables and the dependent variable. Regression analysis produces a regression equation where the coefficients represent the relationship between each independent variable and the dependent variable.	What does a regression analysis tell you
7615	Support Vector Machine can also be used as a regression method, maintaining all the main features that characterize the algorithm (maximal margin). The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences.	Can SVM used for regression
5919	As for exponential smoothing, also ARIMA models are among the most widely used approaches for time series forecasting. The name is an acronym for AutoRegressive Integrated Moving Average. In an AutoRegressive model the forecasts correspond to a linear combination of past values of the variable.	Which algorithm is best for time series forecasting
786	The linear function is called the objective function , of the form f(x,y)=ax+by+c .	How do you find the objective function
8619	The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume.	What is a convolutional layer in CNN
3521	Clustering is the most commonly used unsupervised learning method. This is because typically it is one of the best ways to explore and find out more about data visually.  k-Means clustering: partitions data into k distinct clusters based on distance to the centroid of a cluster.	Why K means clustering is unsupervised learning
6920	Clustering methods are used to identify groups of similar objects in a multivariate data sets collected from fields such as marketing, bio-medical and geo-spatial. They are different types of clustering methods, including: Partitioning methods. Hierarchical clustering.	What are clustering methods
3647	When we say that correlation does not imply cause, we mean that just because you can see a connection or a mutual relationship between two variables, it doesn't necessarily mean that one causes the other.	If correlation does not imply causation then why do researchers use correlation anyways
543	In statistics, single-linkage clustering is one of several methods of hierarchical clustering. It is based on grouping clusters in bottom-up fashion (agglomerative clustering), at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other.	What is linkage in agglomerative clustering
1315	Assuming 0<σ2<∞, by definition σ2=E[(X−μ)2]. Thus, the variance itself is the mean of the random variable Y=(X−μ)2. This suggests the following estimator for the variance ˆσ2=1nn∑k=1(Xk−μ)2.	How do you find the estimated variance
1107	The median is calculated by first sorting all the pixel values from the window into numerical order, and then replacing the pixel being considered with the middle (median) pixel value.	How do you find the median of a filter
8406	Systematic sampling is a type of probability sampling method in which sample members from a larger population are selected according to a random starting point but with a fixed, periodic interval. This interval, called the sampling interval, is calculated by dividing the population size by the desired sample size.	What is systematic random sampling in statistics
3246	F-test is statistical test, that determines the equality of the variances of the two normal populations. T-statistic follows Student t-distribution, under null hypothesis. F-statistic follows Snedecor f-distribution, under null hypothesis. Comparing the means of two populations.	What is the relationship between the F test statistic and t test statistic
3258	The principal advantage of linear regression is its simplicity, interpretability, scientific acceptance, and widespread availability. Linear regression is the first method to use for many problems. Analysts can use linear regression together with techniques such as variable recoding, transformation, or segmentation.	What is the main advantage of using linear regression
555	An algorithm that uses random numbers to decide what to do next anywhere in its logic is called Randomized Algorithm. For example, in Randomized Quick Sort, we use random number to pick the next pivot (or we randomly shuffle the array).	What is randomized selection algorithm
6801	The posterior distribution is a way to summarize what we know about uncertain quantities in Bayesian analysis. It is a combination of the prior distribution and the likelihood function, which tells you what information is contained in your observed data (the “new evidence”).	What is posterior distribution in Bayesian
8268	"A statistical test provides a mechanism for making quantitative decisions about a process or processes. The intent is to determine whether there is enough evidence to ""reject"" a conjecture or hypothesis about the process."	Why do we use statistical tests
729	ratio() to measure similarity between two strings. Pass two strings into difflib. SequenceMatcher(isjunk, a, b) with isJunk set to None to get a SequenceMatcher() object representing the similarity between the strings. Call ratio() on this object to get the ratio of matching characters to total characters.	How do you find the similarity between two strings in python
8250	Correlation means association - more precisely it is a measure of the extent to which two variables are related. There are three possible results of a correlational study: a positive correlation, a negative correlation, and no correlation.	What is correlation in simple terms
6632	Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.	How do you interpret Cohen's kappa
1392	NLP is a technological process that allows computers to derive meaning from user text inputs.  With NLP, you are able to “train” your chatbot on the various interactions it will go through, and help streamline the responses it outputs.	How NLP is used in chatbot
6497	In machine learning we are trying to create solutions to some problem by using data or examples.  Genetic algorithms are stochastic search algorithms which are often used in machine learning applications.	Is a genetic algorithm machine learning
2496	Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable; the method estimates the relationship by minimizing the sum of the squares in the difference between the observed and predicted values of the	What is OLS regression analysis
3386	Here we have few types of classification algorithms in machine learning:Linear Classifiers: Logistic Regression, Naive Bayes Classifier.Nearest Neighbor.Support Vector Machines.Decision Trees.Boosted Trees.Random Forest.Neural Networks.	How do you classify data machine learning
15	Neural networks are often compared to decision trees because both methods can model data that has nonlinear relationships between variables, and both can handle interactions between variables.  A neural network is more of a “black box” that delivers results without an explanation of how the results were derived.	Why is neural network better than decision tree
8086	The least squares criterion is a formula used to measure the accuracy of a straight line in depicting the data that was used to generate it. That is, the formula determines the line of best fit. This mathematical formula is used to predict the behavior of the dependent variables.	What is the least square criterion
2095	The Hopfield Network. The nodes of a Hopfield network can be updated synchronously or asynchronously. Synchronous updating means that at time step (t+1) every neuron is updated based on the network state at time step t.	What is synchronous update in Hopfield model
5006	2.1 The Early Days. Constraint satisfaction, in its basic form, involves finding a value for each one of a set of problem variables where constraints specify that some subsets of values cannot be used together.	What is constraints satisfaction problem in AI
422	Probability theory is the mathematical study of phenomena characterized by randomness or uncertainty. More precisely, probability is used for modelling situations when the result of an experiment, realized under the same circumstances, produces different results (typically throwing a dice or a coin).	What is probability theory in research
3971	To calculate Maddrey discriminant function using SI units, such as micromoles per litre, divide bilirubin value by 17.	How do you find the maddrey discriminant function
359	In terms of linear regression, variance is a measure of how far observed values differ from the average of predicted values, i.e., their difference from the predicted value mean. The goal is to have a value that is low.	What is variance in multiple regression
7757	In other words, accuracy describes the difference between the measurement and the part's actual value, while precision describes the variation you see when you measure the same part repeatedly with the same device.	What is the relationship between precision and accuracy
76	Class limits are the least and greatest numbers that can belong to the class. Class boundaries are the numbers that separate classes without forming gaps between them.	What is the difference between class limits and class boundaries quizlet
3204	Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true.	What is the difference between a Type I error and a Type II error
257	The point of a test set is to give you a final, unbiased performance measure of your entire model building process.	What is the point of a test set
4031	The seeds of modern AI were planted by classical philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning.	How was AI developed
2188	Use Augmented Dickey-Fuller Test (adf test). A p-Value of less than 0.05 in adf. test() indicates that it is stationary.	How do I check if a time series is stationary in R
2910	Linear Discriminant Analysis or Normal Discriminant Analysis or Discriminant Function Analysis is a dimensionality reduction technique which is commonly used for the supervised classification problems. It is used for modeling differences in groups i.e. separating two or more classes.	What is discriminant function in machine learning
230	1 randomly select k data points to act as centroids.2 calculate cosine similarity between each data point and each centroid.  3 assign each data point to the cluster with which it has the *highest* cosine similarity.4 calculate the average of each cluster to get new centroids.More items	How do you use cosine similarity for clustering
8041	Events A and B are independent if the equation P(A∩B) = P(A) · P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together.	How do you prove that A and B are independent
3733	"Loss is often used in the training process to find the ""best"" parameter values for your model (e.g. weights in neural network).  Once you find the optimized parameters above, you use this metrics to evaluate how accurate your model's prediction is compared to the true data."	Why is it useful to track loss while the model is being trained
8101	Use of efficient procedures and rules by the Inference Engine is essential in deducting a correct, flawless solution. In case of knowledge-based ES, the Inference Engine acquires and manipulates the knowledge from the knowledge base to arrive at a particular solution.	Why inference engine is needed
111	Conditional probability is the probability of one event occurring with some relationship to one or more other events. For example: Event A is that it is raining outside, and it has a 0.3 (30%) chance of raining today. Event B is that you will need to go outside, and that has a probability of 0.5 (50%).	What is conditional probability examples
4683	rate of change	What does gradient mean
2876	There is no widely accepted standard notation for the median, but some authors represent the median of a variable x either as x͂ or as μ1/2 sometimes also M. In any of these cases, the use of these or other symbols for the median needs to be explicitly defined when they are introduced.	What is the mathematical symbol for median
807	Blocking refers to operations that block further execution until that operation finishes while non-blocking refers to code that doesn't block execution. Or as Node. js docs puts it, blocking is when the execution of additional JavaScript in the Node. js process must wait until a non-JavaScript operation completes.	What is blocking and non blocking in node JS
1717	Additivity is a property pertaining to a set of interdependent index numbers related by definition or by accounting constraints under which an aggregate is defined as the sum of its components; additivity requires this identity to be preserved when the values of both an aggregate and its components in some reference	What is additivity in statistics
3212	Bayes' theorem, named after 18th-century British mathematician Thomas Bayes, is a mathematical formula for determining conditional probability. Conditional probability is the likelihood of an outcome occurring, based on a previous outcome occurring.	What is Bayes rule in probability
4954	One hidden layer is sufficient for the large majority of problems. Usually, each hidden layer contains the same number of neurons. The larger the number of hidden layers in a neural network, the longer it will take for the neural network to produce the output and the more complex problems the neural network can solve.	How many hidden layers are needed
1088	In natural language processing, perplexity is a way of evaluating language models.  Using the definition of perplexity for a probability model, one might find, for example, that the average sentence xi in the test sample could be coded in 190 bits (i.e., the test sentences had an average log-probability of -190).	What is perplexity in NLP
7132	T-tests are about finding differences between two groups on the mean values of some continuous variable. Correlation is about the linear relationship of two (usually continuous) variables.	What is the difference between at test and a correlation
245	This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction.	What is the difference between a one tailed test and a two tailed test
1254	Weinstein	Who discovered optimism bias
3956	Factor analysis is an exploratory statistical technique to investigate dimensions and the factor structure underlying a set of variables (items) while cluster analysis is an exploratory statistical technique to group observations (people, things, events) into clusters or groups so that the degree of association is	What is the difference between cluster and factor analysis
2627	Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity.  By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. It is hoped that the net effect will be to give estimates that are more reliable.	Why do we use ridge regression
1256	It appears that the median is always closest to the high point (the mode), while the mean tends to be farther out on the tail. In a symmetrical distribution, the mean and the median are both centrally located close to the high point of the distribution.	Where is the mean located in relationship to the median
5027	Increase the power of your analysis.larger sample size.better data collection (reducing error)better/correct model (more complex model, account for covariates, etc.)use a one-sided test instead of a two-sided test.	How do you decrease P value in regression
1962	A sampling distribution is a probability distribution of a statistic obtained from a larger number of samples drawn from a specific population. The sampling distribution of a given population is the distribution of frequencies of a range of different outcomes that could possibly occur for a statistic of a population.	Is a sampling distribution a probability distribution
5713	Intelligence has been defined in many ways: the capacity for logic, understanding, self-awareness, learning, emotional knowledge, reasoning, planning, creativity, critical thinking, and problem-solving.  Intelligence is most often studied in humans but has also been observed in both non-human animals and in plants.	What intelligence means
1992	Forward propagation is how neural networks make predictions. Input data is “forward propagated” through the network layer by layer to the final layer which outputs a prediction.	What is forward propagation in neural network
277	"Analysis of Variance (ANOVA) is a statistical method used to test differences between two or more means. It may seem odd that the technique is called ""Analysis of Variance"" rather than ""Analysis of Means."" As you will see, the name is appropriate because inferences about means are made by analyzing variance."	Why is Anova analysis of variance
8423	Stratified sampling combines random selection with predetermined weightig of a population's demographic characteristics.  Telephone surveys are usually conducted with random phone numbers picked by computer.	What is the difference between random sampling and stratified sampling quizlet
6116	As far as i read in the manual, stream length is simply the number (n) of the sequent number of the random number sample. The bitstream is likely a number of sample size.	What does Stream length in NIST randomness test mean
8684	A sampling distribution is a probability distribution of a statistic obtained from a larger number of samples drawn from a specific population. The sampling distribution of a given population is the distribution of frequencies of a range of different outcomes that could possibly occur for a statistic of a population.	What is a sampling distribution in statistics
5556	0:505:06Suggested clip · 106 secondsPredicting with a Neural Network explained - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How are predictions used in neural networks
6304	The entropy of a substance can be obtained by measuring the heat required to raise the temperature a given amount, using a reversible process. The standard molar entropy, So, is the entropy of 1 mole of a substance in its standard state, at 1 atm of pressure.	How do you measure entropy
76	Linear filtering of an image is accomplished through an operation called convolution. Convolution is a neighborhood operation in which each output pixel is the weighted sum of neighboring input pixels. The matrix of weights is called the convolution kernel, also known as the filter.	Are convolutional filters linear
441	Neural nets are a means of doing machine learning, in which a computer learns to perform some task by analyzing training examples.  Most of today's neural nets are organized into layers of nodes, and they're “feed-forward,” meaning that data moves through them in only one direction.	How do neural networks work
2342	Master limited partnerships (MLPs) are a business venture that exists in the form of a publicly traded limited partnership. They combine the tax benefits of a private partnership—profits are taxed only when investors receive distributions—with the liquidity of a publicly-traded company (PTP).	What is MLP and how does it work
221	The k-means clustering algorithm attempts to split a given anonymous data set (a set containing no information as to class identity) into a fixed number (k) of clusters.  The resulting classifier is used to classify (using k = 1) the data and thereby produce an initial randomized set of clusters.	How does K means clustering work
808	Logistic regression is a powerful machine learning algorithm that utilizes a sigmoid function and works best on binary classification problems, although it can be used on multi-class classification problems through the “one vs. all” method. Logistic regression (despite its name) is not fit for regression tasks.	Can we use logistic regression for classification
378	To find the average, add them together and divide by the number of values (10 in this case). When repeated measurements give different results, we want to know how widely spread the readings are. The spread of values tells us something about the uncertainty of a measurement.	How do you find the uncertainty of a measurement
233	Logistic regression is a classification algorithm traditionally limited to only two-class classification problems. If you have more than two classes then Linear Discriminant Analysis is the preferred linear classification technique.	Is linear discriminant analysis machine learning
527	Statistical hypothesis: A statement about the nature of a population. It is often stated in terms of a population parameter. Null hypothesis: A statistical hypothesis that is to be tested. Alternative hypothesis: The alternative to the null hypothesis.	What is meant by hypothesis in statistics
339	In the statistical analysis of time series, autoregressive–moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the autoregression (AR) and the second for the moving average (MA).	What is autoregressive moving average ARMA modeling in the context of time series analysis
5225	The family of beta(α,β) distributions is an exponential family. η is called the natural parameter.	Is beta distribution Exponential family
1031	Stepwise regression is a modification of the forward selection so that after each step in which a variable was added, all candidate variables in the model are checked to see if their significance has been reduced below the specified tolerance level. If a nonsignificant variable is found, it is removed from the model.	How does forward stepwise regression work
8419	Social engineering is a manipulation technique that exploits human error to gain private information, access, or valuables. In cybercrime, these “human hacking” scams tend to lure unsuspecting users into exposing data, spreading malware infections, or giving access to restricted systems.	What is meant by social engineering in hacking
4821	On the other hand, when the normal approximation is used to approximate a discrete distribution, a continuity correction can be employed so that we can approximate the probability of a specific value of the discrete distribution. The continuity correction requires adding or subtracting .	Why is the correction for continuity used when using the normal approximation to the binomial distribution
1236	The median is another form of an average. It usually represents the middle number in a given sequence of numbers when it's ordered by rank.	Is the median the average
5140	How to Find a Sample Size Given a Confidence Interval and Width (unknown population standard deviation)za/2: Divide the confidence interval by two, and look that area up in the z-table: .95 / 2 = 0.475.  E (margin of error): Divide the given width by 2. 6% / 2.  : use the given percentage. 41% = 0.41.  : subtract. from 1.	How do you determine a sample size
1228	Identifying Confounding A simple, direct way to determine whether a given risk factor caused confounding is to compare the estimated measure of association before and after adjusting for confounding. In other words, compute the measure of association both before and after adjusting for a potential confounding factor.	How do you identify a confounding variable
1543	Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance.	What is bias variance tradeoff in machine learning
483	Summary: The range of a set of data is the difference between the highest and lowest values in the set. To find the range, first order the data from least to greatest. Then subtract the smallest value from the largest value in the set.	How do you determine range
1903	The Altman Z-Score Formula E= Sales / Total Assets (efficiency ratio – measures how much the company's assets are producing in sales). Z-Score Results: Z-Score of < 1.81 represents a company in distress. Z-Score between 1.81 and 2.99 represents the “caution” zone.	How does Altman’s z score formula work
2379	A marginal distribution is a frequency or relative frequency distribution of either the row or column variable in a contingency table.  A conditional distribution lists the relative frequency of each category of the response variable, given a specific value of the explanatory variable in a contingency table.	What is meant by a marginal distribution quizlet
4904	When used as nouns, quantile means one of the class of values of a variate which divides the members of a batch or sample into equal-sized subgroups of adjacent values or a probability distribution into distributions of equal probability, whereas quartile means any of the three points that divide an ordered	In statistics what is the difference between a quartile and a quantile
6525	As the Oxford dictionary states it, Probability means 'The extent to which something is probable; the likelihood of something happening or being the case'. In mathematics too, probability indicates the same – the likelihood of the occurrence of an event. Examples of events can be : Tossing a coin with the head up.	What is probability theory examples
44	Use regression analysis to describe the relationships between a set of independent variables and the dependent variable. Regression analysis produces a regression equation where the coefficients represent the relationship between each independent variable and the dependent variable.	What are regression models used for
43	These represent the squares of the deviation from the mean for each measured value of data. Add the squares of errors together. The final step is to find the sum of the values in the third column. The desired result is the SSE, or the sum of squared errors.	How do you find the sum of squared errors
5946	Overfitting in Machine Learning Overfitting refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.	What is Overfitting in machine learning
844	0:3513:46Suggested clip · 70 secondsInterpreting Odds Ratio for Multinomial Logistic Regression using YouTubeStart of suggested clipEnd of suggested clip	How do you interpret multinomial logistic regression
1187	A nonlinear relationship is a type of relationship between two entities in which change in one entity does not correspond with constant change in the other entity.  However, nonlinear entities can be related to each other in ways that are fairly predictable, but simply more complex than in a linear relationship.	What is a nonlinear association
6707	The Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to maximize the chances of A winning, while on the next turn player B is trying to minimize the chances of A winning (i.e., to maximize B's own chances of winning).	What are some good explanations of the Minimax algorithm
999	You CAN use linear regression with ordinal data, because you can regress any set of numbers against any other.	Can you do linear regression with ordinal data
5726	Gaussian process regression (GPR) is a nonparametric, Bayesian approach to regression that is making waves in the area of machine learning. GPR has several benefits, working well on small datasets and having the ability to provide uncertainty measurements on the predictions.	Is Gaussian process regression machine learning
2869	Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.	What is hierarchical clustering in data mining
2816	The uncertainty of the difference between two means is greater than the uncertainty in either mean.  So the SE of the difference is greater than either SEM, but is less than their sum.	What is the standard error of difference between two means
1396	Linear Activation Function A linear activation function takes the form: A = cx. It takes the inputs, multiplied by the weights for each neuron, and creates an output signal proportional to the input. In one sense, a linear function is better than a step function because it allows multiple outputs, not just yes and no.	What is linear activation function in neural network
7707	Auxiliary Classifiers are type of architectural component that seek to improve the convergence of very deep networks. They are classifier heads we attach to layers before the end of the network.	What is auxiliary classifier
130	Here is a brief review of our original seven techniques for dimensionality reduction:Missing Values Ratio.  Low Variance Filter.  High Correlation Filter.  Random Forests/Ensemble Trees.  Principal Component Analysis (PCA).  Backward Feature Elimination.  Forward Feature Construction.	Which technique can be implemented if you want to reduce the dimensionality of a certain statistical problem
7852	Step 1: Calculate the rate of selection for each group. (Divide by the number of persons selected from a group by the number available from that group.) Step 2: Determine which group has the lowest selection rate, other than 0%.	How is disparate impact calculated
1114	"The obvious difference between ANOVA and a ""Multivariate Analysis of Variance"" (MANOVA) is the “M”, which stands for multivariate. In basic terms, A MANOVA is an ANOVA with two or more continuous response variables. Like ANOVA, MANOVA has both a one-way flavor and a two-way flavor."	What is the difference between Manova and Anova
8576	The mean average precision (mAP) or sometimes simply just referred to as AP is a popular metric used to measure the performance of models doing document/information retrival and object detection tasks.	What is the mean average precision mAP in object detection
287	An RBF is a function that changes with distance from a location. For example, suppose the radial basis function is simply the distance from each location, so it forms an inverted cone over each location. If you take a cross section of the x,z plane for y = 5, you will see a slice of each radial basis function.	How does radial basis function work
7950	A Z-score is a score which indicates how many standard deviations an observation is from the mean of the distribution. Z-scores tend to be used mainly in the context of the normal curve, and their interpretation based on the standard normal table.  Non-normal distributions can also be transformed into sets of Z-scores.	Can z score be used for non normal distribution
5433	Gaussian process regression (GPR) is a nonparametric, Bayesian approach to regression that is making waves in the area of machine learning. GPR has several benefits, working well on small datasets and having the ability to provide uncertainty measurements on the predictions.	What is Gaussian process regression model
1599	In probability theory, an experiment or trial (see below) is any procedure that can be infinitely repeated and has a well-defined set of possible outcomes, known as the sample space. An experiment is said to be random if it has more than one possible outcome, and deterministic if it has only one.	What is the meaning of experiment in probability
800	Selection bias can result when the selection of subjects into a study or their likelihood of being retained in the study leads to a result that is different from what you would have gotten if you had enrolled the entire target population.	How does selection bias affect results
3336	The Central limit Theorem states that when sample size tends to infinity, the sample mean will be normally distributed. The Law of Large Number states that when sample size tends to infinity, the sample mean equals to population mean.	What is the difference between law of large numbers and central limit theorem
4059	You can tell if two random variables are independent by looking at their individual probabilities. If those probabilities don't change when the events meet, then those variables are independent. Another way of saying this is that if the two variables are correlated, then they are not independent.	How do you know if two random variables are independent
4357	Metrics for Evaluating Machine Learning Algorithms Different performance metrics are used to evaluate different Machine Learning Algorithms. For example a classifier used to distinguish between images of different objects; we can use classification performance metrics such as, Log-Loss, Average Accuracy, AUC, etc.	What are metrics in machine learning
706	K-Means clustering algorithm instead converses on local minima which might also correspond to the global minima in some cases but not always.  But that is done by simply making the algorithm choose the set of same random no. for each run.	Does two runs of K mean clustering expected to yield same clustering results
1891	The t-distribution, also known as Student's t-distribution, is a way of describing data that follow a bell curve when plotted on a graph, with the greatest number of observations close to the mean and fewer observations in the tails.	How is the Students t distribution related to the bell curve
3571	Confusion matrix not only gives you insight into the errors being made by your classifier but also types of errors that are being made. This breakdown helps you to overcomes the limitation of using classification accuracy alone. Every column of the confusion matrix represents the instances of that predicted class.	Why do we need confusion matrix in data mining
6032	mAP (mean Average Precision) for Object DetectionPrecision & recall.Precision measures how accurate is your predictions.  Recall measures how good you find all the positives.  IoU (Intersection over union)Precision is the proportion of TP = 2/3 = 0.67.Recall is the proportion of TP out of the possible positives = 2/5 = 0.4.	How do you find the accuracy of an object detection
756	IBM SPSS Statistics for Mac is the ultimate tool for managing your statistics data and research. This super-app affords you complete control over your data.	Can you get SPSS on Mac
873	Let A and G be the Arithmetic Means and Geometric Means respectively of two positive numbers a and b. Then, As, a and b are positive numbers, it is obvious that A > G when G = -√ab.  This proves that the Arithmetic Mean of two positive numbers can never be less than their Geometric Means.	What is the relation between arithmetic mean and geometric mean
7632	The principle of maximum likelihood is a method of obtaining the optimum values of the parameters that define a model. And while doing so, you increase the likelihood of your model reaching the “true” model.	What is the principle of maximum likelihood
954	In a paired sample t-test, the observations are defined as the differences between two sets of values, and each assumption refers to these differences, not the original data values. The paired sample t-test has four main assumptions:  The observations are independent of one another.	What is the paired data assumption
8112	"Analysis of variance (ANOVA) is a collection of statistical models used to analyze the differences among group means and their associated procedures (such as ""variation"" among and between groups).  ANOVAs are useful for comparing (testing) three or more means (groups or variables) for statistical significance."	What is Anova in machine learning
355	The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.	What are gates in Lstm
5971	Data visualization is an important skill in applied statistics and machine learning. Statistics does indeed focus on quantitative descriptions and estimations of data. Data visualization provides an important suite of tools for gaining a qualitative understanding.	Is data visualization part of machine learning
4771	A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous. If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable. Decision boundaries are not always clear cut.	What is decision boundary in classification
7401	Let's discuss some advantages and disadvantages of Linear Regression. Logistic regression is easier to implement, interpret, and very efficient to train. If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting.	What are the advantages and disadvantages of logistic regression
8	In this article, we'll cover some of the frameworks set around deep learning and neural networks, including:TensorFlow.Keras.PyTorch.Theano.DL4J.Caffe.Chainer.Microsoft CNTK.	What are the popular deep learning frameworks
5494	A deep Boltzmann machine (DBM) is a type of binary pairwise Markov random field (undirected probabilistic graphical model) with multiple layers of hidden random variables. It is a network of symmetrically coupled stochastic binary units. It comprises a set of visible units and layers of hidden units .	What is deep Boltzmann machine
1345	Eigenvalues represent the total amount of variance that can be explained by a given principal component. They can be positive or negative in theory, but in practice they explain variance which is always positive. If eigenvalues are greater than zero, then it's a good sign.	What is the meaning of eigenvalues in factor analysis
1875	normal approximation: The process of using the normal curve to estimate the shape of the distribution of a data set. central limit theorem: The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed.	What is normal approximation
537	Regularized regression is a type of regression where the coefficient estimates are constrained to zero. The magnitude (size) of coefficients, as well as the magnitude of the error term, are penalized.  “Regularization” is a way to give a penalty to certain models (usually overly complex ones).	What is regularization in linear regression
3755	The difference between standard deviation and standard error is based on the difference between the description of data and its inference.Comparison Chart.Basis for ComparisonStandard DeviationStandard ErrorFormulaSquare root of varianceStandard deviation divided by square root of sample size.5 more rows•	What is the difference between error and standard error
5980	It's already automating manual and repetitive tasks. Soon it will augment human decisions. Along the way, it will add more to global GDP by 2030 than the current output of China and India—combined. That growth will be more than enough to create many good jobs, while it will also change how current jobs are being done.	How will artificial intelligence change the future
467	There is no rule for determining what size of correlation is considered strong, moderate or weak.  For this kind of data, we generally consider correlations above 0.4 to be relatively strong; correlations between 0.2 and 0.4 are moderate, and those below 0.2 are considered weak.	Is 0.2 A strong correlation
1317	A single layer perceptron (SLP) is a feed-forward network based on a threshold transfer function. SLP is the simplest type of artificial neural networks and can only classify linearly separable cases with a binary target (1 , 0).	What is a single layer Perceptron
1084	Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Importance.	What is data analysis in machine learning
752	The kernel size here refers to the widthxheight of the filter mask. The max pooling layer, for example, returns the pixel with maximum value from a set of pixels within a mask (kernel). That kernel is swept across the input, subsampling it.	What is kernel size
3905	Communalities – This is the proportion of each variable's variance that can be explained by the factors (e.g., the underlying latent continua). It is also noted as h2 and can be defined as the sum of squared factor loadings for the variables.	What is communality in factor analysis
2673	Both LSA and LDA have same input which is Bag of words in matrix format. LSA focus on reducing matrix dimension while LDA solves topic modeling problems.	What is the difference between LSA and LDA
489	Machine Learning AlgorithmsLinear Regression. To understand the working functionality of this algorithm, imagine how you would arrange random logs of wood in increasing order of their weight.  Logistic Regression.  Decision Tree.  SVM (Support Vector Machine)  Naive Bayes.  KNN (K- Nearest Neighbors)  K-Means.  Random Forest.More items•	What are the algorithms in machine learning
6091	As opposed to decision tree and rule set induction, which result in classification models, association rule learning is an unsupervised learning method, with no class labels assigned to the examples.  This would then be a Supervised Learning task , where the NN learns from pre-calssified examples.	Are association rules considered supervised or unsupervised learning
625	8 Methods to Boost the Accuracy of a ModelAdd more data. Having more data is always a good idea.  Treat missing and Outlier values.  Feature Engineering.  Feature Selection.  Multiple algorithms.  Algorithm Tuning.  Ensemble methods.	How do you make a linear regression more accurate
2790	In the context of CNN, a filter is a set of learnable weights which are learned using the backpropagation algorithm. You can think of each filter as storing a single template/pattern.  Filter is referred to as a set of shared weights on the input.	What is a filter in CNN
8509	Prior probability, in Bayesian statistical inference, is the probability of an event before new data is collected. This is the best rational assessment of the probability of an outcome based on the current knowledge before an experiment is performed.	What is a prior probability as used in Bayes rule
4815	The Kappa Architecture was first described by Jay Kreps. It focuses on only processing data as a stream. It is not a replacement for the Lambda Architecture, except for where your use case fits.  The idea is to handle both real-time data processing and continuous reprocessing in a single stream processing engine.	What is the difference between Lambda and Kappa architecture
983	An SVM possesses a number of parameters that increase linearly with the linear increase in the size of the input. A NN, on the other hand, doesn't. Even though here we focused especially on single-layer networks, a neural network can have as many layers as we want.	What is the difference between SVM and neural networks
2318	An independent event is an event in which the outcome isn't affected by another event. A dependent event is affected by the outcome of a second event.	Whats a dependent event and an independent event
4458	While Neural Networks use neurons to transmit data in the form of input values and output values through connections, Deep Learning is associated with the transformation and extraction of feature which attempts to establish a relationship between stimuli and associated neural responses present in the brain.	What is the difference between neural networks and deep learning
27	An example is the weight of luggage loaded onto an airplane. Counting the number of times a ball dropped from a rooftop bounces before it comes to rest comprises numerical data.On the other hand, non-numerical data, also called categorical, qualitative or Yes/No data, is data that can be observed, not measured.	What is non numeric data
3914	In statistics, scale analysis is a set of methods to analyze survey data, in which responses to questions are combined to measure a latent variable.  Any measurement for such data is required to be reliable, valid, and homogeneous with comparable results over different studies.	What is scaling in statistics
5964	Random Forest is perhaps the most popular classification algorithm, capable of both classification and regression. It can accurately classify large volumes of data. The name “Random Forest” is derived from the fact that the algorithm is a combination of decision trees.	Which algorithm is used for prediction
670	Clustering analysis is broadly used in many applications such as market research, pattern recognition, data analysis, and image processing. Clustering can also help marketers discover distinct groups in their customer base. And they can characterize their customer groups based on the purchasing patterns.	Where can cluster analysis be applied
1075	Network StructureGated Recurrent Unit. GRU (Cho14) alternative memory cell design to LSTM.  Layer normalization. Adding layer normalization (Ba16) to all linear mappings of the recurrent network speeds up learning and often improves final performance.  Feed-forward layers first.  Stacked recurrent networks.	How can Lstm improve performance
461	A common strategy is to grow the tree until each node contains a small number of instances then use pruning to remove nodes that do not provide additional information. Pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross-validation set.	What are some of the techniques to decide decision tree pruning
5009	"The method of analyzing an image that has undergone binarization processing is called ""blob analysis"". A blob refers to a lump. Blob analysis is image processing's most basic method for analyzing the shape features of an object, such as the presence, number, area, position, length, and direction of lumps."	What is a blob in image processing
2178	Rank-reduced singular value decomposition T is a computed m by r matrix of term vectors where r is the rank of A—a measure of its unique dimensions ≤ min(m,n). S is a computed r by r diagonal matrix of decreasing singular values, and D is a computed n by r matrix of document vectors.	What does rank of the matrix mean in latent semantic analysis LSA )
2311	A marginal distribution is the percentages out of totals, and conditional distribution is the percentages out of some column.	What is the difference between marginal distribution and conditional distribution
6244	These are the steps we are going to do:Make a stupid model as an example, train and store it.Fetch the variables you need from your stored model.Build the tensor info from them.Create the model signature.Create and save a model builder.Download a Docker image with TensorFlow serving already compile on it.More items•	How do you deploy a TensorFlow model to production
752	0:382:54Suggested clip · 98 secondsClass Boundaries - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you convert class boundaries to class limits
6972	There is no correct value for MSE. Simply put, the lower the value the better and 0 means the model is perfect.	How much mean squared error is good
1309	In the strictest sense, a nocebo response is where a drug-trial's subject's symptoms are worsened by the administration of an inert, sham, or dummy (simulator) treatment, called a placebo.	What is the nocebo response
630	A null hypothesis is a type of hypothesis used in statistics that proposes that there is no difference between certain characteristics of a population (or data-generating process).	What is null hypothesis 1
878	A data distribution is a function or a listing which shows all the possible values (or intervals) of the data. It also (and this is important) tells you how often each value occurs.	What does a distribution tell us about a set of data
5120	A partition of a number is any combination of integers that adds up to that number. For example, 4 = 3+1 = 2+2 = 2+1+1 = 1+1+1+1, so the partition number of 4 is 5. It sounds simple, yet the partition number of 10 is 42, while 100 has more than 190 million partitions.	What is the partition formula
930	The Central Limit Theorem states that the sampling distribution of the sample means approaches a normal distribution as the sample size gets larger — no matter what the shape of the population distribution.	What does the central limit theorem tell us about the sampling distribution of the sample mean
8149	Tableau is considered more user-friendly because of its easy drag-and-drop capabilities. QlikView gives better performance because of its patented “Associative Technology” which allows for in-memory processing of the table and at the same time circumvents the use of OLAP Cubing.	Why do you prefer learning Qlikview than Tableau
2245	Yes. The reason n-1 is used is because that is the number of degrees of freedom in the sample. The sum of each value in a sample minus the mean must equal 0, so if you know what all the values except one are, you can calculate the value of the final one.	Why is the denominator of sample variance n 1 not n
1094	The z-score statistic converts a non-standard normal distribution into a standard normal distribution allowing us to use Table A-2 in your textbook and report associated probabilities. This discussion combines means, standard deviation, z-score, and probability.	What converts a non standard normal distribution into a standard normal distribution
1050	Augustin Louis Cauchy	Who proved the mean value theorem
6151	"Modus Ponens: ""If A is true, then B is true. A is true. Therefore, B is true."" Modus Tollens: ""If A is true, then B is true."	What is the difference between modus ponens and modus tollens
2480	Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.	What is word tokenization in NLP
2546	Concepts in Machine Learning can be thought of as a boolean-valued function defined over a large set of training data.  We have some attributes/features of the day like, Sky, Air Temperature, Humidity, Wind, Water, Forecast and based on this we have a target Concept named EnjoySport.	What is ML concept
2724	Stratification of clinical trials is the partitioning of subjects and results by a factor other than the treatment given. Stratification can be used to ensure equal allocation of subgroups of participants to each experimental condition. This may be done by gender, age, or other demographic factors.	What is stratification in a study
875	Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.	What is Lemmatization in machine learning
20	Contextual bandit is a machine learning framework designed to tackle these—and other—complex situations. With contextual bandit, a learning algorithm can test out different actions and automatically learn which one has the most rewarding outcome for a given situation.	What is a contextual bandit
5568	A test of a statistical hypothesis , where the region of rejection is on only one side of the sampling distribution , is called a one-tailed test. For example, suppose the null hypothesis states that the mean is less than or equal to 10. The alternative hypothesis would be that the mean is greater than 10.	What is an example of a one tailed test
7749	Unfortunately, causality cannot be established by this observational study, and other work must be done to confirm a cause-and-effect relationship between accumulative deep hypnotic time as measured by Bispectral Index <45 and 1-yr postoperative mortality.	Can you determine causation in an observational study
798	Top 8 Text Mining ToolsMonkeyLearn | User-friendly text mining.Aylien | Simple API for text mining.IBM Watson | Powerful AI platform.Thematic | Text mining for customer feedback.Google Cloud NLP | Custom machine learning models.Amazon Comprehend | Pre-trained text mining models.More items•	What are different tools used for text mining
8244	You still use it, the model in terms of Deep Learning. Beyond this, there is a inherent convolutional depth and complexity inherent to the model in of itself which lends itself to training and otherwise.	Are restricted Boltzmann machines still used
5054	When observed outcome of dependent variable can have multiple possible types then logistic regression will be multinomial.	When the observed outcome of dependent variable can have multiple possible types Then the logistic regression is
7528	The cost parameter decides how much an SVM should be allowed to “bend” with the data. For a low cost, you aim for a smooth decision surface and for a higher cost, you aim to classify more points correctly. It is also simply referred to as the cost of misclassification.	What is cost parameter in SVM
940	A model is a simplified representation of a system. over some time period or spatial extent intended to promote understanding of the real system. Why Build a Model? Building models helps us understand the problem. (and its surrounding system) we are investigating solutions for.	What is a model and why using model
5537	The indicator function 1[0,∞) is right differentiable at every real a, but discontinuous at zero (note that this indicator function is not left differentiable at zero).	Is indicator function differentiable
2405	Decision Trees in Machine Learning. Decision Tree models are created using 2 steps: Induction and Pruning. Induction is where we actually build the tree i.e set all of the hierarchical decision boundaries based on our data. Because of the nature of training decision trees they can be prone to major overfitting.	How are decision trees trained
7956	there are mainly five types of class interval such as exclusive class interval, inclusive class interval, less than class interval, more than class interval, mid value class interval , which has been discussed.	What are the types of class interval
5710	Back-propagation is the essence of neural net training.  It is the practice of fine-tuning the weights of a neural net based on the error rate (i.e. loss) obtained in the previous epoch (i.e. iteration).	What is back propagation and how is it used in a neural network
6381	Major advantages include its simplicity and lack of bias. Among the disadvantages are difficulty gaining access to a list of a larger population, time, costs, and that bias can still occur under certain circumstances.	What are the advantages and disadvantages of simple random sampling
5690	2. Stochastic Variational Inference. We derive stochastic variational inference, a stochastic optimization algorithm for mean-field vari- ational inference. Our algorithm approximates the posterior distribution of a probabilistic model with hidden variables, and can handle massive data sets of observations.	What is stochastic variational inference
1416	Homogeneity of variance is an assumption underlying both t tests and F tests (analyses of variance, ANOVAs) in which the population variances (i.e., the distribution, or “spread,” of scores around the mean) of two or more samples are considered equal.	What is homogeneity of variance in statistics
1010	Positive feedback occurs to increase the change or output: the result of a reaction is amplified to make it occur more quickly.  Some examples of positive feedback are contractions in child birth and the ripening of fruit; negative feedback examples include the regulation of blood glucose levels and osmoregulation.	What is an example of positive feedback
817	The generator is a convolutional neural network and the discriminator is a deconvolutional neural network. The goal of the generator is to artificially manufacture outputs that could easily be mistaken for real data. The goal of the discriminator is to identify which outputs it receives have been artificially created.	What is the goal of a generative adversarial network Gan
568	The probability within the region must not exceed 1. A large number---much larger than 1---multiplied by a small number (the size of the region) can be less than 1 if the latter number is small enough.	Can a probability distribution be less than 1
8518	The eigenvalues and eigenvectors of a matrix are often used in the analysis of financial data and are integral in extracting useful information from the raw data. They can be used for predicting stock prices and analyzing correlations between various stocks, corresponding to different companies.	Where do we use eigenvalues
8125	Most recent answer The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer.	How many hidden layers should I use
4454	The Poisson parameter Lambda (λ) is the total number of events (k) divided by the number of units (n) in the data (λ = k/n).  In between, or when events are infrequent, the Poisson distribution is used.	What is lambda in Poisson distribution
939	Linear mixed models (sometimes called “multilevel models” or “hierarchical models”, depending on the context) are a type of regression model that take into account both (1) variation that is explained by the independent variables of interest (like lm() ) – fixed effects, and (2) variation that is not explained by the	What is a linear mixed model analysis
7669	"Yes a perceptron (one fully connected unit) can be used for regression. It will just be a linear regressor. If you use no activation function you get a regressor and if you put a sigmoid activation you get a classifier.  That's why the loss function for classification is called ""logistic regression""."	Can Perceptron be used for regression
3502	Linear regression is used for predicting the continuous dependent variable using a given set of independent features whereas Logistic Regression is used to predict the categorical. Linear regression is used to solve regression problems whereas logistic regression is used to solve classification problems.	Why we use logistic regression instead of linear regression
30	Yes. This is the architechture of logistic regression, which is similar to a single layer feed forward neural network.	Is a single layer feed forward neural network equivalent to a logistic regression algorithm
5844	Binary, multi-class and multi-label classification Cross-entropy is a commonly used loss function for classification tasks.	Which is the loss function most suitable for multi class classification problems
1225	"Depending on the context, an independent variable is sometimes called a ""predictor variable"", regressor, covariate, ""manipulated variable"", ""explanatory variable"", exposure variable (see reliability theory), ""risk factor"" (see medical statistics), ""feature"" (in machine learning and pattern recognition) or ""input"	What's another name for explanatory variables
8668	For example, if you have daily sales data and you expect that it exhibits annual seasonality, you should have more than 365 data points to train a successful model. If you have hourly data and you expect your data exhibits weekly seasonality, you should have more than 7*24 = 168 observations to train a model.	How much data is needed to train a model
2437	The availability bias happens we people often judge the likelihood of an event, or frequency of its occurrence by the ease with which examples and instances come easily to mind. Most consumers are poor at risk assessments – for example they over-estimate the likelihood of attacks by sharks or list accidents.	What is an example of availability bias
1003	Typically, a one-way ANOVA is used when you have three or more categorical, independent groups, but it can be used for just two groups (but an independent-samples t-test is more commonly used for two groups).	Can you use one way Anova for two groups
1014	Linear transformation is a function between two linear spaces over the same field of scalars, which is additive and homogeneous. Linear operator is a linear transformation for which the domain and the codomain spaces are the same and, moreover, in both of them the same basis is considered.	What is the difference between a linear operator and a linear transformation
4081	Typically, a sample survey consists of the following steps:Define the target population.  Select the sampling scheme and sample size.  Develop the questionnaire.  Recruit and train the field investigators.  Obtain information as per the questionnaire.  Scrutinize the information gathered.  Analyze and interpret the information.	What are the steps in sample survey
610	Task parallelism is the simultaneous execution on multiple cores of many different functions across the same or different datasets. Data parallelism (aka SIMD) is the simultaneous execution on multiple cores of the same function across the elements of a dataset.	What is the difference between task function parallelism and data parallelism
6895	The ReLU function is another non-linear activation function that has gained popularity in the deep learning domain. ReLU stands for Rectified Linear Unit. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.	Why is ReLU the most common activation function used in neural networks
3305	The Purpose of Statistics: Statistics teaches people to use a limited sample to make intelligent and accurate conclusions about a greater population. The use of tables, graphs, and charts play a vital role in presenting the data being used to draw these conclusions.	What is statistics and its purpose
816	The standard score (more commonly referred to as a z-score) is a very useful statistic because it (a) allows us to calculate the probability of a score occurring within our normal distribution and (b) enables us to compare two scores that are from different normal distributions.	What is Z score used for
4708	A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem. — Practical recommendations for gradient-based training of deep architectures, 2012.	What should learning rate be
963	In logistic regression, as with any flavour of regression, it is fine, indeed usually better, to have continuous predictors. Given a choice between a continuous variable as a predictor and categorising a continuous variable for predictors, the first is usually to be preferred.	Can you use continuous variables in logistic regression
565	Use caution unless you have good reason and data to support using the substitute value. Regression Substitution: You can use multiple-regression analysis to estimate a missing value. We use this technique to deal with missing SUS scores. Regression substitution predicts the missing value from the other values.	How do you handle missing values in a data set
740	Usually in a conventional neural network, one tries to predict a target vector y from input vectors x. In an autoencoder network, one tries to predict x from x.  Sometimes it is and the neural network simply learns to duplicate the training data instead of learning general concepts from the training data.	What is the difference between a neural network and an autoencoder network
5030	Most machine learning algorithms operate based on the assumption that there are many more samples than predictors.  The number of samples (n) are the actual samples drawn from the domain that you must use to model your predictive modeling problem.	What is N in machine learning
2512	"Pseudorandomness measures the extent to which a sequence of numbers, ""though produced by a completely deterministic and repeatable process, appear to be patternless."" The pattern's seeming randomness is ""the crux of"" much online and other security."	What is pseudo randomisation
904	An S-curve is simply a curve of some object, line or path in the image that curves back and forth horizontally as you proceed vertically, much like the letter S–in fact, usually exactly like the letter S.	What is the S curve in photography
3885	Say we want to estimate the mean of a population. While the most used estimator is the average of the sample, another possible estimator is simply the first number drawn from the sample.  In theory, you could have an unbiased estimator whose variance is asymptotically nonzero, and that would be inconsistent.	Can an estimator be unbiased or inconsistent
7681	One more difference is that Pearson works with raw data values of the variables whereas Spearman works with rank-ordered variables. Now, if we feel that a scatterplot is visually indicating a “might be monotonic, might be linear” relationship, our best bet would be to apply Spearman and not Pearson.	Which correlation coefficient is better to use Spearman or Pearson
388	Active learning is generally defined as any instructional method that engages students in the learning process. In short, active learning requires students to do meaningful learning activities and think about what they are doing.  The students work individually on assignments, and cooperation is limited.	What is the meaning of active learning
7688	In mathematics and statistics, a stationary process (or a strict/strictly stationary process or strong/strongly stationary process) is a stochastic process whose unconditional joint probability distribution does not change when shifted in time.  For many applications strict-sense stationarity is too restrictive.	What is stationarity in a stochastic process
139	Deep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, machine vision, speech recognition, natural language processing, audio recognition, social network filtering, machine	What are deep neural networks used for
8447	The F Distribution The distribution of all possible values of the f statistic is called an F distribution, with v1 = n1 - 1 and v2 = n2 - 1 degrees of freedom.  The mean of the distribution is equal to v2 / ( v2 - 2 ) for v2 > 2.	What is the mean of an F distribution
996	The predictive power of a scientific theory refers to its ability to generate testable predictions.  Theories with strong predictive power are highly valued, because the predictions can often encourage the falsification of the theory.	What is predictive power of a model
514	The agglomerative clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. It's also known as AGNES (Agglomerative Nesting). The algorithm starts by treating each object as a singleton cluster.	What is an agglomerative hierarchical clustering algorithm
985	Methods of Data Labeling in Machine LearningReinforcement Learning. The method utilizes the trial-and-error approach to make predictions within a specific context using feedback from their own experience.  Supervised Learning. This method requires a huge amount of manually labeled data.  Unsupervised Learning. The method leverages raw or unstructured data.	What are the techniques for labeling data in machine learning
2165	Summary: “OLS” stands for “ordinary least squares” while “MLE” stands for “maximum likelihood estimation.”  Maximum likelihood estimation, or MLE, is a method used in estimating the parameters of a statistical model and for fitting a statistical model to data.	What is the difference between OLS and Maximum Likelihood
62	"Writing up resultsFirst, present descriptive statistics in a table.  Organize your results in a table (see Table 3) stating your dependent variable (dependent variable = YES) and state that these are ""logistic regression results.""  When describing the statistics in the tables, point out the highlights for the reader.More items"	How do you write logistic regression results
800	Examples of Deep Learning at Work Aerospace and Defense: Deep learning is used to identify objects from satellites that locate areas of interest, and identify safe or unsafe zones for troops. Medical Research: Cancer researchers are using deep learning to automatically detect cancer cells.	What is an example of deep learning
2272	Variational autoencoders (VAEs) are a deep learning technique for learning latent representations.  They have also been used to draw images, achieve state-of-the-art results in semi-supervised learning, as well as interpolate between sentences. There are many online tutorials on VAEs.	What is variational encoder
1478	A multiplicative error model is one in which the dependent variable is a product of the independent variable and an error term, instead of a sum.	What is multiplicative error
8662	"""The difference between discrete choice models and conjoint models is that discrete choice models present experimental replications of the market with the focus on making accurate predictions regarding the market, while conjoint models do not, using product profiles to estimate underlying utilities (or partworths)"	What is the difference between a discrete choice and a conjoint analysis
4103	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve.	What are some real world examples of normal distribution
8582	So, a covariate is in fact, a type of control variable. Examples of a covariate may be the temperature in a room on a given day of an experiment or the BMI of an individual at the beginning of a weight loss program. Covariates are continuous variables and measured at a ratio or interval level.	What is an example of a covariate
475	Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.	What is w2v
7930	A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.  Restricted Boltzmann machines can also be used in deep learning networks.	What is RBM in machine learning
154	Item response Theory(IRT) is a way to analyze responses to tests or questionnaires with the goal of improving measurement accuracy and reliability.	What is IRT in statistics
1391	Generative model is a class of models for Unsupervised learning where given training data our goal is to try and generate new samples from the same distribution. To train a Generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.)	What are generative models in deep learning
117	Forward chaining as the name suggests, start from the known facts and move forward by applying inference rules to extract more data, and it continues until it reaches to the goal, whereas backward chaining starts from the goal, move backward by using inference rules to determine the facts that satisfy the goal.	What is forward and backward chaining in artificial intelligence
801	Interpolation is also used to simplify complicated functions by sampling data points and interpolating them using a simpler function.  Polynomials are commonly used for interpolation because they are easier to evaluate, differentiate, and integrate - known as polynomial interpolation.	Why do we use interpolation
1416	Batch Normalization during inference During testing or inference phase we can't apply the same batch-normalization as we did during training because we might pass only sample at a time so it doesn't make sense to find mean and variance on a single sample.	Is batch normalization used in inference
2232	If the mean more accurately represents the center of the distribution of your data, and your sample size is large enough, use a parametric test. If the median more accurately represents the center of the distribution of your data, use a nonparametric test even if you have a large sample size.	How do you know whether to use parametric or nonparametric
5679	A* achieves better performance by using heuristics to guide its search. A* combines the advantages of Best-first Search and Uniform Cost Search: ensure to find the optimized path while increasing the algorithm efficiency using heuristics.  If h(n)=0, then A* turns to be Uniform-Cost Search.	Why is a * better than best first search
2983	Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset.  Large numbers of input features can cause poor performance for machine learning algorithms. Dimensionality reduction is a general field of study concerned with reducing the number of input features.	What is dimensionality reduction in machine learning
3073	Normal distributions come up time and time again in statistics. A normal distribution has some interesting properties: it has a bell shape, the mean and median are equal, and 68% of the data falls within 1 standard deviation.	What makes a distribution approximately normal
542	F-test is used either for testing the hypothesis about the equality of two population variances or the equality of two or more population means. The equality of two population means was dealt with t-test. Besides a t-test, we can also apply F-test for testing equality of two population means.	What are the applications of F test
2149	In linear regression the independent variables can be categorical and/or continuous. But, when you fit the model if you have more than two category in the categorical independent variable make sure you are creating dummy variables.	Can we use linear regression for categorical variables
292	Sample size measures the number of individual samples measured or observations used in a survey or experiment. For example, if you test 100 samples of soil for evidence of acid rain, your sample size is 100. If an online survey returned 30,500 completed questionnaires, your sample size is 30,500.	What is an example of sample size
4289	Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).	What are ensemble algorithms
7799	Hopfield in 1982. It consists of a single layer which contains one or more fully connected recurrent neurons. The Hopfield network is commonly used for auto-association and optimization tasks.	What are Hopfield networks used for
4395	Analysis of Variance (ANOVA) consists of calculations that provide information about levels of variability within a regression model and form a basis for tests of significance.	What does the Anova in a regression analysis test for
1436	In mathematics, a divergent series is an infinite series that is not convergent, meaning that the infinite sequence of the partial sums of the series does not have a finite limit. The divergence of the harmonic series was proven by the medieval mathematician Nicole Oresme.	What makes a series divergent
386	"It is technically defined as ""the nth root product of n numbers."" The geometric mean must be used when working with percentages, which are derived from values, while the standard arithmetic mean works with the values themselves. The harmonic mean is best used for fractions such as rates or multiples."	When is it most appropriate to take the arithmetic mean vs geometric mean vs harmonic mean
4459	The law of averages is a false belief, sometimes known as the 'gambler's fallacy,' that is derived from the law of large numbers.  The law of averages is a misconception that probability occurs with a small number of consecutive experiments so they will certainly have to 'average out' sooner rather than later.	Why is the law of averages false
841	Artificial intelligence (AI) makes it possible for machines to learn from experience, adjust to new inputs and perform human-like tasks. Most AI examples that you hear about today – from chess-playing computers to self-driving cars – rely heavily on deep learning and natural language processing.	What is AI How are they used
6332	Discriminant or discriminant function analysis is a. parametric technique to determine which weightings of. quantitative variables or predictors best discriminate. between 2 or more than 2 groups of cases and do so.	What is discriminant analysis PDF
880	TensorFlow Extended (TFX) is an end-to-end platform for deploying production ML pipelines. When you're ready to move your models from research to production, use TFX to create and manage a production pipeline.	What is TFX TensorFlow
7459	Precision is determined by a statistical method called a standard deviation. Standard deviation is how much, on average, measurements differ from each other. High standard deviations indicate low precision, low standard deviations indicate high precision.	What is the relationship between average deviation and precision
426	In statistics, Bessel's correction is the use of n − 1 instead of n in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance.	Is standard deviation divided by N or N 1
3384	Yes, in fact many processors provide two TLBs for this very reason. As an example, the code being accessed by a process may retain the same working set for a long period of time. However, the data the code accesses may change, thus reflecting a change in the working set for data accesses.	Is it possible for a process to have two working sets one representing data and another representing code explain
6467	Quartiles are the values that divide a list of numbers into quarters: Put the list of numbers in order. Then cut the list into four equal parts.In this case all the quartiles are between numbers:Quartile 1 (Q1) = (4+4)/2 = 4.Quartile 2 (Q2) = (10+11)/2 = 10.5.Quartile 3 (Q3) = (14+16)/2 = 15.	How are quartiles calculated
977	LMBP algorithm	Which learning algorithm is used by feed forward neural networks
2316	In reality, for deep learning and big data tasks standard gradient descent is not often used. Rather, a variant of gradient descent called stochastic gradient descent and in particular its cousin mini-batch gradient descent is used.	Is stochastic gradient descent similar to mini batch gradient descent
3490	Using Logarithmic Functions Much of the power of logarithms is their usefulness in solving exponential equations. Some examples of this include sound (decibel measures), earthquakes (Richter scale), the brightness of stars, and chemistry (pH balance, a measure of acidity and alkalinity).	What are logarithms used for in the real world
1275	The Markov condition, sometimes called the Markov assumption, is an assumption made in Bayesian probability theory, that every node in a Bayesian network is conditionally independent of its nondescendents, given its parents. Stated loosely, it is assumed that a node has no bearing on nodes which do not descend from it.	What is the Markov assumption aka Markov property in a Bayesian network
232	8:3111:15Suggested clip · 94 secondsIntroduction to Tensors - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you read a tensor
88	It is usually defined as the ratio of the variance to the mean. As a formula, that's: D = σ2 / μ.	How do you find the variance of a ratio
7943	Statistical Machine Translation. Machine translation (MT) is automated translation. It is the process by which computer software is used to translate a text from one natural language (such as English) to another (such as Spanish).	What is machine translation in AI
493	Reliability refers to how dependably or consistently a test measures a characteristic. If a person takes the test again, will he or she get a similar test score, or a much different score? A test that yields similar scores for a person who repeats the test is said to measure a characteristic reliably.	What is the reliability of a test
4442	": a proposition or theorem formed by contradicting both the subject and predicate or both the hypothesis and conclusion of a given proposition or theorem and interchanging them ""if not-B then not-A "" is the contrapositive of ""if A then B """	What is meant by Contrapositive
6587	A null hypothesis is a type of conjecture used in statistics that proposes that there is no difference between certain characteristics of a population or data-generating process. The alternative hypothesis proposes that there is a difference.	What does the alternative hypothesis predict in null hypothesis testing
673	Random errors are statistical fluctuations (in either direction) in the measured data due to the precision limitations of the measurement device. Random errors usually result from the experimenter's inability to take the same measurement in exactly the same way to get exact the same number.	What are random errors
1491	MSE is used to check how close estimates or forecasts are to actual values. Lower the MSE, the closer is forecast to actual. This is used as a model evaluation measure for regression models and the lower value indicates a better fit.	What are the applications of the mean squared error
3682	n = norm( v ) returns the Euclidean norm of vector v . This norm is also called the 2-norm, vector magnitude, or Euclidean length. n = norm( v , p ) returns the generalized vector p-norm. n = norm( X ) returns the 2-norm or maximum singular value of matrix X , which is approximately max(svd(X)) .	What is 2 norm of a matrix
7467	A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters.  (Dendrogram is often miswritten as dendogram.)	What is Dendrogram in machine learning
4982	Multi-view learning is an emerging direction in machine learning which considers learning with multiple views to improve the generalization performance. Multi-view learning is also known as data fusion or data integration from multiple feature sets.	What is Multiview learning
900	Use loss-based decoding to classify examples — instead of taking the sign of the output of each classifier, com- pute the actual loss, using the training loss function (hinge loss for SVM, square loss for RLSC).	What is the loss function that is used when solving multiclass classification problem
3645	Examples of Factor Analysis Studies Factor analysis provides simplicity after reducing variables. For long studies with large blocks of Matrix Likert scale questions, the number of variables can become unwieldy. Simplifying the data using factor analysis helps analysts focus and clarify the results.	Why do we use factor analysis
8559	The standard deviation of X is σ=√(b−a)212. The probability density function of X is f(x)=1b−a for a≤x≤b. The cumulative distribution function of X is P(X≤x)=x−ab−a.	What is the standard deviation of uniform distribution
8020	To find the relative frequency, divide the frequency by the total number of data values. To find the cumulative relative frequency, add all of the previous relative frequencies to the relative frequency for the current row.	What is the formula for cumulative relative frequency
1095	A normal distribution is determined by two parameters the mean and the variance.  Now the standard normal distribution is a specific distribution with mean 0 and variance 1. This is the distribution that is used to construct tables of the normal distribution.	What is the difference between a normal distribution and the standard normal distribution
1388	A negative binomial random variable is the number X of repeated trials to produce r successes in a negative binomial experiment. The probability distribution of a negative binomial random variable is called a negative binomial distribution.  Suppose we flip a coin repeatedly and count the number of heads (successes).	What does the negative in negative binomial distribution signify
3794	Correlation Coefficient = 0.8: A fairly strong positive relationship. Correlation Coefficient = 0.6: A moderate positive relationship.  Correlation Coefficient = -0.8: A fairly strong negative relationship. Correlation Coefficient = -0.6: A moderate negative relationship.	Is 0.6 A strong correlation
5727	The Machine Learning algorithms that require the feature scaling are mostly KNN (K-Nearest Neighbours), Neural Networks, Linear Regression, and Logistic Regression.	Which machine learning algorithms require feature scaling
6941	Spectral analysis is the process of breaking down a signal into its components at various frequencies, and in the context of acoustics there are two very different ways of doing this, depending on whether the result is desired on a linear frequency scale with constant resolution (in Hz) or on a logarithmic frequency	What is spectral analysis
1682	11 websites to find free, interesting datasetsFiveThirtyEight.  BuzzFeed News.  Kaggle.  Socrata.  Awesome-Public-Datasets on Github.  Google Public Datasets.  UCI Machine Learning Repository.  Data.gov.More items	Where can I find big datasets
6423	Conditional entropy. In information theory, the conditional entropy quantifies the amount of information needed to describe the outcome of a random variable given that the value of another random variable is known.	What does conditional entropy mean
171	This is why it is important to distinguish between the statistical significance of a result and the practical significance of that result.  Null hypothesis testing is a formal approach to deciding whether a statistical relationship in a sample reflects a real relationship in the population or is just due to chance.	What is the difference between statistical significance testing and null hypothesis testing
4184	Other ways of avoiding experimenter's bias include standardizing methods and procedures to minimize differences in experimenter-subject interactions; using blinded observers or confederates as assistants, further distancing the experimenter from the subjects; and separating the roles of investigator and experimenter.	How do you get rid of experimenter bias
2377	Attention models, or attention mechanisms, are input processing techniques for neural networks that allows the network to focus on specific aspects of a complex input, one at a time until the entire dataset is categorized.  Attention models require continuous reinforcement or backpopagation training to be effective.	What is an attention model
2218	Artificial Intelligence is the broader concept of machines being able to carry out tasks in a way that we would consider “smart”.  Machine Learning is a current application of AI based around the idea that we should really just be able to give machines access to data and let them learn for themselves.	What is the difference between artificial intelligence AI and machine learning
5243	"The outcome variable is also called the response or dependent variable, and the risk factors and confounders are called the predictors, or explanatory or independent variables. In regression analysis, the dependent variable is denoted ""Y"" and the independent variables are denoted by ""X""."	Is the explanatory variable The dependent variable
8452	A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous. If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable. Decision boundaries are not always clear cut.	What is decision boundary in SVM
1910	The main difference between CNN and RNN is the ability to process temporal information or data that comes in sequences, such as a sentence for example.  Whereas, RNNs reuse activation functions from other data points in the sequence to generate the next output in a series.	How is RNN different from CNN
927	A condition variable indicates an event and has no value. More precisely, one cannot store a value into nor retrieve a value from a condition variable. If a thread must wait for an event to occur, that thread waits on the corresponding condition variable.	What does it mean to condition on a variable
3151	The filter is a device that allows passing the dc component of the load and blocks the ac component of the rectifier output. Thus the output of the filter circuit will be a steady dc voltage.  Capacitor is used so as to block the dc and allows ac to pass.	Why are filters connected at the output of rectifiers
7577	A random forest is simply a collection of decision trees whose results are aggregated into one final result. Their ability to limit overfitting without substantially increasing error due to bias is why they are such powerful models. One way Random Forests reduce variance is by training on different samples of the data.	Does Random Forest reduce bias
5482	If you don't have enough time to read through the entire post, the following hits on the key components: Bag-of-words: How to break up long text into individual words. Filtering: Different approaches to remove uninformative words. Bag of n-grams: Retain some context by breaking long text into sequences of words.	What is the difference between bag of words and bag of n grams
876	The time complexity of minimax is O(b^m) and the space complexity is O(bm), where b is the number of legal moves at each point and m is the maximum depth of the tree. N-move look ahead is a variation of minimax that is applied when there is no time to search all the way to the leaves of the tree.	What is the complexity of Minimax algorithm
7516	Regression analysis is primarily used for two conceptually distinct purposes. First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning.	Who uses regression analysis
6927	At the foundation of quantum mechanics is the Heisenberg uncertainty principle. Simply put, the principle states that there is a fundamental limit to what one can know about a quantum system.  Heisenberg sometimes explained the uncertainty principle as a problem of making measurements.	Is uncertainty principle a measurement problem
8337	Pixel binning is a clocking scheme used to combine the charge collected by several adjacent CCD pixels, and is designed to reduce noise and improve the signal-to-noise ratio and frame rate of digital cameras.	What is pixel binning
3267	Binary Search: Search a sorted array by repeatedly dividing the search interval in half. Begin with an interval covering the whole array. If the value of the search key is less than the item in the middle of the interval, narrow the interval to the lower half.	How do you perform a binary search
1695	"In information theory, the entropy of a random variable is the average level of ""information"", ""surprise"", or ""uncertainty"" inherent in the variable's possible outcomes. The concept of information entropy was introduced by Claude Shannon in his 1948 paper ""A Mathematical Theory of Communication""."	What is the theory of entropy
2348	TipsUnderstand the concepts. Make sure you understand the concepts first before you memorize them.Start with the hard stuff. Use the stoplight approach if you are having problems applying or understanding key concepts.Create colour-coded flashcards.	How do you understand concepts when studying
2304	In a CNN, each layer has two kinds of parameters : weights and biases. The total number of parameters is just the sum of all weights and biases. Let's define, = Number of weights of the Conv Layer. = Number of biases of the Conv Layer.	What are CNN parameters
8082	Dynamic Partition takes more time in loading data compared to static partition. When you have large data stored in a table then the Dynamic partition is suitable. If you want to partition a number of columns but you don't know how many columns then also dynamic partition is suitable.	What is dynamic partitioning and when is it used
640	Convolution is a mathematical way of combining two signals to form a third signal. It is the single most important technique in Digital Signal Processing. Using the strategy of impulse decomposition, systems are described by a signal called the impulse response.	What is the purpose of convolution
2757	To recap: The test statistic in a paired Wilcoxon signed-rank test (the V value) is the sum of the ranks of the pairwise differences x - y > 0 . Let's create some sample data to understand how V can be zero. We draw samples from two normal distributions with different means.	What is V in Wilcoxon signed rank test
7893	To use the normal approximation method a minimum of 10 successes and 10 failures in each group are necessary (i.e., np≥10 n p ≥ 10 and n(1−p)≥10 n ( 1 − p ) ≥ 10 ).  The null hypothesis is that there is not a difference between the two proportions (i.e., p1=p2 p 1 = p 2 ).	What is the normal approximation method
5129	Even if a model-fitting procedure has been used, R2 may still be negative, for example when linear regression is conducted without including an intercept, or when a non-linear function is used to fit the data.	Can R 2 the coefficient of determination be negative
3023	Skewness is a measure of symmetry, or more precisely, the lack of symmetry.  Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers.	What is skewness and kurtosis in statistics
6839	Thresholding is a technique in OpenCV, which is the assignment of pixel values in relation to the threshold value provided. In thresholding, each pixel value is compared with the threshold value. If the pixel value is smaller than the threshold, it is set to 0, otherwise, it is set to a maximum value (generally 255).	What is image thresholding in OpenCV
972	This is when your model fits the training data well, but it isn't able to generalize and make accurate predictions for data it hasn't seen before.  The training set is used to train the model, while the validation set is only used to evaluate the model's performance.	What is training and validation accuracy
1242	The learning rate hyperparameter controls the rate or speed at which the model learns.  A learning rate that is too small may never converge or may get stuck on a suboptimal solution. When the learning rate is too large, gradient descent can inadvertently increase rather than decrease the training error.	What does lowering learning rate in gradient descent lead to
1304	This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting.	Why is pooling used in convolutional neural network
5638	Examples of Artificial Intelligence: Work & School1 – Google's AI-Powered Predictions.  2 – Ridesharing Apps Like Uber and Lyft.  3 — Commercial Flights Use an AI Autopilot.1 – Spam Filters.2 – Smart Email Categorization.1 –Plagiarism Checkers.  2 –Robo-readers.  1 – Mobile Check Deposits.More items•	What are examples of AI
7946	A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix “bi” means two, or twice).	What is a binomial probability distribution
1441	A moving average is a technique to get an overall idea of the trends in a data set; it is an average of any subset of numbers. The moving average is extremely useful for forecasting long-term trends. You can calculate it for any period of time.	How can you describe the moving average method
7741	Conditional probability is the probability of one event occurring with some relationship to one or more other events. For example: Event A is that it is raining outside, and it has a 0.3 (30%) chance of raining today. Event B is that you will need to go outside, and that has a probability of 0.5 (50%).	How do you know when to use conditional probability
6223	Univariate and multivariate represent two approaches to statistical analysis. Univariate involves the analysis of a single variable while multivariate analysis examines two or more variables. Most multivariate analysis involves a dependent variable and multiple independent variables.	What is the difference between univariate and multivariate regression
3559	0:008:06Suggested clip · 106 secondsSPSS - Correspondence Analysis - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do correspondence analysis in SPSS
1487	0:0010:07Suggested clip · 109 secondsProbability Exponential Distribution Problems - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you solve exponential distribution problems
1357	Control Strategy in Artificial Intelligence scenario is a technique or strategy, tells us about which rule has to be applied next while searching for the solution of a problem within problem space. It helps us to decide which rule has to apply next without getting stuck at any point.	What is control strategy in artificial intelligence
5954	Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.	What is gradient boosting used for
6063	Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.  The concept of physical dimension was introduced by Joseph Fourier in 1822.	What is dimensional analysis and its other name
1701	Each party in a dispute recognises that its own use of the concept is contested by those of other parties. To use an essentially contested concept means to use it against other users. To use such a concept means to use it aggresssively and defensively.	What is contested concept
1011	Logistic regression models are a great tool for analysing binary and categorical data, allowing you to perform a contextual analysis to understand the relationships between the variables, test for differences, estimate effects, make predictions, and plan for future scenarios.	Can logistic regression be used to predict categorical outcome
556	Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for—tasks that involve creativity and empathy among others.	How has AI improved society
2951	What is the F-distribution. A probability distribution, like the normal distribution, is means of determining the probability of a set of events occurring. This is true for the F-distribution as well. The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution.	Is F distribution a normal distribution
823	"The values that divide each part are called the first, second, and third quartiles; and they are denoted by Q1, Q2, and Q3, respectively. Q1 is the ""middle"" value in the first half of the rank-ordered data set. Q2 is the median value in the set. Q3 is the ""middle"" value in the second half of the rank-ordered data set."	What does q1 q2 q3 mean in statistics
2821	"The ""Fast Fourier Transform"" (FFT) is an important measurement method in the science of audio and acoustics measurement. It converts a signal into individual spectral components and thereby provides frequency information about the signal."	What is the purpose of FFT
3072	Note that it is possible to get a negative R-square for equations that do not contain a constant term. Because R-square is defined as the proportion of variance explained by the fit, if the fit is actually worse than just fitting a horizontal line then R-square is negative.	Can you have a negative R squared value
1078	In this case, convergence in distribution implies convergence in probability. We can state the following theorem: Theorem If Xn d→ c, where c is a constant, then Xn p→ c. Since Xn d→ c, we conclude that for any ϵ>0, we have limn→∞FXn(c−ϵ)=0,limn→∞FXn(c+ϵ2)=1.	How do you prove probability convergence
2298	Predictive analytics is the process of using data analytics to make predictions based on data. This process uses data along with analysis, statistics, and machine learning techniques to create a predictive model for forecasting future events.	How do you predict based on data
6616	"The Kruskal-Wallis H test (sometimes also called the ""one-way ANOVA on ranks"") is a rank-based nonparametric test that can be used to determine if there are statistically significant differences between two or more groups of an independent variable on a continuous or ordinal dependent variable."	When Kruskal Wallis test is used
1024	One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture.	How do you address a vanishing gradient problem
370	The chain rule, or general product rule, calculates any component of the joint distribution of a set of random variables using only conditional probabilities. This probability theory is used as a foundation for backpropagation and in creating Bayesian networks.	What is chain rule in artificial intelligence
1649	The n-1 equation is used in the common situation where you are analyzing a sample of data and wish to make more general conclusions. The SD computed this way (with n-1 in the denominator) is your best guess for the value of the SD in the overall population.	Why does the standard deviation formula use n 1
401	Oversampling and undersampling in data analysis are techniques used to adjust the class distribution of a data set (i.e. the ratio between the different classes/categories represented). These terms are used both in statistical sampling, survey design methodology and in machine learning.	What is oversampling in machine learning
1534	An OUTCOME (or SAMPLE POINT) is the result of a the experiment. The set of all possible outcomes or sample points of an experiment is called the SAMPLE SPACE. An EVENT is a subset of the sample space.	What is the difference between sample space and outcome
389	The false discovery rate (FDR) is a statistical approach used in multiple hypothesis testing to correct for multiple comparisons. It is typically used in high-throughput experiments in order to correct for random events that falsely appear significant.	What is the false discovery rate FDR
844	"For an approximate answer, please estimate your coefficient of variation (CV=standard deviation / mean). As a rule of thumb, a CV >= 1 indicates a relatively high variation, while a CV < 1 can be considered low.  Remember, standard deviations aren't ""good"" or ""bad"". They are indicators of how spread out your data is."	What is a good average deviation
1198	1:314:30Suggested clip · 120 secondsCumulative Frequency Distribution (Less than and More than YouTubeStart of suggested clipEnd of suggested clip	How do you construct a less than cumulative frequency distribution
4363	There is various ways to handle missing values of categorical ways.The same steps apply for a categorical variable as well.Ignore observation.Replace by most frequent value.Replace using an algorithm like KNN using the neighbours.Predict the observation using a multiclass predictor.	How do you handle missing values in categorical variables
6271	Two examples of common independent variables are age and time.  They're independent of everything else. The dependent variable (sometimes known as the responding variable) is what is being studied and measured in the experiment. It's what changes as a result of the changes to the independent variable.	What is an independent variable example
7620	Conclusion. Linear regression is more suitable for predicting output which are continuous like house prices, amount of rainfall etc.  The regression line is a straight line. Whereas logistic regression is for classification problems, which predicts a probability range between 0 to 1.	Can linear regression be used for classification
2690	A high-pass filter can be used to make an image appear sharper. These filters emphasize fine details in the image – exactly the opposite of the low-pass filter. High-pass filtering works in exactly the same way as low-pass filtering; it just uses a different convolution kernel.	What is high pass filtering in image processing
5749	A local minimum of a function (typically a cost function in machine learning, which is something we want to minimize based on empirical data) is a point in the domain of a function that has the following property: the function evaluates to a greater value at every other point in a neighborhood around the local minimum	What is local minima in machine learning
2547	The effect of the logit transformation is primarily to pull out the ends of the distribution. Over a broad range of intermediate values of the proportion (p), the relationship of logit(p) and p is nearly linear.	Why do we use logit transformation
3158	It is a mathematical function having a characteristic that can take any real value and map it to between 0 to 1 shaped like the letter “S”. The sigmoid function also called a logistic function.	Is sigmoid logistic function
7016	The most effective tool found for the task for image recognition is a deep neural network, specifically a Convolutional Neural Network (CNN).	Which algorithm is best for image processing
8511	Tensorflow is the most used library used in development of Deep Learning models.  Keras, on the other end, is a high-level API that is built on top of TensorFlow. It is extremely user-friendly and comparatively easier than TensorFlow.	What is the difference between keras and tensorflow
5543	Bimodal Distribution: Two Peaks. Data distributions in statistics can have one peak, or they can have several peaks.  The two peaks in a bimodal distribution also represent two local maximums; these are points where the data points stop increasing and start decreasing.	What does a bimodal distribution mean
1405	Image processing algorithms generally constitute contrast enhancement, noise reduction, edge sharpening, edge detection, segmentation etc. These techniques make the manual diagnosis process of disease detection automatic or semiautomatic.	What is image processing algorithms
7867	Every deception, according to Whaley, is comprised of two parts: dissimulation (covert, hiding what is real) and simulation (overt, showing the false).	What are the elements of deception
1057	Basic principle A neuron (also known as nerve cell) is an electrically excitable cell that takes up, processes and transmits information through electrical and chemical signals. It is one of the basic elements of the nervous system. In order that a human being can react to his environment, neurons transport stimuli.	How do neurons work
40	A test of a statistical hypothesis , where the region of rejection is on both sides of the sampling distribution , is called a two-tailed test. For example, suppose the null hypothesis states that the mean is equal to 10. The alternative hypothesis would be that the mean is less than 10 or greater than 10.	What is an example of a two tailed test
5167	The equation of a hyperplane is w · x + b = 0, where w is a vector normal to the hyperplane and b is an offset.	How do you find the equation of a hyperplane
2436	1:0612:26Suggested clip · 84 secondsEstimating the posterior predictive distribution by sampling - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the sample of a posterior predictive distribution
1249	Training: Training refers to the process of creating an machine learning algorithm.  Inference: Inference refers to the process of using a trained machine learning algorithm to make a prediction.	What is difference between training and inference
527	Usually, people use the cosine similarity as a similarity metric between vectors. Now, the distance can be defined as 1-cos_similarity. The intuition behind this is that if 2 vectors are perfectly the same then similarity is 1 (angle=0) and thus, distance is 0 (1-1=0).	What is cosine distance vs cosine similarity
161	Qualitative Data are not numbers. They may include favorite foods; religions; ethnicities; etc.. Discrete Data are numbers that may take on specific, separated values.  Continuous Data are numbers that may take on all sorts of decimal or fractional values.	What is the difference between qualitative discrete and continuous data
423	Mean: the average score, calculated by dividing the sum of scores by the number of examinees.  Median: the middle raw score of the distribution; 50 percent of the obtained raw scores are higher and 50 percent are lower than the median.	What does distribution of scores mean
3303	Predictive modeling, a tool used in predictive analytics, refers to the process of using mathematical and computational methods to develop predictive models that examine current and historical datasets for underlying patterns and calculate the probability of an outcome.	What are predictive models used for
993	(a) The most significant property of moment generating function is that ``the moment generating function uniquely determines the distribution. '' (b) Let and be constants, and let be the mgf of a random variable . Then the mgf of the random variable.	What is moment generating function and its properties
932	Logistic Regression is a special case of a Neural Network with no hidden layers, that uses the sigmoid activation function and uses the softmax with cross entropy loss.  neural network and logistic regressions are different techniques or algorithms to do the same thing, classification of data.	How is the neural network algorithm similar to logistic regression How are they different
8212	Center: The center is not affected by sample size. The mean of the sample means is always approximately the same as the population mean µ = 3,500. Spread: The spread is smaller for larger samples, so the standard deviation of the sample means decreases as sample size increases.	How does sample size affect sample mean
4021	As with the point-biserial, computing the Pearson correlation for two dichotomous variables is the same as the phi.  If two variables are related, they are correlated. So, when we conduct a chi-square test, and we want to have a rough estimate of how strongly related the two variables are, we can examine phi.	Can you correlate two dichotomous variables
381	R^2 of 0.2 is actually quite high for real-world data. It means that a full 20% of the variation of one variable is completely explained by the other. It's a big deal to be able to account for a fifth of what you're examining. GeneralMayhem on  [–]	What does an r2 value of 0.2 mean
6914	"""Locutus"" came from Latin and means ""the one who speaks"" like in the word locutor."	What does Locutus mean
3168	To deal with categorical variables that have more than two levels, the solution is one-hot encoding. This takes every level of the category (e.g., Dutch, German, Belgian, and other), and turns it into a variable with two levels (yes/no).	How do you handle a categorical variable with many levels
694	A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific.	Is sampling frame the same as population
4518	By keeping both the experimenters and the participants blind, bias is less likely to influence the results of the experiment. A double-blind experiment can be set up when the lead experimenter sets up the study but then has a colleague (such as a graduate student) collect the data from participants.	How does a double blind experiment decrease the amount of bias introduced into experimental results
361	Sampling Frame Error: A type of nonsampling error in a survey caused by a sampling frame (i.e., a list) that is not a perfect representation of the population or universe. That is, the sample list might contain respondents who do not meet the definition of the population or universe.	What is sample frame error
1222	Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and	Where does the hidden Markov model is used
3981	The periodic table	Which model is widely used for classification
5758	Nonparametric tests are also called distribution-free tests because they don't assume that your data follow a specific distribution. You may have heard that you should use nonparametric tests when your data don't meet the assumptions of the parametric test, especially the assumption about normally distributed data.	When should nonparametric statistics be used
85	Most recent answer The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer.	How many hidden layers should I use in neural network
928	MongoDB may be a great non-relational document store, but it just isn't that great for time-series data. So for time-series data with TimescaleDB, you get all the benefits of a reliable relational database (i.e., PostgreSQL) with better performance than a popular NoSQL solution like MongoDB.	Is MongoDB good for time series data
7644	MDS arranges the points on the plot so that the distances among each pair of points correlates as best as possible to the dissimilarity between those two samples. The values on the two axes tell you nothing about the variables for a given sample – the plot is just a two dimensional space to arrange the points.	How do you read an MDS plot
1332	List of Common Machine Learning AlgorithmsLinear Regression.Logistic Regression.Decision Tree.SVM.Naive Bayes.kNN.K-Means.Random Forest.More items•	What are different types of machine learning algorithms
850	Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language. NLG processes turn structured data into text.	How exactly is natural language generation different from natural language understanding
6776	How to Formulate an Effective HypothesisState the problem that you are trying to solve. Make sure that the hypothesis clearly defines the topic and the focus of the experiment.Try to write the hypothesis as an if-then statement.  Define the variables.	How do you write a research hypothesis in statistics
4536	Because our eyes are less sensitive to color detail than to brightness detail, chroma subsampling is used to reduce the amount of data in a video signal while having little or no visible impact on image quality.  The number of pixels that share the same color information is determined by the type of chroma subsampling.	What is Chroma subsampling and why is it important
5805	Dummy variables are useful because they enable us to use a single regression equation to represent multiple groups. This means that we don't need to write out separate equation models for each subgroup. The dummy variables act like 'switches' that turn various parameters on and off in an equation.	What are the advantages of dummy variables in a regression model
459	We present a freely available open-source toolkit for training recurrent neural network based language models. It can be easily used to improve existing speech recognition and machine translation systems.	Is there a recurrent neural networks toolkit
6494	A population is the entire group that you want to draw conclusions about. A sample is the specific group that you will collect data from. The size of the sample is always less than the total size of the population.	What is the difference between a sample and a population
4365	But severe multicollinearity is a major problem, because it increases the variance of the regression coefficients, making them unstable. The more variance they have, the more difficult it is to interpret the coefficients.  You see a positive regression coefficient when the response should decrease as X increases.	How does Multicollinearity affect variance
7356	The binomial distribution is a probability distribution that summarizes the likelihood that a value will take one of two independent values under a given set of parameters or assumptions.	What is definition binomial distribution in statistics
2734	An internal covariate shift occurs when there is a change in the input distribution to our network. When the input distribution changes, hidden layers try to learn to adapt to the new distribution. This slows down the training process.	What is internal covariate shift
7997	A statistic is a number that describes a sample. In inference, we use a statistic to draw a conclusion about a parameter. These conclusions include a probability statement that describes the strength of the evidence or our certainty. For a categorical variable, the parameter and statistics are proportions.	How do probability and statistical inference work together
4967	If x(n), y(n) and z(n) are the samples of the signals, the correlation coefficient between x and y is given by Sigma x(n) * y(n) divided by the root of [Sigma x(n)^2 * y(n)^2], where ' * ' denotes simple multiplication and ^2 denotes squaring.	How do you calculate the correlation between two signals
7696	People also want to know what professions will be most in demand.  This is known as a reward function that will allow AI platforms to come to conclusions instead of arriving at a prediction. Reward Functions are used for reinforcement learning models. Reward Function Engineering determines the rewards for actions.	What is reward in reinforcement learning
518	Recall is the number of relevant documents retrieved by a search divided by the total number of existing relevant documents, while precision is the number of relevant documents retrieved by a search divided by the total number of documents retrieved by that search.	How do you read precision and recall
4246	A convolutional layer within a neural network should have the following attributes:Convolutional kernels defined by a width and height (hyper-parameters).The number of input channels and output channels (hyper-parameter).More items	Which are the attributes convolutional layer within a neural network should have
698	The most significant difference between regression vs classification is that while regression helps predict a continuous quantity, classification predicts discrete class labels. There are also some overlaps between the two types of machine learning algorithms.	When should you use classification vs regression
991	An r of zero indicates that there is no linear relationship between the two variables. There may, however, be a strong nonlinear relationship between the two variables.	What does it mean if r is 0
3399	The eigenvalues and eigenvectors of a matrix are often used in the analysis of financial data and are integral in extracting useful information from the raw data. They can be used for predicting stock prices and analyzing correlations between various stocks, corresponding to different companies.	What are the uses of eigenvalues
6240	Explain the difference between descriptive and inferential statistics. Descriptive statistics describes sets of data. Inferential statistics draws conclusions about the sets of data based on sampling.  A population is a set of units of interest to a study.	What is the difference between descriptive and inferential statistics quizlet
2314	Random forest employs randomization in two places: Each tree is trained using a random sample with replacement from training set.  This can reduce the correlations among trees in the random forests, which improves the predictive performance.	How does randomization in a random forest work
806	The short answer is: Logistic regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.) of its parameters!	Why is logistic regression considered a linear model
3517	To find the expected value, E(X), or mean μ of a discrete random variable X, simply multiply each value of the random variable by its probability and add the products. The formula is given as E(X)=μ=∑xP(x).	How do you find the mean and expected value of a discrete probability distribution
2066	The curse of dimensionality in the k-NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all	Why does K Nearest Neighbor algorithm suffer from curse of dimensionality
6468	"In statistical hypothesis testing, a type I error is the rejection of a true null hypothesis (also known as a ""false positive"" finding or conclusion; example: ""an innocent person is convicted""), while a type II error is the non-rejection of a false null hypothesis (also known as a ""false negative"" finding or conclusion"	How do I understand type I error and type II error when talking about hypothesis
2159	K-Means clustering algorithm fails to give good results when the data contains outliers, the density spread of data points across the data space is different and the data points follow non-convex shapes.	When K means will fail to give good clusters
1363	"As your question is on clustering: In cluster analysis, there usually is no training or test data split. Because you do cluster analysis when you do not have labels, so you cannot ""train"". Training is a concept from machine learning, and train-test splitting is used to avoid overfitting."	Does clustering need training data
5237	Predictive analytics uses predictors or known features to create predictive models that will be used in obtaining an output. A predictive model is able to learn how different points of data connect with each other. Two of the most widely used predictive modeling techniques are regression and neural networks.	How do predictive models work
1376	The Softmax regression is a form of logistic regression that normalizes an input value into a vector of values that follows a probability distribution whose total sums up to 1.	How does Softmax regression work
927	Fashion Apparel Recognition using Convolutional Neural NetworkStep 1: Collect Data.Step 2: Prepare the data.Step 3: Choose the model.Step 4 Train your machine model.Step 5: Evaluation.Step 6: Parameter Tuning.Step 7: Prediction or Inference.	Which are the steps to build a machine learning model
6906	Time Series Model. The time series model comprises a sequence of data points captured, using time as the input parameter.  Random Forest. Random Forest is perhaps the most popular classification algorithm, capable of both classification and regression.  Gradient Boosted Model (GBM)  K-Means.  Prophet.	Which algorithm is best for prediction
6817	If you have n observations and order them to and define and then a future observation is equally likely to be between and for all from to . That is independent of the distribution, and also of the ordering of your sample. That is the sense in which order statistics are independent.	Are order statistics independent
3477	Placement of the IDS device is an important consideration. Most often it is deployed behind the firewall on the edge of your network. This gives the highest visibility but it also excludes traffic that occurs between hosts.	Where is the intrusion detection system located
4712	Yes, absolutely. From my own experience, it's very useful to Adam with learning rate decay. Without decay, you have to set a very small learning rate so the loss won't begin to diverge after decrease to a point.	Does Adam need learning rate decay
4539	Stochastic vs. For example, a stochastic variable is a random variable. A stochastic process is a random process. Typically, random is used to refer to a lack of dependence between observations in a sequence. For example, the rolls of a fair die are random, so are the flips of a fair coin.	What is the difference between random and stochastic
5900	Preference learning is a subfield in machine learning, which is a classification method based on observed preference information. In the view of supervised learning, preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.	What is preference Learning And how is it different from machine learning
3106	It depicts a dystopian future in which humanity is unknowingly trapped inside a simulated reality, the Matrix, created by intelligent machines to distract humans while using their bodies as an energy source.	What is the concept of the Matrix
5402	"Let's see a simple c example to swap two numbers without using third variable.#include<stdio.h>int main(){int a=10, b=20;printf(""Before swap a=%d b=%d"",a,b);a=a+b;//a=30 (10+20)b=a-b;//b=10 (30-20)a=a-b;//a=20 (30-10)More items"	How do you swap values in two variables
7450	The four popular approaches to Artificial Intelligence are self-awareness, the theory of mind, limited memory, and reactive machines.	What are the popular approaches of artificial intelligence
1030	The short answer is: Logistic regression is considered a generalized linear model because the outcome always depends on the sum of the inputs and parameters. Or in other words, the output cannot depend on the product (or quotient, etc.) of its parameters!	Why logistic regression is considered a linear model
903	For large samples, the sample proportion is approximately normally distributed, with mean μˆP=p. and standard deviation σˆP=√pqn. A sample is large if the interval [p−3σˆp,p+3σˆp] lies wholly within the interval [0,1].	How do you find a sample proportion
63	The geometric distribution is a special case of the negative binomial distribution. It deals with the number of trials required for a single success. Thus, the geometric distribution is a negative binomial distribution where the number of successes (r) is equal to 1.	What is a geometric distribution in statistics
2369	Examples in natural systems of swarm intelligence include bird flocking, ant foraging, and fish schooling. Inspired by swarm's such behavior, a class of algorithms is proposed for tackling optimization problems, usually under the title of swarm intelligence algorithms (SIAs) [203].	What are the common aspects of swarm intelligence observed in nature
2209	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	Which is better supervised or unsupervised learning
6117	Note that the CDF gives us P(X≤x). To find P(X<x), for a discrete random variable, we can simply write P(X<x)=P(X≤x)−P(X=x)=FX(x)−PX(x). Let X be a discrete random variable with range RX={1,2,3,}. Suppose the PMF of X is given by PX(k)=12k for k=1,2,3,	How do you calculate CDF from probability
2082	Kernel vs Filter The dimensions of the kernel matrix is how the convolution gets it's name. For example, in 2D convolutions, the kernel matrix is a 2D matrix. A filter however is a concatenation of multiple kernels, each kernel assigned to a particular channel of the input.	What is filter and kernel in CNN
4834	Performance Metrics for Regression Mean Absolute Error (MAE) Mean Squared Error (MSE) Root Mean Squared Error (RMSE) R-Squared.	What are the performance evaluation metrics in regression
490	Grid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.	What is a grid search and why do we use it in machine learning
947	"You can ""use"" deep learning for regression.  You can use a fully connected neural network for regression, just don't use any activation unit in the end (i.e. take out the RELU, sigmoid) and just let the input parameter flow-out (y=x)."	Can deep learning be used for regression
773	Stepwise regression is the step-by-step iterative construction of a regression model that involves the selection of independent variables to be used in a final model. It involves adding or removing potential explanatory variables in succession and testing for statistical significance after each iteration.	What does stepwise regression do
3414	Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.	What is Adam Optimiser
2454	Here are some examples of discrete variables: Number of children per family. Number of students in a class. Number of citizens of a country.	What is an example of a discrete variable
1192	A house price index (HPI) measures the price changes of residential housing as a percentage change from some specific start date (which has HPI of 100). Methodologies commonly used to calculate a HPI are the hedonic regression (HR), simple moving average (SMA) and repeat-sales regression (RSR).	How is HPI calculated
2450	Subject 2. Time-series data is a set of observations collected at usually discrete and equally spaced time intervals.  Cross-sectional data are observations that come from different individuals or groups at a single point in time.	What is the difference between trend time series and cross section analysis
7953	In the literal meaning of the terms, a parametric statistical test is one that makes assumptions about the parameters (defining properties) of the population distribution(s) from which one's data are drawn, while a non-parametric test is one that makes no such assumptions.	What is the meaning of parametric test
4871	The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed.  In general, a lower RMSD is better than a higher one.	What is root mean square error used for
26	Try to avoid implementing cheap tricks to make your code run faster.Optimize your Code using Appropriate Algorithm.  Optimize Your Code for Memory.  printf and scanf Vs cout and cin.  Using Operators.  if Condition Optimization.  Problems with Functions.  Optimizing Loops.  Data Structure Optimization.More items•	How do you optimize code
1337	Aspin-Welch t-test	What test should you use to determine the equality of the two sample means when the population standard deviation is unknown
950	So while L2 regularization does not perform feature selection the same way as L1 does, it is more useful for feature *interpretation*: a predictive feature will get a non-zero coefficient, which is often not the case with L1.	Can we use l2 regularization for feature selection
159	The standard error (SE) of a statistic (usually an estimate of a parameter) is the standard deviation of its sampling distribution or an estimate of that standard deviation.  Mathematically, the variance of the sampling distribution obtained is equal to the variance of the population divided by the sample size.	Is an estimated standard deviation of a sampling distribution
1279	Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.	What is NLP problem
6945	Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used.	What do we use regression for
23	Coef. A regression coefficient describes the size and direction of the relationship between a predictor and the response variable. Coefficients are the numbers by which the values of the term are multiplied in a regression equation.	What are coefficients in logistic regression
5999	A term-document matrix represents the processed text from a text analysis as a table or matrix where the rows represent the text responses, or documents, and the columns represent the words or phrases (the terms).  matrix).	What is text Matrix
685	We can use the regression line to predict values of Y given values of X. For any given value of X, we go straight up to the line, and then move horizontally to the left to find the value of Y. The predicted value of Y is called the predicted value of Y, and is denoted Y'.	How do you predict the value of a linear regression
7246	This occurs when the line-of-best-fit for describing the relationship between x and y is a straight line. The linear relationship between two variables is positive when both increase together; in other words, as values of x get larger values of y get larger. This is also known as a direct relationship.	How do you tell if there is a linear relationship between two variables
3173	One way to show the performance of a reinforcement learning algorithm is to plot the cumulative reward (the sum of all rewards received so far) as a function of the number of steps. One algorithm dominates another if its plot is consistently above the other.	How do you measure the performance of a reinforcement learning agent
8172	Under the efficient-frontier framework, the assumption that investors are risk-averse, i.e., they prefer returns and distaste risks. In other words, investors prefer higher returns and lower risks. The dominance principle is usually used to illustrate the risk-return trade-off.	What is dominance and how does it apply to the efficient frontier
2597	It is acknowledged that current tests do not measure IQ to a level of accuracy of one point: there is a margin of error, usually considered to be about five points either side of the obtained IQ, which should be taken into account when making a diagnosis of ID (The American Association on Mental Retardation 2002).	What is the margin of error of IQ tests
650	How to approach analysing a datasetstep 1: divide data into response and explanatory variables. The first step is to categorise the data you are working with into “response” and “explanatory” variables.  step 2: define your explanatory variables.  step 3: distinguish whether response variables are continuous.  step 4: express your hypotheses.	How do you analyze a data set
1762	import tensorflow as tf. import datetime.  model = create_model() model.  %tensorboard --logdir logs/fit. A brief overview of the dashboards shown (tabs in top navigation bar):  train_dataset = tf. data.  loss_object = tf. keras.  # Define our metrics.  current_time = datetime.  %tensorboard --logdir logs/gradient_tape.More items	How do you use a Tensorflow TensorBoard
1007	Cluster cohesion: Measures the closeness of the objects within the same cluster. A “lower within-cluster” variation indicates good compactness or good clustering. The separation method is implied to measure how well a cluster is separated from other clusters.	Which measures the goodness of a cluster
3037	Moments are a set of statistical parameters to measure a distribution. Four moments are commonly used: 1st, Mean: the average. 2d, Variance: Standard deviation is the square root of the variance: an indication of how closely the values are spread about the mean.	What is meant by moments in statistics
7779	t-test is used to test if two sample have the same mean. The assumptions are that they are samples from normal distribution. f-test is used to test if two sample have the same variance. Same assumptions hold.	What is the difference between t distribution and F distribution
8500	A partition of a set X is a set of non-empty subsets of X such that every element x in X is in exactly one of these subsets (i.e., X is a disjoint union of the subsets).	What is a partition in statistics
1472	A vector space is a space of vectors, ie. each element is a vector. A vector field is, at its core, a function between some space and some vector space, so every point in our base space has a vector assigned to it. A good example would be wind direction maps you see on weather reports.	What is the difference between a vector field and a vector space
363	Qualitative Differences The population standard deviation is a parameter, which is a fixed value calculated from every individual in the population. A sample standard deviation is a statistic. This means that it is calculated from only some of the individuals in a population.	What is the difference between standard deviation and sample standard deviation
856	If there are more variables than equations, you cannot find a unique solution, because there isnt one.	What happens when you have more unknowns than equations
441	Partitioning methods: Given a set of n objects, a partitioning method constructs k partitions of the data, where each partition represents a cluster and k ≤ n. That is, it divides the data into k groups such that each group must contain at least one object.	What are partitioning methods
7789	Collaborative filtering (CF) is a technique used by recommender systems.  In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating).	What holds true for collaborative filtering
8157	Logits are the raw scores output by the last layer of a neural network. Before activation takes place.	What are Logits neural network
5683	A weak correlation means that as one variable increases or decreases, there is a lower likelihood of there being a relationship with the second variable.  Earthquake magnitude and the depth at which it was measured is therefore weakly correlated, as you can see the scatter plot is nearly flat.	What is a weak correlation in a scatter plot
7859	Usually, statistical significance is determined by calculating the probability of error (p value) by the t ratio. The difference between two groups (such as an experiment vs. control group) is judged to be statistically significant when p = 0.05 or less.	How do you know if two results are statistically different
437	Univariate is a term commonly used in statistics to describe a type of data which consists of observations on only a single characteristic or attribute. A simple example of univariate data would be the salaries of workers in industry.	What is an example of univariate data
2257	Stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video. For example, if a neural network's stride is set to 1, the filter will move one pixel, or unit, at a time.	What is stride in deep learning
1051	Bias can damage research, if the researcher chooses to allow his bias to distort the measurements and observations or their interpretation. When faculty are biased about individual students in their courses, they may grade some students more or less favorably than others, which is not fair to any of the students.	What is the problem with bias
1238	With the LassoCV, RidgeCV, and Linear Regression machine learning algorithms.Define the problem.Gather the data.Clean & Explore the data.Model the data.Evaluate the model.Answer the problem.	How do you predict using machine learning
6457	— On the difficulty of training Recurrent Neural Networks, 2013. Gradient clipping involves forcing the gradient values (element-wise) to a specific minimum or maximum value if the gradient exceeded an expected range. Together, these methods are often simply referred to as “gradient clipping.”	How does gradient clipping work
6226	There are several mathematical tools for measuring the statistical randomness of a series. The true randomness of a single event cannot be measured, but it can be investigated by looking at its history. A single event is random, if it's unintentional, no-one has decided it.	Can randomness be measured
5953	The most used algorithm to train neural networks is gradient descent. We'll define it later, but for now hold on to the following idea: the gradient is a numeric calculation allowing us to know how to adjust the parameters of a network in such a way that its output deviation is minimized.	What is gradient neural network
1408	We see that machine learning can do what signal processing can, but has inherently higher complexity, with the benefit of being generalizable to different problems. The signal processing algorithms are optimal for the job in terms of complexity, but are specific to the particular problems they solve.	Is signal processing related to machine learning
2140	The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data; in contrast, data mining uses machine learning and statistical models to uncover clandestine or	What is the difference between data mining and data analysis
180	In multivariate regression there are more than one dependent variable with different variances (or distributions).  But when we say multiple regression, we mean only one dependent variable with a single distribution or variance. The predictor variables are more than one.	What is the difference between multiple regression and multivariate analysis
989	A type 1 error is also known as a false positive and occurs when a researcher incorrectly rejects a true null hypothesis.  The probability of making a type I error is represented by your alpha level (α), which is the p-value below which you reject the null hypothesis.	What is Type I error in statistics
1430	Despite having similar aims and processes, there are two main differences between them: Machine learning works out predictions and recalibrates models in real-time automatically after design. Meanwhile, predictive analytics works strictly on “cause” data and must be refreshed with “change” data.	Is Predictive Analytics same as machine learning
310	In the context of machine learning, regularization is the process which regularizes or shrinks the coefficients towards zero. In simple words, regularization discourages learning a more complex or flexible model, to prevent overfitting. Moving on with this article on Regularization in Machine Learning.	What is regularization in machine learning
8271	You can tell if two random variables are independent by looking at their individual probabilities. If those probabilities don't change when the events meet, then those variables are independent. Another way of saying this is that if the two variables are correlated, then they are not independent.	How do you prove a random variable is independent
2995	The Bayesian Optimization algorithm can be summarized as follows:Select a Sample by Optimizing the Acquisition Function.Evaluate the Sample With the Objective Function.Update the Data and, in turn, the Surrogate Function.Go To 1.	How do I implement a Bayesian optimization
3612	From Wikipedia, the free encyclopedia. In mathematics and statistics, a random number is either Pseudo-random or a number generated for, or part of, a set exhibiting statistical randomness. In common understanding, it's that all have an equal chance; conversely, none have an advantage.	What is meant by random number
1104	"Convenience sampling is a type of nonprobability sampling in which people are sampled simply because they are ""convenient"" sources of data for researchers. In probability sampling, each element in the population has a known nonzero chance of being selected through the use of a random selection procedure."	What is meant by convenience sampling
2847	The joint behavior of two random variables X and Y is determined by the. joint cumulative distribution function (cdf):(1.1) FXY (x, y) = P(X ≤ x, Y ≤ y),where X and Y are continuous or discrete. For example, the probability.  P(x1 ≤ X ≤ x2,y1 ≤ Y ≤ y2) = F(x2,y2) − F(x2,y1) − F(x1,y2) + F(x1,y1).	How do you find the joint distribution of two random variables
1161	Recognizing patterns allows us to predict and expect what is coming. The process of pattern recognition involves matching the information received with the information already stored in the brain. Making the connection between memories and information perceived is a step of pattern recognition called identification.	How do we recognize patterns
8130	Nonprobability sampling is a common technique in qualitative research where researchers use their judgment to select a sample.  In convenience sampling, participants are selected because they are accessible and therefore relatively easy for the researcher to recruit.	Why is non probability sampling used in qualitative research
2612	Here's a step-by-step guide to help you get started.Create a text classifier.  Select 'Topic Classification'  Upload your training data.  Create your tags.  Train your classifier.  Change to Naive Bayes.  Test your Naive Bayes classifier.  Start working with your model.	How do I use naive Bayes classifier
1589	The variance (symbolized by S2) and standard deviation (the square root of the variance, symbolized by S) are the most commonly used measures of spread. We know that variance is a measure of how spread out a data set is. It is calculated as the average squared deviation of each number from the mean of a data set.	What is variance and deviation
716	For digital signature applications, the security strength of a hash function is normally its collision resistance strength. When appropriate processing is applied to the data before it is hashed, the security strength may be more than the collision resistance strength (see Section 5.2. 3).	What is the strength of using a hashing function
1545	A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term. Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.	What is lasso and ridge regularization in machine learning
1461	7:5214:07Suggested clip · 100 secondsHow to Select the Correct Predictive Modeling Technique | Machine YouTubeStart of suggested clipEnd of suggested clip	How do you choose a predictive model
3561	Different Types of Clustering AlgorithmDistribution based methods.Centroid based methods.Connectivity based methods.Density Models.Subspace clustering.Improverd By: Pragya vidyarthi.	What are the different clustering algorithms
886	If the signal is present the person can decide that it is present or absent. These outcomes are called hits and misses. If the signal is absent the person can still decide that the signal is either present or absent. These are called false alarms or correct rejections (CR) respectively.	What is a false alarm in signal detection theory
6660	Advantages and Disadvantages of various CPU scheduling algorithmsThe process with less execution time suffer i.e. waiting time is often quite long.Favors CPU Bound process then I/O bound process.Here, first process will get the CPU first, other processes can get CPU only after the current process has finished it's execution.  This effect results in lower CPU and device utilization.More items•	What are advantages of scheduling algorithms
6215	A representative sample is a subset of a population that seeks to accurately reflect the characteristics of the larger group. For example, a classroom of 30 students with 15 males and 15 females could generate a representative sample that might include six students: three males and three females.	How do you determine if a sample is representative of the population
5288	Dual Booting Can Impact Disk Swap Space. In most cases there shouldn't be too much impact on your hardware from dual booting.  Both Linux and Windows use chunks of the hard disk drive to improve performance while the computer is running.	Does dual booting affect performance
6182	August 2017) (Learn how and when to remove this template message) In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.	What is Latent Dirichlet Allocation in machine learning
8499	A classification is a useful tool for anyone developing statistical surveys. It is a framework which both simplifies the topic being studied and makes it easy to categorise all data or responses received.	What is the necessity of classification in statistical study
1764	The joint behavior of two random variables X and Y is determined by the. joint cumulative distribution function (cdf):(1.1) FXY (x, y) = P(X ≤ x, Y ≤ y),where X and Y are continuous or discrete. For example, the probability.  P(x1 ≤ X ≤ x2,y1 ≤ Y ≤ y2) = F(x2,y2) − F(x2,y1) − F(x1,y2) + F(x1,y1).	How do you find the joint pdf of two random variables
674	The value of the z-score tells you how many standard deviations you are away from the mean.  A positive z-score indicates the raw score is higher than the mean average. For example, if a z-score is equal to +1, it is 1 standard deviation above the mean. A negative z-score reveals the raw score is below the mean average.	What does the Z score represent
580	Fully Convolutional Networks (FCNs) owe their name to their architecture, which is built only from locally connected layers, such as convolution, pooling and upsampling. Note that no dense layer is used in this kind of architecture. This reduces the number of parameters and computation time.	What is fully convolutional neural network
7064	The interquartile range is the difference between the third quartile and the first quartile in a data set, giving the middle 50%. The interquartile range is a measure of spread; it's used to build box plots, determine normal distributions and as a way to determine outliers.	What is the interquartile range of the data set
4853	"Logic, as per the definition of the Oxford dictionary, is ""the reasoning conducted or assessed according to strict principles and validity"". In Artificial Intelligence also, it carries somewhat the same meaning. Logic can be defined as the proof or validation behind any reason provided."	What is logic in artificial intelligence
1355	"if p is a statement variable, the negation of p is ""not p"", denoted by ~p. If p is true, then ~p is false. Conjunction: if p and q are statement variables, the conjunction of p and q is ""p and q"", denoted p q.(p q) ~(p q) p xor qExclusive Orp ~(~p)Double Negation"	What is logically equivalent to P or Q
8623	"In machine learning, a deep belief network (DBN) is a generative graphical model, or alternatively a class of deep neural network, composed of multiple layers of latent variables (""hidden units""), with connections between the layers but not between units within each layer."	What is deep belief neural network
2428	The main goal of randomized trials is therefore to assure that each individual has an equal probability to be assigned to one or the other treatment. Randomization also allows to balance known and unknown confounders in order to make control and treatment groups as balanced as possible.	What are the benefits of randomized controlled trials
3566	A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study. The difference between a population and a sampling frame is that the population is general and the frame is specific.	What is the difference between population and sampling frame in statistics
8121	In multivariate statistics, exploratory factor analysis (EFA) is a statistical method used to uncover the underlying structure of a relatively large set of variables. EFA is a technique within factor analysis whose overarching goal is to identify the underlying relationships between measured variables.	What does exploratory factor analysis do
5181	There are several approaches to avoiding overfitting in building decision trees.Pre-pruning that stop growing the tree earlier, before it perfectly classifies the training set.Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree.	How do you solve overfitting in decision tree
3764	'Learning to learn' is the ability to pursue and persist in learning, to organise one's own learning, including through effective management of time and information, both individually and in groups.	What is the meaning of learning to learn
5521	Mini-Max algorithm uses recursion to search through the game-tree. Min-Max algorithm is mostly used for game playing in AI.	Which searching technique is used in Minimax algorithm
1240	One major disadvantage of non-probability sampling is that it's impossible to know how well you are representing the population. Plus, you can't calculate confidence intervals and margins of error.	What are the disadvantages of non probability sampling
8522	The most used algorithm to train neural networks is gradient descent.  We'll define it later, but for now hold on to the following idea: the gradient is a numeric calculation allowing us to know how to adjust the parameters of a network in such a way that its output deviation is minimized.	What is gradient in neural network training
2919	Interpolation refers to using the data in order to predict data within the dataset. Extrapolation is the use of the data set to predict beyond the data set.	What is the difference between extrapolation and interpolation
5378	There is a broad range of opportunities to study optimization problems that cannot be solved with an exact algorithm.  This work proposes the use of neural networks such as heuristics to resolve optimization problems in those cases where the use of linear programming or Lagrange multipliers is not feasible.	Can neural networks be used for optimization
1658	“Covariance” indicates the direction of the linear relationship between variables. “Correlation” on the other hand measures both the strength and direction of the linear relationship between two variables. Correlation is a function of the covariance.	What does a covariance matrix tell you
1129	In the variational autoencoder model, there are only local latent variables (no datapoint shares its latent z with the latent variable of another datapoint). So we can decompose the ELBO into a sum where each term depends on a single datapoint.	What is the latent variable in variational autoencoders
3753	Cross-sectional data are the result of a data collection, carried out at a single point in time on a statistical unit. With cross-sectional data, we are not interested in the change of data over time, but in the current, valid opinion of the respondents about a question in a survey.	What is meant by cross sectional data
7775	Statistical Methods for Finding the Best Regression ModelAdjusted R-squared and Predicted R-squared: Generally, you choose the models that have higher adjusted and predicted R-squared values.  P-values for the predictors: In regression, low p-values indicate terms that are statistically significant.More items•	How do you know which regression model is better
7354	How to Choose an Optimal Learning Rate for Gradient DescentChoose a Fixed Learning Rate. The standard gradient descent procedure uses a fixed learning rate (e.g. 0.01) that is determined by trial and error.  Use Learning Rate Annealing.  Use Cyclical Learning Rates.  Use an Adaptive Learning Rate.  References.	How do you set the learning rate in gradient descent
8501	Count data models have a dependent variable that is counts (0, 1, 2, 3, and so on). Most of the data are concentrated on a few small discrete values. Examples include: the number of children a couple has, the number of doctors visits per year a person makes, and the number of trips per month that a person takes.	What type of variable is count data
6421	The main difference is obviously that, in a first order reaction, the order of reaction is one by nature. A pseudo first-order reaction is second order reaction by nature but has been altered to make it a first order reaction.	What is the difference between first order and pseudo first order reactions
2334	It is a criterion under which a hypothesis tester decides whether a given hypothesis must be accepted or rejected. The general rule of thumb is that if the value of test statics is greater than the critical value then the null hypothesis is rejected in the favor of the alternate hypothesis.	What is the rejection rule
3397	Jeff Heaton (see page 158 of the linked text), who states that one hidden layer allows a neural network to approximate any function involving “a continuous mapping from one finite space to another.” With two hidden layers, the network is able to “represent an arbitrary decision boundary to arbitrary accuracy.”	How many hidden layers in a multi layer Perceptron are needed to represent any function
2231	The factorial ANOVA should be used when the research question asks for the influence of two or more independent variables on one dependent variable.	When should you use a factorial Anova instead of a simple Anova
1504	In physics, a partition function describes the statistical properties of a system in thermodynamic equilibrium. Partition functions are functions of the thermodynamic state variables, such as the temperature and volume.	What does the partition function represent
4269	A Hash Collision Attack is an attempt to find two input strings of a hash function that produce the same hash result. If two separate inputs produce the same hash output, it is called a collision.	What is a collision is when using a hash function
832	Once you have generated a prediction model (also called training a model), you can put it to use making predictions.  The scoring process examines a dataset and predicts results for each record based on similarities to records analyzed during model training.	What is the predictive score model
5991	Philosophers today usually divide ethical theories into three general subject areas: metaethics, normative ethics, and applied ethics. Metaethics investigates where our ethical principles come from, and what they mean.	What are the two types of ethics
3075	The coefficient for a term represents the change in the mean response associated with a change in that term, while the other terms in the model are held constant. The sign of the coefficient indicates the direction of the relationship between the term and the response.	What are model coefficients
6766	To analyze data and reporting speed AI can be very helpful in improving the data analyzing speed and also to increase the reporting time. The data are analyzed more accurately and the reporting time is also increased. AI can be used to analyze large amounts of data to draw conclusive reports.	How can AI be improved
6150	The first variable in the binomial formula, n, stands for the number of times the experiment runs. The second variable, p, represents the probability of one specific outcome.	What is N in binomial distribution
3317	Answer. The low value of loss function determines whether a model is a good fit for the datasets.	Which of the following values of loss function determines whether a model is a good fit for the dataset
8348	The set of all the possible outcomes is called the sample space of the experiment and is usually denoted by S. Any subset E of the sample space S is called an event.	What is the set of all possible outcomes in a probability experiment
874	Brief Description. The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent.	What is Fourier transform of an image
346	The only difference between a frequency histogram and a relative frequency histogram is that the vertical axis uses relative or proportional frequency instead of simple frequency (see Figure 1).	What is the difference between a relative frequency histogram and a regular histogram
463	Three reasons that you should NOT use deep learning(1) It doesn't work so well with small data. To achieve high performance, deep networks require extremely large datasets.  (2) Deep Learning in practice is hard and expensive. Deep learning is still a very cutting edge technique.  (3) Deep networks are not easily interpreted.	When should you not use deep learning
901	X and Y are independent iff fX,Y (x,y) = g(x)h(y) for all x,y for some functions g and h. Proof. If X and Y are independent then you need only take g(x) = fX(x) and h(y) = fY (y).	How do you know if X and Y is independent
4072	One such step is eliminating duplicate data as discussed above. Another step is resolving any conflicting data. Sometimes, datasets will have information that conflicts with each other, so data normalization is meant to address this conflicting issue and solve it before continuing. A third step is formatting the data.	When and why do we need data normalization
415	Prospective studies usually have fewer potential sources of bias and confounding than retrospective studies. A retrospective study looks backwards and examines exposures to suspected risk or protection factors in relation to an outcome that is established at the start of the study.	Why are prospective studies better than retrospective
840	A critical value of z (Z-score) is used when the sampling distribution is normal, or close to normal. Z-scores are used when the population standard deviation is known or when you have larger sample sizes.  See also: T Critical Value.	Is Z score the same as critical value
5737	K-nearest neighbor is also used in retail to detect patterns in credit card usage. Many new transaction-scrutinizing software applications use kNN algorithms to analyze register data and spot unusual patterns that indicate suspicious activity.	What are industry applications of the K nearest neighbor algorithm
1291	FP. N. FN. TN. where: P = Positive; N = Negative; TP = True Positive; FP = False Positive; TN = True Negative; FN = False Negative.	What is TP TN FP FN
1074	Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information.	What is normalization in machine learning
994	In neural networks, each neuron receives input from some number of locations in the previous layer. In a fully connected layer, each neuron receives input from every element of the previous layer. In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer.	What is the difference between a neural network and a convolutional network
3285	An investor can calculate the coefficient of variation to help determine whether an investment's expected return is worth the volatility it is likely to experience over time. A lower ratio suggests a more favorable tradeoff between risk and return.	Does a higher coefficient of variation mean more risk
7430	4:026:15Suggested clip · 93 secondsFinding the Test Statistic for a Wilcoxon Rank Sum Test in  - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you solve the Wilcoxon rank sum test
2464	We can compute the p-value corresponding to the absolute value of the t-test statistics (|t|) for the degrees of freedom (df): df=n−1. If the p-value is inferior or equal to 0.05, we can conclude that the difference between the two paired samples are significantly different.	How do you find the degrees of freedom for a t test
5279	Modified National Institute of Standards and Technology database	What does Mnist stand for
3281	If your learning rate is set too low, training will progress very slowly as you are making very tiny updates to the weights in your network. However, if your learning rate is set too high, it can cause undesirable divergent behavior in your loss function.	What happens when the learning rate is too large during back propagation
118	"A Poisson distribution is a measure of how many times an event is likely to occur within ""X"" period of time. Example: A video store averages 400 customers every Friday night. What is the probability that 600 customers will come in on any given Friday night? It was named after mathematician Siméon Denis Poisson."	What is Poisson distribution with example
3712	Optuna is an automated hyperparameter optimization software framework that is knowingly invented for the machine learning-based tasks. It emphasizes an authoritative, define-by-run approach user API.	What is Optuna
788	An N-point DFT is expressed as the multiplication , where is the original input signal, is the N-by-N square DFT matrix, and. is the DFT of the signal.	What is N in DFT
8029	Steps for Making decision treeGet list of rows (dataset) which are taken into consideration for making decision tree (recursively at each nodes).Calculate uncertanity of our dataset or Gini impurity or how much our data is mixed up etc.Generate list of all question which needs to be asked at that node.More items•	How do you use the decision tree in machine learning
182	In vector calculus and physics, a vector field is an assignment of a vector to each point in a subset of space. For instance, a vector field in the plane can be visualised as a collection of arrows with a given magnitude and direction, each attached to a point in the plane.	How do you describe a vector field
4799	Subtract the sample mean derived in the previous step from each of the data values, to get the deviation of each value from the sample mean. Multiply each deviation by itself to get the squared deviations of the values. Add up the squared deviations.	How do you find the mean square deviation
8147	Batch normalization works best after the activation function, and here or here is why: it was developed to prevent internal covariate shift. Internal covariate shift occurs when the distribution of the activations of a layer shifts significantly throughout training.	Where do you put batch normalization
8679	3.4. 1 The Logit Link Function. The logit link function is used to model the probability of 'success' as a function of covariates (e.g., logistic regression).	What is logit function used for
912	Image processing is an important component of applications used in the publishing, satellite imagery analysis, medical, and seismic imaging fields.	Where is image processing used
1366	Neural networks work better at predictive analytics because of the hidden layers. Linear regression models use only input and output nodes to make predictions. Neural network also use the hidden layer to make predictions more accurate. That's because it 'learns' the way a human does.	Can neural networks be used for prediction
6333	Elastic net is a popular type of regularized linear regression that combines two popular penalties, specifically the L1 and L2 penalty functions.  Elastic Net is an extension of linear regression that adds regularization penalties to the loss function during training.	What is elastic net in machine learning
3949	How to Deal with MulticollinearityRemove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression.	How can Multicollinearity be reduced
2692	"A validation dataset is a dataset of examples used to tune the hyperparameters (i.e. the architecture) of a classifier. It is sometimes also called the development set or the ""dev set"". An example of a hyperparameter for artificial neural networks includes the number of hidden units in each layer."	What is validation set in machine learning
1468	Without further ado and in no particular order, here are the top 5 machine learning algorithms for those just getting started:Linear regression.  Logical regression.  Classification and regression trees.  K-nearest neighbor (KNN)  Naïve Bayes.	What are the five popular algorithms of machine learning
1285	Heteroscedasticity means unequal scatter. In regression analysis, we talk about heteroscedasticity in the context of the residuals or error term. Specifically, heteroscedasticity is a systematic change in the spread of the residuals over the range of measured values.	What is Homoscedasticity in regression analysis
4382	1 : the quality or state of being reliable. 2 : the extent to which an experiment, test, or measuring procedure yields the same results on repeated trials.	What reliability means
1484	Deep learning techniques do not perform well when dealing with data with complex hierarchical structures. Deep learning identifies correlations between sets of features that are themselves “flat” or non-hierarchical, as in a simple, unstructured list, but much human and linguistic knowledge is more structured.	What deep learning Cannot do
8342	A GLM is absolutely a statistical model, but statistical models and machine learning techniques are not mutually exclusive. In general, statistics is more concerned with inferring parameters, whereas in machine learning, prediction is the ultimate goal.	Are generalized linear models statistical methods or machine learning methods
6036	When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model that fires at the moment of threshold crossing is also called a spiking neuron model.	How do spiking neural networks work
3719	Perceptron networks have several limitations. First, the output values of a perceptron can take on only one of two values (0 or 1) due to the hard-limit transfer function. Second, perceptrons can only classify linearly separable sets of vectors.	What are the limitations of Perceptron
2604	Neural networks are widely used in unsupervised learning in order to learn better representations of the input data.  This process doesn't give you clusters, but it creates meaningful representations that can be used for clustering. You could, for instance, run a clustering algorithm on the hidden layer's activations.	Can neural networks be used for clustering
482	There is currently no theoretical reason to use neural networks with any more than two hidden layers. In fact, for many practical problems, there is no reason to use any more than one hidden layer. Table 5.1 summarizes the capabilities of neural network architectures with various hidden layers.	Is more hidden layers better
322	Answer. Answer: Explanation: Linear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications.	Which technique is well known to utilize class labels in feature selection for classification
3714	Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.	Is gradient descent an optimizer
6482	Divide the number of events by the number of possible outcomes. This will give us the probability of a single event occurring. In the case of rolling a 3 on a die, the number of events is 1 (there's only a single 3 on each die), and the number of outcomes is 6.	How do you find random probability
1429	Hybrid Bayesian networks contain both discrete and continuous conditional probability distributions as numerical inputs. A commonly used type of hybrid Bayesian network is the conditional linear Gaussian (CLG) model [Lauritzen 1992, Cowell et al.	Which type of variables are used to represent the the hybrid Bayesian network
4582	These are generally used when direct sampling from the probability distribution would be difficult. Some of the use cases of MCMC methods are to approximate a target probability distribution or to compute an integral.	When should I use MCMC rather than Stochastic Gradient Descent
4436	Confirmation bias can make people less likely to engage with information which challenges their views.  Even when people do get exposed to challenging information, confirmation bias can cause them to reject it and, perversely, become even more certain that their own beliefs are correct.	Why is confirmation bias a problem
1212	If your p-value is less than or equal to the set significance level, the data is considered statistically significant. As a general rule, the significance level (or alpha) is commonly set to 0.05, meaning that the probability of observing the differences seen in your data by chance is just 5%.	How do you test if a difference is statistically significant
1679	How to Find a Sample Size Given a Confidence Interval and Width (unknown population standard deviation)za/2: Divide the confidence interval by two, and look that area up in the z-table: .95 / 2 = 0.475.  E (margin of error): Divide the given width by 2. 6% / 2.  : use the given percentage. 41% = 0.41.  : subtract. from 1.	How do you determine sample size
4811	CHARACTERISTICS OF A LINEAR MODELIt is a model, in which something progresses or develops directly from one stage to another.A linear model is known as a very direct model, with starting point and ending point.Linear model progresses to a sort of pattern with stages completed one after another without going back to prior phases.More items•	What are the characteristics of linear model
354	"Information theory studies the quantification, storage, and communication of information. It was originally proposed by Claude Shannon in 1948 to find fundamental limits on signal processing and communication operations such as data compression, in a landmark paper titled ""A Mathematical Theory of Communication""."	What do you mean by information theory
1542	A false positive means that the results say you have the condition you were tested for, but you really don't. With a false negative, the results say you don't have a condition, but you really do.	What is the difference between a false positive and a false negative
8098	The four requirements are: each observation falls into one of two categories called a success or failure. there is a fixed number of observations. the observations are all independent. the probability of success (p) for each observation is the same - equally likely.	What are the 4 requirements needed to be a binomial distribution
549	DCGAN is one of the popular and successful network design for GAN. It mainly composes of convolution layers without max pooling or fully connected layers. It uses convolutional stride and transposed convolution for the downsampling and the upsampling. The figure below is the network design for the generator. Source.	What is DC Gan
8507	A term document matrix is a way of representing the words in the text as a table (or matrix) of numbers. The rows of the matrix represent the text responses to be analysed, and the columns of the matrix represent the words from the text that are to be used in the analysis. The most basic version is binary.	What is term document matrix in R
1541	The limitation of Kaplan Meier estimate is that it cannot be used for multivariate analysis as it only studies the effect of one factor at the time. Log-rank test is used to compare two or more groups by testing the null hypothesis.	What are some limitations of Kaplan Meier estimator
5464	Simple random samples involve the random selection of data from the entire population so each possible sample is equally likely to occur. In contrast, stratified random sampling divides the population into smaller groups, or strata, based on shared characteristics.	What is the main difference between systematic random sampling and stratified random sampling
8076	Temporal Difference is an approach to learning how to predict a quantity that depends on future values of a given signal. It can be used to learn both the V-function and the Q-function, whereas Q-learning is a specific TD algorithm used to learn the Q-function.	Is Q learning temporal difference
181	Kalman filters are used to optimally estimate the variables of interests when they can't be measured directly, but an indirect measurement is available. They are also used to find the best estimate of states by combining measurements from various sensors in the presence of noise.	What is Kalman filtering used for
654	1. Interactions in Multiple Linear Regression. Basic Ideas. Interaction: An interaction occurs when an independent variable has a different effect on the outcome depending on the values of another independent variable. Let's look at some examples.	What do interaction terms mean in regression
4251	The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions.	What is the difference between general and generalized linear models
6073	Platt Scaling is most effective when the distortion in the predicted probabilities is sigmoid shaped. Isotonic Regression is a more powerful calibration method that can correct any monotonic distortion. But, Isotonic Regression is prone to over-fitting.	Is there more than Platt scaling and isotonic regression to solve calibration problems
273	The function fX(x) gives us the probability density at point x. It is the limit of the probability of the interval (x,x+Δ] divided by the length of the interval as the length of the interval goes to 0.	What is the probability density function of x
2868	Generally, we use linear regression for time series analysis, it is used for predicting the result for time series as its trends. For example, If we have a dataset of time series with the help of linear regression we can predict the sales with the time.	Can we use linear regression for time series analysis
8671	The technological singularity—also, simply, the singularity—is a hypothetical point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization.	What is it called when AI becomes self aware
1326	Precision and recall both have true positives in the numerator, and different denominators. To average them it really only makes sense to average their reciprocals, thus the harmonic mean. Because it punishes extreme values more.  With the harmonic mean, the F1-measure is 0.	Why F score is harmonic mean
6118	In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions.	What is the multivariate Gaussian distribution
288	Predictive modeling is the subpart of data analytics that uses data mining and probability to predict results. Each model is built up by the number of predictors that are highly favorable to determine future decisions. Once the data is received for a specific predictor, an analytical model is formulated.	What is prediction model in machine learning
7765	Big data is a term that describes the large volume of data – both structured and unstructured – that inundates a business on a day-to-day basis. But it's not the amount of data that's important.  Big data can be analyzed for insights that lead to better decisions and strategic business moves.	What is big data in simple terms
4426	Probability theory is the mathematical framework that allows us to analyze chance events in a logically sound manner. The probability of an event is a number indicating how likely that event will occur. This number is always between 0 and 1, where 0 indicates impossibility and 1 indicates certainty.	What is basic probability theory
1205	The Beta distribution is a continuous probability distribution having two parameters. One of its most common uses is to model one's uncertainty about the probability of success of an experiment.	What is the use of beta distribution
93	"Multinomial logistic regression deals with situations where the outcome can have three or more possible types (e.g., ""disease A"" vs. ""disease B"" vs. ""disease C"") that are not ordered.  Binary logistic regression is used to predict the odds of being a case based on the values of the independent variables (predictors)."	What is the difference between binary logistic regression and multinomial logistic regression
2718	In mathematics, a Fourier transform (FT) is a mathematical transform that decomposes a function (often a function of time, or a signal) into its constituent frequencies, such as the expression of a musical chord in terms of the volumes and frequencies of its constituent notes.	What is Fourier transform in signals and systems
3370	The minimum description length (MDL) principle is a powerful method of inductive inference, the basis of statistical modeling, pattern recognition, and machine learning. It holds that the best explanation, given a limited set of observed data, is the one that permits the greatest compression of the data.	In coding and modeling what is the minimum description length principle about
2598	0:0411:02Suggested clip · 75 secondsControl variables in regression - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you choose control variables in regression
3353	A data point is a discrete unit of information. In a general sense, any single fact is a data point. In a statistical or analytical context, a data point is usually derived from a measurement or research and can be represented numerically and/or graphically.	What are data points in machine learning
750	A regression equation is used in stats to find out what relationship, if any, exists between sets of data. For example, if you measure a child's height every year you might find that they grow about 3 inches a year. That trend (growing three inches a year) can be modeled with a regression equation.	What does a regression equation tell you
5926	Univariate statistics summarize only one variable at a time. Bivariate statistics compare two variables. Multivariate statistics compare more than two variables.	What are the major difference between univariate bivariate and multivariate analysis
538	"The decision rule is: Reject H0 if Z > 1.645. The decision rule is: Reject H0 if Z < 1.645. The decision rule is: Reject H0 if Z < -1.960 or if Z > 1.960. The complete table of critical values of Z for upper, lower and two-tailed tests can be found in the table of Z values to the right in ""Other Resources."""	What are the rules of testing hypothesis
3779	Anchor boxes eliminate the need to scan an image with a sliding window that computes a separate prediction at every potential position.  Because a convolutional neural network (CNN) can process an input image in a convolutional manner, a spatial location in the input can be related to a spatial location in the output.	Why do we use anchor boxes for convolution neural networks
1186	The Rabin-Karp algorithm makes use of hash functions and the rolling hash technique. A hash function is essentially a function that maps one thing to a value. In particular, hashing can map data of arbitrary size to a value of fixed size.	Which technique is used in Rabin Karp algorithm
1310	0:112:51Suggested clip · 118 secondsMath for Liberal Studies: Using the Nearest-Neighbor Algorithm YouTubeStart of suggested clipEnd of suggested clip	Where is the nearest neighbor algorithm
6949	The statistic used to estimate the mean of a population, μ, is the sample mean, . If X has a distribution with mean μ, and standard deviation σ, and is approximately normally distributed or n is large, then is approximately normally distributed with mean μ and standard error ..	How do you tell if a sample mean is normally distributed
1739	Regression and classification are categorized under the same umbrella of supervised machine learning.  The main difference between them is that the output variable in regression is numerical (or continuous) while that for classification is categorical (or discrete).	How regression is different than classification in predictive modeling
2945	Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost.	Why is deep learning needed
3776	Explanation: If a bayesian network is a representation of the joint distribution, then it can solve any query, by summing all the relevant joint entries.	How do you Bayesian network can be used to answer any query
4243	On each edge there are only states moving in one direction, and the direction is opposite for opposite edges. These strange states obtained at the edges are often referred to as chiral edge states. The chirality of the edges is determined by the orientation of the magnetic field (out of the plane vs.	What are edge States
5589	There is a direct relationship between the coefficients produced by logit and the odds ratios produced by logistic. First, let's define what is meant by a logit: A logit is defined as the log base e (log) of the odds. : [1] logit(p) = log(odds) = log(p/q) The range is negative infinity to positive infinity.	What is log odds ratio of logit model
196	Mutual information is one of many quantities that measures how much one random variables tells us about another. It is a dimensionless quantity with (generally) units of bits, and can be thought of as the reduction in uncertainty about one random variable given knowledge of another.	What is meant by mutual information
898	The (BIG) Z is a similar to the small z for one very good reason: It is a standard score. The z score has the sample standard deviation as the denominator, whereas the z-test value has the standard error of the mean ( or a measure of the variability of all the means from the population) as the dominator.	What's with the Z in Z test what similarity does it have to a simple Z or standard score
7685	Discriminant function analysis (DFA) is a statistical procedure that classifies unknown individuals and the probability of their classification into a certain group (such as sex or ancestry group). Discriminant function analysis makes the assumption that the sample is normally distributed for the trait.	What is a discriminant function analysis
3351	Multilevel models are particularly appropriate for research designs where data for participants are organized at more than one level (i.e., nested data). The units of analysis are usually individuals (at a lower level) who are nested within contextual/aggregate units (at a higher level).	When would you use a multilevel model
535	In probability theory and statistics, the exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate.	What is the mean of exponential distribution
387	0:255:16Suggested clip · 118 secondsConvolution of Two Functions - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you perform convolution
2198	n = norm( X ) returns the 2-norm or maximum singular value of matrix X , which is approximately max(svd(X)) . n = norm( X , p ) returns the p-norm of matrix X , where p is 1 , 2 , or Inf : If p = 1 , then n is the maximum absolute column sum of the matrix. If p = 2 , then n is approximately max(svd(X)) .	What is the 2 norm of a matrix
2580	The cumulative distribution function (cdf) of a continuous random variable X is defined in exactly the same way as the cdf of a discrete random variable. F (b) = P (X ≤ b). F (b) = P (X ≤ b) = f(x) dx, where f(x) is the pdf of X.	How can we find the PDF of a continuous random variable
180	Linear filtering is the filtering method in which the value of output pixel is linear combinations of the neighbouring input pixels.  A non-linear filtering is one that cannot be done with convolution or Fourier multiplication. A sliding median filter is a simple example of a non-linear filter.	What is the difference between linear and nonlinear filters
4390	The normal approximation to the binomial is when you use a continuous distribution (the normal distribution) to approximate a discrete distribution (the binomial distribution).	What is normal approximation to the binomial distribution
634	Decision Tree - Overfitting There are several approaches to avoiding overfitting in building decision trees. Pre-pruning that stop growing the tree earlier, before it perfectly classifies the training set. Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree.	What is pre pruning and post pruning in decision tree
2960	The Poisson Distribution formula is: P(x; μ) = (e-μ) (μx) / x! Let's say that that x (as in the prime counting function is a very big number, like x = 10100. If you choose a random number that's less than or equal to x, the probability of that number being prime is about 0.43 percent.	How do you use Poisson distribution
1169	Signal processing is essential for the use of X-rays, MRIs and CT scans, allowing medical images to be analyzed and deciphered by complex data processing techniques. Signals are used in finance, to send messages about and interpret financial data. This aids decision-making in trading and building stock portfolios.	What can you do with signal processing
7633	Absolutely, depth refers to the number of layers whereas receptive field size is specific to ConvNets and refers to the portion of the original input that a layer can see. See here: What is a receptive field in a convolutional neural network? How do I learn convolutional neural network theory?	Is there any difference between the depth and receptive field hyperparameters in Convolutional Neural Networks
7107	In short, fourier series is for periodic signals and fourier transform is for aperiodic signals. Fourier series is used to decompose signals into basis elements (complex exponentials) while fourier transforms are used to analyze signal in another domain (e.g. from time to frequency, or vice versa).	What is the relationship between Fourier series and Fourier transform
428	KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).	How does KNN classification work
1257	The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.	How do you determine the number of neurons in a hidden layer
581	We can use MLE in order to get more robust parameter estimates. Thus, MLE can be defined as a method for estimating population parameters (such as the mean and variance for Normal, rate (lambda) for Poisson, etc.) from sample data such that the probability (likelihood) of obtaining the observed data is maximized.	Why do we use maximum likelihood estimation
5384	A modern approach to reducing generalization error is to use a larger model that may be required to use regularization during training that keeps the weights of the model small. These techniques not only reduce overfitting, but they can also lead to faster optimization of the model and better overall performance.	How do you reduce generalization error
842	In cluster sampling, researchers divide a population into smaller groups known as clusters. They then randomly select among these clusters to form a sample. Cluster sampling is a method of probability sampling that is often used to study large populations, particularly those that are widely geographically dispersed.	What is a cluster in sampling
3437	x = A ./ B divides each element of A by the corresponding element of B . The sizes of A and B must be the same or be compatible. If the sizes of A and B are compatible, then the two arrays implicitly expand to match each other.	How do you split a matrix in Matlab
4410	A probability distribution is a list of outcomes and their associated probabilities.  A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function.	What is the difference between probability and distribution
374	Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.	Which loss function is used in classification
2835	The exponential distribution is a continuous probability distribution used to model the time we need to wait before a given event occurs. It is the continuous counterpart of the geometric distribution, which is instead discrete. Sometimes it is also called negative exponential distribution.	What is the exponential distribution used for
2891	The probability of making a type I error is represented by your alpha level (α), which is the p-value below which you reject the null hypothesis. A p-value of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis.	What does the P level tell us about Type I error
6591	A histogram is a graphical display of data using bars of different heights. In a histogram, each bar groups numbers into ranges. Taller bars show that more data falls in that range. A histogram displays the shape and spread of continuous sample data.	How does a histogram work
7290	"PSD is typically measured in units of Vrms2 /Hz or Vrms/rt Hz , where ""rt Hz"" means ""square root Hertz"". Alternatively, PSD can be expressed in units of dBm/Hz. On a spectrum analyzer such as the PSA, ESA, 856XE/EC or 859XE, power spectral density can be measured with the noise marker."	How is power spectral density measured
7845	The coefficient of determination is a statistical measurement that examines how differences in one variable can be explained by the difference in a second variable, when predicting the outcome of a given event.	What is coefficient of determination what does explain mean
566	The distribution of sample statistics is called sampling distribution.  Next a new sample of sixteen is taken, and the mean is again computed. If this process were repeated an infinite number of times, the distribution of the now infinite number of sample means would be called the sampling distribution of the mean.	What is the difference between a sampling distribution and the distribution of a sample
6121	0:382:54Suggested clip · 77 secondsClass Boundaries - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find class boundaries in statistics
4138	We have now found a test for determining whether a given set of vectors is linearly independent: A set of n vectors of length n is linearly independent if the matrix with these vectors as columns has a non-zero determinant. The set is of course dependent if the determinant is zero.	How do you determine if a subset is linearly independent
5558	In 2019, according to the Gini coefficient, household income distribution in the United States was 0.48. This figure was at 0.43 in 1990, which indicates an increase in income inequality in the U.S. over the past 30 years.	What is the Gini Coefficient for the US
4884	VARIABLE: Characteristic which varies between independent subjects.  CONTINUOUS (SCALE) VARIABLES: Measurements on a proper scale such as age, height etc. INDEPENDENT VARIABLE: The variable we think has an effect on the dependent variable.	What is a continuous independent variable
314	The finite population correction (fpc) factor is used to adjust a variance estimate for an estimated mean or total, so that this variance only applies to the portion of the population that is not in the sample.	What is the finite population correction factor
5086	Covariance measures the total variation of two random variables from their expected values.  Obtain the data.Calculate the mean (average) prices for each asset.For each security, find the difference between each value and mean price.Multiply the results obtained in the previous step.More items	How do you find the covariance
2860	Dropout causes your network to keep only some portion of neurons/weights on each iteration. Sometimes those neurons do not fit the current minibatch well, and this may cause large fluctuations.	What might be the cause of such fluctuations in training loss in Deep neural network using dropout
1431	The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers.	How do you get the best K value in Knn in R
447	A Formal Definition for Concept Learning: Inferring a boolean-valued function from training examples of its input and output. • An example for concept-learning is the learning of bird-concept from the given examples of birds (positive examples) and non-birds (negative examples). •	What is Concept Learning explain with example
1692	The expected value (EV) is an anticipated value for an investment at some point in the future. In statistics and probability analysis, the expected value is calculated by multiplying each of the possible outcomes by the likelihood each outcome will occur and then summing all of those values.	What is expected value of probability distribution
3160	Acts are the actions being considered by the agent -in the example elow, taking the raincoat or not; events are occurrences taking place outside the control of the agent (rain or lack thereof); outcomes are the result of the occurrence (or lack of it) of acts and events (staying dry or not; being burdened by the	What is the meaning of outcomes in decision theory
5136	RL is an increasingly popular technique for organizations that deal regularly with large complex problem spaces. Because RL models learn by a continuous process of receiving rewards and punishments on every action taken, it is able to train systems to respond to unforeseen environments .	Why is reinforcement important in learning
6074	However, RNNs suffer from the problem of vanishing gradients, which hampers learning of long data sequences. The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done.	What is the problem with RNNs and gradients
8192	Backward chaining is the logical process of inferring unknown truths from known conclusions by moving backward from a solution to determine the initial conditions and rules. Backward chaining is often applied in artificial intelligence (AI) and may be used along with its counterpart, forward chaining.	What is the use of backward chaining
314	In statistics, a Bayesian is someone who tries to determine the probability that a theory is true given the observed data. This is in contrast to classical statisticians, who work with the probability of observing certain data assuming a theory.	What does it mean to be a Bayesian
724	When we calculate Z, we will get a value. If this value falls into the middle part, then we cannot reject the null. If it falls outside, in the shaded region, then we reject the null hypothesis. That is why the shaded part is called: rejection region, as you can see below.	How do you find the Z test rejection region
7536	In Reinforcement Learning, this type of decision is called exploitation when you keep doing what you were doing, and exploration when you try something new.  In Reinforcement Learning on the other hand, it is not possible to do that, but there are some techniques that will help figuring out the best strategy.	What is exploration and exploitation in reinforcement learning
5144	Linear programming, mathematical modeling technique in which a linear function is maximized or minimized when subjected to various constraints. This technique has been useful for guiding quantitative decisions in business planning, in industrial engineering, and—to a lesser extent—in the social and physical sciences.	What do you mean by linear programming model
5127	No. You can have dependent events that are not mutually exclusive.	Can two events be dependent and not mutually exclusive
1475	Look up the normal distribution in a statistics table. Statistics tables can be found online or in statistics textbooks. Find the value for the intersection of the correct degrees of freedom and alpha. If this value is less than or equal to the chi-square value, the data is statistically significant.	How do you calculate statistically significant difference
5431	In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain.	What is Markov Chain Monte Carlo method
2751	Graphs that are appropriate for bivariate analysis depend on the type of variable. For two continuous variables, a scatterplot is a common graph. When one variable is categorical and the other continuous, a box plot is common and when both are categorical a mosaic plot is common.	Which plot is used for bivariate analysis
8106	Pierre-Simon Laplace	Who proved the central limit theorem
6682	The potential solutions include the following:Remove some of the highly correlated independent variables.Linearly combine the independent variables, such as adding them together.Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression.	What do you do when a variable is correlated
916	For more tips, read 10 Best Practices for Effective Dashboards.Choose the right charts and graphs for the job.  Use predictable patterns for layouts.  Tell data stories quickly with clear color cues.  Incorporate contextual clues with shapes and designs.  Strategically use size to visualize values.More items	How do you visualize data effectively
7666	A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic , whereas Markov networks are undirected and may be cyclic.  The underlying graph of a Markov random field may be finite or infinite.	What is the difference between Markov Chain Bayesian Network Dynamic Bayesian Network
1427	Decision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves).	What are decision trees in machine learning
5418	The term “multivariate statistics” is appropriately used to include all statistics where there are more than two variables simultaneously analyzed. You are already familiar with bivariate statistics such as the Pearson product moment correlation coefficient and the independent groups t-test.	What is multivariate variable
1379	It began with the “heartless” Tin man from the Wizard of Oz and continued with the humanoid robot that impersonated Maria in Metropolis. By the 1950s, we had a generation of scientists, mathematicians, and philosophers with the concept of artificial intelligence (or AI) culturally assimilated in their minds.	When did AI start and what did it involve
4798	The Poisson probability density function lets you obtain the probability of an event occurring within a given time or space interval exactly x times if on average the event occurs λ times within that interval. f ( x | λ ) = λ x x ! e − λ ; x = 0 , 1 , 2 , … , ∞ .	What is the probability density function of Poisson distribution
1255	When detecting bias, computer programmers normally examine the set of outputs that the algorithm produces to check for anomalous results. Comparing outcomes for different groups can be a useful first step. This could even be done through simulations.	How can algorithmic bias in machine learning be detected
4358	Systematic bias is sampling error that stems from the way in which the research is conducted and can therefore be controled by the researcher. There are three types:  Response bias: A biased view arises, because the answers that are given are not in accordance with the truth.	Why is systematic sampling biased
990	Causation is the relationship between cause and effect. So, when a cause results in an effect, that's a causation.  When we say that correlation does not imply cause, we mean that just because you can see a connection or a mutual relationship between two variables, it doesn't necessarily mean that one causes the other.	How can you tell if correlation does not imply causation
7432	Important classes of stochastic processes are Markov chains and Markov processes. A Markov chain is a discrete-time process for which the future behaviour, given the past and the present, only depends on the present and not on the past. A Markov process is the continuous-time version of a Markov chain.	What are the differences between a Markov chain and a Markov process
686	Decision Tree algorithm has become one of the most used machine learning algorithm both in competitions like Kaggle as well as in business environment. Decision Tree can be used both in classification and regression problem.	Can we use decision tree for regression
3381	Given that the range can easily be computed with information on the maximum and minimum value of the data set, users requiring only a rough indication of the data may prefer to use this indicator over more sophisticated measures of spread, like the standard deviation.	What are the uses of range in statistics
117	An estimator of a given parameter is said to be consistent if it converges in probability to the true value of the parameter as the sample size tends to infinity.	What does it mean for an estimator to be consistent
430	Bootstrapping is building a company from the ground up with nothing but personal savings, and with luck, the cash coming in from the first sales. The term is also used as a noun: A bootstrap is a business an entrepreneur with little or no outside cash or other support launches.	What is mean by bootstrapping
8536	Ap: NIST SP 800-22rev1a (dated April 2010), A Statistical Test Suite for the Validation of Random Number Generators and Pseudo Random Number Generators for Cryptographic Applications, that describes the test suite. Download the NIST Statistical Test Suite.	What is NIST test
1338	If you restrict yourself to linear kernels, both SVMs and LR will give almost identical performance and in some cases, LR will beat SVM.  If we compare logistic regression with SVMs with non-linear kernels, then SVMs beat LRs hands down.	Which is better for binary classification SVM or logistic regression
5339	Depending on the alternative hypothesis operator, greater than operator will be a right tailed test, less than operator is a left tailed test, and not equal operator is a two tailed test.	How do you know if it's a left or right tailed test
8296	"Like random forests, gradient boosting is a set of decision trees. The two main differences are:  Combining results: random forests combine results at the end of the process (by averaging or ""majority rules"") while gradient boosting combines results along the way."	What is the difference between random forest and gradient boosted tree
1934	Face detection is a broader term than face recognition. Face detection just means that a system is able to identify that there is a human face present in an image or video.  Face recognition can confirm identity. It is therefore used to control access to sensitive areas.	What is difference between face detection and face recognition
1162	Mathematically speaking, a decision tree has low bias and high variance. Averaging the result of many decision trees reduces the variance while maintaining that low bias. Combining trees is known as an 'ensemble method'.	Is decision tree an ensemble method
1064	Sentiment analysis (or opinion mining) uses natural language processing and machine learning to interpret and classify emotions in subjective data. Sentiment analysis is often used in business to detect sentiment in social data, gauge brand reputation, and understand customers.	What is sentiment analysis used for
6992	1. A Simple Way of Solving an Object Detection Task (using Deep Learning)First, we take an image as input:Then we divide the image into various regions:We will then consider each region as a separate image.Pass all these regions (images) to the CNN and classify them into various classes.More items•	How do we detect objects
4591	Nonparametric tests are sometimes called distribution-free tests because they are based on fewer assumptions (e.g., they do not assume that the outcome is approximately normally distributed).  There are several statistical tests that can be used to assess whether data are likely from a normal distribution.	Why would you use a nonparametric test
266	A z-test is a statistical test used to determine whether two population means are different when the variances are known and the sample size is large.  A z-statistic, or z-score, is a number representing how many standard deviations above or below the mean population a score derived from a z-test is.	What does the Z in Z test represent
117	Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features.	What is a hyperplane in machine learning
3570	They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.	What are robots useful for
8475	In mathematics, the membership function of a fuzzy set is a generalization of the indicator function for classical sets. In fuzzy logic, it represents the degree of truth as an extension of valuation.	What are membership functions in fuzzy logic
1479	Selection bias can result when the selection of subjects into a study or their likelihood of being retained in the study leads to a result that is different from what you would have gotten if you had enrolled the entire target population.	How does selection bias occur
4302	In contrast, quota sampling in qualitative research is a specific technique for selecting a sample that has been defined using a purposive sampling strategy to define the categories of data sources that are eligible for a study.	What is quota sampling in qualitative research
7114	For a prior distribution expressed as beta(θ|a,b), the prior mean of θ is a/(a + b). Suppose we observe z heads in N flips, which is a proportion of z/N heads in the data. The posterior mean is (z + a)/[(z + a) + (N ‒ z + b)] = (z + a)/(N + a + b).	How do you find the mean of a posterior distribution
329	Theorem 1.2 Suppose that ψ is a simple random point process that has both stationary and independent increments. Then in fact, ψ is a Poisson process. Thus the Poisson process is the only simple point process with stationary and independent increments.	Is Poisson process stationary
1445	Basic rules for logarithmsRule or special caseFormulaProductln(xy)=ln(x)+ln(y)Quotientln(x/y)=ln(x)−ln(y)Log of powerln(xy)=yln(x)Log of eln(e)=12 more rows	What are the logarithm rules
8672	Statistical Validity is the extent to which the conclusions drawn from a statistical test are accurate and reliable. To achieve statistical validity, researchers must have an adequate sample size and pick the right statistical test to analyze the data.	What does statistically valid mean
8186	Properties of F-DistributionThe F-distribution is positively skewed and with the increase in the degrees of freedom ν1 and ν2, its skewness decreases.The value of the F-distribution is always positive, or zero since the variances are the square of the deviations and hence cannot assume negative values.More items	What are the characteristics of F distribution curves
5197	The basic problem that the attention mechanism solves is that it allows the network to refer back to the input sequence, instead of forcing it to encode all information into one fixed-length vector.	What problem does attention solve
6503	The probability distribution associated with a random categorical variable is called a categorical distribution. Categorical data is the statistical data type consisting of categorical variables or of data that has been converted into that form, for example as grouped data.	Do categorical variables have distributions
3549	The value of a dependent variable depends on an independent variable, so a variable cannot be both independent and dependent at the same time. It must be either the cause or the effect, not both!	Can the same variable be used as the dependent and Independent variable after a time lapse
7650	Increasing the temperature will increase the entropy. Changes in volume will lead to changes in entropy. The larger the volume the more ways there are to distribute the molecules in that volume; the more ways there are to distribute the molecules (energy), the higher the entropy.	What causes an increase in entropy
1295	"Training artificial intelligence (AI) without datasets derived from human experts has significant implications for the development of AI with superhuman skills because expert data is ""often expensive, unreliable or simply unavailable."" Demis Hassabis, the co-founder and CEO of DeepMind, said that AlphaGo Zero was so"	What is the significance of AlphaGo Zero in AI research
531	2.1 Steps of Bayesian Data Analysis Choose a statistical model for the data in relation to the research questions. The model should have good theoretical justification and have parameters that are meaningful for the research questions.  Obtain the posterior distributions for the model parameters.	What are the steps involved in Bayesian data analysis
6893	A one-tailed test is a statistical test in which the critical area of a distribution is one-sided so that it is either greater than or less than a certain value, but not both.  A one-tailed test is also known as a directional hypothesis or directional test.	What is a one sided test in statistics
5627	Gradient Descent is the process of minimizing a function by following the gradients of the cost function. This involves knowing the form of the cost as well as the derivative so that from a given point you know the gradient and can move in that direction, e.g. downhill towards the minimum value.	What is gradient descent in logistic regression
7211	Collaborative Filtering Advantages & DisadvantagesNo domain knowledge necessary.Serendipity.Great starting point.Cannot handle fresh items.Hard to include side features for query/item.	What are the limitations of collaborative filtering
10	Normal Approximation to the Binomialn is your sample size,p is your given probability.q is just 1 – p. For example, let's say your probability p is . You would find q by subtracting this probability from 1: q = 1 – . 6 = .	How do you find the probability of normal approximation
5186	In order for the system to function, it's necessary to implement three steps. First, it must detect a face. Then, it must recognize that face nearly instantaneously. Finally, it must take whatever further action is required, such as allowing access for an approved user.	How do you build a face detection and recognition system
6329	Simple regression analysis uses a single x variable for each dependent “y” variable. For example: (x1, Y1). Multiple regression uses multiple “x” variables for each independent variable: (x1)1, (x2)1, (x3)1, Y1).	What is a regression model example
2664	There are mainly four ways of knowledge representation which are given as follows: Logical Representation. Semantic Network Representation. Frame Representation. Production Rules.	What are knowledge representation techniques
8606	Serial correlation is the relationship between a variable and a lagged version of itself over various time intervals. Repeating patterns often show serial correlation when the level of a variable affects its future level.  Serial correlation is also known as autocorrelation or lagged correlation.	Is serial correlation and autocorrelation the same thing
8188	Likelihood ratios range from zero to infinity. The higher the value, the more likely the patient has the condition. As an example, let's say a positive test result has an LR of 9.2. This result is 9.2 times more likely to happen in a patient with the condition than it would in a patient without the condition.	How do you read the likelihood ratio test
1508	In probability theory and statistics, the hypergeometric distribution is a discrete probability distribution that describes the probability of successes (random draws for which the object drawn has a specified feature) in draws, without replacement, from a finite population of size that contains exactly objects with	What is a hypergeometric probability distribution
1987	There are two main methods for tackling a multi-label classification problem: problem transformation methods and algorithm adaptation methods. Problem transformation methods transform the multi-label problem into a set of binary classification problems, which can then be handled using single-class classifiers.	How do you do multi label classification
1476	A continuous random variable Z is said to be a standard normal (standard Gaussian) random variable, shown as Z∼N(0,1), if its PDF is given by fZ(z)=1√2πexp{−z22},for all z∈R.	What is the pdf of normal distribution
7015	The joint behavior of two random variables X and Y is determined by the. joint cumulative distribution function (cdf):(1.1) FXY (x, y) = P(X ≤ x, Y ≤ y),where X and Y are continuous or discrete. For example, the probability.  P(x1 ≤ X ≤ x2,y1 ≤ Y ≤ y2) = F(x2,y2) − F(x2,y1) − F(x1,y2) + F(x1,y1).	How do you find the joint density function of two random variables
3218	An open source software library to carry out numerical computation using data flow graphs, the base language for TensorFlow is C++ or Python, whereas Theano is completely Python based library that allows user to define, optimize and evaluate mathematical expressions evolving multi-dimensional arrays efficiently, as per	Is TensorFlow based on theano
1107	In summary, model parameters are estimated from data automatically and model hyperparameters are set manually and are used in processes to help estimate model parameters. Model hyperparameters are often referred to as parameters because they are the parts of the machine learning that must be set manually and tuned.	What is the difference between a model parameter and a learning algorithm’s hyper parameter
5545	Sometimes we are given a chart showing frequencies of certain groups instead of the actual values.  If we multiply each midpoint by its frequency, and then divide by the total number of values in the frequency distribution, we have an estimate of the mean.	What does frequency distribution mean
7872	A random variable is discrete if it has a finite number of possible outcomes, or a countable number (i.e. the integers are infinite, but are able to be counted). For example, the number of heads you get when flip a coin 100 times is discrete, since it can only be a whole number between 0 and 100.	How do you know if probability is discrete
7736	Multiclass classification with logistic regression can be done either through the one-vs-rest scheme in which for each class a binary classification problem of data belonging or not to that class is done, or changing the loss function to cross- entropy loss.  By default, multi_class is set to 'ovr'.	How will you deal with the multiclass classification problem using logistic regression
702	Answer. True is the answer of Restricted Boltzmann Machine expect data to be labeled for Training as because there are two process for training one which is called as pre-training and training. In pre-training one don't need labeled data.	Does Restricted Boltzmann Machine expect the data to be labeled for training
81	Attention is proposed as a method to both align and translate. Alignment is the problem in machine translation that identifies which parts of the input sequence are relevant to each word in the output, whereas translation is the process of using the relevant information to select the appropriate output.	How does attention work in neural networks
775	Generally, you're evidently not an AI, if we are talking about the computers and algorithms and codes. You cannot prove this topic unless you definitely define what is artificial intelligence and what you are. Generally, you're evidently not an AI, if we are talking about the computers and algorithms and codes.	How do you prove that you are not an artificial intelligence
1028	A mixed model, mixed-effects model or mixed error-component model is a statistical model containing both fixed effects and random effects.  Because of their advantage in dealing with missing values, mixed effects models are often preferred over more traditional approaches such as repeated measures ANOVA.	What is a mixed model in statistics
1278	Fuelled by successes in Computer Go, Monte Carlo tree search (MCTS) has achieved widespread adoption within the games community. Its links to traditional reinforcement learning (RL) methods have been outlined in the past; however, the use of RL techniques within tree search has not been thoroughly studied yet.	Is Monte Carlo Tree Search reinforcement learning
590	In a nutshell, the goal of Bayesian inference is to maintain a full posterior probability distribution over a set of random variables.  Sampling algorithms based on Monte Carlo Markov Chain (MCMC) techniques are one possible way to go about inference in such models.	What is Bayesian sampling
369	If your goal is to target those with buying intent, you can do so by layering an in-market audience on top of your custom affinity audience. This will signal to Google that you want to show ads to people inside the affinity audience you've created who also have shown intent to buy in their search history.	What is layered targeting
706	Standard deviation measures the spread of a data distribution. It measures the typical distance between each data point and the mean. If the data is a sample from a larger population, we divide by one fewer than the number of data points in the sample, n − 1 n-1 n−1 .	What is the sample standard deviation
3708	Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability.	What exactly is a logistic regression algorithm in machine learning What are its applications
744	One of the best is using recurrent neural networks for automatic feature extraction. You can use wors2vec as raw inputs to the network. A step further is to use LSTM nodes in RNNs for modeling long term dependencies.	What is best feature extraction algorithm for Twitter sentiment analysis
3306	A single object of the world from which a model will be learned, or on which a model will be used (e.g., for prediction). In most machine learning work, instances are described by feature vectors; some work uses more complex representations (e.g., containing relations between instances or between parts of instances).	What is an instance in machine learning
21	First you will learn user-user collaborative filtering, an algorithm that identifies other people with similar tastes to a target user and combines their ratings to make recommendations for that user.	What is user user collaborative filtering
502	A false positive is an outcome where the model incorrectly predicts the positive class. And a false negative is an outcome where the model incorrectly predicts the negative class. In the following sections, we'll look at how to evaluate classification models using metrics derived from these four outcomes.	What is false positive rate in machine learning
8360	Sampling process error occurs because researchers draw different subjects from the same population but still, the subjects have individual differences. Keep in mind that when you take a sample, it is only a subset of the entire population; therefore, there may be a difference between the sample and population.	Why does sampling error occur
6887	The three main metrics used to evaluate a classification model are accuracy, precision, and recall. Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.	How do you evaluate the accuracy of a model
3695	The main advantage of KNN over other algorithms is that KNN can be used for multiclass classification. Therefore if the data consists of more than two labels or in simple words if you are required to classify the data in more than two categories then KNN can be a suitable algorithm.	Can Knn be used for multi class classification
7731	Experimental design is a way to carefully plan experiments in advance so that your results are both objective and valid. The terms “Experimental Design” and “Design of Experiments” are used interchangeably and mean the same thing.	What are experimental designs used for
8175	The sample kurtosis is a useful measure of whether there is a problem with outliers in a data set. Larger kurtosis indicates a more serious outlier problem, and may lead the researcher to choose alternative statistical methods.	What are some interesting applications of kurtosis
2569	Autocorrelation is diagnosed using a correlogram (ACF plot) and can be tested using the Durbin-Watson test. The auto part of autocorrelation is from the Greek word for self, and autocorrelation means data that is correlated with itself, as opposed to being correlated with some other data.	How do you calculate autocorrelation
6597	Your explanation, for example, could be, “An observation is something you sense: taste, touch, smell, see, or hear. An inference is something you decide or think about a thing or event after you observe it.”	What is the difference between inference and observation
5261	The next step is known as “Expectation” – step or E-step. In this step, we use the observed data in order to estimate or guess the values of the missing or incomplete data. It is basically used to update the variables. The next step is known as “Maximization”-step or M-step.	What is the significance of expectation and maximization steps in the context of classification
3962	Univariate logistic analysis: When there is one dependent variable, and one independent variable; both are categorical; generally produce Unadjusted model (crude odds ratio) by taking just one independent variable at a time..  Multivariate regression : It's a regression approach of more than one dependent variable.	What is univariate and multivariate logistic regression analysis
1264	In general, solvers return a local minimum (or optimum).  A local minimum of a function is a point where the function value is smaller than at nearby points, but possibly greater than at a distant point. A global minimum is a point where the function value is smaller than at all other feasible points.	What is the difference between a locally and globally optimal strategy
6821	Getting to the point, the basic practical difference between Sigmoid and Softmax is that while both give output in [0,1] range, softmax ensures that the sum of outputs along channels (as per specified dimension) is 1 i.e., they are probabilities. Sigmoid just makes output between 0 to 1.	How does the Sigmoid function differs from Softmax function
154	The p-value is not enough Therefore, a significant p-value tells us that an intervention works, whereas an effect size tells us how much it works. It can be argued that emphasizing the size of effect promotes a more scientific approach, as unlike significance tests, effect size is independent of sample size.	How does effect size affect P value
7217	Stochastic effects have been defined as those for which the probability increases with dose, without a threshold. Nonstochastic effects are those for which incidence and severity depends on dose, but for which there is a threshold dose. These definitions suggest that the two types of effects are not related.	What is the difference between stochastic and non stochastic
5907	Steps in selecting a systematic random sample:Calculate the sampling interval (the number of households in the population divided by the number of households needed for the sample)Select a random start between 1 and sampling interval.Repeatedly add sampling interval to select subsequent households.	How do you select a systematic random sample
6807	You have several options for handling your non normal data. Many tests, including the one sample Z test, T test and ANOVA assume normality. You may still be able to run these tests if your sample size is large enough (usually over 20 items).	Can you use a t test for non normal data
1065	AI and neuroscience researchers agree that current forms of AI cannot have their own emotions, but they can mimic emotion, such as empathy. Synthetic speech also helps reduce the robotic like tone many of these services operate with and emit more realistic emotion.	Could artificial intelligence develop emotions
710	The Akaike information criterion (AIC) is an estimator of out-of-sample prediction error and thereby relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models.	What is an intuitive explanation of the Akaike information criterion
4386	There are two types of sampling methods: Probability sampling involves random selection, allowing you to make statistical inferences about the whole group. Non-probability sampling involves non-random selection based on convenience or other criteria, allowing you to easily collect initial data.	What are the types of probability and non probability sampling
8494	The Fourier Transform is a mathematical technique that transforms a function of time, x(t), to a function of frequency, X(ω).  Making these substitutions in the previous equation yields the analysis equation for the Fourier Transform (also called the Forward Fourier Transform).	What is Fourier transform formula
2559	A hidden Markov model (HMM) is a statistical model that can be used to describe the evolution of observable events that depend on internal factors, which are not directly observable. We call the observed event a `symbol' and the invisible factor underlying the observation a `state'.	Why we use hidden Markov model
3603	Multinomial logistic regression (often just called 'multinomial regression') is used to predict a nominal dependent variable given one or more independent variables. It is sometimes considered an extension of binomial logistic regression to allow for a dependent variable with more than two categories.	What kind of statistics might be used when our variables are multinomial
2042	A continuous random variable is normally distributed or has a normal probability distribution if its relative frequency histogram has the shape of a normal curve.  We can extend this idea to the shape of other distributions. If μ = 0 and σ = 1, almost all of the data should be between -3 and 3, with the center at 0.	How is the continuous normal probability distribution related to a histogram
3389	No, logistic regression does not require any particular distribution for the independent variables. They can be normal, skewed, categorical or whatever. No regression method makes assumptions about the shape of the distribution of either the IVs or the DV.	Does logistic regression require independent variables to be normal distributed
5072	"Q-learning is a model-free reinforcement learning algorithm to learn quality of actions telling an agent what action to take under what circumstances.  ""Q"" names the function that the algorithm computes with the maximum expected rewards for an action taken in a given state."	What is Q function in reinforcement learning
4127	An RNNs is essentially a fully connected neural network that contains a refactoring of some of its layers into a loop.  Among the text usages, the following tasks are among those RNNs perform well at: Sequence labelling. Natural Language Processing (NLP) text classification.	Is NLP neural network
449	A probability distribution may be either discrete or continuous. A discrete distribution means that X can assume one of a countable (usually finite) number of values, while a continuous distribution means that X can assume one of an infinite (uncountable) number of different values.	What is the difference between a discrete probability distribution and a continuous probability distribution
1123	Fine tuning is one approach to transfer learning. In Transfer Learning or Domain Adaptation we train the model with a dataset and after we train the same model with another dataset that has a different distribution of classes, or even with other classes than in the training dataset).	Is fine tuning a pre trained model equivalent to transfer learning
8567	Dropout is a regularization technique for neural network models proposed by Srivastava, et al. in their 2014 paper Dropout: A Simple Way to Prevent Neural Networks from Overfitting (download the PDF). Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly.	What is dropout rate in deep learning
2648	In the context of gradient boosting, the training loss is the function that is optimized using gradient descent, e.g., the “gradient” part of gradient boosting models. Specifically, the gradient of the training loss is used to change the target variables for each successive tree.	What is loss function in gradient boosting
6214	Hinge loss is not differentiable and cannot be used with methods which are differentiable like stochastic gradient descent(SGD). In this case Cross entropy(log loss) can be used.	Is hinge loss differentiable
3450	A Blob is a group of connected pixels in an image that share some common property ( E.g grayscale value ). In the image above, the dark connected regions are blobs, and the goal of blob detection is to identify and mark these regions.	What is blob in object detection
1043	The level of significance which is selected in Step 1 (e.g., α =0.05) dictates the critical value. For example, in an upper tailed Z test, if α =0.05 then the critical value is Z=1.645.	How do you find the critical value of Z in a hypothesis test
3162	It is argued that 'one should not… run away with the concept [of emancipation] to make it all things to all people' (Ayoob 1997: 139).  Thus, security is best understood as an 'essentially contested concept' because any sort of fixed definition of security would be unwise; all static definitions have inherent problems.	Why is security an essentially contested concept
5128	The values could be anywhere from, say, 4.5 feet to 7.2 feet. In general, quantities such as pressure, height, mass, weight, density, volume, temperature, and distance are examples of continuous random variables.	Which of the following are examples of continuous random variables
479	Writing a Questionnaire for a Conjoint Analysis StudyScreener.  Explain how to do the first choice question.  (Optional) Ask people to explain their first choice.  Explain that the following choice questions vary.  Count down the number of choice questions.  Situational data and cueing.  Collect data for validation purposes.  Profiling questions.More items•	How do you write a conjoint analysis survey
4445	Barto (2007), Scholarpedia, 2(11):1604. Temporal difference (TD) learning is an approach to learning how to predict a quantity that depends on future values of a given signal. The name TD derives from its use of changes, or differences, in predictions over successive time steps to drive the learning process.	What is Temporal difference TD learning
7129	a measure of the consistency and freedom from error of a test, as indicated by a correlation coefficient obtained from responses to two or more alternate forms of the test. Also called comparable-forms reliability; equivalent-forms reliability; parallel-forms reliability.	What is alternate form reliability also known as
156	"Binary Search AlgorithmStep 1 - Read the search element from the user.Step 2 - Find the middle element in the sorted list.Step 3 - Compare the search element with the middle element in the sorted list.Step 4 - If both are matched, then display ""Given element is found!!!"" and terminate the function.More items"	What are the four steps of the binary search algorithm
1473	"2 AnswersInspect the topics: Look at the highest-likelihood words in each topic. Do they sound like they form a cohesive ""topic"" or just some random group of words?Inspect the topic assignments: Hold out a few random documents from training and see what topics LDA assigns to them."	How do I verify LDA model
2207	Tokenization is one of the most common tasks when it comes to working with text data.  Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.	What is tokenization NLP
286	When talking about kernels in machine learning, most likely the first thing that comes into your mind is the support vector machines (SVM) model because the kernel trick is widely used in the SVM model to bridge linearity and non-linearity.	In which case we should use kernel tricks
8414	There are three main rules associated with basic probability: the addition rule, the multiplication rule, and the complement rule. You can think of the complement rule as the 'subtraction rule' if it helps you to remember it.	What are the 3 rules of probability
7895	Artificial Intelligence CharacteristicsDeep Learning. Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans, to learn by example.  Facial Recognition.  Automate Simple and Repetitive Tasks.  Data Ingestion.  ChatBots.  Quantum Computing.  Cloud Computing.	What are the characteristics of artificial intelligence
3673	If p is a probability, then p/(1 − p) is the corresponding odds; the logit of the probability is the logarithm of the odds, i.e.  For each choice of base, the logit function takes values between negative and positive infinity.	What is the logit function when P refers to probability of occurrence of an event
3092	NumPy is an open-source numerical Python library. NumPy contains a multi-dimensional array and matrix data structures. It can be utilised to perform a number of mathematical operations on arrays such as trigonometric, statistical, and algebraic routines.  Pandas objects rely heavily on NumPy objects.	Why NumPy is used in Python
2147	: varying with something else so as to preserve certain mathematical interrelations.	What does covariant mean
4631	According to Markowitz, for every point on the efficient frontier, there is at least one portfolio that can be constructed from all available investments that has the expected risk and return corresponding to that point.  The efficient frontier is curved because there is a diminishing marginal return to risk.	Why is the efficient frontier concave
524	IAT is a popular measure in social psychology to measure the relative strength of association between pairs of concepts (Greenwald, McGhee, & Schwartz, 1998).  Studies have found that racial bias IAT studies have a test-retest reliability score of only 0.44, while the IAT overall is just around 0.5.	Is the Implicit Association Test having poor test retest reliability
243	Multiply the Total with disease by the Sensitivity to get the number of True positives. Multiply the Total without disease by the Specificity to get the number of True Negatives. Compute the number of False positives and False negatives by subtraction.	How do you calculate true positive from sensitivity and specificity
779	Non-Negative Matrix Factorization (NMF) is an unsupervised technique so there are no labeling of topics that the model will be trained on. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation.	What is NMF topic modeling
5447	Image processing is often viewed as arbitrarily manipulating an image to achieve an aesthetic standard or to support a preferred reality. However, image processing is more accurately defined as a means of translation between the human visual system and digital imaging devices.	Why image processing is needed
337	Blood Test Helps Determine Patient Response To TNF-Alpha Inhibitors. Baseline levels of serum interferon in rheumatoid arthritis (RA) patients may help rheumatologists determine who may have a poor response to tumour necrosis factor-alpha inhibitor drugs.	Why is TNF alpha test done
194	Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.	What does a normal distribution tell us
1511	A discrete distribution is one in which the data can only take on certain values, for example integers. A continuous distribution is one in which data can take on any value within a specified range (which may be infinite).	How do you determine if a distribution is discrete or continuous
2055	a. If your data is labeled, but you only have a limited amount, you should use a classifier with high bias (for example, Naive Bayes). I'm guessing this is because a higher-bias classifier will have lower variance, which is good because of the small amount of data.	How do I choose the best classifier
7215	These data points which are way too far from zero will be treated as the outliers. In most of the cases a threshold of 3 or -3 is used i.e., if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers.	How do you identify outliers in data science
6040	Gaussian random variables and Gaussian random vectors (vectors whose components are jointly Gaussian, as defined later) play a central role in detection and estimation.  Jointly Gaussian random variables are completely described by their means and covariances, which is part of the simplicity of working with them.	What is a Gaussian vector
6206	The log likelihood This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. Therefore we can work with the simpler log-likelihood instead of the original likelihood.	Why do we use log likelihood
4794	A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term. Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function.	What is the use of regularization What are l1 and l2 regularization
1068	Sampling is done because you usually cannot gather data from the entire population. Even in relatively small populations, the data may be needed urgently, and including everyone in the population in your data collection may take too long.	What are the reasons to use sampling
7363	"""Controlling"" for a variable means adding it to the model so its effect on your outcome variable(s) can be estimated and statistically isolated from the effect of the independent variable you're really interested in.  We could also add other variables such as age, education level, and the like."	What does it mean to control for a variable
3228	The ground-truth bounding boxes (i.e., the hand labeled bounding boxes from the testing set that specify where in the image our object is).	What is ground truth box
617	In positively skewed distributions, the mean is usually greater than the median, which is always greater than the mode. In negatively skewed distributions, the mean is usually less than the median, which is always less than the mode.	What would it mean if the distribution was negatively or positively skewed
409	"""The function we want to minimize or maximize is called the objective function, or criterion.  The loss function computes the error for a single training example, while the cost function is the average of the loss functions of the entire training set."	What is the difference between loss function cost function and objective function
2475	Odds ratios measure how many times bigger the odds of one outcome is for one value of an IV, compared to another value.  That odds ratio is an unstandardized effect size statistic. It tells you the direction and the strength of the relationship between water temperature and the odds that the plant is present.	What is the difference between Cohens d and the odds ratio in statistics
1296	Below are the different regression techniques:Linear Regression.Logistic Regression.Ridge Regression.Lasso Regression.Polynomial Regression.Bayesian Linear Regression.	What are the types of regression
2682	Frequency distribution in statistics is a representation that displays the number of observations within a given interval. The representation of a frequency distribution can be graphical or tabular so that it is easier to understand.	What is a frequency distribution in statistics
5576	Correlation Test Between Two Variables in RR functions.Import your data into R.Visualize your data using scatter plots.Preleminary test to check the test assumptions.Pearson correlation test. Interpretation of the result. Access to the values returned by cor.test() function.Kendall rank correlation test.Spearman rank correlation coefficient.	How do you find the correlation between variables in R
6225	Jakob Bernoulli	Who created the law of averages
4260	The statistical output displays the coded coefficients, which are the standardized coefficients. Temperature has the standardized coefficient with the largest absolute value. This measure suggests that Temperature is the most important independent variable in the regression model.	How do you identify the most important predictor variables in regression models
6712	population is the all people or objects to which you wishes to generalize the findings of your study, for instance if your study is about pregnant teenagers , all of the pregnant tens are your target population. Sample frame is a subset of the population and the people or object that you have access to them.	How does a frame differ from a target population
3803	There are several criteria to be used in evaluating a sorting algorithm:Running time. Typically, an elementary sorting algorithm requires O(N2) steps to sort N randomly arranged items.  Memory requirements. The amount of extra memory required by a sorting algorithm is also an important consideration.  Stability.	What are the basic criteria for selecting appropriate algorithm
736	The median is the middle number in a sorted, ascending or descending, list of numbers and can be more descriptive of that data set than the average.  If there is an odd amount of numbers, the median value is the number that is in the middle, with the same amount of numbers below and above.	What is median value in statistics
324	What one hot encoding does is, it takes a column which has categorical data, which has been label encoded, and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has what value.  So, that's the difference between Label Encoding and One Hot Encoding.	What is the difference between one hot encoding and label encoding
7135	Calculate precision and recall for all objects present in the image. You also need to consider the confidence score for each object detected by the model in the image. Consider all of the predicted bounding boxes with a confidence score above a certain threshold.	How do you evaluate an object's detection model
648	In statistics, latent variables (from Latin: present participle of lateo (“lie hidden”), as opposed to observable variables) are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured).	What is the difference between a latent variable and an observed variable
3482	Statistical analysis is used extensively in science, from physics to the social sciences. As well as testing hypotheses, statistics can provide an approximation for an unknown that is difficult or impossible to measure.	Where can statistical analysis be used
1050	A measure of central location is the single value that best represents a characteristic such as age or height of a group of persons. A measure of dispersion quantifies how much persons in the group vary from each other and from our measure of central location.	What is measures of location and dispersion
742	A true positive is an outcome where the model correctly predicts the positive class. Similarly, a true negative is an outcome where the model correctly predicts the negative class. A false positive is an outcome where the model incorrectly predicts the positive class.	What are true positives and false positives
149	The biggest negative of transfer learning is that it's very hard to do right and very easy to mess up. Especially in NLP this kind of approach has only been mainstream for about a year, which just isn't enough time when model runs take weeks.	What are the disadvantages of transfer learning
5880	Decomposition is a forecasting technique that separates or decomposes historical data into different components and uses them to create a forecast that is more accurate than a simple trend line.	What is decomposition in forecasting
1163	(d) PivotTables allow you to filter data, and crosstab queries do not. Crosstab Query and Pivot table are used to get the aggregated data when the data in rows and columns is intersected. Pivot table are modernized then the cross table queries. These tables have filters which can alter the selection criterion.	What is the difference between crosstab query and pivot table
5353	In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain.	What is Markov Chain Monte Carlo simulation
3326	The t statistic is the coefficient divided by its standard error.  It can be thought of as a measure of the precision with which the regression coefficient is measured. If a coefficient is large compared to its standard error, then it is probably different from 0.	What is T statistics in regression analysis
7584	The rectifier is, as of 2017, the most popular activation function for deep neural networks. A unit employing the rectifier is also called a rectified linear unit (ReLU).	What is ReLU in deep learning
945	"The definition is: ""Entropy is a measure of how evenly energy is distributed in a system. In a physical system, entropy provides a measure of the amount of energy that cannot be used to do work."""	What is entropy in layman's terms
7879	Almost all reinforcement learning algorithms are based on estimating value functions--functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).  The value functions and can be estimated from experience.	What is value function in reinforcement learning
1137	LBPH is one of the easiest face recognition algorithms. It can represent local features in the images. It is possible to get great results (mainly in a controlled environment). It is robust against monotonic gray scale transformations.	Which algorithm is best for face recognition
1087	A random variable is a numerical description of the outcome of a statistical experiment.  For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x). This function provides the probability for each value of the random variable.	What is a random variable in probability theory
244	Sometimes known as the secret to a good horror or drama, non-linear sounds are sounds that are too loud for the normal musical range of an instrument or an animal's vocal chords.  One contained emotionally neutral film scores and the other contained nonlinear sounds.	What is nonlinear noise
267	"Constrained optimization problems are problems for which a function is to be minimized or maximized subject to constraints . Here is called the objective function and is a Boolean-valued formula.  stands for ""maximize subject to constraints "". You say a point satisfies the constraints if is true."	What is constrained optimization problem
2429	- YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find marginal probability
289	Definition: Two events, A and B, are independent if the fact that A occurs does not affect the probability of B occurring. Some other examples of independent events are: Landing on heads after tossing a coin AND rolling a 5 on a single 6-sided die. Choosing a marble from a jar AND landing on heads after tossing a coin.	What is an example of an independent event
6633	It is common to allocate 50 percent or more of the data to the training set, 25 percent to the test set, and the remainder to the validation set. Some training sets may contain only a few hundred observations; others may include millions.	How much data should you allocate for your training validation and test sets
200	This is because geometric mean involves product term. However, for a data which follows log-normal distribution, geometric mean should be same as median.	Is geometric mean the same as median
285	The 7 Steps of Machine Learning1 - Data Collection.2 - Data Preparation.3 - Choose a Model.4 - Train the Model.5 - Evaluate the Model.6 - Parameter Tuning.7 - Make Predictions.More items	What are the correct steps of a machine learning process
3519	Importance sampling is a variance reduction technique that can be used in the Monte Carlo method. The idea behind importance sampling is that certain values of the input random variables in a simulation have more impact on the parameter being estimated than others.	What is the importance of a sampling approach to the estimation of expected values in Monte Carlo algorithms
6772	To explain eigenvalues, we first explain eigenvectors. Almost all vectors change di- rection, when they are multiplied by A. Certain exceptional vectors x are in the same direction as Ax. Those are the “eigenvectors”. Multiply an eigenvector by A, and the vector Ax is a number times the original x.	What is an eigenvalue simple explanation
3759	While e-learning won't replace traditional classrooms, it will change the way we know them today. With improved resources and reduced teacher workloads, classrooms can shift to co-learning spaces. Students can arrive, learn, engage—all at their own pace in a collaborative environment.	Does e learning really have the potential to replace offline learning
320	Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case.	What does kurtosis indicate
1406	How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together.	How do you find the probability distribution
247	Before you run any statistical test, you must first determine your alpha level, which is also called the “significance level.” By definition, the alpha level is the probability of rejecting the null hypothesis when the null hypothesis is true.  Like all probabilities, alpha ranges from 0 to 1.	What is alpha level
2469	Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance.	What is the difference between bias and variance
19	A population regression function is a linear function, which hypothesizes a theoretical relationship between a dependent variable and a set of independent or explanatory variables at a population level. A stochastic error terms is present in the regression model as well.	What is a population regression model
1057	No, because clustering and classification (or supervised learning) are two different philosophies of machine learning.  Alternatively, if you don't have class labels, you can't do classification and only clustering is possible to understand the possible groups within the data.	Is supervised learning commonly carried out after clustering
749	An example of an argument that fits the form modus ponens: If today is Tuesday, then John will go to work.  An argument can be valid but nonetheless unsound if one or more premises are false; if an argument is valid and all the premises are true, then the argument is sound.	What is an example of modus Ponens
7384	• Model capacity is ability to fit variety of functions. – Model with Low capacity struggles to fit training set. – A High capacity model can overfit by memorizing. properties of training set not useful on test set. • When model has higher capacity, it overfits.	What is model capacity in machine learning
5898	Criteria for CausalityStrength: A relationship is more likely to be causal if the correlation coefficient is large and statistically significant.Consistency: A relationship is more likely to be causal if it can be replicated.More items•	How do you know if correlation is causation
31	The Correlation Coefficient When the r value is closer to +1 or -1, it indicates that there is a stronger linear relationship between the two variables. A correlation of -0.97 is a strong negative correlation while a correlation of 0.10 would be a weak positive correlation.	How do you know if a correlation coefficient is strong or weak
438	Thus, Linear regression is better for simpler modelling while neural net is better for complex or multiple-level/category modelling. Neural networks generally outperform linear regression as they have more degrees of freedom. In linear regression variables are treated as a linear combination.	When would you use neural network regression
1084	standard normal distribution	Which standard score has a mean of 0 and a standard deviation of 1
2271	The term general linear model (GLM) usually refers to conventional linear regression models for a continuous response variable given continuous and/or categorical predictors. It includes multiple linear regression, as well as ANOVA and ANCOVA (with fixed effects only).	Is GLM a regression
2229	Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable).	When Should multiple regression be used
3808	Quota sampling means to take a very tailored sample that's in proportion to some characteristic or trait of a population.  For example, if your population consists of 45% female and 55% male, your sample should reflect those percentages.	What is quota sampling and example
6389	Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.	What is the difference between filter and wrapper methods
5820	Machine learning algorithms are able to improve without being explicitly programmed. In other words, they are able to find patterns in the data and apply those patterns to new challenges in the future. Deep learning is a subset of machine learning, which uses neural networks with many layers.	What is learning algorithm in neural network
639	How To Overcome Confirmation Bias And Expand Your MindDon't Be Afraid.  Know That Your Ego Doesn't Want You To Expand Your Mind.  Think For Yourself.  If You Want To Expand Your Mind, You Must Be OK With Disagreements.  Ask Good Questions.  Keep Information Channels Open.	How do you stop confirmation bias
4626	The formula for calculating lambda is: Lambda = (E1 – E2) / E1. Lambda may range in value from 0.0 to 1.0. Zero indicates that there is nothing to be gained by using the independent variable to predict the dependent variable.	How is Lambda calculated
1270	The difference between the two norms is that the standard deviation is calculating the square of the difference whereas the mean absolute deviation is only looking at the absolute difference. Hence large outliers will create a higher dispersion when using the standard deviation instead of the other method.	What is difference between standard deviation and mean deviation
4551	If you are a beginner, I can recommend you as below.Quickly learn Python first.Take a course of AI and Machine learning (several online courses are there). You can try MIT OCW also.Then start with Tutorial of TensorFlow website (https://www.tensorflow.org/versions/0.6.0/tutorials/index.html )	Where can I start learning how to use TensorFlow
3340	The amount that the weights are updated during training is referred to as the step size or the “learning rate.” Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.	What is step size in deep learning
3864	In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived.	What is validate model data
4479	This post is about various evaluation metrics and how and when to use them.Accuracy, Precision, and Recall: A.  F1 Score: This is my favorite evaluation metric and I tend to use this a lot in my classification projects.  Log Loss/Binary Crossentropy.  Categorical Crossentropy.  AUC.	What metrics would you use in a classification problem
2789	TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.	What is TensorBoard used for
2950	"Contrapositive: The contrapositive of a conditional statement of the form ""If p then q"" is ""If ~q then ~p"". Symbolically, the contrapositive of p q is ~q ~p. A conditional statement is logically equivalent to its contrapositive."	Which is the contrapositive of P Q
531	The converse of the conditional statement is “If Q then P.” The contrapositive of the conditional statement is “If not Q then not P.” The inverse of the conditional statement is “If not P then not Q.”	What is converse inverse and Contrapositive
794	"Mathwords: Contrapositive. Switching the hypothesis and conclusion of a conditional statement and negating both. For example, the contrapositive of ""If it is raining then the grass is wet"" is ""If the grass is not wet then it is not raining."""	What is a Contrapositive example
6641	The F-distribution is either zero or positive, so there are no negative values for F. This feature of the F-distribution is similar to the chi-square distribution. The F-distribution is skewed to the right. Thus this probability distribution is nonsymmetrical.	What are the properties of F distribution
6102	1 . Two main measures for the efficiency of an algorithm areProcessor and memory.Complexity and capacity.Time and space.Data and space.	What are two main measures for the efficiency of an algorithm
16	Bias is calculated as the product of two components: non-response rate and the difference between the observed and non-respondent answers. Increasing either of the two components will lead to an increase in bias.	How do you calculate nonresponse bias
160	The least squares regression line is the line that best fits the data. Its slope and y-intercept are computed from the data using formulas.  The sum of the squared errors SSE of the least squares regression line can be computed using a formula, without having to compute all the individual errors.	What is the least squares regression line
2723	A null hypothesis is a type of hypothesis used in statistics that proposes that there is no difference between certain characteristics of a population (or data-generating process). For example, a gambler may be interested in whether a game of chance is fair.	What is a null hypothesis example
2498	In statistics, imputation is the process of replacing missing data with substituted values.  That is to say, when one or more values are missing for a case, most statistical packages default to discarding any case that has a missing value, which may introduce bias or affect the representativeness of the results.	What is imputing missing data
1186	A Z-score is a numerical measurement that describes a value's relationship to the mean of a group of values. Z-score is measured in terms of standard deviations from the mean.  A Z-Score is a statistical measurement of a score's relationship to the mean in a group of scores.	What is Z scores in statistics
6286	Techniques of Forecasting: Simple Moving Average (SMA) Exponential Smoothing (SES) Autoregressive Integration Moving Average (ARIMA) Neural Network (NN)	What are different time series forecasting techniques
4968	Noisy data can appear as normal data. So noise objects are not always outliers.	Are noise objects always outliers
8019	Generally, a value of r greater than 0.7 is considered a strong correlation. Anything between 0.5 and 0.7 is a moderate correlation, and anything less than 0.4 is considered a weak or no correlation.	Is 0.4 A strong correlation
1318	The P-value is the probability that a chi-square statistic having 2 degrees of freedom is more extreme than 19.58. We use the Chi-Square Distribution Calculator to find P(Χ2 > 19.58) = 0.0001. Interpret results. Since the P-value (0.0001) is less than the significance level (0.05), we cannot accept the null hypothesis.	What is the relationship between the chi square statistic and the p value
3018	The main difference between cluster sampling and stratified sampling is that in cluster sampling the cluster is treated as the sampling unit so sampling is done on a population of clusters (at least in the first stage). In stratified sampling, the sampling is done on elements within each stratum.	What is the difference between cluster sampling and stratified sampling
321	How do you calculate precision and recall for multiclass classification using confusion matrix?Precision = TP / (TP+FP)Recall = TP / (TP+FN)	How do you calculate precision and recall for multi class problems
4580	Deep Learning is the evolution of Machine Learning and it will definitely help in making machines better than what Machine Learning does. But one thing to note is that Deep Learning models require a very large amount of data to train the model otherwise it won't work as expected.	Can deep learning replace machine learning
1160	To find percent agreement for two raters, a table (like the one above) is helpful.Count the number of ratings in agreement. In the above table, that's 3.Count the total number of ratings. For this example, that's 5.Divide the total by the number in agreement to get a fraction: 3/5.Convert to a percentage: 3/5 = 60%.	How is inter rater agreement calculated
8226	Relu : More computationally efficient to compute than Sigmoid like functions since Relu just needs to pick max(0, x) and not perform expensive exponential operations as in Sigmoids. Relu : In practice, networks with Relu tend to show better convergence performance than sigmoid.	Why we use ReLU instead of sigmoid
1485	The problem is to find the probability of landing at a given spot after a given number of steps, and, in particular, to find how far away you are on average from where you started. Why do we care about this game? The random walk is central to statistical physics.	What is random walk problem
8375	The set of all the possible outcomes is called the sample space of the experiment and is usually denoted by S. Any subset E of the sample space S is called an event.  E = {2,4,6} is an event, which can be described in words as ”the number is even”. Example 3 Tossing a coin twice.	What is the difference between sample space and event
548	Regression and classification are categorized under the same umbrella of supervised machine learning.  The main difference between them is that the output variable in regression is numerical (or continuous) while that for classification is categorical (or discrete).	How is regression related to classification
8334	The mean is an important measure because it incorporates the score from every subject in the research study. The required steps for its calculation are: count the total number of cases—referred in statistics as n; add up all the scores and divide by the total number of cases.	Why is the mean useful in statistics
774	AB testing is essentially an experiment where two or more variants of a page are shown to users at random, and statistical analysis is used to determine which variation performs better for a given conversion goal.	What is a B testing and how does it work
3423	Recall quantifies the number of positive class predictions made out of all positive examples in the dataset. F-Measure provides a single score that balances both the concerns of precision and recall in one number.	What is the recall score for the machine learning model
8620	Pattern recognition requires repetition of experience. Semantic memory, which is used implicitly and subconsciously is the main type of memory involved with recognition.  The development of neural networks in the outer layer of the brain in humans has allowed for better processing of visual and auditory patterns.	How do humans recognize patterns
1021	A random variable, usually written X, is a variable whose possible values are numerical outcomes of a random phenomenon. There are two types of random variables, discrete and continuous.	What is random variable and its types
1781	AI or artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning and self-correction. Some of the applications of AI include expert systems, speech recognition and machine vision.	What are the applications of artificial intelligence
6270	AI works by combining large amounts of data with fast, iterative processing and intelligent algorithms, allowing the software to learn automatically from patterns or features in the data.  Cognitive computing is a subfield of AI that strives for a natural, human-like interaction with machines.	What makes an AI an AI
2930	3:1615:06Suggested clip · 98 secondsOrdinal logistic regression using SPSS (July, 2019) - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you run ordinal regression in SPSS
7649	Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data.  Deep learning allows machines to solve complex problems even when using a data set that is very diverse, unstructured and inter-connected.	What is meant by deep learning
4802	Within an artificial neural network, a neuron is a mathematical function that model the functioning of a biological neuron. Typically, a neuron compute the weighted average of its input, and this sum is passed through a nonlinear function, often called activation function, such as the sigmoid.	What is a neuron in a neural network
495	A sample is a randomly chosen selection of elements from an underlying population. Sample covariance measures the strength and the direction of the relationship between the elements of two samples, and the sample correlation is derived from the covariance.	What does sample covariance mean
605	A true positive is an outcome where the model correctly predicts the positive class. Similarly, a true negative is an outcome where the model correctly predicts the negative class. A false positive is an outcome where the model incorrectly predicts the positive class.	What is false positive and true positive
5656	Hyperbolic Tangent (Sigmoid) Kernel The Sigmoid Kernel comes from the Neural Networks field, where the bipolar sigmoid function is often used as an activation function for artificial neurons.  This kernel was quite popular for support vector machines due to its origin from neural network theory.	What is a sigmoid kernel
638	The Top 5 Uses of Image Recognition#1. Automated Image Organization – from Cloud Apps to Telecoms.#2. Stock Photography and Video Websites.#3. Visual Search for Improved Product Discoverability.#4. Image Classification for Websites with Large Visual Databases.#5.  #6.  Celebrating the Power of Image Recognition.	What can you do with image recognition
7330	Such algorithms are called greedy because while the optimal solution to each smaller instance will provide an immediate output, the algorithm doesn't consider the larger problem as a whole.  Greedy algorithms work by recursively constructing a set of objects from the smallest possible constituent parts.	Why is it called greedy algorithm
378	Gaussian Noise is a statistical noise having a probability density function equal to normal distribution, also known as Gaussian Distribution. Random Gaussian function is added to Image function to generate this noise. It is also called as electronic noise because it arises in amplifiers or detectors.	What is Gaussian noise in image processing
284	In mathematics, a divergent series is an infinite series that is not convergent, meaning that the infinite sequence of the partial sums of the series does not have a finite limit. If a series converges, the individual terms of the series must approach zero.	What makes a function divergent
121	A simple random sample is a subset of a statistical population in which each member of the subset has an equal probability of being chosen. A simple random sample is meant to be an unbiased representation of a group.  Random sampling is used in science to conduct randomized control tests or for blinded experiments.	What is a random sample in statistics
4570	A mode of a continuous probability distribution is often considered to be any value x at which its probability density function has a locally maximum value, so any peak is a mode. In symmetric unimodal distributions, such as the normal distribution, the mean (if defined), median and mode all coincide.	What is the mode of a continuous random variable
6119	k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.	What is required for K means clustering
1067	It basically defined on probability estimates and measures the performance of a classification model where the input is a probability value between 0 and 1. It can be understood more clearly by differentiating it with accuracy.	What is performance measure in machine learning
1244	They are used for different purposes. Gradient descent, in its vanilla form, minimizes an unconstrained optimization problem. To handle constraints, you can use some modifications like projected gradient descent.	Does SVM use gradient descent
416	The mass density (ρ) of a substance is the mass of one unit volume of the substance.  The relative density is the ratio of the mass of the substance in air at 20 °C to that of an equal volume of water at the same temperature.	What is density and relative density
6409	Research has shown that the Wechsler test is one of the most well-designed tests to measure intelligence.  However, as most tests of this nature are, the tests are only as reliable as the person giving them.	Is the Wechsler IQ test accurate
8493	(Example: a test with 90% specificity will correctly return a negative result for 90% of people who don't have the disease, but will return a positive result — a false-positive — for 10% of the people who don't have the disease and should have tested negative.)	What is an acceptable false positive rate
2677	A manifest variable is a variable or factor that can be directly measured or observed. It is the opposite of a latent variable, which is a factor that cannot be directly observed, and which needs a manifest variable assigned to it as an indicator to test whether it is present.	What are latent and manifest variables
821	Probability is about a finite set of possible outcomes, given a probability. Likelihood is about an infinite set of possible probabilities, given an outcome.	What is likelihood in probability
1293	The fact is almost all big data sets, generated by systems powered by ML/AI based models, are known to be biased. However, most ML modelers are not aware of these biases and even if they are, they do not know what to do about it.  Most (almost all) big datasets generated by ML powered systems are biased.	Is all data biased
19	Essentially, multivariate analysis is a tool to find patterns and relationships between several variables simultaneously. It lets us predict the effect a change in one variable will have on other variables.  Multivariate analysis is also highly graphical in its approach.	What is multivariate analysis when is it used
68	Neural network activation functions are a crucial component of deep learning. Activation functions determine the output of a deep learning model, its accuracy, and also the computational efficiency of training a model—which can make or break a large scale neural network.	Why do neural networks need an activation function
1121	Message queues enable asynchronous communication, which means that the endpoints that are producing and consuming messages interact with the queue, not each other. Producers can add requests to the queue without waiting for them to be processed. Consumers process messages only when they are available.	Why do we need message queue
1136	5 Techniques to Prevent Overfitting in Neural NetworksSimplifying The Model. The first step when dealing with overfitting is to decrease the complexity of the model.  Early Stopping. Early stopping is a form of regularization while training a model with an iterative method, such as gradient descent.  Use Data Augmentation.  Use Regularization.  Use Dropouts.	How can we avoid overfitting in deep learning neural networks
1715	We will learn Classification algorithms, types of classification algorithms, support vector machines(SVM), Naive Bayes, Decision Tree and Random Forest Classifier in this tutorial.	What are the different classifiers in machine learning
3516	The short answer is yes—because most regression models will not perfectly fit the data at hand. If you need a more complex model, applying a neural network to the problem can provide much more prediction power compared to a traditional regression.	Can we use neural network for regression
6400	Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use.	What does Optimizer do in neural network
5673	Entropy is the measure of disorder in a thermodynamic system.Difference Between Enthalpy and EntropyEnthalpy is a kind of energyEntropy is a propertyIt is the sum of internal energy and flows energyIt is the measurement of the randomness of moleculesIt is denoted by symbol HIt is denoted by symbol S5 more rows	What is the difference between entropy and entropy
1558	The purpose and importance of the null hypothesis and alternative hypothesis are that they provide an approximate description of the phenomena. The purpose is to provide the researcher or an investigator with a relational statement that is directly tested in a research study.	Why is null hypothesis important
7773	The role of a fully connected layer in a CNN architecture The objective of a fully connected layer is to take the results of the convolution/pooling process and use them to classify the image into a label (in a simple classification example).	What is the function of fully connected layer of CNN
7074	A sampling frame is a list or other device used to define a researcher's population of interest. The sampling frame defines a set of elements from which a researcher can select a sample of the target population.	What is the meaning of sample frame
5156	Some additional simple scoring methods include:Counts. Count the number of times each word appears in a document.Frequencies. Calculate the frequency that each word appears in a document out of all the words in the document.	How do you calculate bag words
246	Similar to supervised learning, a neural network can be used in a way to train on unlabeled data sets. This type of algorithms are categorized under unsupervised learning algorithms and are useful in a multitude of tasks such as clustering.	Can neural networks be used for unsupervised learning
7479	Nearly 150 years ago, Charles Darwin proposed that morality was a byproduct of evolution, a human trait that arose as natural selection shaped man into a highly social species—and the capacity for morality, he argued, lay in small, subtle differences between us and our closest animal relatives.	How did morality evolve
8343	Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.	How is a normal distribution related to a probability distribution
119	In probability theory and statistics, the moment-generating function of a real-valued random variable is an alternative specification of its probability distribution.  There are particularly simple results for the moment-generating functions of distributions defined by the weighted sums of random variables.	What is meant by moment generating function
7672	Markov chains are an important concept in stochastic processes. They can be used to greatly simplify processes that satisfy the Markov property, namely that the future state of a stochastic variable is only dependent on its present state.	What is the use of Markov chain
7761	Cautious and uncertain, AI systems will seek additional information and learn to navigate the confusing situations they encounter. Of course, self-driving cars shouldn't have to ask questions. If a car's image detection spots a foreign object up ahead, for instance, it won't have time to ask humans for help.	How do AI systems deal with uncertainty
682	Parametric statistics generally require interval or ratio data. An example of this type of data is age, income, height, and weight in which the values are continuous and the intervals between values have meaning. In contrast, nonparametric statistics are typically used on data that nominal or ordinal.	Which types of data are normally used with nonparametric statistics
508	Critic Loss: D(x) - D(G(z)) The discriminator tries to maximize this function. In other words, it tries to maximize the difference between its output on real instances and its output on fake instances.	What is discriminator loss
4354	Hyperparameters are the variables which determines the network structure(Eg: Number of Hidden Units) and the variables which determine how the network is trained(Eg: Learning Rate). Hyperparameters are set before training(before optimizing the weights and bias).	What is the role of Hyperparameters in deep learning
7126	The median filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image).	Is median filter linear
6221	"ROC curves in logistic regression are used for determining the best cutoff value for predicting whether a new observation is a ""failure"" (0) or a ""success"" (1).  Your observed outcome in logistic regression can ONLY be 0 or 1. The predicted probabilities from the model can take on all possible values between 0 and 1."	What is ROC in logistic regression
2698	Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events. You might recall that information quantifies the number of bits required to encode and transmit an event.	What does cross entropy mean
106	A and B are mutually exclusive events if they cannot occur at the same time. This means that A and B do not share any outcomes and P(A AND B) = 0. For example, suppose the sample space S = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}.	How do you know if A and B is mutually exclusive
951	The binomial theorem is an algebraic method of expanding a binomial expression. Essentially, it demonstrates what happens when you multiply a binomial by itself (as many times as you want). For example, consider the expression (4x+y)7 ( 4 x + y ) 7 .	What is binomial expansion method
7932	In statistics, the likelihood-ratio test assesses the goodness of fit of two competing statistical models based on the ratio of their likelihoods, specifically one found by maximization over the entire parameter space and another found after imposing some constraint.	What does the likelihood ratio test tell us
415	Here is a simpler rule: If two SEM error bars do overlap, and the sample sizes are equal or nearly equal, then you know that the P value is (much) greater than 0.05, so the difference is not statistically significant.  If the sample sizes are very different, this rule of thumb does not always work.	What can be concluded if a difference between two samples is not statistically significant
6461	Most implementations of random forest (and many other machine learning algorithms) that accept categorical inputs are either just automating the encoding of categorical features for you or using a method that becomes computationally intractable for large numbers of categories. A notable exception is H2O.	Can random forest handle categorical variables
7353	In particular, a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space.	What is meant by random experiment
4892	The best fit line is the one that minimises sum of squared differences between actual and estimated results. Taking average of minimum sum of squared difference is known as Mean Squared Error (MSE). Smaller the value, better the regression model.	How do you tell if a regression model is a good fit
4248	Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.	Is Adam Optimizer gradient descent
1647	Joint probability is the probability of two events occurring simultaneously. Marginal probability is the probability of an event irrespective of the outcome of another variable. Conditional probability is the probability of one event occurring in the presence of a second event.	What is the difference between conditional probability and joint probability
575	Asymptotic Analysis is the big idea that handles above issues in analyzing algorithms. In Asymptotic Analysis, we evaluate the performance of an algorithm in terms of input size (we don't measure the actual running time). We calculate, how the time (or space) taken by an algorithm increases with the input size.	How asymptotic analysis can be used to assess the effectiveness of an algorithm
1297	Each sample contains different elements so the value of the sample statistic differs for each sample selected. These statistics provide different estimates of the parameter. The sampling distribution describes how these different values are distributed.	What is the difference between a sample and a sampling distribution
1598	Clustering and Association are two types of Unsupervised learning.  Important clustering types are: 1)Hierarchical clustering 2) K-means clustering 3) K-NN 4) Principal Component Analysis 5) Singular Value Decomposition 6) Independent Component Analysis.	Which of the following is an unsupervised learning algorithm
3166	The three main metrics used to evaluate a classification model are accuracy, precision, and recall. Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.	How do you evaluate machine learning models
4556	The standard error can be used to gauge the precision of a statistical estimate or to permit a judgement being made of the divergence between expected and observed values.  clearly the concept of standard error of an estimate and its various uses in practice.	Can the standard error be used to gauge the precision of a statistical estimate or to permit a judgement being made of divergence between expected and observed values
5649	Systematic errors are biases in measurement which lead to a situation wherein the mean of many separate measurements differs significantly from the actual value of the measured attribute in one direction. A systematic error makes the measured value always smaller or larger than the true value, but not both.	How error affects the measurement
837	So, to find the residual I would subtract the predicted value from the measured value so for x-value 1 the residual would be 2 - 2.6 = -0.6. Mentor: That is right! The residual of the independent variable x=1 is -0.6.	How do you find the residual in multiple regression
6014	Here are 11 tips for making the most of your large data sets.Cherish your data. “Keep your raw data raw: don't manipulate it without having a copy,” says Teal.  Visualize the information.Show your workflow.  Use version control.  Record metadata.  Automate, automate, automate.  Make computing time count.  Capture your environment.More items•	How do you handle large datasets
264	Classification SVM Type 1 (also known as C-SVM classification); Classification SVM Type 2 (also known as nu-SVM classification); Regression SVM Type 1 (also known as epsilon-SVM regression); Regression SVM Type 2 (also known as nu-SVM regression).	What are the two classification methods that SVM can handle
3456	Convenience sampling (also known as grab sampling, accidental sampling, or opportunity sampling) is a type of non-probability sampling that involves the sample being drawn from that part of the population that is close to hand. This type of sampling is most useful for pilot testing.	What is convenience sampling used for
2924	Linear Regression Is Limited to Linear Relationships By its nature, linear regression only looks at linear relationships between dependent and independent variables. That is, it assumes there is a straight-line relationship between them.	What are the limitations of linear regression
4147	A derivative is a continuous description of how a function changes with small changes in one or multiple variables. We're going to look into many aspects of that statement. For example.	What is a derivative in machine learning
5330	Regression analysis is a form of inferential statistics. The p-values help determine whether the relationships that you observe in your sample also exist in the larger population. The p-value for each independent variable tests the null hypothesis that the variable has no correlation with the dependent variable.	What is the P value in regression
1246	From Simple English Wikipedia, the free encyclopedia. The entropy of an object is a measure of the amount of energy which is unavailable to do work. Entropy is also a measure of the number of possible arrangements the atoms in a system can have. In this sense, entropy is a measure of uncertainty or randomness.	What is entropy in simple terms
6768	Sampling Distribution of Sample Variance This is the variance of the population. The variance of this sampling distribution can be computed by finding the expected value of the square of the sample variance and subtracting the square of 2.92.	What is the distribution of sample variance
1289	The chi-square distribution is used in the common chi-square tests for goodness of fit of an observed distribution to a theoretical one, the independence of two criteria of classification of qualitative data, and in confidence interval estimation for a population standard deviation of a normal distribution from a	What is the chi square distribution used for
4749	Yes, this is possible and I have heard it termed as joint regression or multivariate regression.  Regression analysis involving more than one independent variable and more than one dependent variable is indeed (also) called multivariate regression. This methodology is technically known as canonical correlation analysis.	Can you use more than one independent variable in regression models
4728	Ensemble learning is the process by which multiple models, such as classifiers or experts, are strategically generated and combined to solve a particular computational intelligence problem. Ensemble learning is primarily used to improve the (classification, prediction, function approximation, etc.)	What is Ensemble Learning explain with example
964	The quantizing of an analog signal is done by discretizing the signal with a number of quantization levels. Quantization is representing the sampled values of the amplitude by a finite set of levels, which means converting a continuous-amplitude sample into a discrete-time signal.	How quantization is done
5948	An SVM performs classification tasks by constructing hyperplanes in a multidimensional space that separates cases of different class labels. You can use an SVM when your data has exactly two classes, e.g. binary classification problems, but in this article we'll focus on a multi-class support vector machine in R.	How SVM can be used for multi class classification
191	The k-means clustering algorithm uses the Euclidean distance [1,4] to measure the similarities between objects. Both iterative algorithm and adaptive algorithm exist for the standard k-means clustering. K-means clustering algorithms need to assume that the number of groups (clusters) is known a priori.	Which distance function is used in K means clustering
5890	Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater.	What is loss in machine learning
1370	The most popular is definitely KMP, if you need fast string matching without any particular usecase in mind it's what you should use. Here are your options(with time complexity): Brute Force O(nm) Knuth–Morris–Pratt algorithm - O(n)	Which is the best algorithm for checking string similarity metric
392	Geoffrey HintonGeoffrey Hinton CC FRS FRSCHinton in 2013BornGeoffrey Everest Hinton 6 December 1947 Wimbledon, LondonAlma materUniversity of Cambridge (BA) University of Edinburgh (PhD)Known forApplications of Backpropagation Boltzmann machine Deep learning Capsule neural network10 more rows	Who invented backpropagation algorithm
1307	Logistic regression is basically a supervised classification algorithm. In a classification problem, the target variable(or output), y, can take only discrete values for given set of features(or inputs), X. Contrary to popular belief, logistic regression IS a regression model.	Is logistic regression actually a regression algorithm
4932	A threshold transfer function is sometimes used to quantify the output of a neuron in the output layer.  All possible connections between neurons are allowed. Since loops are present in this type of network, it becomes a non-linear dynamic system which changes continuously until it reaches a state of equilibrium.	What is threshold function in neural network
1285	The normal distribution is a probability distribution. As with any probability distribution, the proportion of the area that falls under the curve between two points on a probability distribution plot indicates the probability that a value will fall within that interval.	How is the concept of probability related to the normal distribution
408	Advertisements. Bayesian classification is based on Bayes' Theorem. Bayesian classifiers are the statistical classifiers. Bayesian classifiers can predict class membership probabilities such as the probability that a given tuple belongs to a particular class.	What are Bayesian classifiers in data mining
890	Significance levels The convention in most biological research is to use a significance level of 0.05. This means that if the P value is less than 0.05, you reject the null hypothesis; if P is greater than or equal to 0.05, you don't reject the null hypothesis.	What does it mean to reject the null hypothesis at the .05 level
6900	The lower quartile, or first quartile, is denoted as Q1 and is the middle number that falls between the smallest value of the dataset and the median. The second quartile, Q2, is also the median.	Is median the same with second quartile
1243	A discrete distribution is one in which the data can only take on certain values, for example integers. A continuous distribution is one in which data can take on any value within a specified range (which may be infinite).	What is discrete distributions and continuous distributions
1394	The equation of a hyperplane is w · x + b = 0, where w is a vector normal to the hyperplane and b is an offset.	How do you calculate Hyperplane
7782	In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression.	What is a feature set in machine learning
2414	In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.  Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex.	What is meant by convolutional neural network
1882	Another most important role of training data for machine learning is classifying the data sets into various categorized which is very much important for supervised machine learning.  It helps them to recognize and classify the similar objects in future, thus training data is very important for such classification.	Why is data important for machine learning
6243	The equation of a hyperplane is w · x + b = 0, where w is a vector normal to the hyperplane and b is an offset.	What is the equation of hyperplane
5779	As long as the growth factor used is assumed to be normally distributed (as we assume with the rate of return), then the lognormal distribution makes sense. Normal distribution cannot be used to model stock prices because it has a negative side, and stock prices cannot fall below zero.	Do stock prices follow a normal distribution
7484	Representation is basically the space of allowed models (the hypothesis space), but also takes into account the fact that we are expressing models in some formal language that may encode some models more easily than others (even within that possible set).	What is representation in machine learning
3299	Definition: In simple words, data mining is defined as a process used to extract usable data from a larger set of any raw data. It implies analysing data patterns in large batches of data using one or more software.  Data mining is also known as Knowledge Discovery in Data (KDD).	What does data mining mean
968	Predictive ModelingClean the data by removing outliers and treating missing data.Identify a parametric or nonparametric predictive modeling approach to use.Preprocess the data into a form suitable for the chosen modeling algorithm.Specify a subset of the data to be used for training the model.More items	How do you do predictive modeling
2610	Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). These new reduced set of features should then be able to summarize most of the information contained in the original set of features.	Why feature extraction is done
3214	The type of inference exhibited here is called abduction or, somewhat more commonly nowadays, Inference to the Best Explanation.1.1 Deduction, induction, abduction. Abduction is normally thought of as being one of three major types of inference, the other two being deduction and induction.  1.2 The ubiquity of abduction.	What are the two basic types of inferences
4688	Top reasons to use feature selection are: It enables the machine learning algorithm to train faster. It reduces the complexity of a model and makes it easier to interpret. It improves the accuracy of a model if the right subset is chosen.	Why do we need feature selection in machine learning
841	In cluster sampling, researchers divide a population into smaller groups known as clusters.You thus decide to use the cluster sampling method.Step 1: Define your population.  Step 2: Divide your sample into clusters.  Step 3: Randomly select clusters to use as your sample.  Step 4: Collect data from the sample.	How do you use cluster sampling
3124	In computer science and machine learning, pattern recognition is a technology that matches the information stored in the database with the incoming data.	What is pattern recognition in machine learning
252	The relationship between correlation coefficient and a scatterplot is that the two of them describe how similar the variables are.  A scatterplot that looks like a blobby thing without direction has a correlation coefficient closer to 0, meaning the two variables aren't correlated.	In what way are a scatterplot and a correlation coefficient similar
418	TensorFlow applications can be run on most any target that's convenient: a local machine, a cluster in the cloud, iOS and Android devices, CPUs or GPUs. If you use Google's own cloud, you can run TensorFlow on Google's custom TensorFlow Processing Unit (TPU) silicon for further acceleration.	Where can I use TensorFlow
1802	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve.	Why data should be normally distributed
1260	Semi-supervised learning takes a middle ground. It uses a small amount of labeled data bolstering a larger set of unlabeled data. And reinforcement learning trains an algorithm with a reward system, providing feedback when an artificial intelligence agent performs the best action in a particular situation.	Is reinforcement learning considered a branch of semi supervised learning
8573	AlphaGo Zero is a version of DeepMind's Go software AlphaGo.  By playing games against itself, AlphaGo Zero surpassed the strength of AlphaGo Lee in three days by winning 100 games to 0, reached the level of AlphaGo Master in 21 days, and exceeded all the old versions in 40 days.	What is significant about Alpha Go Zero
866	How to Perform Systematic Sampling: StepsStep 1: Assign a number to every element in your population.  Step 2: Decide how large your sample size should be.  Step 3: Divide the population by your sample size.  Step 1: Assign a number to every element in your population.Step 2: Decide how large your sample size should be.More items•	How do you calculate sample size in systematic sampling
3556	A type of research design where one sample is drawn from the population of interest only once.	What is a single cross sectional study
44	I guess if you squint at it sideways, binary search is greedy in the sense that you're trying to cut down your search space by as much as you can in each step. It just happens to be a greedy algorithm in a search space with structure making that both efficient, and always likely to find the right answer.	Is binary search a greedy algorithm
5416	Moran's I is a correlation coefficient that measures the overall spatial autocorrelation of your data set. In other words, it measures how one object is similar to others surrounding it. If objects are attracted (or repelled) by each other, it means that the observations are not independent.	What does Moran's I measure
600	Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms.	How are statistics and data science related
583	In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.	What is Hyperparameter tuning in machine learning
573	Averaging Likert Responses Because Likert and Likert-like survey questions are neatly ordered with numerical responses, it's easy and tempting to average them by adding the numeric value of each response, and then dividing by the number of respondents.	How do you calculate average Likert scale results
4616	"A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents.  Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning."	What is multi agent system in AI
1166	Exploratory structural equation modeling (ESEM) is an approach for analysis of latent variables using exploratory factor analysis to evaluate the measurement model.  ESEM is recommended when non-ignorable cross-factor loadings exist.	What is exploratory structural equation modeling
5383	Categories with a large difference between observed and expected values make a larger contribution to the overall chi-square statistic. In these results, the contribution values from each category sum to the overall chi-square statistic, which is 0.65.	What is the contribution to the chi square statistic
905	For quick and visual identification of a normal distribution, use a QQ plot if you have only one variable to look at and a Box Plot if you have many. Use a histogram if you need to present your results to a non-statistical public. As a statistical test to confirm your hypothesis, use the Shapiro Wilk test.	How do you check if your data is normally distributed
8007	Regularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the model's performance on the unseen data as well.	What are regularization techniques
6960	Data are rarely randomly distributed in high-dimensions and are highly correlated, often with spurious correlations. The distances between a data point and its nearest and farthest neighbours can become equidistant in high dimensions, potentially compromising the accuracy of some distance-based analysis tools.	What are the implications of using highly dimensional data
498	Lift can be found by dividing the confidence by the unconditional probability of the consequent, or by dividing the support by the probability of the antecedent times the probability of the consequent, so: The lift for Rule 1 is (3/4)/(4/7) = (3*7)/(4 * 4) = 21/16 ≈ 1.31.	How do you calculate lift in association rules
5329	Divisive Clustering: The divisive clustering algorithm is a top-down clustering approach, initially, all the points in the dataset belong to one cluster and split is performed recursively as one moves down the hierarchy.	What is divisive clustering
1263	In clustering, a group of different data objects is classified as similar objects. One group means a cluster of data. Data sets are divided into different groups in the cluster analysis, which is based on the similarity of the data. After the classification of data into various groups, a label is assigned to the group.	What are the features of cluster analysis
415	If the mean more accurately represents the center of the distribution of your data, and your sample size is large enough, use a parametric test. If the median more accurately represents the center of the distribution of your data, use a nonparametric test even if you have a large sample size.	How do you decide between parametric and nonparametric
5196	A control problem involves a system that is described by state variables.  The problem is to find a time control stratergy to make the system reach the terget state that is find conditions for application of force as a function of the control variables of the system (V,W,Th).	What is control problem
5731	Neural Networks are networks used in Machine Learning that work similar to the human nervous system. It is designed to function like the human brain where many things are connected in various ways.  There are many kinds of artificial neural networks used for the computational model.	What is neural network and its types
1413	There is no plausible way for the brain to use backpropagation.  The way in which neurons connect and communicate in the brain do not allow any mechanism that could accommodate the backpropagation principle.	Can the brain do back propagation
2731	The standard deviation is a statistic that measures the dispersion of a dataset relative to its mean and is calculated as the square root of the variance.  If the data points are further from the mean, there is a higher deviation within the data set; thus, the more spread out the data, the higher the standard deviation.	What mean standard deviation
7647	A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input.	What is the use of convolutional neural network
5746	Abstract. Markov chain Monte Carlo (MCMC) is a simulation technique that can be used to find the posterior distribution and to sample from it. Thus, it is used to fit a model and to draw samples from the joint posterior distribution of the model parameters.  The software OpenBUGS and Stan are MCMC samplers.	What is Markov Chain Monte Carlo and why it matters
1010	The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic	What is Backpropagation and how does it work
8012	The level of statistical significance is often expressed as a p-value between 0 and 1. The smaller the p-value, the stronger the evidence that you should reject the null hypothesis.  A p-value higher than 0.05 (> 0.05) is not statistically significant and indicates strong evidence for the null hypothesis.	Is the level of significance the same as the P value
297	Decision trees help you to evaluate your options. Decision Trees are excellent tools for helping you to choose between several courses of action. They provide a highly effective structure within which you can lay out options and investigate the possible outcomes of choosing those options.	What are decision trees good for
2054	Change Detection means updating the DOM whenever data is changed.  In its default strategy, whenever any data is mutated or changed, Angular will run the change detector to update the DOM. In the onPush strategy, Angular will only run the change detector when a new reference is passed to @Input() data.	What is change detection in angular
268	Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning, particularly using Google's own TensorFlow software.	What is TPU deep learning
1931	The F distribution is the probability distribution associated with the f statistic. In this lesson, we show how to compute an f statistic and how to find probabilities associated with specific f statistic values.	What does F distribution mean
440	In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.  In setting a learning rate, there is a trade-off between the rate of convergence and overshooting.	What is a learning rate in machine learning
1569	Simple Linear Regression Math by HandCalculate average of your X variable.Calculate the difference between each X and the average X.Square the differences and add it all up.  Calculate average of your Y variable.Multiply the differences (of X and Y from their respective averages) and add them all together.More items	How do you calculate linear regression by hand
2327	A Poisson process is a non-deterministic process where events occur continuously and independently of each other.  A Poisson distribution is a discrete probability distribution that represents the probability of events (having a Poisson process) occurring in a certain period of time.	What is the difference between Poisson process and Poisson distribution
4633	Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures.  Descriptive statistics are typically distinguished from inferential statistics. With descriptive statistics you are simply describing what is or what the data shows.	What is meant by descriptive statistics
3535	A posterior probability value is a prior probability value that has been a | Course Hero. Study Resources. by Textbook. by Literature Title.	What is posterior probability Coursehero
5309	7 Types of Classification AlgorithmsLogistic Regression.Naïve Bayes.Stochastic Gradient Descent.K-Nearest Neighbours.Decision Tree.Random Forest.Support Vector Machine.	What are the different classification algorithms
2659	Although side effects believed to be caused by statins can be annoying, consider the benefits of taking a statin before you decide to stop taking your medication. Remember that statin medications can reduce your risk of a heart attack or stroke, and the risk of life-threatening side effects from statins is very low.	Are statins really worth taking
192	5.2 Selector syntax A simple selector is either a type selector or universal selector followed immediately by zero or more attribute selectors, ID selectors, or pseudo-classes, in any order. The simple selector matches if all of its components match.	What is Selector and its types
3906	Characteristics of an AlgorithmUnambiguous − Algorithm should be clear and unambiguous.  Input − An algorithm should have 0 or more well-defined inputs.Output − An algorithm should have 1 or more well-defined outputs, and should match the desired output.More items	How do you write an algorithm for data structure
321	Sigmoid and tanh should not be used as activation function for the hidden layer. This is because of the vanishing gradient problem, i.e., if your input is on a higher side (where sigmoid goes flat) then the gradient will be near zero.  The best function for hidden layers is thus ReLu.	Why is the sigmoid activation function not recommended for hidden units but it is fine for an output unit
857	We find the robust standard deviation estimate by multiplying the MAD by a factor that happens to have a value close to 1.5. This gives us a robust value ('sigma- hat') of B . . If we use this method on data without outliers, it provides estimates that are close to x and s, so no harm is done.	How is robust standard deviation calculated
7220	Artificial Intelligence is the broader concept of machines being able to carry out tasks in a way that we would consider “smart”.  Machine Learning is a current application of AI based around the idea that we should really just be able to give machines access to data and let them learn for themselves.	What is difference between artificial intelligence and machine learning
964	If you have only one independent variable, R-squared(R2) remains the same. Because in single variable linear model - R2 is nothing but a square of the correlation between two variables.	How does R squared change if dependent and independent variables are switched in an ordinary linear regression
1489	Companies use machine learning models to make practical business decisions, and more accurate model outcomes result in better decisions. The cost of errors can be huge, but optimizing model accuracy mitigates that cost.  The benefits of improving model accuracy help avoid considerable time, money, and undue stress.	Why Accuracy is important in machine learning
3845	Gram matrix is simply the matrix of the inner product of each vector and its corresponding vectors in same. It found use in the current machine learning is due to deep learning loss where while style transferring the loss function is computed using the gram matrix.	What is Gram matrix in machine learning
1417	To find “q” or the studentized range statistic, refer to your table on page A-32 of your text. On the table 'k' or the number of groups is found along the top, and degrees of freedom within is down the side.	How do you find q in Tukey's HSD
1328	Estimation, in statistics, any of numerous procedures used to calculate the value of some property of a population from observations of a sample drawn from the population.  A point estimate, for example, is the single number most likely to express the value of the property.	What is the meaning of estimation in statistics
825	2 Answers. Boosting is based on weak learners (high bias, low variance).  Boosting reduces error mainly by reducing bias (and also to some extent variance, by aggregating the output from many models). On the other hand, Random Forest uses as you said fully grown decision trees (low bias, high variance).	What is the difference in bias and variance in 1 Random Forest 2 gradient boosting Why this difference
6197	When you have a statistically significant interaction, reporting the main effects can be misleading. Therefore, you will need to report the simple main effects.	Do you report main effects if there is an interaction
4829	A random effect model is a model all of whose factors represent random effects. (See Random Effects.) Such models are also called variance component models. Random effect models are often hierarchical models. A model that contains both fixed and random effects is called a mixed model.	What is random effect in mixed model
86	Order Statistics Definition Order statistics are sample values placed in ascending order. The study of order statistics deals with the applications of these ordered values and their functions. Let's say you had three weights: X1 = 22 kg, X2 = 44 kg, and X3 = 12 kg.	What is order statistics and why do we use it
6031	• A random process is a time-varying function that assigns the outcome of a random experiment to each time instant: X(t). • For a fixed (sample path): a random process is a time varying function, e.g., a signal.	What is random process in communication
4025	Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple's Siri and and Google's voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data.	What is recurrent neural network in machine learning
5955	Recall is the true positive rate, also referred to as sensitivity, measures the probability of ground truth objects being correctly detected.	What is recall in object detection
4505	When a table shows relative frequencies for different categories of a categorical variable, it is called a relative frequency table. The first table shows relative frequencies as a proportion, and the second table shows relative frequencies as a percentage.	What is a relative frequency table
149	Matrix items are ideal when an item is sold with different options. The most common example is a shirt that is available in different colors and sizes but they can be used for anything that is sold with different options. Matrix items provide benefits that simplifies the process for users and improves data accuracy.	What are matrix items
3117	Quantization of charge means that charge can take up only particular discrete values. The generally observed value of electric charge, q, of a substance is the integral multiples of e. Wherenbe the number of particles taken, e is the charge of one electron.	What is quantization of charge Class 12
678	Linear SVM is a parametric model, an RBF kernel SVM isn't, and the complexity of the latter grows with the size of the training set.  So, the rule of thumb is: use linear SVMs (or logistic regression) for linear problems, and nonlinear kernels such as the Radial Basis Function kernel for non-linear problems.	Is RBF kernel linear
3667	At the heart of this definition are three conditions, called the axioms of probability theory. Axiom 1: The probability of an event is a real number greater than or equal to 0. Axiom 2: The probability that at least one of all the possible outcomes of a process (such as rolling a die) will occur is 1.	What is axioms of probability in statistics
2124	The leading explanation: signal detection theory, which at its most basic, states that the detection of a stimulus depends on both the intensity of the stimulus and the physical/psychological state of the individual. Basically, we notice things based on how strong they are and on how much we're paying attention.	Which is best explained by signal detection theory
192	Designing partitions for query performanceLimit the size of each partition so that the query response time is within target.If you use horizontal partitioning, design the shard key so that the application can easily select the right partition.  Consider the location of a partition.	How do you partition data
2423	The function takes a loaded dataset as input and returns the dataset split into two subsets. Ideally, you can split your original dataset into input (X) and output (y) columns, then call the function passing both arrays and have them split appropriately into train and test subsets.	How do you split dataset into training and test set
2456	Pearson's correlation coefficient is the test statistics that measures the statistical relationship, or association, between two continuous variables.  It gives information about the magnitude of the association, or correlation, as well as the direction of the relationship.	What does Pearson's correlation coefficient tell you
4469	3:0910:31Suggested clip · 114 secondsLoglinear Analysis in SPSS with Assumption Testing - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do log linear analysis in SPSS
3247	The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler. For the case of simplicity we will focus on the SVD for real-valued matrices and ignore the case for complex numbers.	What does Singular Value Decomposition mean
1354	Tips for Training Recurrent Neural NetworksAdaptive learning rate. We usually use adaptive optimizers such as Adam (Kingma14) because they can better handle the complex training dynamics of recurrent networks that plain gradient descent.Gradient clipping.  Normalizing the loss.  Truncated backpropagation.  Long training time.  Multi-step loss.	How do you train a recurrent neural network
385	It allows us to operate in the original feature space without computing the coordinates of the data in a higher dimensional space.  In essence, what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions.	Why does the kernel trick work
16	Humans are error-prone and biased, but that doesn't mean that algorithms are necessarily better.  But these systems can be biased based on who builds them, how they're developed, and how they're ultimately used. This is commonly known as algorithmic bias.	Can an algorithm be biased
6041	Log-loss is an appropriate performance measure when you're model output is the probability of a binary outcome. The log-loss measure considers confidence of the prediction when assessing how to penalize incorrect classification.	What is log loss and how it helps to improve performance
2285	The very first is a Box Plot. A box plot is a graphical display for describing the distribution of data. Box plots use the median and the lower and upper quartiles. An outlier can easily be detected via Box plot where any point above or below the whiskers represent an outlier.	How does machine learning determine outliers
222	2:585:44Suggested clip · 117 secondsC1 R: Using R to Conduct a Randomization Test - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do a randomization test in R
8521	Essentially, multivariate analysis is a tool to find patterns and relationships between several variables simultaneously. It lets us predict the effect a change in one variable will have on other variables.	What is the use of multivariate analysis
186	Association rules mining is another key unsupervised data mining method, after clustering, that finds interesting associations (relationships, dependencies) in large sets of data items.	Is association rule mining unsupervised learning
3547	False positive. A false positive result is when PowerAI Vision labels or categorizes an image when it should not have. For example, categorizing an image of a cat as a dog. True negative. A true negative result is when PowerAI Vision correctly does not label or categorize an image.	What is false positive in object detection
29	Starting TensorBoardOpen up the command prompt (Windows) or terminal (Ubuntu/Mac)Go into the project home directory.If you are using Python virtuanenv, activate the virtual environment you have installed TensorFlow in.Make sure that you can see the TensorFlow library through Python.More items•	How do you use a TensorBoard
1176	"Because we use the natural exponential, we hugely increase the probability of the biggest score and decrease the probability of the lower scores when compared with standard normalization. Hence the ""max"" in softmax."	Why is exponential function used in softmax function in machine learning
1878	Here is step by step on how to compute K-nearest neighbors KNN algorithm:Determine parameter K = number of nearest neighbors.Calculate the distance between the query-instance and all the training samples.Sort the distance and determine nearest neighbors based on the K-th minimum distance.More items	How is KNN algorithm calculated
1120	The mean of the sum of squares (SS) is the variance of a set of scores, and the square root of the variance is its standard deviation. This simple calculator uses the computational formula SS = ΣX2 - ((ΣX)2 / N) - to calculate the sum of squares for a single set of scores.	How do you find the sum of squares with mean and standard deviation
6025	Loss functions and optimizations. Machines learn by means of a loss function. It's a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number.	What is loss function in deep learning
6432	The focus will especially be on applications of stochastic processes as key technologies in various research areas, such as Markov chains, renewal theory, control theory, nonlinear theory, queuing theory, risk theory, communication theory engineering and traffic engineering.	What are the applications of stochastic process
2163	Boosting differs somewhat from bagging as it does not involve bootstrap sampling. Instead models are generated sequentially and iteratively, meaning that it is necessary to have information about model before iteration is produced. Boosting was motivated by Kearns and Valiant (1989).	Does boosting use bootstrapping
8057	A normality test is used to determine whether sample data has been drawn from a normally distributed population (within some tolerance). A number of statistical tests, such as the Student's t-test and the one-way and two-way ANOVA require a normally distributed sample population.	What does a normality test show
4913	An independent event is an event in which the outcome isn't affected by another event. A dependent event is affected by the outcome of a second event.	What is the difference between events that are dependent and events that are independent
504	Another way researchers try to minimize selection bias is by conducting experimental studies, in which participants are randomly assigned to the study or control groups (i.e. randomized controlled studies or RCTs). However, selection bias can still occur in RCTs.	How can self selection bias be reduced
4650	➢ To determine the critical region for a normal distribution, we use the table for the standard normal distribution. If the level of significance is α = 0.10, then for a one tailed test the critical region is below z = -1.28 or above z = 1.28.	How do you find the critical region of a normal distribution
2254	Feature scaling is essential for machine learning algorithms that calculate distances between data.  Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.	Why is scaling important in machine learning
6006	When the data are already in digital form the 'reduction' of the data typically involves some editing, scaling, encoding, sorting, collating, and producing tabular summaries. When the observations are discrete but the underlying phenomenon is continuous then smoothing and interpolation are often needed.	What are data reduction techniques
1389	In the discussion above, Poisson regression coefficients were interpreted as the difference between the log of expected counts, where formally, this can be written as β = log( μx+1) – log( μx ), where β is the regression coefficient, μ is the expected count and the subscripts represent where the predictor variable, say	How do you interpret the coefficient in Poisson regression
240	Skip connections in deep architectures, as the name suggests, skip some layer in the neural network and feeds the output of one layer as the input to the next layers (instead of only the next one). As previously explained, using the chain rule, we must keep multiplying terms with the error gradient as we go backwards.	What is Skip connections in CNN
4210	"Entries in the ""Total"" row and ""Total"" column are called marginal frequencies or the marginal distribution.  Entries in the body of the table are called joint frequencies."	What is a frequency marginal distribution
7621	Accuracy = (sensitivity) (prevalence) + (specificity) (1 - prevalence). The numerical value of accuracy represents the proportion of true positive results (both true positive and true negative) in the selected population. An accuracy of 99% of times the test result is accurate, regardless positive or negative.	How do you find the accuracy of a test
7671	Researchers use convenience sampling not just because it is easy to use, but because it also has other research advantages. In pilot studies, convenience sample is usually used because it allows the researcher to obtain basic data and trends regarding his study without the complications of using a randomized sample.	Why is convenience sampling used
1820	The value to be gained from taking a decision. Net gain is calculated by adding together the expected value of each outcome and deducting the costs associated with the decision.	How do you calculate decision trees
1176	An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value.  The vertical axis represents the size of the area (total number of pixels) that is captured in each one of these zones.	What is a histogram in image processing
6095	They are continuous vs discrete distributions. A first difference is that multinomial distribution M(N,p) is discrete (it generalises binomial disrtibution) whereas Dirichlet distribution is continuous (it generalizes Beta distribution).	What makes the Dirichlet distribution different from a multinomial distribution
617	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is the difference between supervised and unsupervised learning algorithms
7463	Advantages of Spiking Neural Networks Spiking neural networks are interesting for a few reasons. First, information can be transmitted using very weak signals as rate encoding is very robust to noise. Second, they bring new learning algorithms for unsupervised learning.	Why are spiking neural networks important
1927	Basically CV<10 is very good, 10-20 is good, 20-30 is acceptable, and CV>30 is not acceptable.	What is a good coefficient of variation percentage
2190	The simplest divergence test, called the Divergence Test, is used to determine whether the sum of a series diverges based on the series's end-behavior. It cannot be used alone to determine wheter the sum of a series converges. Allow a series n that has infinitely many elements.	Why does the divergence test work
1181	You likely have run into the Softmax function, a wonderful activation function that turns numbers aka logits into probabilities that sum to one. Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes.	What is Softmax in deep learning
1485	In other words, the value of the empirical distribution function at a given point is obtained by:counting the number of observations that are less than or equal to ;dividing the number thus obtained by the total number of observations, so as to obtain the proportion of observations that is less than or equal to .	How do you find the empirical distribution function
7072	In physics, electronics, control systems engineering, and statistics, the frequency domain refers to the analysis of mathematical functions or signals with respect to frequency, rather than time.  The inverse Fourier transform converts the frequency-domain function back to the time-domain function.	What is frequency domain in Fourier Transform
7511	Which intuitively says that the probability of has to be “really high”. In other words, if your value is smaller than E[X], then the upper bound of it taking that value is 1 (basically sort of an uninteresting statement, since you already knew the upper bound was 1 or greater).	What is an intuitive explanation of Markovs inequality
3538	Linear discriminant function analysis (i.e., discriminant analysis) performs a multivariate test of differences between groups.  In addition, discriminant analysis is used to determine the minimum number of dimensions needed to describe these differences.	What is discriminant analysis in SPSS
5441	Preference bias is simply what representation(s) a supervised learning algorithm prefers. For example, a decision tree algorithm might prefer shorter, less complex trees. In other words, it is our algorithm's belief about what makes a good hypothesis.	What is preference bias
3594	How to find accuracy of ARIMA model?Problem description: Prediction on CPU utilization.  Step 1: From Elasticsearch I collected 1000 observations and exported on Python.Step 2: Plotted the data and checked whether data is stationary or not.Step 3: Used log to convert the data into stationary form.Step 4: Done DF test, ACF and PACF.More items•	How do you know if Arima model is accurate
5318	"A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve."	What is a multi agent
1557	The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative.	How do you calculate false positives and negatives
2911	Well, database normalization is the process of structuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. In simpler terms, normalization makes sure that all of your data looks and reads the same way across all records.	What does it mean to normalize data
669	Normal distribution describes continuous data which have a symmetric distribution, with a characteristic 'bell' shape. Binomial distribution describes the distribution of binary data from a finite sample. Thus it gives the probability of getting r events out of n trials.	What is the difference between normal distribution and binomial
6699	Overfitting: Good performance on the training data, poor generliazation to other data. Underfitting: Poor performance on the training data and poor generalization to other data.	What is overfitting and underfitting in machine learning 1
3011	The purpose of Causal Analysis and Resolution (CAR) is to identify causes of defects and other problems and take action to prevent them from occurring in the future. Introductory Notes The Causal Analysis and Resolution process area involves the following: Identifying and analyzing causes of defects and other problems.	What is causal analysis and resolution
6581	In statistics, a confounder (also confounding variable, confounding factor, or lurking variable) is a variable that influences both the dependent variable and independent variable, causing a spurious association. Confounding is a causal concept, and as such, cannot be described in terms of correlations or associations.	What is meant by confounding in statistics
5323	A z-test is a statistical test to determine whether two population means are different when the variances are known and the sample size is large. It can be used to test hypotheses in which the z-test follows a normal distribution. A z-statistic, or z-score, is a number representing the result from the z-test.	What does the Z test tell you
2064	Epsilon greedy policy is a way of selecting random actions with uniform distribution from a set of available actions.  This policy selects random actions in twice if the value of epsilon is 0.2. Consider a following example, There is a robot with capability to move in 4 direction. Up,down,left,right.	What is Epsilon greedy policy
315	11 Ways to Build Your Confidence and Appear More Attractive.  Always be ready to tell a good story.  Demonstrate inquisitiveness.  Practice good posture.  Stop worrying about what people think.  Eliminate negative self-talk.  Smile.  Learn from your mistakes without dwelling on them.More items•	How can a guy be more attractive and confident
5559	Use. Cluster sampling is typically used in market research. It's used when a researcher can't get information about the population as a whole, but they can get information about the clusters. For example, a researcher may be interested in data about city taxes in Florida.	When would cluster sampling be used
3095	MSE loss is used for regression tasks. As the name suggests, this loss is calculated by taking the mean of squared differences between actual(target) and predicted values.	How is loss function calculated
3567	An artificial neural network's learning rule or learning process is a method, mathematical logic or algorithm which improves the network's performance and/or training time. Usually, this rule is applied repeatedly over the network.	What are the learning rules in neural network
4344	A sampling frame is a list or other device used to define a researcher's population of interest. The sampling frame defines a set of elements from which a researcher can select a sample of the target population.	What is meant by sampling frame
7041	"Build the model on the training set and then use the test set as a holdout sample to test your trained model using the test data. Compare the predicted values with the actual values by calculating the error using measures such as the ""Mean Absolute Percent Error"" (MAPE) for example."	How can you tell if the predictive model is accurate
266	At a high level, a computer cluster is a group of two or more computers, or nodes, that run in parallel to achieve a common goal. This allows workloads consisting of a high number of individual, parallelizable tasks to be distributed among the nodes in the cluster.	What are clusters used for
1358	Coming to the debate of Artificial Intelligence Vs Human Intelligence, recent AI achievements imitate human intelligence more closely than before, however, machines are still way beyond what human brains are capable of doing.  Meanwhile, real-world scenarios need a holistic human approach.	Can artificial intelligence replace the human intelligence
1239	The training and testing error is the score that your train and test sets score using your error metrics. If your train error is low and test error high, you are likely overfitting to your train data.	When training error is very low and testing error is high that is called
351	What I understand is hidden layers are intermediate layers between the input and the output layer. These could be of various types, For example, the convolutional layer in convnets is a hidden layer. A dense layer is a kind of hidden layer where every node is connected to every other node in the next layer.	Is there a difference between hidden layer and dense layer in neural networks
4279	Find the F Statistic (the critical value for this test). The F statistic formula is: F Statistic = variance of the group means / mean of the within group variances. You can find the F Statistic in the F-Table.	How do you find the F distribution
55	Autocorrelation can cause problems in conventional analyses (such as ordinary least squares regression) that assume independence of observations. In a regression analysis, autocorrelation of the regression residuals can also occur if the model is incorrectly specified.	Why is autocorrelation a problem for times series analysis
585	Standard deviation is the deviation from the mean, and a standard deviation is nothing but the square root of the variance. Mean is an average of all set of data available with an investor or company. Standard deviation used for measuring the volatility of a stock.	What the difference between standard deviation and mean
5343	Normalization is the process of organizing data into a related table; it also eliminates redundancy and increases the integrity which improves performance of the query. To normalize a database, we divide the database into tables and establish relationships between the tables.	What is normalization and its types
4881	Definition: Stratified sampling is a type of sampling method in which the total population is divided into smaller groups or strata to complete the sampling process. The strata is formed based on some common characteristics in the population data.	What is a strata in sampling
1336	The latent space representation of our data contains all the important information needed to represent our original data point. This representation must then represent the features of the original data. In other words, the model learns the data features and simplifies its representation to make it easier to analyze.	What is latent representation
351	11 Applications of Artificial Intelligence in Business:Chatbots:  Artificial Intelligence in eCommerce:  AI to Improve Workplace Communication:  Human Resource Management:  AI in Healthcare:  Intelligent Cybersecurity:  Artificial Intelligence in Logistics and Supply Chain:  Sports betting Industry:More items•	What are the different applications of artificial intelligence
5781	The t-test is a method that determines whether two populations are statistically different from each other, whereas ANOVA determines whether three or more populations are statistically different from each other.	How is t test different from Anova
3639	The Akaike information criterion (AIC) is a mathematical method for evaluating how well a model fits the data it was generated from. In statistics, AIC is used to compare different possible models and determine which one is the best fit for the data.	What does Akaike information criterion mean
295	Random Forest Algorithm The Random Forest ML Algorithm is a versatile supervised learning algorithm that's used for both classification and regression analysis tasks.	Which algorithms can be used for both classification and regression tasks
4259	However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning. Instead, if you want to learn deep learning then you can go straight to learning the deep learning models if you want to.	Can I directly learn deep learning
3881	All RNNs have feedback loops in the recurrent layer. This lets them maintain information in 'memory' over time.  LSTM networks are a type of RNN that uses special units in addition to standard units. LSTM units include a 'memory cell' that can maintain information in memory for long periods of time.	How is Lstm different from RNN
5780	It turns out that it is easy to calculate the expected number of errors: it is the sum of the error probabilities. The most probable number of errors (E*) is also easy to calculate. First calculate E = expected errors = sum P_e. Then round down to the nearest integer, and this is the most probable number of errors.	How do you calculate expected error
4941	In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4. 5 algorithm, and is typically used in the machine learning and natural language processing domains.	What is id3 algorithm and how do we use it in decision tree regression
6793	Expert System is an application using AI to build a knowledge base and use that knowledge base to solve such problems where human experts are needed to solve the problem. Artificial Intelligence targets to make machines intelligent.  Expert System is an application using Artificial Intelligence.	What is the difference between artificial intelligence and expert system
1165	A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.	What is convolutional neural network algorithm
6242	Chebyshev's inequality says that at least 1−1K2 of data from a sample must fall within K standard deviations from the mean, where K is any positive real number greater than one.	What is K in Chebyshev's inequality
2857	Abstract. Dunn's test is the appropriate nonparametric pairwise multiple- comparison. procedure when a Kruskal–Wallis test is rejected, and it is now im- plemented for Stata in the dunntest command. dunntest produces multiple com- parisons following a Kruskal–Wallis k-way test by using Stata's built-in kwallis command.	What is a Dunn test
134	Linear Discriminant Analysis or Normal Discriminant Analysis or Discriminant Function Analysis is a dimensionality reduction technique which is commonly used for the supervised classification problems. It is used for modeling differences in groups i.e. separating two or more classes.	What is linear discriminant analysis in machine learning
988	Logarithms are defined as the solutions to exponential equations and so are practically useful in any situation where one needs to solve such equations (such as finding how long it will take for a population to double or for a bank balance to reach a given value with compound interest).	What is a logarithm and why is it useful
873	Characteristics of Normal Distribution Here, we see the four characteristics of a normal distribution. Normal distributions are symmetric, unimodal, and asymptotic, and the mean, median, and mode are all equal.	What are the 4 properties of a normal distribution
7820	A scatterplot displays the strength, direction, and form of the relationship between two quantitative variables. A correlation coefficient measures the strength of that relationship.  The correlation r measures the strength of the linear relationship between two quantitative variables.	How is a linear relationship between two variables measured in statistics
7540	KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).	How does the K Nearest Neighbour algorithm work
2291	Natural Language Processing (NLP) is a branch of Artificial Intelligence (AI) that studies how machines understand human language. Its goal is to build systems that can make sense of text and perform tasks like translation, grammar checking, or topic classification.	What type of machine learning is NLP
307	Values near −1 indicate a strong negative linear relationship, values near 0 indicate a weak linear relationship, and values near 1 indicate a strong positive linear relationship. The correlation is an appropriate numerical measure only for linear relationships and is sensitive to outliers.	How can you tell the difference between a strong linear association and a weak linear association
1059	The sample variance will always be a smaller value than the population variance. The sample variance will always be a larger value than the population variance.	How is the sample variance computed differently from the population variance
2139	The probability density function f(x), abbreviated pdf, if it exists, is the derivative of the cdf. Each random variable X is characterized by a distribution function FX(x).	Is the PDF the derivative of the CDF
3581	Extrapolation is a statistical method beamed at understanding the unknown data from the known data. It tries to predict future data based on historical data. For example, estimating the size of a population after a few years based on the current population size and its rate of growth.	What is extrapolation method
3207	The degree of freedom is not a property of the distribution, it's the name of the distribution. It refers to the number of degrees of freedom of some variable that has the distribution.	Statistics academic discipline What is an intuitive explanation of degrees of freedom
1211	The range of ReLu is [0, inf). This means it can blow up the activation.  Imagine a network with random initialized weights ( or normalised ) and almost 50% of the network yields 0 activation because of the characteristic of ReLu ( output 0 for negative values of x ).	What is the output range of ReLU activation function
290	Unsupervised: Given only samples X of the data, we compute a function f such that y = f(X) is “simpler”. Clustering: y is discrete • Y is continuous: Matrix factorization, Kalman filtering, unsupervised neural networks. Unsupervised: Cluster some hand-written digit data into 10 classes.	Is matrix factorization supervised or unsupervised
7751	Word2Vec takes texts as training data for a neural network. The resulting embedding captures whether words appear in similar contexts. GloVe focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together.	What is the difference between word2vec and GloVe
4864	The measurable variable, as the name suggests, is the variable that is measured in an experiment. It is the dependent variable (DV), which depends on changes to the independent variable (IV). Any experiment studies the effects on the DV resulting from changes to the IV.	Is an independent variable measurable
6853	A consistent learning algorithm is simply required to output a hypothesis that is consistent with all the training data provided to it.  This notion of consistency is closely related to the empirical risk minimisation principle in the machine learning literature, where the risk is defined using the zero-one loss.	What is consistency in machine learning
7818	We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks.	Can generative models generate adversarial examples
8230	Stochastic Gradient Descent (SGD): Hence, in Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration.  This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration.	How does stochastic gradient descent work
8077	The experiment results show that the accuracy of the model performance has a significant improvement by using hyperparameter optimization algorithms. Both Bayesian optimization and grid search perform almost equally well. However, Bayesian optimization runs faster than grid search.	Does Bayesian hyperparameter tuning give better accuracy results then the more traditional grid search in machine learning
6678	Training. AlphaGo Zero's neural network was trained using TensorFlow, with 64 GPU workers and 19 CPU parameter servers.  In the first three days AlphaGo Zero played 4.9 million games against itself in quick succession.	Does AlphaGo use TensorFlow
363	Rotation-invariant convolutional neural networks [9] rotate the input images in different angles, than compute different images with the same convolutional filters, the output feature maps of those are than concatenated together, Page 3 and one or more dense layers are stacked on top of it to achieve rotation	How do convolutional neural networks for images achieve rotational invariance
5784	For discrete data key distributions are: Bernoulli, Binomial, Poisson and Multinomial.	What are the types of discrete distributions
321	2. HIDDEN MARKOV MODELS. A hidden Markov model (HMM) is a statistical model that can be used to describe the evolution of observable events that depend on internal factors, which are not directly observable. We call the observed event a `symbol' and the invisible factor underlying the observation a `state'.	What are hidden Markov models used for
3628	Linear regression attempts to model the relationship between two variables by fitting a linear equation (= a straight line) to the observed data.  If you have a hunch that the data follows a straight line trend, linear regression can give you quick and reasonably accurate results.	What is linear regression in simple terms
8306	Huber loss is convex, differentiable, and also robust to outliers.	Is Huber loss convex
162	The parameter lambda is called as the regularization parameter which denotes the degree of regularization.  This is mainly because the weight W has a lot of parameters ( each neuron of each hidden layer ) while b has just one parameter which means the biases typically require less data than the weights to ﬁt accurately.	What is lambda in neural network
7553	0:007:21Suggested clip · 102 secondsBayesian posterior sampling - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you sample a posterior distribution
1568	Compressed sensing addresses the issue of high scan time by enabling faster acquisition by measuring fewer Fourier coefficients. This produces a high-quality image with relatively lower scan time.	How does compressed sensing work
6726	A disadvantage is when researchers can't classify every member of the population into a subgroup. Stratified random sampling is different from simple random sampling, which involves the random selection of data from the entire population so that each possible sample is equally likely to occur.	What are the disadvantages of stratified sampling
4969	: a principle of choice for a decision problem: one should choose the action which minimizes the loss that can be suffered even under the worst circumstances.	What is the Minimax principle
5581	Gini index < 0.2 represents perfect income equality, 0.2–0.3 relative equality, 0.3–0.4 adequate equality, 0.4–0.5 big income gap, and above 0.5 represents severe income gap.	What does a Gini coefficient of 0.3 mean
8628	To calculate the total variance, you would subtract the average actual value from each of the actual values, square the results and sum them. From there, divide the first sum of errors (explained variance) by the second sum (total variance), subtract the result from one, and you have the R-squared.	How do you calculate R squared value
735	0:0411:21Suggested clip · 104 secondsThe Binomial Theorem - Example 1 - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you solve a binomial theorem
5833	Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent is simply used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible.	What is the use of gradient descent in machine learning
5131	Survival analysis is a branch of statistics for analyzing the expected duration of time until one or more events happen, such as death in biological organisms and failure in mechanical systems.  Even in biological problems, some events (for example, heart attack or other organ failure) may have the same ambiguity.	What is survival analysis statistics
6450	"The normal distribution, commonly known as the bell curve, occurs throughout statistics. It is actually imprecise to say ""the"" bell curve in this case, as there are an infinite number of these types of curves. Above is a formula that can be used to express any bell curve as a function of x."	What is the equation of a bell curve
2062	When a sampling unit is drawn from a finite population and is returned to that population, after its characteristic(s) have been recorded, before the next unit is drawn, the sampling is said to be “with replacement”.	What is sampling with replacement in statistics
2021	Feature extraction is process of computing preselected features of EMG signals to be fed to a processing scheme (such as classifier) to improve the performance of the EMG based control system.	What is feature extraction in signal processing
4427	Nodes are then organized into layers to comprise a network. A single-layer artificial neural network, also called a single-layer, has a single layer of nodes, as its name suggests. Each node in the single layer connects directly to an input variable and contributes to an output variable.	What are layers in a neural network
1134	A machine-learning algorithm that involves a Gaussian process uses lazy learning and a measure of the similarity between points (the kernel function) to predict the value for an unseen point from training data.	What is Gaussian process in machine learning
3774	The main types of probability sampling methods are simple random sampling, stratified sampling, cluster sampling, multistage sampling, and systematic random sampling.	What are the kinds of random sampling
6331	The Pearson's correlation coefficient is calculated as the covariance of the two variables divided by the product of the standard deviation of each data sample. It is the normalization of the covariance between the two variables to give an interpretable score.	How do you find the correlation between many variables
902	An affine layer, or fully connected layer, is a layer of an artificial neural network in which all contained nodes connect to all nodes of the subsequent layer. Affine layers are commonly used in both convolutional neural networks and recurrent neural networks.	What is an affine layer in machine learning
5978	Random Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.	Is Random Forest an ensemble method
691	Cohen's kappa.  Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.	What is a good kappa statistic
1134	When to use it Use Spearman rank correlation when you have two ranked variables, and you want to see whether the two variables covary; whether, as one variable increases, the other variable tends to increase or decrease.	Why do we use Spearman rank correlation
268	"In this way the keypoint is ""orientation invariant"" in the sense that if the same keypoint were found in a rotated image, the dominant orientation alignment/subtraction would guarantee the same (or similar) set of orientation histograms and therefore, keypoint signature."	How does sift achieve rotation invariance
3544	Sensitivity or the true positive rate is the probability that a test will result positive (indicate disease) amongst the subject with the disease. This is also a measure of the avoidance of false negatives.Variables and formulas.ConceptFormulaFalse Negative Rate100 x False Negative / (True Positive + False Negative)3 more rows•	How do you calculate a false negative
5810	And the Machine Learning – The Naïve Bayes Classifier. It is a classification technique based on Bayes' theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.	What is naive Bayes classifier in machine learning
3597	Dropout is a technique used to prevent a model from overfitting. Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase.	What is dropout layer in neural network
575	Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.  Tokenization can be done to either separate words or sentences.	Why is tokenization important in NLP
5767	The value of the z-score tells you how many standard deviations you are away from the mean. If a z-score is equal to 0, it is on the mean. A positive z-score indicates the raw score is higher than the mean average. For example, if a z-score is equal to +1, it is 1 standard deviation above the mean.	What does the Z statistic tell you
5474	When we decompose a complex problem we often find patterns among the smaller problems we create.  Pattern recognition is one of the four cornerstones of Computer Science. It involves finding the similarities or patterns among small, decomposed problems that can help us solve more complex problems more efficiently.	How does pattern recognition solve problems
3722	A random forest is simply a collection of decision trees whose results are aggregated into one final result. Their ability to limit overfitting without substantially increasing error due to bias is why they are such powerful models. One way Random Forests reduce variance is by training on different samples of the data.	Is Random Forest a decision tree
2804	Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).  Clustering can therefore be formulated as a multi-objective optimization problem.	What is meant by clustering
828	A nerve is essentially a collection of axon bundles found in the peripheral nervous system. The axons are wrapped in three layers connective tissue for protection and insulation. A neuron, on the other hand, has only one axon, it may be branched and extend in more than one direction.	What is the relationship between a nerve and a neuron quizlet
589	Log loss is used when we have {0,1} response. This is usually because when we have {0,1} response, the best models give us values in terms of probabilities. In simple words, log loss measures the UNCERTAINTY of the probabilities of your model by comparing them to the true labels.	Why do we use log loss in logistic regression
3422	Qualitative Variables - Variables that are not measurement variables. Their values do not result from measuring or counting. Examples: hair color, religion, political party, profession. Designator - Values that are used to identify individuals in a table.	What is a qualitative variable
258	S-Curves are used to visualize the progress of a project over time. They plot either cumulative work, based on person-hours, or costs over time. The name is derived from the fact that the data usually takes on an S-shape, with slower progress at the beginning and end of a project.	What is S curve used for
8312	This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction.	How do you determine when to use a one tailed or two tailed test for Anova
1893	There are four assumptions associated with a linear regression model: Linearity: The relationship between X and the mean of Y is linear. Homoscedasticity: The variance of residual is the same for any value of X. Independence: Observations are independent of each other.	What are the assumptions that must be met to use regression analysis
949	The process of training an ML model involves providing an ML algorithm (that is, the learning algorithm) with training data to learn from. The term ML model refers to the model artifact that is created by the training process.  You can use the ML model to get predictions on new data for which you do not know the target.	How are machine learning models trained
2704	Systematic Sampling Versus Cluster Sampling Cluster sampling breaks the population down into clusters, while systematic sampling uses fixed intervals from the larger population to create the sample.  Cluster sampling divides the population into clusters and then takes a simple random sample from each cluster.	What is the difference between the cluster and systematic sampling
1832	Mean rank. The mean rank is the average of the ranks for all observations within each sample. Minitab uses the mean rank to calculate the H-value, which is the test statistic for the Kruskal-Wallis test.  If two or more observations are tied, Minitab assigns the average rank to each tied observation.	What is mean rank in Kruskal Wallis test
105	To improve CNN model performance, we can tune parameters like epochs, learning rate etc..Train with more data: Train with more data helps to increase accuracy of mode. Large training data may avoid the overfitting problem.  Early stopping: System is getting trained with number of iterations.  Cross validation:	How can I improve my convolutional neural network performance
1043	When training data is split into small batches, each batch is jargoned as a minibatch.  If one updates model parameters after processing the whole training data (i.e., epoch), it would take too long to get a model update in training, and the entire training data probably won't fit in the memory.	What is a minibatch in a neural network
1835	This axiom is controversial because although it seems like a relatively intuitive idea, there are still some issues.  Also, the axiom of choice is equivalent to the statement that any set can be well-ordered, i.e., every nonempty set can be endowed with a total order such that every nonempty subset has a least element.	Why is the axiom of choice so controversial
255	13. What is the difference between unimodal, bimodal, and multimodal data? Unimodal data has a distribution that is single-peaked (one mode). Bimodal data has two peaks (2 modes) and multimodal data refer to distributions with more than two clear peaks.	What is the difference between unimodal and multimodal
6646	Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters).	How does Lasso regression work
5484	With inferential statistics, you take data from samples and make generalizations about a population.  This means taking a statistic from your sample data (for example the sample mean) and using it to say something about a population parameter (i.e. the population mean). Hypothesis tests.	Which is best described as inferential statistics
1007	Correlation coefficients are indicators of the strength of the relationship between two different variables. A correlation coefficient that is greater than zero indicates a positive relationship between two variables. A value that is less than zero signifies a negative relationship between two variables.	What do different correlation coefficients mean
6290	Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.	What is the use of tokenization in NLP
385	The objective is to reduce the error e, which is the difference between the neuron response a, and the target vector t. The perceptron learning rule learnp calculates desired changes to the perceptron's weights and biases given an input vector p, and the associated error e.	What is the objective of Perceptron learning
6454	In a normal distribution, the mean and the median are the same number while the mean and median in a skewed distribution become different numbers: A left-skewed, negative distribution will have the mean to the left of the median. A right-skewed distribution will have the mean to the right of the median.	What are some ways to distinguish between a skewed and normal distribution
64	A scatterplot displays the strength, direction, and form of the relationship between two quantitative variables. A correlation coefficient measures the strength of that relationship.  The correlation r measures the strength of the linear relationship between two quantitative variables.	How is a scatterplot similar to a correlation coefficient
302	A coefficient of correlation of +0.8 or -0.8 indicates a strong correlation between the independent variable and the dependent variable. An r of +0.20 or -0.20 indicates a weak correlation between the variables.	Is 0.8 A strong correlation
6190	The difference between interval and ratio scales comes from their ability to dip below zero. Interval scales hold no true zero and can represent values below zero. For example, you can measure temperature below 0 degrees Celsius, such as -10 degrees. Ratio variables, on the other hand, never fall below zero.	What is the difference between interval and ratio as scale of measurement
8667	A qualitative variable is a variable that expresses a quality. Values do not have numerical meaning and cannot be ordered numerically. Height, mass, age, and shoe size would all be measured in terms of numbers. So, these categories do not contain qualitative data.	Is age a qualitative variable
3194	2. HIDDEN MARKOV MODELS. A hidden Markov model (HMM) is a statistical model that can be used to describe the evolution of observable events that depend on internal factors, which are not directly observable. We call the observed event a `symbol' and the invisible factor underlying the observation a `state'.	What is hidden Markov model used for
300	The linear regression coefficients b 1 and b 3 describe the autoregressive effects, or the effect of a construct on itself measured at a later time. The autoregressive effects describe the stability of the constructs from one occasion to the next.	What are autoregressive effects
7941	"Variables that can only take on a finite number of values are called ""discrete variables."" All qualitative variables are discrete. Some quantitative variables are discrete, such as performance rated as 1,2,3,4, or 5, or temperature rounded to the nearest degree."	Which variables are discrete
4308	Measures of dispersion include: variance, standard deviation, and interquartile range. 3. 50th percentile states the value its not a measure of despersion.	Which one is the not measure of dispersion The Range 50th percentile Inter Quartile Range Variance
1117	In essence, what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions. With that saying, the application of the kernel trick is not limited to the SVM algorithm.	What is the purpose of the kernel trick
8685	We use binary cross-entropy loss for classification models which output a probability p. The range of the sigmoid function is [0, 1] which makes it suitable for calculating probability.	Which loss function is used for binary classification
2522	An operating system (OS) is a set of functions or programs that coordinate a user program's access to the computer's resources (i.e. memory and CPU).  These functions are called the MicroStamp11's kernel functions.	How kernel functions are called
4155	Pros: It is easy and fast to predict class of test data set. It also perform well in multi class prediction. When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.	Can naive Bayes be used for multiclass classification
4828	A search algorithm is applied to a state space representation to find a solution path. Each search algorithm applies a particular search strategy. If states in the solution space can be revisited more than once a directed graph is used to represent the solution space.	Which method used for state space search problem
4662	In their first layers, convolutional neural nets have 'filters'.  Then, the filter is slid (or convolved), so it is now multiplied by a different section of the input, but the filter still has the same weights.  Hence the shared weights.	ANNs How do convolutional nets use weight sharing
6125	A normal distribution is symmetrical and bell-shaped. The Empirical Rule is a statement about normal distributions. The 95% Rule states that approximately 95% of observations fall within two standard deviations of the mean on a normal distribution.	How can you use the empirical rule to describe data that are bell shaped
3554	A disadvantage is when researchers can't classify every member of the population into a subgroup. Stratified random sampling is different from simple random sampling, which involves the random selection of data from the entire population so that each possible sample is equally likely to occur.	What are the drawbacks disadvantage of stratified sampling
480	Essentially, the process goes as follows:Select k centroids. These will be the center point for each segment.Assign data points to nearest centroid.Reassign centroid value to be the calculated mean value for each cluster.Reassign data points to nearest centroid.Repeat until data points stay in the same cluster.	How do you calculate K means clustering
4952	Machine learning models require all input and output variables to be numeric. This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model. The two most popular techniques are an Ordinal Encoding and a One-Hot Encoding.	How does machine learning work with categorical data
452	Values between 0.7 and 1.0 (−0.7 and −1.0) indicate a strong positive (negative) linear relationship through a firm linear rule. It is the correlation coefficient between the observed and modelled (predicted) data values. It can increase as the number of predictor variables in the model increases; it does not decrease.	What is the good range of correlation values to include in the regression model
1131	The geometric distribution would represent the number of people who you had to poll before you found someone who voted independent. You would need to get a certain number of failures before you got your first success. If you had to ask 3 people, then X=3; if you had to ask 4 people, then X=4 and so on.	How do you use geometric distribution
1029	Shannon entropy is never negative since it is minus the logarithm of a probability between zero and one. Minus a minus yields a positive for Shannon entropy. Like thermodynamic entropy, Shannon's information entropy is an index of disorder—unexpected or surprising bits.	Can entropy be negative information theory
594	Insufficient training data is another cause of algorithmic bias. If the data used to train the algorithm are more representative of some groups of people than others, the predictions from the model may also be systematically worse for unrepresented or under-representative groups.	What causes algorithmic bias
3923	four ways	How many ways represent knowledge in artificial intelligence
1505	Abstract. Survival analysis, or more generally, time-to-event analysis, refers to a set of methods for analyzing the length of time until the occurrence of a well-defined end point of interest.  The occurrence of a well-defined event such as patient mortality is often a primary outcome in medical research.	What is a time to event analysis
1108	Unlikely to CNN, RNN learns to recognize image features across time. Although RNN can be used for image classification theoretically, only a few researches about RNN image classifier can be found.	Can RNN be used for image classification
505	An example of cluster sampling is area sampling or geographical cluster sampling. Each cluster is a geographical area. Because a geographically dispersed population can be expensive to survey, greater economy than simple random sampling can be achieved by grouping several respondents within a local area into a cluster.	What is cluster example
1759	Systematic sampling is frequently used to select a specified number of records from a computer file. Stratified sampling is commonly used probability method that is superior to random sampling because it reduces sampling error. A stratum is a subset of the population that share at least one common characteristic.	What is stratified and systematic sampling
8004	Set the equation equal to zero for each set of parentheses in the fully-factored binomial. For 2x^3 - 16 = 0, for example, the fully factored form is 2(x - 2)(x^2 + 2x + 4) = 0. Set each individual equation equal to zero to get x - 2 = 0 and x^2 + 2x + 4 = 0. Solve each equation to get a solution to the binomial.	How do you solve Binomials
2118	Time series data means that data is in a series of particular time periods or intervals. The data is considered in three types: Time series data: A set of observations on the values that a variable takes at different times. Cross-sectional data: Data of one or more variables, collected at the same point in time.	What is time series data in statistics
1948	Convolutional Neural Networks have a different architecture than regular Neural Networks.  Every layer is made up of a set of neurons, where each layer is fully connected to all neurons in the layer before. Finally, there is a last fully-connected layer — the output layer — that represent the predictions.	How does convolutional neural network work
8361	"Statistics can never ""prove"" anything. All a statistical test can do is assign a probability to the data you have, indicating the likelihood (or probability) that these numbers come from random fluctuations in sampling."	Can statistics be used to prove anything
78	Stream processing is the processing of data in motion, or in other words, computing on data directly as it is produced or received. The majority of data are born as continuous streams: sensor events, user activity on a website, financial trades, and so on – all these data are created as a series of events over time.	What is stream processing in big data
197	Put simply: random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction. Random forest has nearly the same hyperparameters as a decision tree or a bagging classifier.  Random forest adds additional randomness to the model, while growing the trees.	How does a Random Forest model work
4555	The MSE is a measure of the quality of an estimator—it is always non-negative, and values closer to zero are better.  For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated.	Is MSE equal to variance
8281	The decision of which statistical test to use depends on the research design, the distribution of the data, and the type of variable.  In general, if the data is normally distributed, parametric tests should be used. If the data is non-normal, non-parametric tests should be used.	What kind of statistical test should I use
8555	The Mean Squared Error (MSE) is a measure of how close a fitted line is to data points.  The MSE has the units squared of whatever is plotted on the vertical axis. Another quantity that we calculate is the Root Mean Squared Error (RMSE). It is just the square root of the mean square error.	What is the difference between RMSE and MSE
443	Efficiency: For an unbiased estimator, efficiency indicates how much its precision is lower than the theoretical limit of precision provided by the Cramer-Rao inequality. A measure of efficiency is the ratio of the theoretically minimal variance to the actual variance of the estimator.	What is the efficiency of an estimator
2049	12 Common Logical Fallacies and How to Debunk Them12 Common Logical Fallacies and How to Debunk Them.  Ad Hominem.  Appeal to Authority.  Bandwagon Argument, or ad populum.  The Strawman.  Circular Reasoning.  The Genetic Fallacy.  Anecdotal Evidence.More items•	What are the 12 fallacies
6546	TensorFlow provides a collection of workflows to develop and train models using Python, JavaScript, or Swift, and to easily deploy in the cloud, on-prem, in the browser, or on-device no matter what language you use. The tf. data API enables you to build complex input pipelines from simple, reusable pieces.	What is TensorFlow and how do you use it
961	Bootstrapping: When you estimate something based on another estimation. In the case of Q-learning for example this is what is happening when you modify your current reward estimation rt by adding the correction term maxa′Q(s′,a′) which is the maximum of the action value over all actions of the next state.	What is bootstrapping in reinforcement learning
7924	"The purpose of a neural network is to learn to recognize patterns in your data. Once the neural network has been trained on samples of your data, it can make predictions by detecting similar patterns in future data. Software that learns is truly ""Artificial Intelligence""."	What is the purpose of a neural network
3051	BFS vs DFS BFS stands for Breadth First Search. DFS stands for Depth First Search.  DFS(Depth First Search) uses Stack data structure. 3. BFS can be used to find single source shortest path in an unweighted graph, because in BFS, we reach a vertex with minimum number of edges from a source vertex.	What is BFS and DFS with example
5701	In terms of the courtroom example, a type I error corresponds to convicting an innocent defendant. Type II error. The second kind of error is the failure to reject a false null hypothesis as the result of a test procedure.	What is Type I and Type II error give examples
76	Logistic regression is quite different than linear regression in that it does not make several of the key assumptions that linear and general linear models (as well as other ordinary least squares algorithm based models) hold so close: (1) logistic regression does not require a linear relationship between the dependent	Does logistic regression data need to be normally distributed
1713	In statistics, the multiple comparisons, multiplicity or multiple testing problem occurs when one considers a set of statistical inferences simultaneously or infers a subset of parameters selected based on the observed values.	What is a multiple hypothesis testing problem in statistics
1353	Log likelihood is just the log of the likelihood.  The difference between two log likelihoods (on the same data) does have meaning. It is an indicator of how much better one model fits than another, and it is used in a lot of ways which, again, you can read about in books on logistic regression.	What does log likelihood mean in logistic regression
97	Genetic algorithms usually perform well on discrete data, whereas neural networks usually perform efficiently on continuous data. Genetic algorithms can fetch new patterns, while neural networks use training data to classify a network.  Genetic algorithms calculate the fitness function repeatedly to get a good solution.	What is the difference between genetic algorithm and neural network
7364	The following IIMs are part of CAP 2021:IIM Ranchi – Conducting body of CAP 2021.IIM Trichy.IIM Raipur.IIM Udaipur.IIM Kashipur.IIM Amritsar.IIM Bodh Gaya.IIM Sambalpur.	Which IIM calls are out
874	Replaces an image by the norm of its gradient, as estimated by discrete filters. The Raw filter of the detail panel designates two filters that correspond to the two components of the gradient in the principal directions.	What is a gradient norm
2886	Machine learning algorithms are almost always optimized for raw, detailed source data. Thus, the data environment must provision large quantities of raw data for discovery-oriented analytics practices such as data exploration, data mining, statistics, and machine learning.	What type of data does machine learning need
7293	Descriptive statistics summarize the characteristics of a data set. Inferential statistics allow you to test a hypothesis or assess whether your data is generalizable to the broader population.	What is the difference between descriptive and inferential statistics
2470	Here are some strategies that would aid in the effectiveness of your AI deployment:Combine machine learning automation & human evaluation with your data.  Marry the data research efforts with project management best practices.  Develop a flexible development methodology.  Centralize your AI and ML data.More items	How can we improve artificial intelligence
1859	You can use the covariance to determine the direction of a linear relationship between two variables as follows:If both variables tend to increase or decrease together, the coefficient is positive.If one variable tends to increase as the other decreases, the coefficient is negative.	How do you interpret correlation and covariance
1264	Properties. Unlike the classical conditional entropy, the conditional quantum entropy can be negative.  Positive conditional entropy of a state thus means the state cannot reach even the classical limit, while the negative conditional entropy provides for additional information.	Can conditional entropy negative
4874	Often, researchers choose significance levels equal to 0.01, 0.05, or 0.10; but any value between 0 and 1 can be used. Test method. Use the chi-square test for independence to determine whether there is a significant relationship between two categorical variables.	How do you determine the significance level in a chi square test
913	Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable.	What does linear regression tell you
418	Do you know how to choose the right machine learning algorithm among 7 different types?1-Categorize the problem.  2-Understand Your Data.  Analyze the Data.  Process the data.  Transform the data.  3-Find the available algorithms.  4-Implement machine learning algorithms.  5-Optimize hyperparameters.More items	How do you choose the best model in machine learning
2385	The coefficient of variation (CV) is a measure of relative variability. It is the ratio of the standard deviation to the mean (average). For example, the expression “The standard deviation is 15% of the mean” is a CV.	What is difference between standard deviation and coefficient of variation
4964	Binning is the process of combining charge from adjacent pixels in a CCD during readout. The two primary benefits of binning are improved signal-to-noise ratio (SNR) and the ability to increase frame rate, albeit at the expense of reduced spatial resolution.	What is binning in camera
218	Interview Answer Risk means potential threat that calls for identification and careful monitoring of KPI's.	What does risk mean to you interview question
1767	The Four Probability Rules P(A or B)=P(A)+P(B)−P(A and B)  Specifically, if event A is already known to have occurred and probability of event B is desired, then we have the following rule. P(B, given A)=P(A and B)P(A) In set notation, this is written as P(B|A)=P(A∩B)P(A).	What are the 4 laws of probability
5032	It can work on categorical data and will give you a statistical likelihood of which categorical value (or values) a cluster is most likely to take on.	Can we use clustering for categorical data
2077	Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data.  Models and algorithms based on the principle of competitive learning include vector quantization and self-organizing maps (Kohonen maps).	What is competitive learning algorithm in neural network
4278	Find a confidence level for a data set by taking half of the size of the confidence interval, multiplying it by the square root of the sample size and then dividing by the sample standard deviation. Look up the resulting Z or t score in a table to find the level.	How is confidence level calculated
908	Calculating Standard Error of the MeanFirst, take the square of the difference between each data point and the sample mean, finding the sum of those values.Then, divide that sum by the sample size minus one, which is the variance.Finally, take the square root of the variance to get the SD.	How do you find the standard error of the mean difference
1162	5:5217:59Suggested clip · 118 secondsHow to Use SPSS-Hierarchical Multiple Regression - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you control for a variable in multiple regression SPSS
1299	2:324:34Suggested clip · 65 secondsSignal Processing - 24 Convolution - Explained - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you explain convolution
7120	If X takes values in [a, b] and Y takes values in [c, d] then the pair (X, Y ) takes values in the product [a, b] × [c, d]. The joint probability density function (joint pdf) of X and Y is a function f(x, y) giving the probability density at (x, y).	How do you find the joint probability density function
4724	"Fans believe ""What if I told you"" was said by Morpheus when he was explaining the Matrix to Neo (Keanu Reeves).  Per KnowYourMeme, chances are good the ""What if I told you"" line was merely a reworded take on Morpheus' actual dialogue in the scene: ""Do you want to know what 'it' is?""."	What if I told you matrix
8228	Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.	What does tokenization mean in NLP
2427	Advantages. The main advantage of multivariate analysis is that since it considers more than one factor of independent variables that influence the variability of dependent variables, the conclusion drawn is more accurate. The conclusions are more realistic and nearer to the real-life situation.	What are the advantages of multivariate analysis
4896	The bootstrap method is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples. Importantly, samples are constructed by drawing observations from a large data sample one at a time and returning them to the data sample after they have been chosen.	How does bootstrapping work
944	The purpose of machine learning is to discover patterns in your data and then make predictions based on often complex patterns to answer business questions, detect and analyse trends and help solve problems.	What are the objectives of machine learning
6571	If we assume that there is some variation in our data, we will be able to disregard the possibility that either of these standard deviations is zero. Therefore the sign of the correlation coefficient will be the same as the sign of the slope of the regression line.	Is there a relationship between the correlation coefficient and the slope of a linear regression line
7275	Z-tests are statistical calculations that can be used to compare population means to a sample's. T-tests are calculations used to test a hypothesis, but they are most useful when we need to determine if there is a statistically significant difference between two independent sample groups.	What is the difference between z test and t test
529	Examples of Sentiment Analysis For instance, sentiment analysis may be performed on Twitter to determine overall opinion on a particular trending topic. Companies and brands often utilize sentiment analysis to monitor brand reputation across social media platforms or across the web as a whole.	What is sentiment analysis example
4319	When there is lack of domain understanding for feature introspection , Deep Learning techniques outshines others as you have to worry less about feature engineering . Deep Learning really shines when it comes to complex problems such as image classification, natural language processing, and speech recognition.	Why is deep learning so effective
471	Conditional probability is probability of a second event given a first event has already occurred.  A dependent event is when one event influences the outcome of another event in a probability scenario.	What is the difference between dependent and conditional probability
1040	Face detection algorithms typically start by searching for human eyes -- one of the easiest features to detect. The algorithm might then attempt to detect eyebrows, the mouth, nose, nostrils and the iris.  The training improves the algorithms' ability to determine whether there are faces in an image and where they are.	How do face recognition algorithms detect human faces
4577	The conversion of a frequency distribution to a probability distribution is also called an adjusted histogram. This is true for continuous random variables. To convert a frequency distribution to a probability distribution, divide area of the bar or interval of x by the total area of all the Bars.	How can a histogram be converted into a probability distribution
1423	Statistics is a very good major in terms of job market and salary scale, it also open doors for many graduate courses, unless you are poor at math ,statistics is worth taking.	Is a statistics degree useful
1221	0:172:45Suggested clip · 110 secondsStats: Complement Rule - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the complement in statistics
656	The purpose of statistical inference is to estimate this sample to sample variation or uncertainty.	What is the purpose of statistical inferences
1381	For example, a perfect precision and recall score would result in a perfect F-Measure score:F-Measure = (2 * Precision * Recall) / (Precision + Recall)F-Measure = (2 * 1.0 * 1.0) / (1.0 + 1.0)F-Measure = (2 * 1.0) / 2.0.F-Measure = 1.0.	How do you calculate precision and recall
1218	The two are different. Stoichiometry looks at balancing equations whereas dimensional analysis is looking at the units particular equations take and allowing you to make a determination of final units (and possibly the correctness of your derivation of units for any equations).	Is dimensional analysis the same thing as stoichiometry
1156	Neural networks work better at predictive analytics because of the hidden layers. Linear regression models use only input and output nodes to make predictions. Neural network also use the hidden layer to make predictions more accurate. That's because it 'learns' the way a human does.	How does a neural network predict
707	6 Freebies to Help You Increase the Performance of Your Object Detection ModelsVisually Coherent Image Mix-up for Object Detection (+3.55% mAP Boost)Classification Head Label Smoothening (+2.16% mAP Boost)Data Pre-processing (Mixed Results)Training Scheduler Revamping (+1.44% mAP Boost)More items	How can you improve the accuracy of an object detection
6256	The Linear Regression Equation The equation has the form Y= a + bX, where Y is the dependent variable (that's the variable that goes on the Y axis), X is the independent variable (i.e. it is plotted on the X axis), b is the slope of the line and a is the y-intercept.	How do you calculate linear regression
206	Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate).	What is a correlation in statistics
5315	A term document matrix is a way of representing the words in the text as a table (or matrix) of numbers. The rows of the matrix represent the text responses to be analysed, and the columns of the matrix represent the words from the text that are to be used in the analysis.	What is term document matrix algorithm
972	The Loss Function is one of the important components of Neural Networks. Loss is nothing but a prediction error of Neural Net. And the method to calculate the loss is called Loss Function. In simple words, the Loss is used to calculate the gradients. And gradients are used to update the weights of the Neural Net.	What is loss in neural network
8124	Definition. The Vector-Space Model (VSM) for Information Retrieval represents documents and queries as vectors of weights. Each weight is a measure of the importance of an index term in a document or a query, respectively.  The documents are then returned by the system by decreasing cosine.	What is vector space model in information retrieval
7351	Early stopping is a method that allows you to specify an arbitrarily large number of training epochs and stop training once the model performance stops improving on the validation dataset.	What is early stopping in neural network
917	The variance is the average of the sum of squares (i.e., the sum of squares divided by the number of observations). The standard deviation is the square root of the variance.	How do you find the variance of a sum of squares
8363	Ensemble methods are learning models that achieve performance by combining the opinions of multiple learners.  Ensemble methods are learning models that achieve performance by combining the opinions of multiple learners.	How do ensemble methods work and why are they superior to individual models
4128	LDA (Linear Discriminant Analysis) is used when a linear boundary is required between classifiers and QDA (Quadratic Discriminant Analysis) is used to find a non-linear boundary between classifiers. LDA and QDA work better when the response classes are separable and distribution of X=x for all class is normal.	What is the difference between LDA and QDA
4787	A z-score tells you how many standard deviations from the mean your result is. You can use your knowledge of normal distributions (like the 68 95 and 99.7 rule) or the z-table to determine what percentage of the population will fall below or above your result.	When should Z scores be used
1216	Time is a continuous variable. You could turn age into a discrete variable and then you could count it. For example: A person's age in years.	Is time a continuous or discrete variable
7449	Acceptance sampling is a statistical measure used in quality control. It allows a company to determine the quality of a batch of products by selecting a specified number for testing.  Acceptance sampling solves these problems by testing a representative sample of the product for defects.	What is acceptance number in sampling
7387	Today, neural networks are used for solving many business problems such as sales forecasting, customer research, data validation, and risk management. For example, at Statsbot we apply neural networks for time-series predictions, anomaly detection in data, and natural language understanding.	What can you do with a neural network
411	Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for—tasks that involve creativity and empathy among others.	What's the impact of artificial intelligence and technology on society
168	A trimmed mean is stated as a mean trimmed by x%, where x is the sum of the percentage of observations removed from both the upper and lower bounds.	How do you find the trimmed mean in statistics
6719	Each neuron in a layer receives an input from all the neurons present in the previous layer—thus, they're densely connected. In other words, the dense layer is a fully connected layer, meaning all the neurons in a layer are connected to those in the next layer.	What is a dense layer in neural networks
4083	In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.	What is Latent Dirichlet Allocation used for
8141	In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as image objects).  Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images.	What is segmentation in pattern recognition
491	Moments are a set of statistical parameters to measure a distribution. Four moments are commonly used: 1st, Mean: the average. 2d, Variance: Standard deviation is the square root of the variance: an indication of how closely the values are spread about the mean.	What are moments in statistics
3630	A significant advantage of a decision tree is that it forces the consideration of all possible outcomes of a decision and traces each path to a conclusion. It creates a comprehensive analysis of the consequences along each branch and identifies decision nodes that need further analysis.	What are the advantages of decision trees
4721	Although both techniques have certain similarities, the difference lies in the fact that classification uses predefined classes in which objects are assigned, while clustering identifies similarities between objects, which it groups according to those characteristics in common and which differentiate them from other	What is the difference between Clustering and Classification in Machine Learning
1390	If the car is behind door 1, Monty will not choose it. He'll open door 2 and show a goat 1/2 of the time. If the car is behind door 2, Monty will always open door 3, as he never reveals the car. If the car is behind door 3, Monty will open door 2 100% of the time.	What is the answer to the Monty Hall problem
2658	Techniques for Handling the Missing DataListwise or case deletion.  Pairwise deletion.  Mean substitution.  Regression imputation.  Last observation carried forward.  Maximum likelihood.  Expectation-Maximization.  Multiple imputation.More items•	How do you treat missing data
1217	All the classes may have the same class size or they may have different classes sizes depending on how you group your data. The class interval is always a whole number.	Can class intervals be different
1052	This lesson explains how to conduct a chi-square goodness of fit test. The test is applied when you have one categorical variable from a single population. It is used to determine whether sample data are consistent with a hypothesized distribution.	What does the chi square goodness of fit test actually test
3912	The macrostructure of sleep has a small but consistent correlation with intelligence, with possible nonlinear effects.  Individual differences in intelligence may either cause or be a consequence of individual differences in the macrostructure of sleep, such as timing or duration.	Is there a significant difference in the amount of sleep required based on intelligence
377	Tokenization is a common task in Natural Language Processing (NLP).  Tokens are the building blocks of Natural Language. Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords.	How does tokenization work in NLP
3818	The standard error of the regression (S), also known as the standard error of the estimate, represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable.	What does the standard error tell you in a regression
1229	The covariance between X and Y is defined as Cov(X,Y)=E[(X−EX)(Y−EY)]=E[XY]−(EX)(EY).The covariance has the following properties:Cov(X,X)=Var(X);if X and Y are independent then Cov(X,Y)=0;Cov(X,Y)=Cov(Y,X);Cov(aX,Y)=aCov(X,Y);Cov(X+c,Y)=Cov(X,Y);Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z);more generally,	How do you find the covariance of a random variable
680	A qualitative variable, also called a categorical variable, is a variable that isn't numerical. It describes data that fits into categories. For example: Eye colors (variables include: blue, green, brown, hazel).	Is colour a qualitative variable
4360	While Sensitivity measure is used to determine the proportion of actual positive cases, which got predicted correctly, Specificity measure is used to determine the proportion of actual negative cases, which got predicted correctly.	What is sensitivity and specificity in machine learning
5813	The interpretation of the odds ratio depends on whether the predictor is categorical or continuous. Odds ratios that are greater than 1 indicate that the even is more likely to occur as the predictor increases. Odds ratios that are less than 1 indicate that the event is less likely to occur as the predictor increases.	How do you interpret odds ratios for continuous variables
1753	Train Generative Adversarial Network (GAN)Load Training Data.Define Generator Network.Define Discriminator Network.Define Model Gradients, Loss Functions and Scores.Specify Training Options.Train Model.Generate New Images.More items	How do you train a generative adversarial network
4082	Machine learning is changing the world by transforming all segments including healthcare services, education, transport, food, entertainment, and different assembly line and many more. It will impact lives in almost every aspect, including housing, cars, shopping, food ordering, etc.	How is Machine Learning changing the world
523	Abstract: The generalized likelihood ratio test (GLRT), which is commonly used in composite hypothesis testing problems, is investigated. Conditions for asymptotic optimality of the GLRT in the Neyman-Pearson sense are studied and discussed.	What is GLRT
1181	A null hypothesis is a type of hypothesis used in statistics that proposes that there is no difference between certain characteristics of a population (or data-generating process). For example, a gambler may be interested in whether a game of chance is fair.	What is a null hypothesis statement
6352	For a spontaneous reaction, the sign on Delta G must be negative. Gibbs free energy relates enthalpy, entropy and temperature. A spontaneous reaction will always occur when Delta H is negative and Delta S is positive, and a reaction will always be non-spontaneous when Delta H is positive and Delta S is negative.	What is the relation between Delta H and Delta S
6064	Wilks' lambda is a measure of how well each function separates cases into groups. It is equal to the proportion of the total variance in the discriminant scores not explained by differences among the groups. Smaller values of Wilks' lambda indicate greater discriminatory ability of the function.	What does the Wilks lambda value mean
7424	The essential benefit achieved by using a rolling hash such as the Rabin fingerprint is that it is possible to compute the hash value of the next substring from the previous one by doing only a constant number of operations, independent of the substrings' lengths.	What is benefit of using rolling hash function in Rabin Karp algorithm
7894	It is not a stretch to say in a few weeks of study you could be familiar with the framework API. However, if you want to present novel ideas in deep learning through research it would likely take 1 - 4 years given your background.	How long does it take to learn deep learning
121	Instead of data distribution across several programs, ERP consolidates information in one place for better decision-making, including data accumulated from sources across the supply chain.  It also boosts the quality of a company's data, resulting in more reliable information for decision-making.	How does ERP help in decision making
4413	The first step would be to get comfortable with the concepts of permutations and combinations.  Step 1- learn permutations and combinations from 11th class NCERT.Step 2 - practice as many questions as you can on this topic .  Step 3 - once you have done that, read probability from 11th NCERT.More items	How do you master probability and statistics
550	It also makes life easier because we only need one table (the Standard Normal Distribution Table), rather than doing calculations individually for each value of mean and standard deviation.	Why do we convert normal distribution into standard normal distribution
1250	The mean squared error tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. The squaring is necessary to remove any negative signs. It also gives more weight to larger differences.	Why mean square error is used
995	p = randperm( n ) returns a row vector containing a random permutation of the integers from 1 to n without repeating elements. p = randperm( n , k ) returns a row vector containing k unique integers selected randomly from 1 to n .	What is Randperm
4534	"While machine learning is based on the idea that machines should be able to learn and adapt through experience, AI refers to a broader idea where machines can execute tasks ""smartly."" Artificial Intelligence applies machine learning, deep learning and other techniques to solve actual problems."	How does machine learning relate to AI
8377	The t-value measures the size of the difference relative to the variation in your sample data. Put another way, T is simply the calculated difference represented in units of standard error. The greater the magnitude of T, the greater the evidence against the null hypothesis.	What does the t statistic tell you
1164	The hidden-curriculum concept is based on the recognition that students absorb lessons in school that may or may not be part of the formal course of study—for example, how they should interact with peers, teachers, and other adults; how they should perceive different races, groups, or classes of people; or what ideas	What is it like when an academic culture is shaped by a hidden curriculum
912	Feature Selection. Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.	What is feature selection and feature extraction
2848	Explanation: The objective of perceptron learning is to adjust weight along with class identification.	What is the objective of Perceptron learning *
3411	Latent classes divide the cases into their respective dimensions in relation to the variable. For example, cluster analysis groups similar cases and puts them into one group. The numbers of clusters in the cluster analysis are called the latent classes. In SEM, the number of constructs is called the latent classed.	What is Latent class cluster analysis
1094	The hazard rate refers to the rate of death for an item of a given age (x). It is part of a larger equation called the hazard function, which analyzes the likelihood that an item will survive to a certain point in time based on its survival to an earlier time (t).	What does hazard rate mean
366	Each kernel function (the part) just takes in your input and compares it to some and tells you how much it matches. All those kernels each output their match amount which is then put together via a weighted linear combination. So yeah, it's template matching.	How are kernel methods glorified template matching
93	The Poisson Distribution is a tool used in probability theory statistics. It is used to test if a statement regarding a population parameter is correct. Hypothesis testing to predict the amount of variation from a known average rate of occurrence, within a given time frame.	What are the applications of Poisson distribution
264	If x is a lognormally distributed random variable, then y = ln(x) is a normally distributed random variable. The location parameter is equal to the mean of the logarithm of the data points, and the shape parameter is equal to the standard deviation of the logarithm of the data points.	How do you find the parameter of a lognormal distribution
6550	Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.	Why do we use Adam Optimizer
7306	A Seq2Seq model is a model that takes a sequence of items (words, letters, time series, etc) and outputs another sequence of items.  The encoder captures the context of the input sequence in the form of a hidden state vector and sends it to the decoder, which then produces the output sequence.	What is Seq2Seq model
734	Types of predictive modelsForecast models. A forecast model is one of the most common predictive analytics models.  Classification models.  Outliers Models.  Time series model.  Clustering Model.  The need for massive training datasets.  Properly categorising data.	What are the types of predictive models
3007	What i.i.d. assumption states is that random variables are independent and identically distributed. You can formally define what does it mean, but informally it says that all the variables provide the same kind of information independently of each other (you can read also about related exchangeability).	What is the IID assumption
1817	The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively.	How is ROC AUC calculated
208	In a hypothesis test, we:Evaluate the null hypothesis, typically denoted with H0.  Always write the alternative hypothesis, typically denoted with Ha or H1, using less than, greater than, or not equals symbols, i.e., (≠, >, or <).More items	How do you state the null and alternative hypothesis in words
2262	The normal distribution is a continuous probability distribution. This has several implications for probability. The total area under the normal curve is equal to 1. The probability that a normal random variable X equals any particular value is 0.	What is the probability density function of normal distribution
7990	Now we'll check out the proven way to improve the accuracy of a model:Add more data. Having more data is always a good idea.  Treat missing and Outlier values.  Feature Engineering.  Feature Selection.  Multiple algorithms.  Algorithm Tuning.  Ensemble methods.	How can you improve the accuracy of a linear regression model
1830	Use In Exponential Distributions It is defined as the reciprocal of the scale parameter and indicates how quickly decay of the exponential function occurs. When the rate parameter = 1, there is no decay. Values close to 1 (e.g. 0.8 or 0.9) indicate a slow decay.	What is exponential distribution rate
434	"Examine the table and note that a ""Z"" score of 0.0 lists a probability of 0.50 or 50%, and a ""Z"" score of 1, meaning one standard deviation above the mean, lists a probability of 0.8413 or 84%."	How many percent of a score distribution is between Z 0 and Z 1
7833	Therefore, the probability of committing a type II error is 2.5%.	What is the probability of making a Type II error
2620	Machine learning algorithms are almost always optimized for raw, detailed source data. Thus, the data environment must provision large quantities of raw data for discovery-oriented analytics practices such as data exploration, data mining, statistics, and machine learning.	What kind of data does machine learning use
4394	An ROC curve shows the relationship between clinical sensitivity and specificity for every possible cut-off. The ROC curve is a graph with: The x-axis showing 1 – specificity (= false positive fraction = FP/(FP+TN)) The y-axis showing sensitivity (= true positive fraction = TP/(TP+FN))	What does a ROC curve tell you
439	"The Chi-square test is intended to test how likely it is that an observed distribution is due to chance. It is also called a ""goodness of fit"" statistic, because it measures how well the observed distribution of data fits with the distribution that is expected if the variables are independent."	Why does the chi squared test work
8024	"""Correlation is not causation"" means that just because two things correlate does not necessarily mean that one causes the other.  Correlations between two things can be caused by a third factor that affects both of them."	What does it mean to say correlation doesn't imply causation
803	A distinction of sampling bias (albeit not a universally accepted one) is that it undermines the external validity of a test (the ability of its results to be generalized to the rest of the population), while selection bias mainly addresses internal validity for differences or similarities found in the sample at hand.	What is the difference between sampling bias and selection bias
609	In probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed.  A log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive.	What does log normally distributed mean
4512	The p-values is affected by the sample size. Larger the sample size, smaller is the p-values.  Increasing the sample size will tend to result in a smaller P-value only if the null hypothesis is false.	How does sample mean affect P value
444	Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.	Are generative models unsupervised
7381	The probability of committing a type II error is equal to one minus the power of the test, also known as beta. The power of the test could be increased by increasing the sample size, which decreases the risk of committing a type II error.	What is the probability of a Type II error
3792	How to find the mean of the probability distribution: StepsStep 1: Convert all the percentages to decimal probabilities. For example:  Step 2: Construct a probability distribution table.  Step 3: Multiply the values in each column.  Step 4: Add the results from step 3 together.	How do you find the mean of a probability distribution
8208	Generative Adversarial Networks takes up a game-theoretic approach, unlike a conventional neural network. The network learns to generate from a training distribution through a 2-player game. The two entities are Generator and Discriminator. These two adversaries are in constant battle throughout the training process.	How does generative adversarial networks work
1069	"For a classification problem Random Forest gives you probability of belonging to class. SVM gives you distance to the boundary, you still need to convert it to probability somehow if you need probability.  SVM gives you ""support vectors"", that is points in each class closest to the boundary between classes."	What is the difference between SVM and random forest
4424	Loss function characterizes how well the model performs over the training dataset, regularization term is used to prevent overfitting [7], and λ balances between the two. Conventionally, λ is called hyperparameter.  Different ML algorithms use different loss functions and/or regularization terms.	Is loss function a Hyperparameter
3819	If we want to indicate the uncertainty around the estimate of the mean measurement, we quote the standard error of the mean. The standard error is most useful as a means of calculating a confidence interval. For a large sample, a 95% confidence interval is obtained as the values 1.96×SE either side of the mean.	Why do we use standard error
7713	Joint probability is calculated by multiplying the probability of event A, expressed as P(A), by the probability of event B, expressed as P(B). For example, suppose a statistician wishes to know the probability that the number five will occur twice when two dice are rolled at the same time.	How do you find joint probability
569	It is a Softmax activation plus a Cross-Entropy loss.  If we use this loss, we will train a CNN to output a probability over the C classes for each image. It is used for multi-class classification.	Why is categorical cross entropy
8437	In statistics, a contingency table (also known as a cross tabulation or crosstab) is a type of table in a matrix format that displays the (multivariate) frequency distribution of the variables.  They provide a basic picture of the interrelation between two variables and can help find interactions between them.	What is contingency table in data mining
978	Deep learning is a class of machine learning algorithms that uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.	What is deep learning examples
1323	When used as nouns, quantile means one of the class of values of a variate which divides the members of a batch or sample into equal-sized subgroups of adjacent values or a probability distribution into distributions of equal probability, whereas quartile means any of the three points that divide an ordered	What is a quartile how is it related to quantile
1356	While PCA is based on Euclidean distances, PCoA can handle (dis)similarity matrices calculated from quantitative, semi-quantitative, qualitative, and mixed variables.  When the distance metric is Euclidean, PCoA is equivalent to Principal Components Analysis.	What is the difference between PCA and PCoA
1289	Linear regression models are used to show or predict the relationship between two variables or factors. The factor that is being predicted (the factor that the equation solves for) is called the dependent variable.	What does a linear model tell you
1493	A histogram (graph) of these values provides the sampling distribution of the statistic. The law of large numbers holds that as n increases, a statistic such as the sample mean (X) converges to its true mean (f)—that is, the sampling distribution of the mean collapses on the population mean.	What is the law of large numbers with respect to histograms
5287	(algorithm) The assignment of start and end times to a set of tasks, subject to certain constraints.	What is meant by task scheduling
3224	Increase the sample size. Often, the most practical way to decrease the margin of error is to increase the sample size.  Reduce variability. The less that your data varies, the more precisely you can estimate a population parameter.  Use a one-sided confidence interval.  Lower the confidence level.	How do you reduce the margin of error in statistics
6178	TL;DR: Entropy is not quantized. Entropy is often stated to be the logarithm of the number of Quantum States accessible to the system.  Entropy is often stated to be the logarithm of the number of Quantum States accessible to the system.	Is entropy quantized
7084	The area to the left of x (point of interest) is equal to probability of the x-axis variable being less than the value of x (point of interest).  The probability density is the y-axis. The PDF works for discrete and continuous data distributions.	What is the Y axis on a probability density function
6158	The law of averages is often mistaken by many people as the law of large numbers, but there is a big difference. The law of averages is a spurious belief that any deviation in expected probability will have to average out in a small sample of consecutive experiments, but this is not necessarily true.	Is the law of averages true
349	Analysis methods you might considerNegative binomial regression – Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean.  Poisson regression – Poisson regression is often used for modeling count data.More items	How do you know when to use a negative binomial distribution
3203	Regression attempts to establish how X causes Y to change and the results of the analysis will change if X and Y are swapped. With correlation, the X and Y variables are interchangeable.  Correlation is a single statistic, whereas regression produces an entire equation.	What is the univariate correlation matrix Is it different from the Pearson correlation analysis
1358	"The coefficient of determination is a measurement used to explain how much variability of one factor can be caused by its relationship to another related factor. This correlation, known as the ""goodness of fit,"" is represented as a value between 0.0 and 1.0."	What does the coefficient of determination tell you
7020	The sampling distribution assumes that the null hypothesis is true. When we compare an obtained test statistic to the sampling distribution, we're asking how likely it is that we would get that statistic if we were sampling from a population that has the null hypothesis characteristics (e.g., P = 0.50).	How is sampling distribution used in hypothesis testing
6652	Artificial neural networks (ANN) is the key tool of machine learning.  Neural networks (NN) constitute both input & output layer, as well as a hidden layer containing units that change input into output so that output layer can utilise the value.	Why is neural network NN so important in AI example
1776	On average the interpolation search makes about log(log(n)) comparisons (if the elements are uniformly distributed), where n is the number of elements to be searched. In the worst case (for instance where the numerical values of the keys increase exponentially) it can make up to O(n) comparisons.	What is the average case complexity of interpolation search
1226	In mathematics, the membership function of a fuzzy set is a generalization of the indicator function for classical sets. In fuzzy logic, it represents the degree of truth as an extension of valuation.	What is meant by membership function in fuzzy logic
967	Using the change of variable x=λy, we can show the following equation that is often useful when working with the gamma distribution: Γ(α)=λα∫∞0yα−1e−λydyfor α,λ>0.For any positive real number α:Γ(α)=∫∞0xα−1e−xdx;∫∞0xα−1e−λxdx=Γ(α)λα,for λ>0;Γ(α+1)=αΓ(α);Γ(n)=(n−1)!, for n=1,2,3,⋯;Γ(12)=√π.	How do you find gamma distribution
1794	Hypothesis Testing > Results from a statistical tests will fall into one of two regions: the rejection region— which will lead you to reject the null hypothesis, or the acceptance region, where you provisionally accept the null hypothesis.	What is acceptance and rejection region
4339	As Justin Rising points out, the order statistics are clearly not independent of each other. . If the observations are independent and identically distributed from a continuous distribution, then any ordering of the samples is equally likely.	Are the order statistics independent
3029	The Wilcoxon signed rank test is a nonparametric test that compares the median of a set of numbers against a hypothetical median. The Wilcoxon rank sum test is a nonparametric test to compare two unmatched groups. It is equivalent to the Mann-Whitney test.	What is the difference between the Wilcoxon rank sum test and the Wilcoxon signed rank test
4623	A trimmed mean (similar to an adjusted mean) is a method of averaging that removes a small designated percentage of the largest and smallest values before calculating the mean.  The use of a trimmed mean helps eliminate the influence of outliers or data points on the tails that may unfairly affect the traditional mean.	What is a trimmed mean and why is it used
7814	So, for example, if our random variable were the number obtained by rolling a fair 3-sided die, the expected value would be (1 * 1/3) + (2 * 1/3) + (3 * 1/3) = 2.	How do you find the expected value example
7037	Average Linkage is a type of hierarchical clustering in which the distance between one cluster and another cluster is considered to be equal to the average distance from any member of one cluster to any member of the other cluster.	What is average linkage clustering
8232	Data Analysis. Data Analysis is the process of systematically applying statistical and/or logical techniques to describe and illustrate, condense and recap, and evaluate data.  An essential component of ensuring data integrity is the accurate and appropriate analysis of research findings.	What is meant by data analysis
95	Deep learning is a subset of machine learning so technically machine learning is required for machine learning. However, it is not necessary for you to learn the machine learning algorithms that are not a part of machine learning in order to learn deep learning.	Does deep learning require machine learning
3324	Definition. The class intervals are the subsets into which the data is grouped. The width of the class intervals will be a compromise between having intervals short enough so that not all of the observations fall in the same interval, but long enough so that you do not end up with only one observation per interval.	What is class interval in statistics with example
2906	Typically, a regression analysis is done for one of two purposes: In order to predict the value of the dependent variable for individuals for whom some information concerning the explanatory variables is available, or in order to estimate the effect of some explanatory variable on the dependent variable.	What is the purpose of a regression model
7719	A Markov chain is a mathematical system that experiences transitions from one state to another according to certain probabilistic rules. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed.	How does Markov chain work
6567	Collaborative filtering (CF) is a technique used by recommender systems.  For example, a collaborative filtering recommendation system for television tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes).	What is collaborative filtering recommender systems
1056	TensorFlow Lite inferenceAndroid Platform.iOS Platform.Linux Platform.	Which devices support TensorFlow Lite for inference
3909	Feature Selection.  The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.	What's the difference between feature selection and feature extraction
4951	A point to remember is that the main purpose of acceptance sampling is to decide whether or not the lot is likely to be acceptable, not to estimate the quality of the lot. Acceptance sampling is employed when one or several of the following hold: Testing is destructive. The cost of 100% inspection is very high.	What is the primary reason for acceptance sampling instead of 100% inspection
8675	Multi-view data is common in real-world datasets, where different views describe distinct perspec- tives.  Multi-view data is prevalent in many real-world applications. For instance, the same news can be obtained from various language sources; an image can be described by different low level visual features.	What is multi view data
7607	To see the accuracy of clustering process by using K-Means clustering method then calculated the square error value (SE) of each data in cluster 2. The value of square error is calculated by squaring the difference of the quality score or GPA of each student with the value of centroid cluster 2.	How do you find the accuracy of K means
652	In an upper-tailed test the decision rule has investigators reject H0 if the test statistic is larger than the critical value. In a lower-tailed test the decision rule has investigators reject H0 if the test statistic is smaller than the critical value.	What is the decision rule for rejecting null hypothesis
1792	ReLu bounded negative outputs to 0 & above. This works well in hidden layers than the final output layer.  It is not typical, since in this case, the ouput value is not bounded in a range.	Why ReLu is not used in output layer
1033	The biggest advantage of Deep Learning is that we do not need to manually extract features from the image. The network learns to extract features while training. You just feed the image to the network (pixel values).	Do we require feature extraction in deep learning
2859	Type I and II Errors and Significance Levels. Rejecting the null hypothesis when it is in fact true is called a Type I error.  Most people would not consider the improvement practically significant. Caution: The larger the sample size, the more likely a hypothesis test will detect a small difference.	Does sample size affect type 1 error
6474	The main downside was that it was a pretty large network in terms of the number of parameters to be trained. VGG-19 neural network which is bigger then VGG-16, but because VGG-16 does almost as well as the VGG-19 a lot of people will use VGG-16.	Is vgg19 better than vgg16
353	How Change Detection WorksDeveloper updates the data model, e.g. by updating a component binding.Angular detects the change.Change detection checks every component in the component tree from top to bottom to see if the corresponding model has changed.If there is a new value, it will update the component's view (DOM)	How does change detection work in angular
6648	The reason for using L1 norm to find a sparse solution is due to its special shape. It has spikes that happen to be at sparse points. Using it to touch the solution surface will very likely to find a touch point on a spike tip and thus a sparse solution.	Why does l1 regularization result in sparse models
671	Adaptive Resonance Theory, or ART, algorithms overcome the computational problems of back propagation and Deep Learning.  These biological models exemplify complementary computing, and use local laws for match learning and mismatch learning that avoid the problems of Deep Learning.	Is the Adaptive Resonance theory a part of deep learning or not
3775	There are three types of proposition: fact, value and policy.	What are the two types of proposition
7022	If the hazard ratio is less than 1, then the predictor is protective (i.e., associated with improved survival) and if the hazard ratio is greater than 1, then the predictor is associated with increased risk (or decreased survival).	How do you interpret the hazard ratio in Cox Regression
700	Bayes' theorem is a formula that describes how to update the probabilities of hypotheses when given evidence. It follows simply from the axioms of conditional probability, but can be used to powerfully reason about a wide range of problems involving belief updates.	Is conditional probability the same as Bayes Theorem
8068	A classic use of a statistical test occurs in process control studies. For example, suppose that we are interested in ensuring that photomasks in a production process have mean linewidths of 500 micrometers. The null hypothesis, in this case, is that the mean linewidth is 500 micrometers.	What is an example of a statistical test
1907	With stratified sampling, the sample includes elements from each stratum. With cluster sampling, in contrast, the sample includes elements only from sampled clusters. Multistage sampling. With multistage sampling, we select a sample by using combinations of different sampling methods.	What is the difference between stratified sampling and multistage sampling
1529	2 Multivariate Data. Multivariate data contains, at each sample point, multiple scalar values that represent different simulated or measured quantities.	What is a multivariate data set
877	From Wikipedia, the free encyclopedia. Cohen's kappa coefficient (κ) is a statistic that is used to measure inter-rater reliability (and also Intra-rater reliability) for qualitative (categorical) items.	What does Kappa mean in statistics
482	The metric our intuition tells us we should maximize is known in statistics as recall, or the ability of a model to find all the relevant cases within a dataset. The precise definition of recall is the number of true positives divided by the number of true positives plus the number of false negatives.	What is recall in statistics
5153	Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.	What is the difference between feature extraction and feature selection
472	Some of the most popular methods for outlier detection are:Z-Score or Extreme Value Analysis (parametric)Probabilistic and Statistical Modeling (parametric)Linear Regression Models (PCA, LMS)Proximity Based Models (non-parametric)Information Theory Models.More items	What are the different techniques to remove outliers
1352	There is really only one advantage to using a random forest over a decision tree: It reduces overfitting and is therefore more accurate.	What is the main advantage of using a random forest over a decision tree
5115	The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer.	How do you choose the number of hidden layers in a neural network
4060	The singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values.  The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning.	What is singular value decomposition in machine learning
2980	Fixed effect factor: Data has been gathered from all the levels of the factor that are of interest.  Random effect factor: The factor has many possible levels, interest is in all possible levels, but only a random sample of levels is included in the data.	What is a fixed vs random effect
2824	If there are only two variables, one is continuous and another one is categorical, theoretically, it would be difficult to capture the correlation between these two variables.	Is it possible capture the correlation between continuous and categorical variable if yes how
7055	A point estimate of a population parameter is a single value of a statistic. For example, the sample mean x is a point estimate of the population mean μ. Similarly, the sample proportion p is a point estimate of the population proportion P.	What is the value of the sample statistic that might be used to estimate the population parameter
324	How to choose the size of the convolution filter or Kernel size1x1 kernel size is only used for dimensionality reduction that aims to reduce the number of channels. It captures the interaction of input channels in just one pixel of feature map.  2x2 and 4x4 are generally not preferred because odd-sized filters symmetrically divide the previous layer pixels around the output pixel.	How do I choose CNN filter size
2296	An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n − 1)–order Markov model.	What is N gram approach
3190	Discrete Probability Distributions If a random variable is a discrete variable, its probability distribution is called a discrete probability distribution. An example will make this clear. Suppose you flip a coin two times. This simple statistical experiment can have four possible outcomes: HH, HT, TH, and TT.	What is a discrete probability
6613	Randomization in an experiment is where you choose your experimental participants randomly.  If you use randomization in your experiments, you guard against bias. For example, selection bias (where some groups are underrepresented) is eliminated and accidental bias (where chance imbalances happen) is minimized.	What is the randomization condition
3405	1:3610:15Suggested clip · 117 secondsConducting a Multiple Regression using Microsoft Excel Data YouTubeStart of suggested clipEnd of suggested clip	How do you do a regression in Excel with multiple variables
1124	The derivative of the sigmoid function is the sigmoid function times one minus itself.	What is the derivative of a sigmoid function
1346	Solve each equation to get a solution to the binomial. For x^2 - 9 = 0, for example, x - 3 = 0 and x + 3 = 0. Solve each equation to get x = 3, -3. If one of the equations is a trinomial, such as x^2 + 2x + 4 = 0, solve it using the quadratic formula, which will result in two solutions (Resource).	How do you solve a binomial equation
6915	The interpretation of the odds ratio depends on whether the predictor is categorical or continuous. Odds ratios that are greater than 1 indicate that the even is more likely to occur as the predictor increases. Odds ratios that are less than 1 indicate that the event is less likely to occur as the predictor increases.	How do you interpret the odds ratio for a continuous variable
7812	The Moment Generating Function of the Binomial Distribution (3) dMx(t) dt = n(q + pet)n−1pet = npet(q + pet)n−1. Evaluating this at t = 0 gives (4) E(x) = np(q + p)n−1 = np.	What is the moment generating function of binomial distribution
3374	Mean Absolute Error (MAE): This measures the absolute average distance between the real data and the predicted data, but it fails to punish large errors in prediction. Mean Square Error (MSE): This measures the squared average distance between the real data and the predicted data.	What is the difference between squared error and absolute error
993	A regression line (LSRL - Least Squares Regression Line) is a straight line that describes how a response variable y changes as an explanatory variable x changes. The line is a mathematical model used to predict the value of y for a given x.  No line will pass through all the data points unless the relation is PERFECT.	What does a least squares regression model predict
280	The basic strength of inductive reasoning is its use in predicting what might happen in the future or in establishing the possibility of what you will encounter. The main weakness of inductive reasoning is that it is incomplete, and you may reach false conclusions even with accurate observations.	What are the advantages and disadvantages of using inductive reasoning
775	Initially, I started with 22,500 labeled samples and used that to create a classifier using fastText and this platform.  At only 5,000 labeled samples, the transfer learning model provided an over 34% improvement in accuracy over fastText, and maintained an 86% accuracy rate.	Does FastText support transfer learning
4696	Fine-tuning deep learning involves using weights of a previous deep learning algorithm for programming another similar deep learning process. Weights are used to connect each neuron in one layer to every neuron in the next layer in the neural network.	What is fine tuning neural network
8380	Linear regression is a linear method to model the relationship between your independent variables and your dependent variables. Advantages include how simple it is and ease with implementation and disadvantages include how is' lack of practicality and how most problems in our real world aren't “linear”.	What are the advantages and disadvantages of linear regression
1740	A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.	What is word embedding in deep learning
2540	The data used in cluster analysis can be interval, ordinal or categorical. However, having a mixture of different types of variable will make the analysis more complicated.	What are the different types of data used for cluster analysis
738	How to Measure VariabilityThe Range. The range is the difference between the largest and smallest values in a set of values.  The Interquartile Range (IQR) The interquartile range (IQR) is a measure of variability, based on dividing a data set into quartiles.  The Variance.  The Standard Deviation.  Effect of Changing Units.	How do you calculate variability
2382	Ridge regression has an additional factor called λ (lambda) which is called the penalty factor which is added while estimating beta coefficients. This penalty factor penalizes high value of beta which in turn shrinks beta coefficients thereby reducing the mean squared error and predicted error.	Why does ridge regression reduce variance
6161	"""Correlation is not causation"" means that just because two things correlate does not necessarily mean that one causes the other.  Correlations between two things can be caused by a third factor that affects both of them."	What does Correlation is not causation mean
5889	Let X be a discrete random variable with the Bernoulli distribution with parameter p: X∼Bern(p) Then the variance of X is given by: var(X)=p(1−p)	How do you find the variance of a Bernoulli distribution
691	The Pearson correlation evaluates the linear relationship between two continuous variables.  The Spearman correlation coefficient is based on the ranked values for each variable rather than the raw data. Spearman correlation is often used to evaluate relationships involving ordinal variables.	What are the differences between correlation coefficient and rank correlation
4291	Classification requires labels. Therefore you first cluster your data and save the resulting cluster labels. Then you train a classifier using these labels as a target variable. By saving the labels you effectively seperate the steps of clustering and classification.	How do you classify after clustering
4996	The 22 Design design where two factors (say factor A\,\! and factor B\,\!) are investigated at two levels. A single replicate of this design will require four runs ({{2}^{2}}=2\times 2=4\,\!) The effects investigated by this design are the two main effects, A\,\! and B,\,\! and the interaction effect AB\,\!.	How many effects will be in factorial experiment
1417	Augmented reality (AR) adds digital elements to a live view often by using the camera on a smartphone. Virtual reality (VR) implies a complete immersion experience that shuts out the physical world.	What is difference between VR and AR
6291	"The SVM typically tries to use a ""kernel function"" to project the sample points to high dimension space to make them linearly separable, while the perceptron assumes the sample points are linearly separable."	What is the difference between the perceptron learning algorithm and SVM
2810	Logistic regression is used to predict the class (or category) of individuals based on one or multiple predictor variables (x). It is used to model a binary outcome, that is a variable, which can have only two possible values: 0 or 1, yes or no, diseased or non-diseased.	What kind of outcomes does logistic regression predict
91	Definition: Random sampling is a part of the sampling technique in which each sample has an equal probability of being chosen. A sample chosen randomly is meant to be an unbiased representation of the total population.  An unbiased random sample is important for drawing conclusions.	Why is it important to use a random sample
131	Inverted Dropout is how Dropout is implemented in practice in the various deep learning frameworks because it helps to define the model once and just change a parameter (the keep/drop probability) to run train and test on the same model.	Why is the dropout inverted
1441	Q-Learning is a value-based reinforcement learning algorithm which is used to find the optimal action-selection policy using a Q function. Our goal is to maximize the value function Q. The Q table helps us to find the best action for each state.  Initially we explore the environment and update the Q-Table.	What is Q function explain Q learning with suitable example
626	In General, A Discriminative model ‌models the decision boundary between the classes. A Generative Model ‌explicitly models the actual distribution of each class.  A Discriminative model ‌learns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems.	What's the difference between a generative and discriminative model
283	Precision and recall at k: Definition Precision at k is the proportion of recommended items in the top-k set that are relevant. Its interpretation is as follows. Suppose that my precision at 10 in a top-10 recommendation problem is 80%. This means that 80% of the recommendation I make are relevant to the user.	What is precision K
5951	Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to.	What is multi label classification in machine learning
297	Multicollinearity can also be detected with the help of tolerance and its reciprocal, called variance inflation factor (VIF). If the value of tolerance is less than 0.2 or 0.1 and, simultaneously, the value of VIF 10 and above, then the multicollinearity is problematic.	How do you test for Multicollinearity
3550	The distribution pX (x) is called the target distribution, while qX (x) is the sampling distribution or the proposal distribution.	In importance sampling what is the difference between p x and q x
386	Robust standard errors address the problem of errors that are not independent and identically distributed. The use of robust standard errors will not change the coefficient estimates provided by OLS, but they will change the standard errors and significance tests.	Why use robust standard errors Stata
586	Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater.	What is the loss in machine learning
7174	For a perfectly normal distribution the mean, median and mode will be the same value, visually represented by the peak of the curve. The normal distribution is often called the bell curve because the graph of its probability density looks like a bell.	Why is the bell curve used to represent the normal distribution Why not a different shape
4044	One way to prove Chebyshev's inequality is to apply Markov's inequality to the random variable Y = (X − μ)2 with a = (kσ)2. Chebyshev's inequality then follows by dividing by k2σ2.	How do you prove Chebyshev's inequality
5750	In statistics, the kth order statistic of a statistical sample is equal to its kth-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference.	What is KTH order statistic
4634	Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression.  A Random Forest operates by constructing several decision trees during training time and outputting the mean of the classes as the prediction of all the trees.	What is a Random Forest Regression
6909	No Normality RequiredComparison of Statistical Analysis Tools for Normally and Non-Normally Distributed DataTools for Normally Distributed DataEquivalent Tools for Non-Normally Distributed DataANOVAMood's median test; Kruskal-Wallis testPaired t-testOne-sample sign testF-test; Bartlett's testLevene's test3 more rows	What test to use if data is not normally distributed
133	Continuous probability distribution: A probability distribution in which the random variable X can take on any value (is continuous). Because there are infinite values that X could assume, the probability of X taking on any one specific value is zero.  The normal distribution is one example of a continuous distribution.	What is the definition of probability when the variable is continuous
255	Confounding means the distortion of the association between the independent and dependent variables because a third variable is independently associated with both. A causal relationship between two variables is often described as the way in which the independent variable affects the dependent variable.	What is confounding in statistics
4088	Convolutional neural networks work because it's a good extension from the standard deep-learning algorithm. Given unlimited resources and money, there is no need for convolutional because the standard algorithm will also work. However, convolutional is more efficient because it reduces the number of parameters.	Why is convolutional neural network better
1404	Image embedding refers to a set of techniques used for reduction the dimensionality of the input data processed by general NNs, including deep NNs.  Image embedding refers to a set of techniques used for reduction the dimensionality of the input data processed by general NNs, including deep NNs.	What are image Embeddings
7288	PD analysis is a method used by larger institutions to calculate their expected loss. A PD is assigned to each risk measure and represents as a percentage the likelihood of default.  LGD represents the amount unrecovered by the lender after selling the underlying asset if a borrower defaults on a loan.	What is PD and LGD
3585	Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.	What are decision trees commonly used for
6730	The binomial is a type of distribution that has two possible outcomes (the prefix “bi” means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail. A Binomial Distribution shows either (S)uccess or (F)ailure.	What is an example of a binomial distribution
2098	relu . The difference is that relu is an activation function whereas LeakyReLU is a Layer defined under keras. layers .  For activation functions you need to wrap around or use inside layers such Activation but LeakyReLU gives you a shortcut to that function with an alpha value.	What is difference between ReLU and LeakyReLU
7974	We capture the notion of being close to a number with a probability density function which is often denoted by ρ(x). If the probability density around a point x is large, that means the random variable X is likely to be close to x. If, on the other hand, ρ(x)=0 in some interval, then X won't be in that interval.	How do you interpret probability density function
673	In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.  Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex.	What is the meaning of convolutional neural networks
4989	Ultimately, the difference between inference and prediction is one of fulfillment: while itself a kind of inference, a prediction is an educated guess (often about explicit details) that can be confirmed or denied, an inference is more concerned with the implicit.	What is difference between inference and prediction
2502	a theory that attempts to explain how imagery works in performance enhancement. It suggests that imagery develops and enhances a coding system that creates a mental blueprint of what has to be done to complete an action.	What is symbolic learning
1252	Yes, there are. One example is the WEKA MOA framework [1]. This framework implements standard algorithms in the literature of concept drift detection.  The nice thing about this framework is that it allows users to generate new data streams which contains concept drifts of different types.	Is there a good library for concept drift detection algorithms
5195	A regression line is a straight line that de- scribes how a response variable y changes as an explanatory variable x changes. We often use a regression line to predict the value of y for a given value of x.	What does a regression line mean
185	Time-series data is a set of observations collected at usually discrete and equally spaced time intervals.  Cross-sectional data are observations that come from different individuals or groups at a single point in time.	What is the difference between time series data and cross sectional data
8580	Statistics is generally considered a prerequisite to the field of applied machine learning. We need statistics to help transform observations into information and to answer questions about samples of observations.	How is statistics related to machine learning
5665	Mean Square Error, Quadratic loss, L2 Loss Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.	What are the loss functions used for regression and classification
2453	In project management terms, an s-curve is a mathematical graph that depicts relevant cumulative data for a project—such as cost or man-hours—plotted against time.  An s-curve in project management is typically used to track the progress of a project.	What is S curve model
2217	2 Answers. If M is your matrix, then it represents a linear f:Rn→Rn, thus when you do M(T) by row times column multiplication you obtain a vectorial expression for your f(T). Thus ∂M∂T is just the derivative of the vector MT, which you do component-wise.	Can you take the derivative of a matrix
1044	A Turing Test is a method of inquiry in artificial intelligence (AI) for determining whether or not a computer is capable of thinking like a human being. The test is named after Alan Turing, the founder of the Turing Test and an English computer scientist, cryptanalyst, mathematician and theoretical biologist.	What is the Turing test for artificial intelligence
2822	p(x) = the likelihood that random variable takes a specific value of x. The sum of all probabilities for all possible values must equal 1. Furthermore, the probability for a particular value or range of values must be between 0 and 1. Probability distributions describe the dispersion of the values of a random variable.	What are the rules for probability distributions
1031	The biggest advantage of linear regression models is linearity: It makes the estimation procedure simple and, most importantly, these linear equations have an easy to understand interpretation on a modular level (i.e. the weights).	What are the advantages of linear regression
865	Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables. For instance, when we predict rent based on square feet alone that is simple linear regression.	What is the difference between simple regression and multivariate regression
5502	Very expensive voltmeters are often made to measure “true RMS”, because that is what is desired. Low-cost voltmeters approximate the RMS value. To approximate the RMS value for a sine wave, one could simply find the peak value of the sine wave and multiply it by .	Do voltmeters measure RMS or peak
2228	Data Drift Defined Data drift is unexpected and undocumented changes to data structure, semantics, and infrastructure that is a result of modern data architectures. Data drift breaks processes and corrupts data, but can also reveal new opportunities for data use.	What is data drift
8564	2:296:43Suggested clip · 121 secondsHow to calculate Confidence Intervals and Margin of Error - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the margin of error for a confidence interval
174	Service-Level Objective (SLO) Availability, in SRE terms, defines whether a system is able to fulfill its intended function at a point in time.	What is Slo in Sre
7716	Conjugate priors are useful because they reduce Bayesian updating to modifying the parameters of the prior distribution (so-called hyperparameters) rather than computing integrals.	Why conjugate priors are useful in Bayesian statistics
5400	Unsupervised learning algorithms are used to group cases based on similar attributes, or naturally occurring trends, patterns, or relationships in the data. These models also are referred to as self-organizing maps. Unsupervised models include clustering techniques and self-organizing maps.	What technique is considered unsupervised learning
1389	inter-rater reliability	What does Kappa coefficient mean
1096	It is important to realize that this conclusion may or may not be correct. Our acceptance or rejection of an hypothesis, and the reality of the truth or falsity of the hypothesis, creates four possibilities, shown below.	How many possibilities may be there when statistical hypothesis is tested
496	Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model.	What is data augmentation in deep learning
6160	The tensor of inertia gives us an idea about how the mass is distributed in a rigid body. Analogously, we can define the tensor of inertia about point O, by writing equation(4) in matrix form.  It follows from the definition of the products of inertia, that the tensors of inertia are always symmetric.	What do you mean by inertia tensor
465	Definition: An image processing method that creates a bitonal (aka binary) image based on setting a threshold value on the pixel intensity of the original image.  The thresholding process is sometimes described as separating an image into foreground values (black) and background values (white).	What is threshold in image processing
530	Key Takeaways. Standard deviation looks at how spread out a group of numbers is from the mean, by looking at the square root of the variance. The variance measures the average degree to which each point differs from the mean—the average of all data points.	What is standard deviation and variance
4544	Usually, Deep Learning takes more time to train as compared to Machine Learning. The main reason is that there are so many parameters in a Deep Learning algorithm. Whereas Machine Learning takes much less time to train, ranging from a few seconds to a few hours.	Which is better deep learning or machine learning
413	There are several undeniable truths about statistics: First and foremost, they can be manipulated, massaged and misstated.  Second, if bogus statistical information is repeated often enough, it eventually is considered to be true.	Can statistics be manipulated
1454	In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if-then rules rather than through conventional procedural code.	What is artificial intelligence and expert system
560	where our data set is expressed by the matrix X∈Rn×d X ∈ R n × d . Following from this equation, the covariance matrix can be computed for a data set with zero mean with C=XXTn−1 C = X X T n − 1 by using the semi-definite matrix XXT X X T .	How is covariance matrix calculated
1383	Like z-scores, t-scores are also a conversion of individual scores into a standard form. However, t-scores are used when you don't know the population standard deviation; You make an estimate by using your sample.	Why do we use t distribution instead of Z
162	In statistics, a Poisson distribution is a statistical distribution that shows how many times an event is likely to occur within a specified period of time. It is used for independent events which occur at a constant rate within a given interval of time.	What is the Poisson distribution used for
621	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	How can you distinguish between supervised and unsupervised learning
123	Clustering is a Machine Learning technique that involves the grouping of data points.  Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.	What is clustering in ML
5511	Classification accuracy is our starting point. It is the number of correct predictions made divided by the total number of predictions made, multiplied by 100 to turn it into a percentage.	What is classification accuracy
1005	Statement of the Multiplication Rule In order to use the rule, we need to have the probabilities of each of the independent events. Given these events, the multiplication rule states the probability that both events occur is found by multiplying the probabilities of each event.	Do you multiply independent events probability
651	Bayesian decision theory refers to a decision theory which is informed by Bayesian probability. It is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs.	What is Bayesian decision theory
958	Cross-entropy is commonly used in machine learning as a loss function. Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions.	What is cross entropy in deep learning
3568	Correlation is the concept of linear relationship between two variables.  It is linear relationship nor any other relationship. Whereas correlation coefficient is a measure that measures linear relationship between two variables.	Is there any difference between correlation and correlation coefficient
5866	Popular Answers (1) That's right that LDA is an unsupervised method.	Is Latent Dirichlet Allocation supervised or unsupervised
1164	Convolution is the process of adding each element of the image to its local neighbors, weighted by the kernel. This is related to a form of mathematical convolution.	What is convolution computer vision
870	"To recap the differences between the two: Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned. Deep learning structures algorithms in layers to create an ""artificial neural network” that can learn and make intelligent decisions on its own."	What is the difference between machine learning and deep learning
6504	This is the “q-value.” A p-value of 5% means that 5% of all tests will result in false positives. A q-value of 5% means that 5% of significant results will result in false positives.	How do you interpret Q values
862	Define hypotheses.  The test statistic is a z-score (z) defined by the following equation. z = (x - M ) / [ σ /sqrt(n) ] where x is the observed sample mean, M is the hypothesized population mean (from the null hypothesis), and σ is the standard deviation of the population.	What is the formula for the one sample z test statistic
2606	Positive feedback occurs to increase the change or output: the result of a reaction is amplified to make it occur more quickly.  Some examples of positive feedback are contractions in child birth and the ripening of fruit; negative feedback examples include the regulation of blood glucose levels and osmoregulation.	What is an example of a positive feedback
2720	In marketing terms, a multi-armed bandit solution is a 'smarter' or more complex version of A/B testing that uses machine learning algorithms to dynamically allocate traffic to variations that are performing well, while allocating less traffic to variations that are underperforming.	What is multi armed bandit testing
878	color difference	What does Delta E mean in color
4277	The Wald Chi-Square test statistic is the squared ratio of the Estimate to the Standard Error of the respective predictor. The probability that a particular Wald Chi-Square test statistic is as extreme as, or more so, than what has been observed under the null hypothesis is given by Pr > ChiSq.	What is the Wald chi square statistic
6953	The chi-squared test applies an approximation assuming the sample is large, while the Fisher's exact test runs an exact procedure especially for small-sized samples.	What is the difference between chi square and Fisher's exact test
5753	Feature transformation is simply a function that transforms features from one representation to another.  feature values may cause problems during the learning process, e.g. data represented in different scales.	What is feature transformation in machine learning
3237	Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner.  We will use a simple example to understand the GBM algorithm.	What are different ensemble learning algorithms
6617	He doesn't explicitly betray Kaneki, but it seems like it because someone who seemed like such a nice guy, giving advice to Kaneki and helping retrieve him from Aogiri, ended up being a sadistic and manipulative person.	Did Uta betray kaneki
1506	An example of statistics is a report of numbers saying how many followers of each religion there are in a particular country. An example of statistics is a math class offered in high schools and colleges. The definition of a statistic is a number, or a person who is an unnamed piece of data to be studied.	What are examples of statistics
4028	The input layer (often called a feature vector) has a node for each feature used for prediction and usually an extra bias node. You usually need only 1 hidden layer, and discerning its ideal size tricky. Having too many hidden layer nodes can result in overfitting and slow training.	What is the size of input layer
2467	T-test. A t-test is used to compare the mean of two given samples. Like a z-test, a t-test also assumes a normal distribution of the sample. A t-test is used when the population parameters (mean and standard deviation) are not known.	Is at test a statistical test
7188	The moment generating function M(t) can be found by evaluating E(etX). By making the substitution y=(λ−t)x, we can transform this integral into one that can be recognized. And therefore, the standard deviation of a gamma distribution is given by σX=√kλ.	How do you find the MGF of a gamma distribution
3348	The Bernoulli distribution is a discrete probability distribution that covers a case where an event will have a binary outcome as either a 0 or 1.	What is Bernoulli distribution in machine learning
308	The one-way multivariate analysis of variance (one-way MANOVA) is used to determine whether there are any differences between independent groups on more than one continuous dependent variable. In this regard, it differs from a one-way ANOVA, which only measures one dependent variable.	When would you use a multivariate Anova
260	Some applications of unsupervised machine learning techniques include: Clustering allows you to automatically split the dataset into groups according to similarity. Often, however, cluster analysis overestimates the similarity between groups and doesn't treat data points as individuals.	What are the applications of unsupervised learning
2624	Finding and Making the RulesFrequent Itemset Generation:- find all itemsets whose support is greater than or equal to the minimum support threshold.Rule generation: generate strong association rules from the frequent itemset whose confidence greater than or equal to minimum confidence threshold.	How do you find strong association rules
6688	Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.	What is mean in machine learning
291	Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. Intuitively, underfitting occurs when the model or the algorithm does not fit the data well enough. Specifically, underfitting occurs if the model or algorithm shows low variance but high bias.	What is underfitting in machine learning
1459	The cumulative distribution function (c.d.f.) of a discrete random variable X is the function F(t) which tells you the probability that X is less than or equal to t. So if X has p.d.f. P(X = x), we have: F(t) = P(X £ t) = SP(X = x).	How do you find the CDRE of a discrete random variable
1940	The Z score is a test of statistical significance that helps you decide whether or not to reject the null hypothesis. The p-value is the probability that you have falsely rejected the null hypothesis. Z scores are measures of standard deviation.  Both statistics are associated with the standard normal distribution.	Is Z score the same as test statistic
377	Cohen's kappa coefficient (κ) is a statistic that is used to measure inter-rater reliability (and also Intra-rater reliability) for qualitative (categorical) items.	What is Cohen's kappa used for
2161	In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration.	What is stopping criterion in machine learning
4239	Coverage, is the extent to which the real, observed population matches the ideal or normative population. A population is the domain from which observations for a particular topic can be drawn.	What is statistical coverage
3134	Basically, the test compares the fit of two models. The null hypothesis is that the smaller model is the “best” model; It is rejected when the test statistic is large. In other words, if the null hypothesis is rejected, then the larger model is a significant improvement over the smaller one.	What is the null hypothesis for likelihood ratio test
8610	In data parallel model, tasks are assigned to processes and each task performs similar types of operations on different data. Data parallelism is a consequence of single operations that is being applied on multiple data items. Data-parallel model can be applied on shared-address spaces and message-passing paradigms.	What is data parallel model
491	The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.	What is the difference between statistics and machine learning
757	At the pooling layer, forward propagation results in an N×N pooling block being reduced to a single value - value of the “winning unit”. Backpropagation of the pooling layer then computes the error which is acquired by this single value “winning unit”.	Do pooling layers affect backpropagation
687	A priori probability refers to the likelihood of an event occurring when there is a finite amount of outcomes and each is equally likely to occur. The outcomes in a priori probability are not influenced by the prior outcome.  A priori probability is also referred to as classical probability.	What is a priori in statistics
4073	Average (or mean) filtering is a method of 'smoothing' images by reducing the amount of intensity variation between neighbouring pixels. The average filter works by moving through the image pixel by pixel, replacing each value with the average value of neighbouring pixels, including itself.	What is average filtering
2920	The standard error tells you how accurate the mean of any given sample from that population is likely to be compared to the true population mean. When the standard error increases, i.e. the means are more spread out, it becomes more likely that any given mean is an inaccurate representation of the true population mean.	What does Standard Error tell you
207	Box-Cox Transformation is a type of power transformation to convert non-normal data to normal data by raising the distribution to a power of lambda (λ). The algorithm can automatically decide the lambda (λ) parameter that best transforms the distribution into normal distribution.	How do you convert a normal distribution to a non normal distribution
104	Basically, you're just pre-setting some of the weights of the new network. Be sure to initialize the new connections to have similar distributions. Make the last layer a concatenation of their results and then add another few layers. Make the last layer a concatenation of their results and the original input.	How do I connect two neural networks
1594	Article. Cards. TF-IDF is an abbreviation for Term Frequency-Inverse Document Frequency and is a very common algorithm to transform text into a meaningful representation of numbers. The technique is widely used to extract features across various NLP applications.	What is TF IDF vector
8300	It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.	What is naive Bayes classifier in data mining
1321	SVM tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data, while logistic regression does not, instead it can have different decision boundaries with different weights that are near the optimal point.	Is SVM better than logistic regression
3188	"In regression analysis, the dependent variable is denoted ""Y"" and the independent variables are denoted by ""X""."	How do you identify independent and dependent variables in regression analysis
864	Statistical power, or the power of a hypothesis test is the probability that the test correctly rejects the null hypothesis. That is, the probability of a true positive result.  statistical power is the probability that a test will correctly reject a false null hypothesis.	What is statistical power in research
5007	A CNN LSTM can be defined by adding CNN layers on the front end followed by LSTM layers with a Dense layer on the output. It is helpful to think of this architecture as defining two sub-models: the CNN Model for feature extraction and the LSTM Model for interpreting the features across time steps.	How do I combine CNN and Lstm
5125	If our model is too simple and has very few parameters then it may have high bias and low variance.  This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can't be more complex and less complex at the same time.	Why is there a bias variance tradeoff
312	Partial correlation holds variable X3 constant for both the other two variables. Whereas, Semipartial correlation holds variable X3 for only one variable (either X1 or X2). Hence, it is called 'semi'partial. There should be linear relationship between all the three variables.	What is the distinction between semi partial and partial correlations
3924	In the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available.  An algorithm that implements classification, especially in a concrete implementation, is known as a classifier.	What is classification in machine learning
6084	Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data.	What are data Visualisation techniques
4431	The measure of central tendency which is most strongly influenced by extreme values in the 'tail' of the distribution is: the mean. The mean height of a student group is 167 cm.	Which of the following measures of central tendency is most affected by extreme values quizlet
1282	A stochastic process is defined as a collection of random variables X={Xt:t∈T} defined on a common probability space, taking values in a common set S (the state space), and indexed by a set T, often either N or [0, ∞) and thought of as time (discrete or continuous respectively) (Oliver, 2009).	What is a stochastic process in laymans terms
216	The sample proportion is what you expect the results to be. This can often be determined by using the results from a previous survey, or by running a small pilot study. If you are unsure, use 50%, which is conservative and gives the largest sample size.	What is proportion in sample size calculation
6635	Define spreading activation. The process through which activity in one node in a network flows outward to other nodes through associative links.	What is spreading activation quizlet
5716	"This paper describes the concept of adaptive noise cancelling, an alternative method of estimating signals corrupted by additive noise or interference. The method uses a ""primary"" input containing the corrupted signal and a ""reference"" input containing noise correlated in some unknown way with the primary noise."	What is adaptive noise
1429	The residual learning framework eases the training of these networks, and enables them to be substantially deeper — leading to improved performance in both visual and non-visual tasks. These residual networks are much deeper than their 'plain' counterparts, yet they require a similar number of parameters (weights).	Why do we learn residual
7996	The Gini coefficient is equal to the area below the line of perfect equality (0.5 by definition) minus the area below the Lorenz curve, divided by the area below the line of perfect equality.	How is Gini coefficient calculated
1975	An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”.	What is Autoencoder neural network
7919	The Spearman rank-order correlation coefficient (Spearman's correlation, for short) is a nonparametric measure of the strength and direction of association that exists between two variables measured on at least an ordinal scale.	What test is used to determine if there is a correlation relationship between 2 variables of ordinal type
5118	Nonstandard units of measurement are units of measurement that aren't typically used, such as a pencil, an arm, a toothpick, or a shoe. We can use just about anything as a nonstandard unit of measurement, as we saw was the case with Mr. FuzzyPaws.	What is an example of a non standard unit of measurement
7868	KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).	How does K nearest neighbors algorithm work
3951	clustering	What does K mean in K means algorithm
4367	To create a stratified random sample, there are seven steps: (a) defining the population; (b) choosing the relevant stratification; (c) listing the population; (d) listing the population according to the chosen stratification; (e) choosing your sample size; (f) calculating a proportionate stratification; and (g) using	How do you do stratified sampling
8336	Back-propagation is just a way of propagating the total loss back into the neural network to know how much of the loss every node is responsible for, and subsequently updating the weights in such a way that minimizes the loss by giving the nodes with higher error rates lower weights and vice versa.	What Back Propagation is usually used for in neural networks
2661	Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. Optimizers help to get results faster.	What is an optimizer in deep learning
299	Control Charts: A discrete distribution is one in which the data can only take on certain values, for example integers. A continuous distribution is one in which data can take on any value within a specified range (which may be infinite).	What is the difference between discrete and continuous probability distribution
8219	Here are 5 common machine learning problems and how you can overcome them.1) Understanding Which Processes Need Automation.  2) Lack of Quality Data.  3) Inadequate Infrastructure.  4) Implementation.  5) Lack of Skilled Resources.	What are the issues of machine learning
705	Training deep learning neural networks is very challenging. The best general algorithm known for solving this problem is stochastic gradient descent, where model weights are updated each iteration using the backpropagation of error algorithm. Optimization in general is an extremely difficult task.	Is neural network hard to learn
5552	The regularization parameter (lambda) serves as a degree of importance that is given to miss-classifications. SVM pose a quadratic optimization problem that looks for maximizing the margin between both classes and minimizing the amount of miss-classifications.  For non-linear-kernel SVM the idea is the similar.	What is regularization parameter in SVM
7260	A random variate is a variable generated from uniformly distributed pseudorandom numbers. Depending on how they are generated, a random variate can be uniformly or nonuniformly distributed. Random variates are frequently used as the input to simulation models (Neelamkavil 1987, p. 119).	What is random variate generation
1786	So you can see that the ch-sq is the statistical measurement, while the P value is the level of probability that the result was due to chance alone. As the chi-sq statistic becomes larger, the P value becomes smaller.	What is the difference between chi square and p value
6820	A decision tree is a specific type of flow chart used to visualize the decision making process by mapping out different courses of action, as well as their potential outcomes.	What is a decision making tree
8282	λ(t)=f(t)S(t), which some authors give as a definition of the hazard function. In words, the rate of occurrence of the event at duration t equals the density of events at t, divided by the probability of surviving to that duration without experiencing the event. λ(t)=−ddtlogS(t).	How do you calculate hazard function
7969	The objective of Unsupervised Anomaly Detection is to detect previously unseen rare objects or events without any prior knowledge about these. The only information available is that the percentage of anomalies in the dataset is small, usually less than 1%.	What is unsupervised anomaly detection
5853	The short answer is yes—because most regression models will not perfectly fit the data at hand. If you need a more complex model, applying a neural network to the problem can provide much more prediction power compared to a traditional regression.	Can neural networks be used for linear regression
2616	Correction factor is defined / given by. Square of the gross total of observed values /Total number of observed values. The sum of squares (SS), used in ANOVA, is actually the sum of squares of the deviations of observed values from their mean.	What is correction factor in statistics
517	In factorial ANOVA, each level and factor are paired up with each other (“crossed”). This helps you to see what interactions are going on between the levels and factors. If there is an interaction then the differences in one factor depend on the differences in another.	What does a factorial Anova tell us
1189	When a data set has outliers or extreme values, we summarize a typical value using the median as opposed to the mean. When a data set has outliers, variability is often summarized by a statistic called the interquartile range, which is the difference between the first and third quartiles.	What does the interquartile range mean
241	In machine learning, the vanishing gradient problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.  The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value.	Why is vanishing gradient a problem
4270	Spatial pooling mimics the action of the receptive fields of the various layers of the cortex, primarily layers L2/3, L5 & L6. This also incorporates the inhibitory action of the inter-neurons. This inhibitory bit is simulated with the k-winner part of the Numenta implementation.	What is spatial pooling
6528	Multivariate analysis is a set of statistical techniques used for analysis of data that contain more than one variable.  Multivariate analysis refers to any statistical technique used to analyse more complex sets of data.	What do mean by multivariate analysis
1014	How to Get Started with AIPick a topic you are interested in.Find a quick solution.Improve your simple solution.Share your solution.Repeat steps 1-4 for different problems.Complete a Kaggle competition.Use machine learning professionally.	How do I start learning artificial intelligence
4587	The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.	How do you handle categorical data
405	In the context of neural networks So, in a neural network context, the receptive field is defined as the size of the region in the input that produces the feature. Basically, it is a measure of association of an output feature (of any layer) to the input region (patch).	What is receptive field size
5917	Time series regression is a statistical method for predicting a future response based on the response history (known as autoregressive dynamics) and the transfer of dynamics from relevant predictors.  Time series regression is commonly used for modeling and forecasting of economic, financial, and biological systems.	What is a time series regression analysis
3165	It is a particular Monte Carlo method that numerically computes a definite integral. While other algorithms usually evaluate the integrand at a regular grid, Monte Carlo randomly chooses points at which the integrand is evaluated. This method is particularly useful for higher-dimensional integrals.	How does Monte Carlo integration work
3922	Artificial Intelligence ExamplesManufacturing robots.Smart assistants.Proactive healthcare management.Disease mapping.Automated financial investing.Virtual travel booking agent.Social media monitoring.Inter-team chat tool.More items	What products use artificial intelligence
70	To import and publish data from TwitterClick the name of the dashboard to run it.From the toolbar, click the arrow next to the Add Data icon , and then select Import Data. The Connect to Your Data page opens.	How do I import data from twitter
6711	Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Theano features: tight integration with NumPy – Use numpy. ndarray in Theano-compiled functions.	What is theano in deep learning
1252	k in kNN algorithm represents the number of nearest neighbor points which are voting for the new test data's class. If k=1, then test examples are given the same label as the closest example in the training set.	What is K in the K nearest neighbors algorithm in Python
380	"The binomial distribution model allows us to compute the probability of observing a specified number of ""successes"" when the process is repeated a specific number of times (e.g., in a set of patients) and the outcome for a given patient is either a success or a failure."	What are the application of binomial distribution
1606	Let V be a vector space. A linearly independent spanning set for V is called a basis. Equivalently, a subset S ⊂ V is a basis for V if any vector v ∈ V is uniquely represented as a linear combination v = r1v1 + r2v2 + ··· + rkvk, where v1,,vk are distinct vectors from S and r1,,rk ∈ R.	How do you find the basis of a vector space
1260	Partial correlation is a measure of the strength and direction of a linear relationship between two continuous variables whilst controlling for the effect of one or more other continuous variables (also known as 'covariates' or 'control' variables).	What is partial correlation analysis
6703	Wright	Who gave the theory of path coefficient
6285	Other examples that may follow a Poisson distribution include the number of phone calls received by a call center per hour and the number of decay events per second from a radioactive source.	What is Poisson distribution example
3599	Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data).	How does deep learning solve semi supervised learning
6623	tl;dr: Bagging and random forests are “bagging” algorithms that aim to reduce the complexity of models that overfit the training data. In contrast, boosting is an approach to increase the complexity of models that suffer from high bias, that is, models that underfit the training data.	Does random forest use boosting
6054	Distributions of data can have few or many peaks. Distributions with one clear peak are called unimodal, and distributions with two clear peaks are called bimodal.	What is difference between unimodal and bimodal
3693	N-grams of texts are extensively used in text mining and natural language processing tasks. They are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios).	Why do we use n grams
5904	Rudolf E. Kálmán	Who invented the Kalman filter
3896	In General, A Discriminative model ‌models the decision boundary between the classes. A Generative Model ‌explicitly models the actual distribution of each class.  A Discriminative model ‌learns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems.	What is the difference between discriminative model and generative model
2870	Matt came to know an old man who went by the name of Stick.  Basically, Daredevil was able to use all his senses (except sight) to actually 'see'. He can actually put together an environment in his head by adding together all the elements that his senses pick up. The picture created in his head is 'a world on fire.	How does daredevil know where he's going
5008	Matrix factorization is a class of collaborative filtering algorithms used in recommender systems. Matrix factorization algorithms work by decomposing the user-item interaction matrix into the product of two lower dimensionality rectangular matrices.	What is the use of matrix factorization
2643	Adaptive learning rate methods are an optimization of gradient descent methods with the goal of minimizing the objective function of a network by using the gradient of the function and the parameters of the network.	What is Adaptive learning rate
3655	Inductive logic programming is the subfield of machine learning that uses first-order logic to represent hypotheses and data. Because first-order logic is expressive and declarative, inductive logic programming specifically targets problems involving structured data and background knowledge.	What is inductive logic programming in machine learning
1960	A continuous variable can take on any score or value within a measurement scale. In addition, the difference between each of the values has a real meaning. Familiar types of continuous variables are income, temperature, height, weight, and distance. There are two main types of continuous variables: interval and ratio.	What type of variable is continuous
7090	Cross correlation and autocorrelation are very similar, but they involve different types of correlation: Cross correlation happens when two different sequences are correlated. Autocorrelation is the correlation between two of the same sequences. In other words, you correlate a signal with itself.	What is the difference between correlation and autocorrelation
376	Generally speaking, non-probability sampling can be a more cost-effective and faster approach than probability sampling, but this depends on a number of variables including the target population being studied. Certain types of non-probability sampling can also introduce bias into the sample and results.	What is the advantage of probability sampling over non probability sampling
1312	Income, although you may consider it to be technically discrete, would likely be treated as a continuous variable. Other discrete variables (such as the number of ER visits per year for a sample of hospitals) may also be treated as continuous even though they are technically discrete.	Is income a discrete variable
3248	In machine learning, classification refers to a predictive modeling problem where a class label is predicted for a given example of input data. Examples of classification problems include: Given an example, classify if it is spam or not. Given a handwritten character, classify it as one of the known characters.	What is classification model in machine learning
277	Measurement uncertainty is critical to risk assessment and decision making. Organizations make decisions every day based on reports containing quantitative measurement data. If measurement results are not accurate, then decision risks increase. Selecting the wrong suppliers, could result in poor product quality.	Why do statistics and uncertainties matter
888	Body parts are not used as standard unit of measurement because length of palm and hand are different for different persons which causes error in measurement.	Why are body parts not used as standard units of measurement
58	The independence between inputs means that each input has a different normalization operation, allowing arbitrary mini-batch sizes to be used. The experimental results show that layer normalization performs well for recurrent neural networks.	Why do we normalize layers
940	The binomial distribution is a common discrete distribution used in statistics, as opposed to a continuous distribution, such as the normal distribution.	Is binomial distribution discrete or continuous
1702	χ2 can be used to test whether two variables are related or independent from one another or to test the goodness-of-fit between an observed distribution and a theoretical distribution of frequencies.	What is the test statistic for Chi Square
469	General linear modeling in SPSS for Windows The general linear model (GLM) is a flexible statistical model that incorporates normally distributed dependent variables and categorical or continuous independent variables.	What is general linear model in SPSS
723	Two approaches to avoiding overfitting are distinguished: pre-pruning (generating a tree with fewer branches than would otherwise be the case) and post-pruning (generating a tree in full and then removing parts of it). Results are given for pre-pruning using either a size or a maximum depth cutoff.	How can we avoid overfitting in a decision tree
4200	Properties of Log Base 2Zero Exponent Rule : loga 1 = 0.Change of Base Rule : logb (x) = ln x / ln b or logb (x) = log10 x / log10 b.Logb b = 1 Example : log22 = 1.Logb bx = x Example : log22x = x.	How do you calculate log2
344	Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses. The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data.	Which algorithm falls under unsupervised learning
5158	5:4711:51Suggested clip · 87 secondsInterpreting the Odds Ratio in Logistic Regression using SPSS YouTubeStart of suggested clipEnd of suggested clip	How do you interpret odds ratio in logistic regression SPSS
2	A random process is a time-varying function that assigns the outcome of a random experiment to each time instant: X(t).  If one scans all possible outcomes of the underlying random experiment, we shall get an ensemble of signals.	What is meant by random process
103	Advantages and disadvantagesAre simple to understand and interpret.  Have value even with little hard data.  Help determine worst, best and expected values for different scenarios.Use a white box model.  Can be combined with other decision techniques.	What are the advantages and disadvantages of decision tree
5184	Properties, Uses and Limitations of a Dimensional AnalysisTo check the correctness of a physical equation.To derive the relation between different physical quantities involved in a physical phenomenon.To change from one system of units to another.	What are the uses and limitations of dimensional analysis
492	Morpheus: If real is what you can feel, smell, taste and see, then 'real' is simply electrical signals interpreted by your brain.	What is the Matrix Morpheus quote
316	Validation and Derivation Procedures serve two different purposes. Validation Procedures compare multiple Question responses for the same patient for the purpose of ensuring that patient data is valid. Derivation Procedures use calculations to derive values from collected data.	Whats the difference between derivation sample and validation sample
7534	Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.	What is data augmentation in machine learning
389	Agents can be grouped into five classes based on their degree of perceived intelligence and capability. Model-based reflex agent.  Goal-based agents. Utility-based agent.	What is agent and types of agent in AI
7360	K-means clustering algorithm can be significantly improved by using a better initialization technique, and by repeating (re-starting) the algorithm. When the data has overlapping clusters, k-means can improve the results of the initialization technique.	How do you improve K means algorithm
396	In statistics, the method of moments is a method of estimation of population parameters. It starts by expressing the population moments (i.e., the expected values of powers of the random variable under consideration) as functions of the parameters of interest.  The solutions are estimates of those parameters.	What is the method of moments estimator
2600	In machine learning, classification refers to a predictive modeling problem where a class label is predicted for a given example of input data. Examples of classification problems include: Given an example, classify if it is spam or not. Given a handwritten character, classify it as one of the known characters.	What is classification in machine learning with example
4681	Distributional similarity is the idea that the meaning of words can be understood from their context. This should not be confused with the term distributed representation, which refers to the idea of representing information with relatively dense vectors as opposed to a one-hot representation.	What is distributional similarity
6284	Factorial analysis of variance (ANOVA) is a statistical procedure that allows researchers to explore the influence of two or more independent variables (factors) on a single dependent variable.	What is a factorial analysis of variance
8403	Naive Bayes classifier assume that the effect of the value of a predictor (x) on a given class (c) is independent of the values of other predictors. This assumption is called class conditional independence. P(c|x) is the posterior probability of class (target) given predictor (attribute).	What are the assumptions for naïve Bayes classifier
3862	"Data from ordinal or nominal (categorical) variables are not properly analyzed using the theory or tests based on the normal distribution.  However, it makes no sense to discuss ""sex"" (a categorical variable) as a normally distributed variable."	Can categorical data be normally distributed
700	Generally, a machine learning pipeline describes or models your ML process: writing code, releasing it to production, performing data extractions, creating training models, and tuning the algorithm. An ML pipeline should be a continuous process as a team works on their ML platform.	What is pipeline in machine learning
3530	While the returns for stocks usually have a normal distribution, the stock price itself is often log-normally distributed. This is because extreme moves become less likely as the stock's price approaches zero.	Why do prices and income follow a log normal distribution
4730	An Inverted file is an index data structure that maps content to its location within a database file, in a document or in a set of documents.  The inverted file is the most popular data structure used in document retrieval systems to support full text search.	What is inverted file in data structure
8113	Generalized Linear Models (GLMs)  The term general linear model (GLM) usually refers to conventional linear regression models for a continuous response variable given continuous and/or categorical predictors. It includes multiple linear regression, as well as ANOVA and ANCOVA (with fixed effects only).	What is linear in a generalized linear model
820	Latent Semantic Analysis is a technique for creating a vector representation of a document.  This in turn means you can do handy things like classifying documents to determine which of a set of known topics they most likely belong to.	Can Latent Semantic Analysis used for document classification
10	Basic steps:Assign a number of points to coordinates in n-dimensional space.  Calculate Euclidean distances for all pairs of points.  Compare the similarity matrix with the original input matrix by evaluating the stress function.  Adjust coordinates, if necessary, to minimize stress.	How do you apply multidimensional scaling
726	Brownian motion lies in the intersection of several important classes of processes. It is a Gaussian Markov process, it has continuous paths, it is a process with stationary independent increments (a Lévy process), and it is a martingale. Several characterizations are known based on these properties.	Is Brownian motion a Markov process
574	Ensemble is a machine learning concept in which multiple models are trained using the same learning algorithm. Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data.	Is bagging an example of ensemble learning
481	Regression analysis is a form of inferential statistics. The p-values help determine whether the relationships that you observe in your sample also exist in the larger population. The p-value for each independent variable tests the null hypothesis that the variable has no correlation with the dependent variable.	What is P in regression analysis
3782	Decision Tree Splitting Method #1: Reduction in VarianceFor each split, individually calculate the variance of each child node.Calculate the variance of each split as the weighted average variance of child nodes.Select the split with the lowest variance.Perform steps 1-3 until completely homogeneous nodes are achieved.	How do you determine the best split in decision tree
3795	Machine Learning(ML) generally means that you're training the machine to do something(here, image processing) by providing set of training data's.	Is image processing part of machine learning
7677	Chaos theory is an interdisciplinary theory stating that, within the apparent randomness of chaotic complex systems, there are underlying patterns, interconnectedness, constant feedback loops, repetition, self-similarity, fractals, and self-organization.	How do you explain the chaos theory
1	Anthropology definitions The definition of anthropology is the study of various elements of humans, including biology and culture, in order to understand human origin and the evolution of various beliefs and social customs. An example of someone who studies anthropology is Ruth Benedict.	What is anthropology and its example
2472	Use Simple Random Sampling One of the most effective methods that can be used by researchers to avoid sampling bias is simple random sampling, in which samples are chosen strictly by chance. This provides equal odds for every member of the population to be chosen as a participant in the study at hand.	How do you avoid sampling bias
5326	Linear Growth Model Organisms generally grow in spurts that are dependent on both environment and genetics. Under controlled laboratory conditions, however, one can often observe a constant rate of growth. These periods of constant growth are often referred to as the linear portions of the growth curve.	What is a linear growth curve
8687	Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks. Perceptron is a linear classifier (binary). Also, it is used in supervised learning. It helps to classify the given input data.	Is neural network a linear classifier
1171	In spatial analysis, four major problems interfere with an accurate estimation of the statistical parameter: the boundary problem, scale problem, pattern problem (or spatial autocorrelation), and modifiable areal unit problem.  In analysis with area data, statistics should be interpreted based upon the boundary.	What are spatial problems
4818	In the mathematical field of numerical analysis, interpolation is a type of estimation, a method of constructing new data points within the range of a discrete set of known data points.  It is often required to interpolate, i.e., estimate the value of that function for an intermediate value of the independent variable.	What is interpolation algorithm
3270	To get started, you need to identify the two terms from your binomial (the x and y positions of our formula above) and the power (n) you are expanding the binomial to. For example, to expand (2x-3)³, the two terms are 2x and -3 and the power, or n value, is 3.	How do you do binomial theorem
223	John McCarthy	Who invented artificial intelligence
5146	So in summary, hidden state is overall state of what we have seen so far. Cell state is selective memory of the past. Both these states are trainable with data.	What is the difference between a hidden state and a cell state in Lstm intuitively
6186	Nonparametric statistics should be considered when the sample sizes are small and the underlying distribution is not clear. If it is important to detect small effects, one should be very cautious about one's choice of the test statistic.	Is there a good reference about when to use nonparametric statistical procedures
1405	Variance (σ2) in statistics is a measurement of the spread between numbers in a data set. That is, it measures how far each number in the set is from the mean and therefore from every other number in the set.	What is variance in statistics
982	Bagging (Bootstrap Aggregating) is an ensemble method. First, we create random samples of the training data set (sub sets of training data set). Then, we build a classifier for each sample. Finally, results of these multiple classifiers are combined using average or majority voting.	How do ensemble models work
1109	The Relationship Between a CDF and a PDF In technical terms, a probability density function (pdf) is the derivative of a cumulative density function (cdf). Futhermore, the area under the curve of a pdf between negative infinity and x is equal to the value of x on the cdf.	What is the difference between a CDF and a PDF
4581	A frequency count is a measure of the number of times that an event occurs. Thus, a relative frequency of 0.50 is equivalent to a percentage of 50%.	What is relative frequency percentage
6501	In statistics, a type of probability distribution in which all outcomes are equally likely.  A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same.	What is uniform distribution in statistics
8472	Interval data is like ordinal except we can say the intervals between each value are equally split. The most common example is temperature in degrees Fahrenheit.  Ratio data is interval data with a natural zero point. For example, time is ratio since 0 time is meaningful.	What is interval or ratio data
1306	Decision tree classifier – Decision tree classifier is a systematic approach for multiclass classification. It poses a set of questions to the dataset (related to its attributes/features). The decision tree classification algorithm can be visualized on a binary tree.	Which algorithm is best for multiclass classification
3564	It is clear that correlated features means that they bring the same information, so it is logical to remove one of them.	Why do we remove correlated variables
5345	and is commonly used as an estimator for σ. Nevertheless, S is a biased estimator of σ.	Is s an unbiased estimator of Sigma
1325	The maximum entropy principle is defined as modeling a given set of data by finding the highest entropy to satisfy the constraints of our prior knowledge.  The maximum entropy model is a conditional probability model p(y|x) that allows us to predict class labels given a set of features for a given data point.	What is maximum entropy model in NLP
943	fX(x) dx For fX(x) to be a proper distribution, it must satisfy the following two conditions: 1. The PDF fX(x) is positive-valued; fX(x) ≥ 0 for all values of x ∈ X. 2. The rule of total probability holds; the total area under fX(x) is 1; ∫	What is a proper probability distribution
865	The coefficients in a Cox regression relate to hazard; a positive coefficient indicates a worse prognosis and a negative coefficient indicates a protective effect of the variable with which it is associated.	How do you interpret Cox regression coefficients
1417	Knowing the number of scores and ranking them in order from lowest to highest, you can use the formula R = P / 100 (N + 1) to calculate the percentile rank.	How do you find rank in statistics
313	A hypothesis test for a population mean when the population standard deviation, σ, is unknown is conducted in the same way as if the population standard deviation is known. The only difference is that the t-distribution is invoked, instead of the standard normal distribution (z-distribution).	Which distribution is used to test the population mean when the population standard deviation is unknown
6441	The Poisson parameter Lambda (λ) is the total number of events (k) divided by the number of units (n) in the data (λ = k/n).	How do you find lambda in a Poisson distribution
5625	The mean value of x is thus the first moment of its distribution, while the fact that the probability distribution is normalized means that the zeroth moment is always 1.  The variance of x is thus the second central moment of the probability distribution when xo is the mean value or first moment.	What are moments prove that first moment is average and second moment is variance
6825	An image kernel is a small matrix used to apply effects like the ones you might find in Photoshop or Gimp, such as blurring, sharpening, outlining or embossing.  The matrix on the left contains numbers, between 0 and 255, which each correspond to the brightness of one pixel in a picture of a face.	What is kernel size in image processing
13	A machine learning task is the type of prediction or inference being made, based on the problem or question that is being asked, and the available data. For example, the classification task assigns data to categories, and the clustering task groups data according to similarity.	What are the tasks in machine learning
4678	Deep learning is an AI function that mimics the workings of the human brain in processing data for use in detecting objects, recognizing speech, translating languages, and making decisions. Deep learning AI is able to learn without human supervision, drawing from data that is both unstructured and unlabeled.	What is deep learning and how does it relate to AI
803	Continuous probability functions are also known as probability density functions. You know that you have a continuous distribution if the variable can assume an infinite number of values between any two values. Continuous variables are often measurements on a scale, such as height, weight, and temperature.	What kind of distribution is called a probability density
1278	Therefore, the average running time of QUICKSORT on uniformly distributed permutations (random data) and the expected running time of randomized QUICKSORT are both O(n + n lg n) = O(n lg n). This is the same growth rate as merge sort and heap sort.	What is randomized quicksort analyze the expected running time of randomized quicksort with the help of a suitable example
1510	Tips for improving deductive reasoning skillsBe curious.Be observational.Increase your knowledge.Break problems into smaller pieces.	How can deductive reasoning skills be improved
6572	We will use the RAND() function to generate a random value between 0 and 1 on our Y-axis and then get the inverse of it with the NORM. INV function which will result in our random normal value on the X-axis. Mean – This is the mean of the normal distribution.	How do you generate a random number from a normal distribution
555	more  A symbol for a value we don't know yet. It is usually a letter like x or y. Example: in x + 2 = 6, x is the variable.	What is variable and example
7176	The law of large numbers, in probability and statistics, states that as a sample size grows, its mean gets closer to the average of the whole population.  In a financial context, the law of large numbers indicates that a large entity which is growing rapidly cannot maintain that growth pace forever.	What does the law of large numbers tell us
1131	In a hypothesis test, we: Evaluate the null hypothesis, typically denoted with H0. The null is not rejected unless the hypothesis test shows otherwise. The null statement must always contain some form of equality (=, ≤ or ≥)	What is the symbol for null hypothesis in statistics
2526	In every factor analysis, there are the same number of factors as there are variables.  The eigenvalue is a measure of how much of the variance of the observed variables a factor explains. Any factor with an eigenvalue ≥1 explains more variance than a single observed variable.	What is eigenvalue in factor analysis
7271	How Deep Learning Algorithms WorkMultilayer Perceptron Neural Network (MLPNN)  Backpropagation.  Convolutional Neural Network (CNN)  Recurrent Neural Network (RNN)  Long Short-Term Memory (LSTM)  Generative Adversarial Network (GAN)  Restricted Boltzmann Machine (RBM)  Deep Belief Network (DBN)	What are different deep learning algorithms
5048	Different performance metrics are used to evaluate different Machine Learning Algorithms. For now, we will be focusing on the ones used for Classification problems. We can use classification performance metrics such as Log-Loss, Accuracy, AUC(Area under Curve) etc.	What is performance metrics in machine learning
960	Notice that simple linear regression has k=1 predictor variable, so k+1 = 2. Thus, we get the formula for MSE that we introduced in that context of one predictor. S=√MSE S = M S E estimates σ and is known as the regression standard error or the residual standard error.	How do you find the standard error of multiple linear regression
8664	Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.	Why is gradient descent important in machine learning
4	Maximum sample rate: This parameter needs to be looked at carefully when an ADC's input channels are multiplexed. For ADCs using flash and SAR (successive approximate register) architectures, the sample rate for each channel can be calculated by dividing the specified sample rate by the number of channels.	How do you calculate sampling frequency for ADC
6140	Classification Algorithms in Data Mining. It is one of the Data Mining. That is used to analyze a given data set and takes each instance of it. It assigns this instance to a particular class.  So classification is the process to assign class label from a data set whose class label is unknown.	What is classification algorithms in data mining
5300	Graphically, the p value is the area in the tail of a probability distribution. It's calculated when you run hypothesis test and is the area to the right of the test statistic (if you're running a two-tailed test, it's the area to the left and to the right).	What is the P value of the test statistic
8413	An offset variable is one that is treated like a regression covariate whose parameter is fixed to be 1.0.  Offset variables are most often used to scale the modeling of the mean in Poisson regression situations with a log link.	What is an offset in Poisson regression
5826	An IQ (Intelligence Quotient) score from a standardized test of intelligences is a good example of an interval scale score.  IQ scores are created so that a score of 100 represents the average IQ of the population and the standard deviation (or average variability) of scores is 15.	Is IQ an interval scale
4603	Accuracy reflects how close a measurement is to a known or accepted value, while precision reflects how reproducible measurements are, even if they are far from the accepted value. Measurements that are both precise and accurate are repeatable and very close to true values.	What is difference between accuracy and precision
3500	Definition. A study design that randomly assigns participants into an experimental group or a control group. As the study is conducted, the only expected difference between the control and experimental groups in a randomized controlled trial (RCT) is the outcome variable being studied.	What kind of study is a randomized controlled trial
727	There are basically two methods to reduce autocorrelation, of which the first one is most important:Improve model fit. Try to capture structure in the data in the model.  If no more predictors can be added, include an AR1 model.	How do you fix autocorrelation
905	The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above . 05 indicates normality.	How do you know if a distribution is Gaussian
7544	The correlation structure between the dependent variables provides additional information to the model which gives MANOVA the following enhanced capabilities: Greater statistical power: When the dependent variables are correlated, MANOVA can identify effects that are smaller than those that regular ANOVA can find.	Why use a Manova instead of Anova
7776	The only difference between Greedy BFS and A* BFS is in the evaluation function. For Greedy BFS the evaluation function is f(n) = h(n) while for A* the evaluation function is f(n) = g(n) + h(n).	What is the difference between greedy best first search and A * search algorithm
2781	Meta-learning, also known as “learning to learn”, intends to design models that can learn new skills or adapt to new environments rapidly with a few training examples.  Humans, in contrast, learn new concepts and skills much faster and more efficiently.	How does meta learning work
633	Any study that attempts to predict human behavior will tend to have R-squared values less than 50%. However, if you analyze a physical process and have very good measurements, you might expect R-squared values over 90%.	What is a good R squared value for regression
1023	Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors.	What is the purpose of principal component analysis
715	The role of sigma in the Gaussian filter is to control the variation around its mean value. So as the Sigma becomes larger the more variance allowed around mean and as the Sigma becomes smaller the less variance allowed around mean.  it simply means that we apply a kernel on every pixel in the image.	What is Sigma in Gaussian blur
6834	The area under the graph is the definite integral. By definition, definite integral is the sum of the product of the lengths of intervals and the height of the function that is being integrated with that interval, which includes the formula of the area of the rectangle. The figure given below illustrates it.	What is the geometrical interpretation of integration
1331	In OLS regression, a linear relationship between the dependent and independent variable is a must, but in logistic regression, one does not assume such things. The relationship between the dependent and independent variable may be linear or non-linear.	What is the difference between OLS and logistic regression
6579	The amount that the weights are updated during training is referred to as the step size or the “learning rate.” Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0.	What is step size in neural network
1644	Word2Vec takes texts as training data for a neural network. The resulting embedding captures whether words appear in similar contexts. GloVe focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together.	What is the difference between Word2Vec and GloVe
2602	The desired precision of the estimate (also sometimes called the allowable or acceptable error in the estimate) is half the width of the desired confidence interval. For example if you would like the confidence interval width to be about 0.1 (10%) you would enter a precision of +/- 0.05 (5%).	What is precision in sample size calculation
5640	In statistics, the likelihood-ratio test assesses the goodness of fit of two competing statistical models based on the ratio of their likelihoods, specifically one found by maximization over the entire parameter space and another found after imposing some constraint.	What does a likelihood ratio test mean
966	Some of the methods commonly used for binary classification are:Decision trees.Random forests.Bayesian networks.Support vector machines.Neural networks.Logistic regression.Probit model.	What are some binary classification algorithms
8372	Sparse coding is the representation of items by the strong activation of a relatively small set of neurons. For each stimulus, this is a different subset of all available neurons.	What is sparse coding in neural network
1466	Use this type of sampling to indicate if a particular trait or characteristic exists in a population. Researchers widely use the non-probability sampling method when they aim at conducting qualitative research, pilot studies, or exploratory research.	What are the uses of non probability sampling
143	This approach works when the sample size is relatively large (greater than or equal to 30). Use the first or third formulas when the population size is known.How to Choose Sample Size for a Simple Random Sample.Sample statisticPopulation sizeSample sizeProportionUnknownn = [ ( z2 * p * q ) + ME2 ] / ( ME2 )3 more rows	What is the formula for random sampling
1267	In order to be considered a normal distribution, a data set (when graphed) must follow a bell-shaped symmetrical curve centered around the mean. It must also adhere to the empirical rule that indicates the percentage of the data set that falls within (plus or minus) 1, 2 and 3 standard deviations of the mean.	How do you know if a distribution is normal
5706	The term “sigmoid” means S-shaped, and it is also known as a squashing function, as it maps the whole real range of z into [0,1] in the g(z). This simple function has two useful properties that: (1) it can be used to model a conditional probability distribution and (2) its derivative has a simple form.	What is sigmoid function
6192	In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).  In cluster sampling, the sampling unit is the whole cluster; Instead of sampling individuals from within each group, a researcher will study whole clusters.	What is cluster sampling vs stratified sampling
2289	How It Works. Connected component labeling works by scanning an image, pixel-by-pixel (from top to bottom and left to right) in order to identify connected pixel regions, i.e. regions of adjacent pixels which share the same set of intensity values V.	How does connected component image labeling work on colored images
829	K-means is an unsupervised learning algorithm as it infers a clustering (or labels) for a set of provided samples that do not initially have labels. The goal of k-means is to partition the n samples from your dataset in to k clusters where each datapoint belongs to the single cluster for which it is nearest to.	Is K means clustering considered supervised or unsupervised machine learning
4115	Each feature, or column, represents a measurable piece of data that can be used for analysis: Name, Age, Sex, Fare, and so on. Features are also sometimes referred to as “variables” or “attributes.” Depending on what you're trying to analyze, the features you include in your dataset can vary widely.	What is data feature
2252	"Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons a candidate (""backtracks"") as soon as it determines that the candidate cannot possibly be completed to a"	What is backtracking algorithm
7933	Batch means a group of training samples. In gradient descent algorithms, you can calculate the sum of gradients with respect to several examples and then update the parameters using this cumulative gradient. If you 'see' all training examples before one 'update', then it's called full batch learning.	What is meant by batch learning
1486	A pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling.	Why is pooling used in CNN
6176	Getting and preparing the data Each line of the text file contains a list of labels, followed by the corresponding document. All the labels start by the __label__ prefix, which is how fastText recognize what is a label or what is a word. The model is then trained to predict the labels given the word in the document.	How is fastText trained
433	"Constrained optimization problems are problems for which a function is to be minimized or maximized subject to constraints . Here is called the objective function and is a Boolean-valued formula.  stands for ""maximize subject to constraints "". You say a point satisfies the constraints if is true."	What is a constrained optimization problem
319	For a sequence of random variables {Xn}, if there exists a real number c such that for every small positive number σ the probability that the absolute difference between Xn and c is less than σ has the limit of 1 when n → ∞, namely, then we say that {xn } converges in probability to constant c, and c is called the	What is probability limit
3709	"A 'weak' learner (classifer, predictor, etc) is just one which performs relatively poorly--its accuracy is above chance, but just barely.  Weak learner also suggests that many instances of the algorithm are being pooled (via boosting, bagging, etc) together into to create a ""strong"" ensemble classifier."	Why do we use weak learners in boosting
150	Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.	What is batch normalization layer
1414	The estimated regression equations show the equation for y hat i.e. predicted y. The regression model on the other hand shows equation for the actual y. This is an abstract model and uses population terms (which are specified in Greek symbols).	What is the difference between regression and estimated regression
3201	The false alarm probability is the probability that exceeds a certain threshold when there is no signal.	What is false alarm probability
8426	Connectionism theory is based on the principle of active learning and is the result of the work of the American psychologist Edward Thorndike. This work led to Thorndike's Laws. According to these Laws, learning is achieved when an individual is able to form associations between a particular stimulus and a response.	What is Thorndike's connectionism learning theory
205	A function that represents a discrete probability distribution is called a probability mass function. A function that represents a continuous probability distribution is called a probability density function. Functions that represent probability distributions still have to obey the rules of probability.	What is the difference between probability density function and probability distribution function
3947	Euclidean distance	Which distance metric can be used in Knn
562	Bellman equation is the basic block of solving reinforcement learning and is omnipresent in RL. It helps us to solve MDP. To solve means finding the optimal policy and value functions. The optimal value function V*(S) is one that yields maximum value.	What is Bellman equation in reinforcement learning
299	A) (ii) Disadvantages of Mohr Method  Mohr's method is suitable only for titration of chloride, bromide and cyanide alone.  Errors can be introduced due to the need of excess titrant before the endpoint colour is visible.	What are the limitations of Mohr's method
8132	Gradient Boosting Machines vs. XGBoost.  While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation.	What is the difference between XGBoost and gradient boost
1045	A Bloom filter is a data structure designed to tell you, rapidly and memory-efficiently, whether an element is present in a set. The price paid for this efficiency is that a Bloom filter is a probabilistic data structure: it tells us that the element either definitely is not in the set or may be in the set.	What is Bloom filter in big data
830	The weaknesses of decision tree methods : Decision trees are less appropriate for estimation tasks where the goal is to predict the value of a continuous attribute. Decision trees are prone to errors in classification problems with many class and relatively small number of training examples.	What are the issues in decision tree induction
1397	They are data records that differ dramatically from all others, they distinguish themselves in one or more characteristics. In other words, an outlier is a value that escapes normality and can (and probably will) cause anomalies in the results obtained through algorithms and analytical systems.	What is an outlier in data science
2360	Gradient is a vector that is tangent of a function and points in the direction of greatest increase of this function. Gradient is zero at a local maximum or minimum because there is no single direction of increase. In mathematics, gradient is defined as partial derivative for every input variable of function.	What is gradient in data science
1327	In a dataset a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set.	What is the difference between training and test dataset
6464	robust is a programmer's command that computes a robust variance estimator based on a varlist of equation-level scores and a covariance matrix.  The robust variance estimator goes by many names: Huber/White/sandwich are typically used in the context of robustness against heteroskedasticity.	What does robust do Stata
80	Random assignment helps reduce the chances of systematic differences between the groups at the start of an experiment and, thereby, mitigates the threats of confounding variables and alternative explanations. However, the process does not always equalize all of the confounding variables.	How does random assignment control for confounding variables
3772	The equation used to calculate kappa is: Κ = PR(e), where Pr(a) is the observed agreement among the raters and Pr(e) is the hypothetical probability of the raters indicating a chance agreement. The formula was entered into Microsoft Excel and it was used to calculate the Kappa coefficient.	How is kappa calculated
4831	Factor analysis is a way to condense the data in many variables into a just a few variables. For this reason, it is also sometimes called “dimension reduction.” You can reduce the “dimensions” of your data into one or more “super-variables.” The most common technique is known as Principal Component Analysis (PCA).	What is factor analysis and why it is used
8115	Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features.	What is a hyperplane in machine learning
1398	“A method of estimating the parameters of a distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable.”	What is the likelihood function of normal distribution
2842	Medical Definition of alpha state : a state of wakeful relaxation that is associated with increased alpha wave activity When electroencephalograms show a brain wave pattern of 9 to 12 cycles per second, the subject is said to be in alpha state, usually described as relaxed, peaceful, or floating.—	What is the alpha state
8318	Markov models are useful to model environments and problems involving sequential, stochastic decisions over time. Representing such environments with decision trees would be confusing or intractable, if at all possible, and would require major simplifying assumptions [2].	Why Markov model is useful
1016	If the mean more accurately represents the center of the distribution of your data, and your sample size is large enough, use a parametric test. If the median more accurately represents the center of the distribution of your data, use a nonparametric test even if you have a large sample size.	How do you know if its parametric or nonparametric
573	There are essentially three stopping criteria that can be adopted to stop the K-means algorithm: Centroids of newly formed clusters do not change. Points remain in the same cluster. Maximum number of iterations are reached.	What is the ideal stopping criteria for the K Means algorithm
7346	Scikit-learn is a free machine learning library for Python. It features various algorithms like support vector machine, random forests, and k-neighbours, and it also supports Python numerical and scientific libraries like NumPy and SciPy .  Then we'll dive into scikit-learn and use preprocessing.	Why Sklearn is used in Python
6547	Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).	When does ensemble learning work
556	A generalized linear model is a flexible generalization of ordinary linear regression models which allows for the response variables (dependent) to have error distribution other than normal distribution.  GLM was developed to unify other statistical methods (linear, logistic, Poisson regression).	What is the difference between GLM and linear regression
5539	The exponential distribution is one of the widely used continuous distributions. It is often used to model the time elapsed between events. We will now mathematically define the exponential distribution, and derive its mean and expected value.	Why is the exponential distribution important
7729	This term is used in statistics in its ordinary sense, but most frequently occurs in connection with samples from different populations which may or may not be identical. If the populations are identical they are said to be homogeneous, and by extension, the sample data are also said to be homogeneous.	What is a homogeneous group in statistics
186	"When your child sits the eleven plus exam, the number of questions answered correctly decides the ""Raw Score"". If there are more than one tests, the score may be the sum of the raw scores.  A standardized test score is calculated by translating the raw score into a completely different scale."	How is 11+ standardized calculated
1437	The Cox proportional hazards model92 is the most popular model for the analysis of survival data. It is a semiparametric model; it makes a parametric assumption concerning the effect of the predictors on the hazard function, but makes no assumption regarding the nature of the hazard function λ(t) itself.	Why is the Cox proportional hazards model referred to as semiparametric
1292	Precision refers to how close estimates from different samples are to each other. For example, the standard error is a measure of precision. When the standard error is small, estimates from different samples will be close in value; and vice versa.	What is sample precision
3799	The Bernoulli distribution represents the success or failure of a single Bernoulli trial. The Binomial Distribution represents the number of successes and failures in n independent Bernoulli trials for some given value of n.  Another example is the number of heads obtained in tossing a coin n times.	How is a Bernoulli process associated with the binomial distribution
5811	Can you list out the critical assumptions of linear regression?  What is Heteroscedasticity?  What is the primary difference between R square and adjusted R square?  Can you list out the formulas to find RMSE and MSE?	What are the interview questions on regression modeling
74	Each node in the decision tree works on a random subset of features to calculate the output. The random forest then combines the output of individual decision trees to generate the final output.  The Random Forest Algorithm combines the output of multiple (randomly created) Decision Trees to generate the final output.	How is random forest different from Decision Tree
3074	Classification is one of the most fundamental concepts in data science. Classification algorithms are predictive calculations used to assign data to preset categories by analyzing sets of training data.၂၀၂၀၊ ဩ ၂၆	What are classification algorithms in machine learning
420	Retail. Supermarkets, for example, use joint purchasing patterns to identify product associations and decide how to place them in the aisles and on the shelves. Data mining also detects which offers are most valued by customers or increase sales at the checkout queue.	What is data mining give an example
247	The number of true positives is placed in the top left cell of the confusion matrix. The data rows (emails) belonging to the positive class (spam) and incorrectly classified as negative (normal emails). These are called False Negatives (FN).	What is positive class in confusion matrix
1036	Frequency tables, pie charts, and bar charts are the most appropriate graphical displays for categorical variables. Below are a frequency table, a pie chart, and a bar graph for data concerning Penn State's undergraduate enrollments by campus in Fall 2017. Note that in the bar chart, the bars are separated by a space.	How can categorical data be represented
8140	Lab Color is a more accurate color space.  It specifies a color using a 3-axis system. The a-axis (green to red), b-axis (blue to yellow) and Lightness axis. The best thing about Lab Color is that it's device-independent. That means that it's easier to achieve exactly the same color across different media.	What does lab color mean
3553	Markov analysis is a method used to forecast the value of a variable whose predicted value is influenced only by its current state, and not by any prior activity. In essence, it predicts a random variable based solely upon the current circumstances surrounding the variable.	What is Markov model used for
2433	Population variance (σ2) tells us how data points in a specific population are spread out.  Here N is the population size and the xi are data points. μ is the population mean.	What is N in population variance
2937	Face-detection algorithms focus on the detection of frontal human faces. It is analogous to image detection in which the image of a person is matched bit by bit. Image matches with the image stores in database. Any facial feature changes in the database will invalidate the matching process.	What is face detection algorithm
3144	Classification Accuracy It is the ratio of number of correct predictions to the total number of input samples. It works well only if there are equal number of samples belonging to each class. For example, consider that there are 98% samples of class A and 2% samples of class B in our training set.	What is accuracy score in machine learning
6356	The best way to fix it is to perform a log transform of the same data, with the intent to reduce the skewness. After taking logarithm of the same data the curve seems to be normally distributed, although not perfectly normal, this is sufficient to fix the issues from a skewed dataset as we saw before.	How do you handle right skewed data
1320	Key TakeawaysA Bernoulli (success-failure) experiment is performed n times, and the trials are independent.The probability of success on each trial is a constant p ; the probability of failure is q=1−p q = 1 − p .The random variable X counts the number of successes in the n trials.	How do you find the probability of a binomial random variable
604	Particle filtering uses a set of particles (also called samples) to represent the posterior distribution of some stochastic process given noisy and/or partial observations.  The state-space model can be nonlinear and the initial state and noise distributions can take any form required.	What is an explanation for particle filters
2210	Firstly generate observations from a standard normal distribution then multiply by the standard deviation and add the mean.	How do you sample from a normal distribution with known mean and variance
486	No, the normal distribution cannot be skewed. It is a symmetric distribution with mean, median and mode being equal. However, a small sample from a normally distributed variable may be skewed.	Can normal distribution be skewed
7636	The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function.	Why do we need activation function in a Perceptron
7123	Confirmation bias can lead even the most experienced experts astray. Doctors, for example, will sometimes get attached to a diagnosis and then look for evidence of the symptoms they suspect already exist in a patient while ignoring markers of another disease or injury.	What are the dangers of confirmation bias
5597	Inferential statistics takes data from a sample and makes inferences about the larger population from which the sample was drawn.	What is inferential statistics 1
6035	AI has the potential to accelerate the process of achieving the global education goals through reducing barriers to access learning, automating management processes, and optimizing methods in order to improve learning outcomes.	How AI can improve education
1237	This type of distribution is useful when you need to know which outcomes are most likely, the spread of potential values, and the likelihood of different results. In this blog post, you'll learn about probability distributions for both discrete and continuous variables.	What is the importance of probability distribution
4641	Six quick tips to improve your regression modelingA.1. Fit many models.  A.2. Do a little work to make your computations faster and more reliable.  A.3. Graphing the relevant and not the irrelevant.  A.4. Transformations.  A.5. Consider all coefficients as potentially varying.  A.6. Estimate causal inferences in a targeted way, not as a byproduct of a large regression.	How can statistical models be improved
6001	SVMs (linear or otherwise) inherently do binary classification. However, there are various procedures for extending them to multiclass problems.  A binary classifier is trained for each pair of classes. A voting procedure is used to combine the outputs.	Do SVMs only do binary classification
1412	the state of being likely or probable; probability. a probability or chance of something: There is a strong likelihood of his being elected.	What is meant by likelihood
2112	Difference between deep learning and reinforcement learning The difference between them is that deep learning is learning from a training set and then applying that learning to a new data set, while reinforcement learning is dynamically learning by adjusting actions based in continuous feedback to maximize a reward.	What is the difference between deep learning and reinforcement learning
5928	Logarithmic Loss, or simply Log Loss, is a classification loss function often used as an evaluation metric in Kaggle competitions.  Log Loss quantifies the accuracy of a classifier by penalising false classifications.	What is log loss function
8654	Decision tree is unstable because training a tree with a slightly different sub-sample causes the structure of the tree to change drastically. It overfits by learning from noise data as well and optimises for that particular sample, which causes its variable importance order to change significantly.	Why are decision trees unstable
987	Created by the Google Brain team, TensorFlow is an open source library for numerical computation and large-scale machine learning. TensorFlow bundles together a slew of machine learning and deep learning (aka neural networking) models and algorithms and makes them useful by way of a common metaphor.	What is the purpose of TensorFlow
1002	A spectrum is simply a chart or a graph that shows the intensity of light being emitted over a range of energies.  Spectra can be produced for any energy of light, from low-energy radio waves to very high-energy gamma rays. Each spectrum holds a wide variety of information.	What is a spectral
8465	So the standard error of a mean provides a statement of probability about the difference between the mean of the population and the mean of the sample.  This is called the 95% confidence interval , and we can say that there is only a 5% chance that the range 86.96 to 89.04 mmHg excludes the mean of the population.	How does standard error relate to confidence interval
938	For independent random variables X and Y, the variance of their sum or difference is the sum of their variances: Variances are added for both the sum and difference of two independent random variables because the variation in each variable contributes to the variation in each case.	How do you find the variance of an independent variable
4733	Exponential beta value is interpreted with the reference category, where the probability of the dependent variable will increase or decrease. In continuous variables, it is interpreted with one unit increase in the independent variable, corresponding to the increase or decrease of the units of the dependent variable.	What is beta in logistic regression
8381	Among all continuous probability distributions with support [0, ∞) and mean μ, the exponential distribution with λ = 1/μ has the largest differential entropy. In other words, it is the maximum entropy probability distribution for a random variate X which is greater than or equal to zero and for which E[X] is fixed.	What does Lambda represent in exponential distribution
8370	Batch normalization is a technique that can improve the learning rate of a neural network. It does so by minimizing internal covariate shift which is essentially the phenomenon of each layer's input distribution changing as the parameters of the layer above it change during training.	What is batch normalization Pytorch
4142	Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used to solve complex problems.  Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.	What is meant by knowledge representation
2779	In general, K-means is a heuristic algorithm that partitions a data set into K clusters by minimizing the sum of squared distance in each cluster.  In this paper, the simulation of basic k-means algorithm is done, which is implemented using Euclidian distance metric.	What is the distance metric used in the standard implementation of K means to calculate the cluster assignments
4434	Like machine learning or deep learning, NLP is a subset of AI.  SAS offers a clear and basic explanation of the term: “Natural language processing makes it possible for humans to talk to machines.” It's the branch of AI that enables computers to understand, interpret, and manipulate human language.	What is natural language processing in AI
2079	Regularized regression is a type of regression where the coefficient estimates are constrained to zero. The magnitude (size) of coefficients, as well as the magnitude of the error term, are penalized.  “Regularization” is a way to give a penalty to certain models (usually overly complex ones).	What is Regularisation in regression
1751	Non-linearity in neural networks simply mean that the output at any unit cannot be reproduced from a linear function of the input.	What is non linearity in neural networks
3448	A low pass filter is a fixed filter just filters out frequencies above a passband. A Kalman filter can be used for state estimation, prediction of values in time and smoothing. A Kalman filter is a consequence of state variable models and LQG system theory. It has a gain which changes at each time step.	Is Kalman filter a low pass filter
884	Ridge regression has two main benefits. First, adding a penalty term reduces overfitting. Second, the penalty term guarantees that we can find a solution. I think the second part is easier to explain.	What are the benefits of using ridge regression over ordinary linear regression
473	While the previous study (Wu et al., 2015) suggests that ingroup derogation is a specialized mechanism which disregards explicit disease-relevant information mediated by outgroup members, a different pattern was observed in Experiment 2.	What is ingroup derogation
5091	This is because of the logistic distribution having heavier tails (than the normal distribution): Any outliers would not carry as much weight under the assumptions of the logistic (blue) distribution.  In a logistic regression does a very small P value for a predictor mean a good predictor or a bad predictor?	Why is logistic regression considered robust to outliers compared to a least square method
3245	A linear regression model extended to include more than one independent variable is called a multiple regression model. It is more accurate than to the simple regression.  The principal adventage of multiple regression model is that it gives us more of the information available to us who estimate the dependent variable.	Why is multiple regression preferable to single regression
5242	Econometrics is often “theory driven” while statistics tends to be “data driven”.  Typically, econometricians test theory using data, but often do little if any exploratory data analysis. On the other hand, I tend to build models after looking at data sets.	Is econometrics the same as statistics
2278	In probability theory and statistics, the marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables.	What does marginal distribution mean
1186	It might take about 2-4 hours of coding and 1-2 hours of training if done in Python and Numpy (assuming sensible parameter initialization and a good set of hyperparameters). No GPU required, your old but gold CPU on a laptop will do the job. Longer training time is expected if the net is deeper than 2 hidden layers.	How long does training a neural network take
7399	Collaborative filtering (CF) is a technique used by recommender systems.  In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating).	What is collaborative based filtering
1738	0:365:49Suggested clip · 61 secondsTensorboard Explained in 5 Min - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you read a TensorBoard
8345	Leaving a long gap between testing periods would not really help. For a test-retest coefficient to be an accurate estimate of reliability, there should be no change to the underlying trait (i.e. memory ability). If a long delay is enforced, subjects' actual memory capacities will change.	Can we really measure test retest reliability in memory tests
7334	Many instances of binomial distributions can be found in real life. For example, if a new drug is introduced to cure a disease, it either cures the disease (it's successful) or it doesn't cure the disease (it's a failure). If you purchase a lottery ticket, you're either going to win money, or you aren't.	How is binomial distribution used in real life
551	17:1525:32Suggested clip · 110 secondsStructural Equation Modeling: what is it and what can we use it for YouTubeStart of suggested clipEnd of suggested clip	How do you use structural equation modeling
6631	A sampling unit is a selection of a population that is used as an extrapolation of the population. For example, a household is used as a sampling unit, under the assumption that the polling results from this unit represents the opinions of a larger group. Related Courses. Guide to Audit Sampling.	What is a sampling unit in auditing
1756	The key to interpreting a hierarchical cluster analysis is to look at the point at which any given pair of cards “join together” in the tree diagram. Cards that join together sooner are more similar to each other than those that join together later.	How do you interpret hierarchical clustering
1031	Description. The alternating least squares (ALS) algorithm factorizes a given matrix R into two factors U and V such that R≈UTV.  Since matrix factorization can be used in the context of recommendation, the matrices U and V can be called user and item matrix, respectively.	What is ALS algorithm
5034	One of the major disadvantages of the backpropagation learning rule is its ability to get stuck in local minima. The error is a function of all the weights in a multidimensional space.	What are the limitations of back propagation network
4785	T-tests are called t-tests because the test results are all based on t-values. T-values are an example of what statisticians call test statistics. A test statistic is a standardized value that is calculated from sample data during a hypothesis test.	Is the test statistic the T value
3577	"This is in contrast to the Bernoulli, binomial, and hypergeometric distributions, where the number of possible values is finite.  Whereas, in the geometric and negative binomial distributions, the number of ""successes"" is fixed, and we count the number of trials needed to obtain the desired number of ""successes""."	What is the difference between hypergeometric distribution and geometric distribution
487	by Tim Bock. Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.	What is hierarchical clustering algorithm
2517	Sampling helps a lot in research. It is one of the most important factors which determines the accuracy of your research/survey result. If anything goes wrong with your sample then it will be directly reflected in the final result.	What is the importance of sampling technique
184	The F-distribution is a skewed distribution of probabilities similar to a chi-squared distribution. But where the chi-squared distribution deals with the degree of freedom with one set of variables, the F-distribution deals with multiple levels of events having different degrees of freedom.	What is af distribution
7519	In multivariate regression there are more than one dependent variable with different variances (or distributions). The predictor variables may be more than one or multiple.  But when we say multiple regression, we mean only one dependent variable with a single distribution or variance.	Is multiple regression A multivariate analysis
7640	Machine learning field allows you to code in a way so that the application or system can learn to solve the problem on it's own. Learning is a iterative process.	Is machine learning an iterative process
5676	Chunking in NLP is Changing a perception by moving a “chunk”, or a group of bits of information, in the direction of a Deductive or Inductive conclusion through the use of language.  you will start to get smaller pieces of information about a car.	What is chunking in NLP
2115	The main problem of using adaptive learning rate optimizers including Adam, RMSProp, etc. is the difficulty of being stuck on local minima while not converging to the global minimum.  These can lead to bad decisions of the optimizer and being stuck on local optima instead of finding global minima.	Why Adam Optimizer is bad
959	ProcedureFrom the cluster management console, select Workload > Spark > Deep Learning.Select the Datasets tab.Click New.Create a dataset from Images for Object Classification.Provide a dataset name.Specify a Spark instance group.Specify image storage format, either LMDB for Caffe or TFRecords for TensorFlow.More items	How do you create a dataset of an image
3357	You can regularize your network by introducing a dropout layer soon after the convolution layer. So a typical layer of Conv->Relu becomes Conv->Dropout->Relu. You may play around with the architecture rather than simply use pre-defined ones like VGG or AlexNet.	How is regularization implemented in the VGGNet 16 network
4958	An example of a mutually exclusive event is when a coin is a tossed and there are two events that can occur, either it will be a head or a tail. Hence, both the events here are mutually exclusive.Difference between Mutually exclusive and independent eventsMutually exclusive eventsIndependent events4 more rows	What is the difference between mutually exclusive and independent events explain by example examples
7447	So year is a discretized measure of a continuous interval variable, so quantitative.	Is year a quantitative variable
8328	Coefficient of correlation is “R” value which is given in the summary table in the Regression output. R square is also called coefficient of determination. Multiply R times R to get the R square value. In other words Coefficient of Determination is the square of Coefficeint of Correlation.	Is correlation coefficient r or r2
724	The Binomial Theorem: Formulas. The Binomial Theorem is a quick way (okay, it's a less slow way) of expanding (or multiplying out) a binomial expression that has been raised to some (generally inconveniently large) power. For instance, the expression (3x – 2)10 would be very painful to multiply out by hand.	How does the binomial theorem work
4185	In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.	What does regularization mean in machine learning
674	A symbol defining a class, such as 56 to 65 in Table (1), is called a class interval. The end numbers, 56 and 65, are called class limits; the smaller number (56) is the lower class limit, and the larger number (65) is the upper class limit.	What is the difference between class intervals and class boundaries
204	Advantages of Machine LearningContinuous Improvement. Machine Learning algorithms are capable of learning from the data we provide.  Automation for everything.  Trends and patterns identification.  Wide range of applications.  Data Acquisition.  Highly error-prone.  Algorithm Selection.  Time-consuming.	What are the advantages of machine learning
385	Then in July, Google launched AutoML for machine translation and natural language processing. These products have been adopted by Disney and Urban Outfitters in their practical applications. Behind AutoML is its engine called Neural Architecture Search, invented by Quoc Le, a pioneer in the AI Field.	Who invented AutoML
2890	Examples of False Alarm Ratios The FAR would be: number of false alarms / the total number of warnings or alarms: 8/20 = 0.40. In weather reporting, the false alarm ratio for tornado warnings is the number of false tornado warnings per total number of tornado warnings.	How is false alarm rate calculated
8506	It's a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction.	What is a loss function in ML
1182	A conjoint analysis step by step guide.Step 1: The Problem & Attribute.  Step 2: The Preference Model.  Step 3: The Data Collection.  Step 4: Presentation of Alternatives.  Step 5: The Experimental Design.  Step 6: Measurement Scale.  Step 7: Estimation Method.  Conclusion.	What are the steps in conjoint analysis
8690	Definitions. The median (middle quartile) marks the mid-point of the data and is shown by the line that divides the box into two parts. Half the scores are greater than or equal to this value and half are less. The middle “box” represents the middle 50% of scores for the group.	How do you interpret box plots
1033	Categorical Imperative sees an action as right or wrong, based on moral duty, without taking the consequences into account. Rule utilitarianism views an action as right or wrong only when we take the consequences of the action into account.	How does rule utilitarianism differ from kantianism
501	The points of our n dimensional space that obey a single one of our linear constraints as equalities, define a hyperplane.  It is a plane-like region of n-1 dimensions in an n dimensional space. A hyperplane that actual forms part of the boundary of the feasible region is called an n-1 face of that region.	What is Hyperplane in LPP
7196	"""Mean"" usually refers to the population mean. This is the mean of the entire population of a set.  The mean of the sample group is called the sample mean."	Is sample mean the same as mean
1351	The Poisson parameter Lambda (λ) is the total number of events (k) divided by the number of units (n) in the data (λ = k/n).  In between, or when events are infrequent, the Poisson distribution is used.	What does Lambda mean in Poisson distribution
553	"The sampling distribution of the sample mean can be thought of as ""For a sample of size n, the sample mean will behave according to this distribution."" Any random draw from that sampling distribution would be interpreted as the mean of a sample of n observations from the original population."	What does a sampling distribution of sample means represent
1677	An example of Multiple stage sampling by clusters – An organization intends to survey to analyze the performance of smartphones across Germany. They can divide the entire country's population into cities (clusters) and select cities with the highest population and also filter those using mobile devices.	What is an example of cluster sampling
2599	In summary, model parameters are estimated from data automatically and model hyperparameters are set manually and are used in processes to help estimate model parameters. Model hyperparameters are often referred to as parameters because they are the parts of the machine learning that must be set manually and tuned.	What is the difference between parameter and Hyperparameter
215	The finite population correction (fpc) factor is used to adjust a variance estimate for an estimated mean or total, so that this variance only applies to the portion of the population that is not in the sample.	What is finite population correction
6292	An odds ratio is a measure of association between the presence or absence of two properties.  The value of the odds ratio tells you how much more likely someone under 25 might be to make a claim, for example, and the associated confidence interval indicates the degree of uncertainty associated with that ratio.	What is odds ratio and confidence interval
163	The central limit theorem states that the sampling distribution of the mean approaches a normal distribution, as the sample size increases.  Therefore, as a sample size increases, the sample mean and standard deviation will be closer in value to the population mean μ and standard deviation σ .	Does the mean increase with sample size
2110	Though unsupervised learning also could be used for anomaly detection, they are shown to perform very poorly compared to supervised or semi-supervised learning3.  Reinforcement learning brings the full power of Artificial Intelligence to anomaly detection.	Can reinforcement learning be used for anomaly detection
7471	To calculate how much weight you need, divide the known population percentage by the percent in the sample. For this example: Known population females (51) / Sample Females (41) = 51/41 = 1.24. Known population males (49) / Sample males (59) = 49/59 = .	How do you do weightage to a variable
132	We obtain the moment generating function MX(t) from the expected value of the exponential function. We can then compute derivatives and obtain the moments about zero. M′X(t)=0.35et+0.5e2tM″X(t)=0.35et+e2tM(3)X(t)=0.35et+2e2tM(4)X(t)=0.35et+4e2t.	How do you find the moments of a moment generating function
6144	The standard error is also inversely proportional to the sample size; the larger the sample size, the smaller the standard error because the statistic will approach the actual value. The standard error is considered part of descriptive statistics. It represents the standard deviation of the mean within a dataset.	How does sample size effect standard error
6638	In exclusive form, the lower and upper limits are known as true lower limit and true upper limit of the class interval. Thus, class limits of 10 - 20 class intervals in the exclusive form are 10 and 20. In inclusive form, class limits are obtained by subtracting 0.5 from lower limitand adding 0.5 to the upper limit.	What is class interval and class limit
1018	: to aim an attack at someone or something. : to direct an action, message, etc., at someone or something.	What does it mean to target someone
143	Convergence is the ability to turn the two eyes inward toward each other to look at a close object. We depend on this visual skill for near-work activities such as desk work at school, working on a smartphone type device, or even in sports when catching a ball.	What is convergence in vision
38	A true positive is an outcome where the model correctly predicts the positive class. Similarly, a true negative is an outcome where the model correctly predicts the negative class. A false positive is an outcome where the model incorrectly predicts the positive class.	What is false positive and true positive
3911	The difference between MLE/MAP and Bayesian inference MLE gives you the value which maximises the Likelihood P(D|θ). And MAP gives you the value which maximises the posterior probability P(θ|D).  MLE and MAP returns a single fixed value, but Bayesian inference returns probability density (or mass) function.	What is the difference between MLE and map
3990	The value of the step size s depends on the fauntion. If it is too small the algorithm will be too slow. If it is too large the algrithm may over shoot the global minimum and behave eratically. Usually we set s to something like 0.01 and then adjust according to the results.	What is the step size in gradient descent
6759	Calculating the SVD consists of finding the eigenvalues and eigenvectors of AAT and ATA. The eigenvectors of ATA make up the columns of V , the eigenvectors of AAT make up the columns of U. Also, the singular values in S are square roots of eigenvalues from AAT or ATA.	How do you calculate the decomposition of a SVD
5116	We can interpret the Poisson regression coefficient as follows: for a one unit change in the predictor variable, the difference in the logs of expected counts is expected to change by the respective regression coefficient, given the other predictor variables in the model are held constant.	How do you interpret Poisson regression
4644	We discuss some wonders in the field of image processing with machine learning advancements. Image processing can be defined as the technical analysis of an image by using complex algorithms. Here, image is used as the input, where the useful information returns as the output.	How does image processing work in machine learning
599	Principal Component Analysis (PCA) is used to explain the variance-covariance structure of a set of variables through linear combinations. It is often used as a dimensionality-reduction technique.	Why do we use principal component analysis
3967	SVMs and decision trees are discriminative models because they learn explicit boundaties between classes. SVM is a maximal margin classifier, meaning that it learns a decision boundary that maximizes the distance between samples of the two classes, given a kernel.	Is decision tree generative or discriminative
7057	Compressed sensing (also known as compressive sensing, compressive sampling, or sparse sampling) is a signal processing technique for efficiently acquiring and reconstructing a signal, by finding solutions to underdetermined linear systems.	What is compressed sensing compressive sampling in laymans terms
6465	MGF Properties If two random variables have the same MGF, then they must have the same distribution. That is, if X and Y are random variables that both have MGF M(t), then X and Y are distributed the same way (same CDF, etc.). You could say that the MGF determines the distribution.	What are the properties of moment generating function
6219	A tensor is a generalization of vectors and matrices and is easily understood as a multidimensional array.  It is a term and set of techniques known in machine learning in the training and operation of deep learning models can be described in terms of tensors.	What is tensor in machine learning
5895	Data is the currency of applied machine learning. Therefore, it is important that it is both collected and used effectively. Data sampling refers to statistical methods for selecting observations from the domain with the objective of estimating a population parameter.	What is sampling in deep learning
4466	Linear mixed models (sometimes called “multilevel models” or “hierarchical models”, depending on the context) are a type of regression model that take into account both (1) variation that is explained by the independent variables of interest (like lm() ) – fixed effects, and (2) variation that is not explained by the	What is a mixed model regression
2742	It's also important to understand what to focus on and what to do first.Pick a topic you are interested in.  Find a quick solution.  Improve your simple solution.  Share your solution.  Repeat steps 1-4 for different problems.  Complete a Kaggle competition.  Use machine learning professionally.	What are the steps to learn artificial intelligence
608	The normal distribution can be used as an approximation to the binomial distribution, under certain circumstances, namely: If X ~ B(n, p) and if n is large and/or p is close to ½, then X is approximately N(np, npq)	Can the normal distribution be used to approximate this probability
5672	The Poisson distribution is a discrete function, meaning that the event can only be measured as occurring or not as occurring, meaning the variable can only be measured in whole numbers. Fractional occurrences of the event are not a part of the model. it was named after French mathematician Siméon Denis Poisson.	Is a Poisson distribution discrete or continuous
4670	The ReLu (Rectified Linear Unit) Layer ReLu refers to the Rectifier Unit, the most commonly deployed activation function for the outputs of the CNN neurons. Mathematically, it's described as: Unfortunately, the ReLu function is not differentiable at the origin, which makes it hard to use with backpropagation training.	What is ReLU layer in CNN
1818	fits that relationship. That line is called a Regression Line and has the equation ŷ= a + b x. The Least Squares Regression Line is the line that makes the vertical distance from the data points to the regression line as small as possible.	What is the equation of the least squares regression line
4462	First, linear regression needs the relationship between the independent and dependent variables to be linear. It is also important to check for outliers since linear regression is sensitive to outlier effects.  Multicollinearity occurs when the independent variables are too highly correlated with each other.	Why is it important to examine the assumption of linearity in regression
3321	Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance.	What do you understand by bias variance trade off
7501	A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.	What is network ACLs in AWS
7394	The level of statistical significance is often expressed as a p-value between 0 and 1. The smaller the p-value, the stronger the evidence that you should reject the null hypothesis. A p-value less than 0.05 (typically ≤ 0.05) is statistically significant.	What is the relationship between the P value of a study and its statistical significance
5219	A GLM consists of three components: A random component, A systematic component, and. A link function.	What are the three components of a generalized linear model
4461	In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.	What is the learning rate in machine learning
107	In complete-linkage clustering, the link between two clusters contains all element pairs, and the distance between clusters equals the distance between those two elements (one in each cluster) that are farthest away from each other.	What does measuring distance between clusters mean in case of complete linkage
1167	x̄ = ( Σ xi ) / nAdd up the sample items.Divide sum by the number of samples.The result is the mean.Use the mean to find the variance.Use the variance to find the standard deviation.	How do you find the mean of a random sample
3642	The resulting digital time record is then mathematically transformed into a frequency spectrum using an algorithm known as the Fast Fourier Transform, or FFT. The FFT is simply a clever set of operations which implements Fourier's theorem. The resulting spectrum shows the frequency components of the input signal.	What is an FFT spectrum
4911	EXAMPLES OF DATA MINING APPLICATIONS Marketing. Data mining is used to explore increasingly large databases and to improve market segmentation.  It is commonly applied to credit ratings and to intelligent anti-fraud systems to analyse transactions, card transactions, purchasing patterns and customer financial data.	What is data mining and example
8245	The t-value measures the size of the difference relative to the variation in your sample data. Put another way, T is simply the calculated difference represented in units of standard error. The greater the magnitude of T, the greater the evidence against the null hypothesis.	What does the t statistic indicate
5121	Supervised: Use the target variable (e.g. remove irrelevant variables).Wrapper: Search for well-performing subsets of features. RFE.Filter: Select subsets of features based on their relationship with the target. Feature Importance Methods.Intrinsic: Algorithms that perform automatic feature selection during training.	How do you choose best features for classification
3042	Hashing provides a more reliable and flexible method of data retrieval than any other data structure. It is faster than searching arrays and lists. In the same space it can retrieve in 1.5 probes anything stored in a tree that will otherwise take log n probes.	What are the advantages of hashing
274	In statistical hypothesis testing, the null distribution is the probability distribution of the test statistic when the null hypothesis is true. For example, in an F-test, the null distribution is an F-distribution. Null distribution is a tool scientists often use when conducting experiments.	What distribution does the test statistic follow under the null hypothesis
839	Cluster analysis is an exploratory analysis that tries to identify structures within the data. Cluster analysis is also called segmentation analysis or taxonomy analysis. More specifically, it tries to identify homogenous groups of cases if the grouping is not previously known.	What does a cluster analysis help identify
1330	Momentum [1] or SGD with momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging. It is one of the most popular optimization algorithms and many state-of-the-art models are trained using it.	What does momentum do in SGD
3945	Center: The center is not affected by sample size. The mean of the sample means is always approximately the same as the population mean µ = 3,500. Spread: The spread is smaller for larger samples, so the standard deviation of the sample means decreases as sample size increases.	What effect does the sample size have on the standard deviation of all possible sample means
379	Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.	What is bootstrapping used for
8202	Yes, you can use linear regression for prediction as long as the value of the unseen exploratory variable (x) is within the range of the x that was used to fit the linear model.	Does linear regression predict future values
5327	Multivariate analysis is a set of statistical techniques used for analysis of data that contain more than one variable.  Multivariate analysis refers to any statistical technique used to analyse more complex sets of data.	What is the meaning of multivariate analysis
1138	2. Why is it important to examine a residual plot even if a scatterplot appears to be linear? An examination of the of the residuals often leads us to discover groups of observations that are different from the rest.	Why is it important to examine a residual plot
804	A decision tree is a flowchart-like diagram that shows the various outcomes from a series of decisions. It can be used as a decision-making tool, for research analysis, or for planning strategy. A primary advantage for using a decision tree is that it is easy to follow and understand.	What is decision tree diagram
4830	The following are examples of discrete probability distributions commonly used in statistics:Binomial distribution.Geometric Distribution.Hypergeometric distribution.Multinomial Distribution.Negative binomial distribution.Poisson distribution.	What are the different types of discrete probability distributions
4895	Metaphor in Psychology Metaphors derive their power from how confused we are as human beings. Our brains have evolved to confuse the literal and the symbolic by cramming viscerally similar functions in the same brain areas. For example: The insula processes both physical and moral disgust.	How do metaphors influence the brain
397	"Definition. A sigmoid function is a bounded, differentiable, real function that is defined for all real input values and has a non-negative derivative at each point and exactly one inflection point. A sigmoid ""function"" and a sigmoid ""curve"" refer to the same object."	What is sigmoid function & squashing
1251	Like I said before, the AUC-ROC curve is only for binary classification problems. But we can extend it to multiclass classification problems by using the One vs All technique. So, if we have three classes 0, 1, and 2, the ROC for class 0 will be generated as classifying 0 against not 0, i.e. 1 and 2.	Can the AUC ROC can be used for both classification as well as regression problems
4636	Unsupervised learning is where you only have input data (X) and no corresponding output variables. The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data.	What kind of data does unsupervised learning uses to train the model
799	For trials with categorical outcomes (such as noting the presence or absence of a term), one way to estimate the probability of an event from data is simply to count the number of times an event occurred divided by the total number of trials.	What is estimation in probability
8609	Parameters are key to machine learning algorithms.  In this case, a parameter is a function argument that could have one of a range of values. In machine learning, the specific model you are using is the function and requires parameters in order to make a prediction on new data.	What is a parameter machine learning
2208	There are various ways to modify a study design to actively exclude or control confounding variables (3) including Randomization, Restriction and Matching. In randomization the random assignment of study subjects to exposure categories to breaking any links between exposure and confounders.	How do you deal with confounders within a statistical study
8357	Enthalpy ( H ) is defined as the amount of energy released or absorbed during a chemical reaction. Entropy ( S ) defines the degree of randomness or disorder in a system. where at constant temperature, the change on free energy is defined as: ΔG=ΔH−TΔS .	What is the relation between enthalpy and entropy
2372	Unsupervised Learning is the second type of machine learning, in which unlabeled data are used to train the algorithm, which means it used against data that has no historical labels.	What category of machine learning algorithm finds patterns in the data when the data is not labeled
7091	The reason for using L1 norm to find a sparse solution is due to its special shape. It has spikes that happen to be at sparse points. Using it to touch the solution surface will very likely to find a touch point on a spike tip and thus a sparse solution.	Why does l1 norm cause sparsity
1263	Parametric tests are those that make assumptions about the parameters of the population distribution from which the sample is drawn. This is often the assumption that the population data are normally distributed. Non-parametric tests are “distribution-free” and, as such, can be used for non-Normal variables.	What is the difference parametric and nonparametric tests
1645	Normalization is the process of organizing data into a related table; it also eliminates redundancy and increases the integrity which improves performance of the query. To normalize a database, we divide the database into tables and establish relationships between the tables.	What is normalization and types of normalization
563	AUC (Area Under Curve)-ROC (Receiver Operating Characteristic) is a performance metric, based on varying threshold values, for classification problems. As name suggests, ROC is a probability curve and AUC measure the separability.	What is metrics in machine learning
6608	The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above . 05 indicates normality.	How do you test if a data set is normally distributed
3904	Correlation between a continuous and categorical variable There are three big-picture methods to understand if a continuous and categorical are significantly correlated — point biserial correlation, logistic regression, and Kruskal Wallis H Test.	How do you find the correlation between categorical and continuous variables
2561	According to the realistic conflict theory, ingroup bias arises from competition for resources between groups. Since different groups are all competing for the same available resources, it serves the best interests of the group to favor members while spurning outsiders.	Why does ingroup bias occur
4478	Maximum likelihood estimation involves defining a likelihood function for calculating the conditional probability of observing the data sample given a probability distribution and distribution parameters. This approach can be used to search a space of possible distributions and parameters.	What is maximum likelihood estimation in machine learning
1178	Independence of Random Variables If two random variables, X and Y, are independent, they satisfy the following conditions. P(x|y) = P(x), for all values of X and Y. P(x ∩ y) = P(x) * P(y), for all values of X and Y.	Are X and Y independent random variables
8073	R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  After fitting a linear regression model, you need to determine how well the model fits the data.	What does the R Squared tell us
2552	The function fX(x) gives us the probability density at point x. It is the limit of the probability of the interval (x,x+Δ] divided by the length of the interval as the length of the interval goes to 0. Remember that P(x<X≤x+Δ)=FX(x+Δ)−FX(x).	How do you find probability density
6750	The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(∗). Both functions will take any number and rescale it to fall between 0 and 1.	What is the difference between logit and probit model
3080	5 Techniques to Prevent Overfitting in Neural NetworksSimplifying The Model. The first step when dealing with overfitting is to decrease the complexity of the model.  Early Stopping. Early stopping is a form of regularization while training a model with an iterative method, such as gradient descent.  Use Data Augmentation.  Use Regularization.  Use Dropouts.	How does neural network deal with Overfitting
632	The discount factor essentially determines how much the reinforcement learning agents cares about rewards in the distant future relative to those in the immediate future. If γ=0, the agent will be completely myopic and only learn about actions that produce an immediate reward.	Why is the discount factor important in reinforcement learning
6085	A Blob is a group of connected pixels in an image that share some common property ( E.g grayscale value ). In the image above, the dark connected regions are blobs, and the goal of blob detection is to identify and mark these regions.	What is blob in OBject detection
2729	The purpose of factor analysis is to reduce many individual items into a fewer number of dimensions. Factor analysis can be used to simplify data, such as reducing the number of variables in regression models.	What is the purpose of factor analysis
8579	In statistics and probability, quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample in the same way. There is one fewer quantile than the number of groups created.	What is quantile distribution
313	Our Big Data Hadoop certification training course lets you master the concepts of the Hadoop framework, Big Data tools, and methodologies to prepare you for success in your role as a Big Data Developer. Learn how various components of the Hadoop ecosystem fit into the Big Data processing lifecycle.	What is big data Course
6568	Performing Accuracy Assessment In Erdas Imagine img' that you created in a viewer. Click on the Raster tab –> Classification –> Supervised –> Accuracy Assessment. A new window will open which is the main window for the accuracy assessment tool. A new window will open to set the settings for the accuracy assessment.	How do you do accuracy assessment in erdas
5527	In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable.	What does the maximum likelihood estimate mean
2243	An activation function is a function used in artificial neural networks which outputs a small value for small inputs, and a larger value if its inputs exceed a threshold.  Activation functions are useful because they add non-linearities into neural networks, allowing the neural networks to learn powerful operations.	What is meant by activation function
1284	Spatiotemporal models arise when data are collected across time as well as space and has at least one spatial and one temporal property. An event in a spatiotemporal dataset describes a spatial and temporal phenomenon that exists at a certain time t and location x.	What is spatio temporal model
6298	Categorical Data is the data that generally takes a limited number of possible values. Also, the data in the category need not be numerical, it can be textual in nature. All machine learning models are some kind of mathematical model that need numbers to work with.	What is categorical features in machine learning
2667	The softmax function is simply a generalisation of the logistic function, which simply squashes values into a given range.  the reason for using the softmax is to ensure these logits all sum up to 1, thereby fulfilling the constraints of a probability density.	Why is the Softmax function necessary in the output layer
8139	Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks.  Spreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents.	What does spreading activation mean
5062	Use. Cluster sampling is typically used in market research. It's used when a researcher can't get information about the population as a whole, but they can get information about the clusters.  Cluster sampling is often more economical or more practical than stratified sampling or simple random sampling.	Why do we use cluster sampling
1742	The purpose of the activation function is to introduce non-linearity into the output of a neuron. We know, neural network has neurons that work in correspondence of weight, bias and their respective activation function.	Why does CNN use activation function
630	Introduction[edit] Shift Invariance simply refers to the 'invariance' that a CNN has to recognising images. It allows the CNN to detect features/objects even if it does not look exactly like the images in it's training period. Shift invariance covers 'small' differences, such as movements shifts of a couple of pixels.	What is spatial invariance in CNN
7081	Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension.	What is dimension reduction in Data Science
5722	How to avoid selection biasesUsing random methods when selecting subgroups from populations.Ensuring that the subgroups selected are equivalent to the population at large in terms of their key characteristics (this method is less of a protection than the first, since typically the key characteristics are not known).	How can we prevent self selection bias
6987	If your TST (Mantoux) or Quantiferon blood test was found to be positive, this means you have a latent TB infection, but usually not the active disease.	What does it mean when a QuantiFERON is positive
818	Since a Naive Bayes text classifier is based on the Bayes's Theorem, which helps us compute the conditional probabilities of occurrence of two events based on the probabilities of occurrence of each individual event, encoding those probabilities is extremely useful.	Why is naive Bayes used for text classification
7742	Data Structure - Depth First TraversalRule 1 − Visit the adjacent unvisited vertex. Mark it as visited. Display it. Push it in a stack.Rule 2 − If no adjacent vertex is found, pop up a vertex from the stack. (It will pop up all the vertices from the stack, which do not have adjacent vertices.)Rule 3 − Repeat Rule 1 and Rule 2 until the stack is empty.	How do I find depth first search
675	An unbiased estimator is an accurate statistic that's used to approximate a population parameter.  That's just saying if the estimator (i.e. the sample mean) equals the parameter (i.e. the population mean), then it's an unbiased estimator.	What is an unbiased estimator in statistics
3780	How to calculate percentileRank the values in the data set in order from smallest to largest.Multiply k (percent) by n (total number of values in the data set).  If the index is not a round number, round it up (or down, if it's closer to the lower number) to the nearest whole number.Use your ranked data set to find your percentile.	How is percentile calculated
402	In Excel 2016 for Mac: Click Data > Solver. In Excel for Mac 2011: Click the Data tab, under Analysis, click Solver. In Set Objective, enter a cell reference or name for the objective cell. Note: The objective cell must contain a formula.	How do you set objectives in solver
593	. Thus logit regression is simply the GLM when describing it in terms of its link function, and logistic regression describes the GLM in terms of its activation function.	Whats the difference between logit and logistic regression 1
792	ANSWER. A false positive means that the results say you have the condition you were tested for, but you really don't. With a false negative, the results say you don't have a condition, but you really do.	What is meant by false positive and false negative
2961	Connection to stratified sampling Quota sampling is the non-probability version of stratified sampling. In stratified sampling, subsets of the population are created so that each subset has a common characteristic, such as gender.	Is stratified sampling non probability
1144	Repeated measures design is a research design that involves multiple measures of the same variable taken on the same or matched subjects either under different conditions or over two or more time periods. For instance, repeated measurements are collected in a longitudinal study in which change over time is assessed.	What are repeated measures in statistics
6422	"The sampling distribution of the sample mean can be thought of as ""For a sample of size n, the sample mean will behave according to this distribution."" Any random draw from that sampling distribution would be interpreted as the mean of a sample of n observations from the original population."	What is the sampling distribution of the sample mean
6683	Fine-tuning, in general, means making small adjustments to a process to achieve the desired output or performance. Fine-tuning deep learning involves using weights of a previous deep learning algorithm for programming another similar deep learning process.	What is fine tuning in deep learning
1106	Handling overfittingReduce the network's capacity by removing layers or reducing the number of elements in the hidden layers.Apply regularization , which comes down to adding a cost to the loss function for large weights.Use Dropout layers, which will randomly remove certain features by setting them to zero.	What do you do if a deep learning model is Overfitting
1101	It is quite simple: if you are running a logit regression, a negative coefficient simply implies that the probability that the event identified by the DV happens decreases as the value of the IV increases.	What do negative coefficients mean in a logit regression
2154	Having good test re-test reliability signifies the internal validity of a test and ensures that the measurements obtained in one sitting are both representative and stable over time.	Why is test retest reliability important
6399	While Neural Networks use neurons to transmit data in the form of input values and output values through connections, Deep Learning is associated with the transformation and extraction of feature which attempts to establish a relationship between stimuli and associated neural responses present in the brain.	Is deep learning and neural networks the same
2805	A target function, in machine learning, is a method for solving a problem that an AI algorithm parses its training data to find. Once an algorithm finds its target function, that function can be used to predict results (predictive analysis).	What is a target function in machine learning
5420	If the model fit to the data were correct, the residuals would approximate the random errors that make the relationship between the explanatory variables and the response variable a statistical relationship. Therefore, if the residuals appear to behave randomly, it suggests that the model fits the data well.	How will you check if the model fits the data
623	Like random forests, gradient boosting is a set of decision trees. The two main differences are: How trees are built: random forests builds each tree independently while gradient boosting builds one tree at a time.	What is the difference between random forest and gradient boosting
4446	: having or involving a number of independent mathematical or statistical variables multivariate calculus multivariate data analysis.	What does multivariate mean
795	A linear relationship can also be found in the equation distance = rate x time. Because distance is a positive number (in most cases), this linear relationship would be expressed on the top right quadrant of a graph with an X and Y-axis.	How do you find the linear relationship between two variables
2419	What a p-value tells you about statistical significance. When you perform a statistical test a p-value helps you determine the significance of your results in relation to the null hypothesis.	What is the use and importance of the P value in statistics
7952	Correspondence Analysis (CA) is a multivariate graphical technique designed to explore relationships among categorical variables. Epidemiologists frequently collect data on multiple categorical variables with to the goal of examining associations amongst these variables.	What is correspondence analysis used for
6903	Fisher's exact test is a statistical test used to determine if there are nonrandom associations between two categorical variables. . For each one, calculate the associated conditional probability using (2), where the sum of these probabilities must be 1.	What is Fisher's exact test used for
5468	A function of one or more parameters containing a noise term. where the noise is (without loss of generality) assumed to be additive. SEE ALSO: Noise, Stochastic Optimization.	What is a stochastic function
2693	Linear regression is one of the most common techniques of regression analysis. Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables.	Is multiple regression the same as multiple linear regression
697	"To see what the bias term represents, simply set all to 0. The resulting log odds is the bias term. In other words, the bias term is the ""default"" log odds for the case that all predictors equal 0 (or equal to reference value for categorical predictors). For example, if = 2.5, then the log odds of the outcome is 2.5."	How do you calculate bias in logistic regression
6067	The term general linear model (GLM) usually refers to conventional linear regression models for a continuous response variable given continuous and/or categorical predictors. It includes multiple linear regression, as well as ANOVA and ANCOVA (with fixed effects only).	What does a general linear model show
2373	A distribution in statistics is a function that shows the possible values for a variable and how often they occur.	What is a distribution in statistics
6087	1 Answer. In word2vec, you train to find word vectors and then run similarity queries between words. In doc2vec, you tag your text and you also get tag vectors.  If two authors generally use the same words then their vector will be closer.	How is Doc2Vec different from word2vec
7811	Bayesian learning has many advantages over other learning programs: Interpolation Bayesian learning methods interpolate all the way to pure engineering. When faced with any learning problem, there is a choice of how much time and effort a human vs. a computer puts in.	What are the advantages of Bayesian methods in machine learning
399	Linear filters process time-varying input signals to produce output signals, subject to the constraint of linearity. This results from systems composed solely of components (or digital algorithms) classified as having a linear response.	What are the filter characteristics of linear systems
6628	How to deploy an Object Detection Model with TensorFlow servingCreate a production ready model for TF-Serving.  Create TF-serving environment using Docker.  Creating a client to request the model server running in the Docker container for inference on a test image.	How do you deploy an object detection model with TensorFlow serving
319	In Bayesian statistical inference, a prior probability distribution, often simply called the prior, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account.  Priors can be created using a number of methods.	What is a prior in Bayesian statistics
725	The critical value is a factor used to compute the margin of error, as shown in the equations below. When the sampling distribution of the statistic is normal or nearly normal, the critical value can be expressed as a t score or as a z-score.	Is critical value the same as Z score
4227	The law of averages is sometimes known as “Gambler's Fallacy. ” It evokes the idea that an event is “due” to happen.  The law of averages says it's due to land on black! ” Of course, the wheel has no memory and its probabilities do not change according to past results.	Why is the law of averages considered a logical fallacy
539	Restricted Boltzmann Machines are shallow, two-layer neural nets that constitute the building blocks of deep-belief networks. The first layer of the RBM is called the visible, or input layer, and the second is the hidden layer. Each circle represents a neuron-like unit called a node.	What are the two layers of a restricted Boltzmann machine
14	Binomial theorem, statement that for any positive integer n, the nth power of the sum of two numbers a and b may be expressed as the sum of n + 1 terms of the form. Binomial theorem.	What does the binomial theorem state
4199	Logistic regression is a classification algorithm, used when the value of the target variable is categorical in nature. Logistic regression is most commonly used when the data in question has binary output, so when it belongs to one class or another, or is either a 0 or 1.	Why logistic regression is used for classification
1223	While implementing the decision tree we will go through the following two phases:Building Phase. Preprocess the dataset. Split the dataset from train and test using Python sklearn package. Train the classifier.Operational Phase. Make predictions. Calculate the accuracy.	How do you implement a decision tree
3000	MSE is the average of the squared error that is used as the loss function for least squares regression: It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE.	What is mean square error in machine learning
1119	The delta rule is a straight-forward application of gradient descent (i.e. hill climbing), and is easy to do because in a neural network with a single hidden layer, the neurons have direct access to the error signal.	What is Delta in backpropagation
264	5. Image Processing Using Machine LearningFeature mapping using the scale-invariant feature transform (SIFT) algorithm.Image registration using the random sample consensus (RANSAC) algorithm.Image Classification using artificial neural networks.Image classification using convolutional neural networks (CNNs)Image Classification using machine learning.More items	How is image processing used in machine learning
1216	load_model functionv2. 0. Load a model from a shortcut link, package or data path. If called with a shortcut link or package name, spaCy will assume the model is a Python package and import and call its load() method.	Which function is used to load a model in spaCy
3738	A term document matrix is a way of representing the words in the text as a table (or matrix) of numbers. The rows of the matrix represent the text responses to be analysed, and the columns of the matrix represent the words from the text that are to be used in the analysis. The most basic version is binary.	What is document term matrix in R
4822	Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used to solve complex problems.  Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.	What is knowledge representation in artificial intelligence
8545	The toss of a coin, throw of a dice and lottery draws are all examples of random events.	What is an example of a random event
4692	Answer: Agglomerative Hierarchical clustering method allows the clusters to be read from bottom to top and it follows this approach so that the program always reads from the sub-component first then moves to the parent whereas, divisive uses top-bottom approach in which the parent is visited first then the child.	What is the difference between agglomerative and divisive hierarchical clustering
170	The slope of a least squares regression can be calculated by m = r(SDy/SDx). In this case (where the line is given) you can find the slope by dividing delta y by delta x. So a score difference of 15 (dy) would be divided by a study time of 1 hour (dx), which gives a slope of 15/1 = 15.	How do you interpret the slope of the least squares regression line
1573	The name 'exponential smoothing' is attributed to the use of the exponential window function during convolution. It is no longer attributed to Holt, Winters & Brown. , and the weights assigned to previous observations are proportional to the terms of the geometric progression. .	Why is it called exponential smoothing
236	Face detection and recognition process. The facial recognition process begins with an application for the camera, installed on any compatible device in communication with said camera.  This application is then able to use computer vision and a deep neural network in order to find a prospective face within its stream.	Is facial recognition computer vision
7562	You will need to know the standard deviation of the population in order to calculate the sampling distribution. Add all of the observations together and then divide by the total number of observations in the sample.	How do you find the sampling distribution
4255	In Bayesian statistical inference, a prior probability distribution, often simply called the prior, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account.  Priors can be created using a number of methods.	What is the role of prior distribution in Bayesian learning
3169	Galton fit a line to each set of heights, and added a reference line to show the average adult height (68.25 inches).  So here's the irony: The term regression, as Galton used it, didn't refer to the statistical procedure he used to determine the fit lines for the plotted data points.	Why is it called regression line
503	The terms dummy variable and binary variable are sometimes used interchangeably. However, they are not exactly the same thing.  If your dummy variable has only two options, like 1=Male and 2=female, then that dummy variable is also a binary variable.	Which is a more acceptable term binary variable or dummy variable
6028	In Convolutional Neural Networks, Filters detect spatial patterns such as edges in an image by detecting the changes in intensity values of the image.	What are filters in convolutional neural networks
5507	Denying the antecedent, sometimes also called inverse error or fallacy of the inverse, is a formal fallacy of inferring the inverse from the original statement. It is committed by reasoning in the form: If P, then Q. Therefore, if not P, then not Q.	What is denying the antecedent in relation to a propositional fallacy
218	Summary. A Random Variable is a variable whose possible values are numerical outcomes of a random experiment. Random Variables can be discrete or continuous. An important example of a continuous Random variable is the Standard Normal variable, Z.	What is an example of a continuous random variable
445	A common strategy is to grow the tree until each node contains a small number of instances then use pruning to remove nodes that do not provide additional information. Pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross-validation set.	How do you prune a decision tree
1280	Regularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the model's performance on the unseen data as well.	What is regularization in CNN
7937	Google uses artificial neural networks to power voice search.	Is Google an example of artificial intelligence
1924	In image processing, thresholding is used to split an image into smaller segments, or junks, using at least one color or gray scale value to define their boundary. The advantage of obtaining first a binary image is that it reduces the complexityof the data and simplifies the process of recognition and classification.	Why thresholding is used in image processing
5996	In statistics and research, internal consistency is typically a measure based on the correlations between different items on the same test (or the same subscale on a larger test). It measures whether several items that propose to measure the same general construct produce similar scores.	What is Internal Consistency in testing
688	"At a high level, there are two families of fairness definitions.  ""Statistical"" definitions of fairness ask for equality of some error metric (like false positive rate) evaluated over ""protected"" populations. These are easy to check and satisfy, but don't provide guarantees to individuals."	What is statistical fairness
740	Quota Sampling also has its pros and cons. As this process sets criteria to choose samples, disadvantages are mainly due to its non-random nature. Some of the disadvantages are as follows: Since quota sampling is a non-random sampling method, it is impossible to find the sampling error.	What is quota sampling advantages and disadvantages
6044	To convert a frequency distribution to a probability distribution, divide area of the bar or interval of x by the total area of all the Bars. A simpler formula is: , N is the total Frequency and w is the interval of x.	How do you construct a probability distribution from a frequency distribution
296	Partitioning is when an area of data storage is sub-divided to improve performance. Think of it as an organizational tool. If all your collected data is in one large space without organization the digital tools used for analyzing it will have a more difficult time finding the information in order to analyze it.	What is partitioning in ETL
7667	“Statistical significance helps quantify whether a result is likely due to chance or to some factor of interest,” says Redman. When a finding is significant, it simply means you can feel confident that's it real, not that you just got lucky (or unlucky) in choosing the sample.	Why statistical significance is important
996	Metrics like accuracy, precision, recall are good ways to evaluate classification models for balanced datasets, but if the data is imbalanced and there's class disparity, then other methods like ROC/AUC perform better in evaluating the model performance.	What are the metrics chosen to evaluate model performance
8402	Introduction to K-Means ClusteringStep 1: Choose the number of clusters k.  Step 2: Select k random points from the data as centroids.  Step 3: Assign all the points to the closest cluster centroid.  Step 4: Recompute the centroids of newly formed clusters.  Step 5: Repeat steps 3 and 4.	How do you use K means clustering
879	This serves the process of symmetry-breaking and gives much better accuracy. In this method, the weights are initialized very close to zero, but randomly. This helps in breaking symmetry and every neuron is no longer performing the same computation.	Why neural networks initialize randomly
734	The parameters of the distribution are m and s2, where m is the mean (expectation) of the distribution and s2 is the variance. We write X ~ N(m, s2) to mean that the random variable X has a normal distribution with parameters m and s2. If Z ~ N(0, 1), then Z is said to follow a standard normal distribution.	How do you write a normal distribution
3004	The Bonferroni procedure ignores dependencies among the data and is therefore much too conservative if the number of tests is large. Hence, we agree with Perneger that the Bonferroni method should not be routinely used.	Why is Bonferroni conservative
6751	In machine learning, the radial basis function kernel, or RBF kernel, is a popular kernel function used in various kernelized learning algorithms. In particular, it is commonly used in support vector machine classification.	What is RBF in machine learning
5515	Interpret the key results for Binary Logistic RegressionStep 1: Determine whether the association between the response and the term is statistically significant.Step 2: Understand the effects of the predictors.Step 3: Determine how well the model fits your data.Step 4: Determine whether the model does not fit the data.	How do you interpret logit regression results
964	Lastly, the formula for Cohen's Kappa is the probability of agreement take away the probability of random agreement divided by 1 minus the probability of random agreement. Figure 7: Cohen's Kappa coefficient formula.	How do you calculate Cohen's kappa
217	Standard deviation is sensitive to outliers. A single outlier can raise the standard deviation and in turn, distort the picture of spread. For data with approximately the same mean, the greater the spread, the greater the standard deviation.	Is standard deviation affected by outliers
5180	Padding is a term relevant to convolutional neural networks as it refers to the amount of pixels added to an image when it is being processed by the kernel of a CNN. For example, if the padding in a CNN is set to zero, then every pixel value that is added will be of value zero.	What is padding in deep learning
785	A probability distribution is a statistical function that describes all the possible values and likelihoods that a random variable can take within a given range.  These factors include the distribution's mean (average), standard deviation, skewness, and kurtosis.	What is a probability distribution explain your answer
584	"Linear least squares regression is by far the most widely used modeling method. It is what most people mean when they say they have used ""regression"", ""linear regression"" or ""least squares"" to fit a model to their data."	Is Least Squares the same as linear regression
3835	In an economic model, an exogenous variable is one whose value is determined outside the model and is imposed on the model, and an exogenous change is a change in an exogenous variable. In contrast, an endogenous variable is a variable whose value is determined by the model.	What is the difference between an exogenous variable and an endogenous variable
5957	Chisquare Test, Different Types and its Application using RChi-Square Test.Chi-square test of independence.2 x 2 Contingency Table.Chi-square test of significance.Chi-square Test in R.Chi Square Goodness of Fit (One Sample Test)Chi-square Goodness of Test in R.Fisher's exact test.More items•	What are the two types of chi square tests
5997	Divide the number of events by the number of possible outcomes.Determine a single event with a single outcome.  Identify the total number of outcomes that can occur.  Divide the number of events by the number of possible outcomes.  Determine each event you will calculate.  Calculate the probability of each event.More items•6 days ago	How do you calculate probability
8035	Bag-of-words refers to what kind of information you can extract from a document (namely, unigram words). Vector space model refers to the data structure for each document (namely, a feature vector of term & term weight pairs).  Only the unigram words themselves, making for a bunch of words to represent the document.	What is the difference between bag of words TF IDF and vector space model
2404	Examples. According to the law of large numbers, if a large number of six-sided dice are rolled, the average of their values (sometimes called the sample mean) is likely to be close to 3.5, with the precision increasing as more dice are rolled.	What is the law of large numbers example
5002	The additive effect of allele M2 is the average change in genotypic values seen by substituting an M2 allele for an M1 allele. To find this effect, simply construct a new variable, called X1 here, that equals the number of M2 alleles for the individual's genotype.	How is additive effect calculated
6068	Steps for Making decision treeGet list of rows (dataset) which are taken into consideration for making decision tree (recursively at each nodes).Calculate uncertanity of our dataset or Gini impurity or how much our data is mixed up etc.Generate list of all question which needs to be asked at that node.More items•	How do you make a decision in tree machine learning
1495	Summary. The k-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems. It's easy to implement and understand, but has a major drawback of becoming significantly slows as the size of that data in use grows.	What is K Nearest Neighbor machine learning
740	Residual = Observed – Predicted positive values for the residual (on the y-axis) mean the prediction was too low, and negative values mean the prediction was too high; 0 means the guess was exactly correct.	How do you interpret a residual plot in regression
614	Convolutional Neural Networks	Which neural network works best for image data
557	Multicollinearity happens when one predictor variable in a multiple regression model can be linearly predicted from the others with a high degree of accuracy. This can lead to skewed or misleading results. Luckily, decision trees and boosted trees algorithms are immune to multicollinearity by nature .	Are decision trees affected by Multicollinearity
1433	In statistics, a population is the entire pool from which a statistical sample is drawn. A population may refer to an entire group of people, objects, events, hospital visits, or measurements. A population can thus be said to be an aggregate observation of subjects grouped together by a common feature.	What does population in statistics mean
3238	In Vapnik–Chervonenkis theory, the Vapnik–Chervonenkis (VC) dimension is a measure of the capacity (complexity, expressive power, richness, or flexibility) of a set of functions that can be learned by a statistical binary classification algorithm.  It was originally defined by Vladimir Vapnik and Alexey Chervonenkis.	What is VC dimension in machine learning
662	What is a term document matrix?Clean your text responses using Insert > More > Text Analysis > Setup Text Analysis.  Add your term-document matrix using Insert > More > Text Analysis > Techniques > Create Term Document Matrix.	How do you create a term Matrix
962	The classification report visualizer displays the precision, recall, F1, and support scores for the model. There are four ways to check if the predictions are right or wrong: TN / True Negative: the case was negative and predicted negative. TP / True Positive: the case was positive and predicted positive.	How do you read a classification report
5453	"A non-stationary process with a deterministic trend has a mean that grows around a fixed trend, which is constant and independent of time.  It specifies the value at time ""t"" by the last period's value, a drift, a trend, and a stochastic component."	What is a non stationary process
8476	Answered March 1, 2016. Differentiability is the only condition of an activation function.	What is the condition to use a function as an activation function in neural network
1827	"Poisson regression is used to predict a dependent variable that consists of ""count data"" given one or more independent variables. The variable we want to predict is called the dependent variable (or sometimes the response, outcome, target or criterion variable)."	What is Poisson regression used for
2621	Train and serve a TensorFlow model with TensorFlow ServingTable of contents.Create your model. Import the Fashion MNIST dataset. Train and evaluate your model.Save your model.Examine your saved model.Serve your model with TensorFlow Serving. Add TensorFlow Serving distribution URI as a package source:  Make a request to your model in TensorFlow Serving. Make REST requests.	How do you use TensorFlow serving
1228	The Fourier Series is a specialized tool that allows for any periodic signal (subject to certain conditions) to be decomposed into an infinite sum of everlasting sinusoids.  Practically, this allows the user of the Fourier Series to understand a periodic signal as the sum of various frequency components.	What is Fourier series in signal and system
8502	The notion of the distance matrix between individual points is not particularly useful in k-means clustering. The matrix of distances between data points and the centroids is, however, quite central.	Is a distance matrix useful to k means
4567	Q17. Which of the following is true about “Ridge” or “Lasso” regression methods in case of feature selection? “Ridge regression” will use all predictors in final model whereas “Lasso regression” can be used for feature selection because coefficient values can be zero.	Which of the following is true about Ridge or lasso regression methods in case of feature selection
3531	Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting.	Is bootstrapping the same as bagging
5632	Y-hat = b0 + b1(x) - This is the sample regression line. You must calculate b0 & b1 to create this line. Y-hat stands for the predicted value of Y, and it can be obtained by plugging an individual value of x into the equation and calculating y-hat.	How do you calculate y hat in regression
8170	TensorFlow is more of a low-level library; basically, we can think of TensorFlow as the Lego bricks (similar to NumPy and SciPy) that we can use to implement machine learning algorithms whereas scikit-learn comes with off-the-shelf algorithms, e.g., algorithms for classification such as SVMs, Random Forests, Logistic	What is Scikit and TensorFlow
8463	Inferential statistics helps to suggest explanations for a situation or phenomenon. It allows you to draw conclusions based on extrapolations, and is in that way fundamentally different from descriptive statistics that merely summarize the data that has actually been measured.	How useful is inferential statistics in decision making
495	Sentiment analysis – otherwise known as opinion mining – is a much bandied about but often misunderstood term. In essence, it is the process of determining the emotional tone behind a series of words, used to gain an understanding of the the attitudes, opinions and emotions expressed within an online mention.	How does sentiment analysis work generally
353	Probability distributions are a fundamental concept in statistics. They are used both on a theoretical level and a practical level. Some practical uses of probability distributions are: To calculate confidence intervals for parameters and to calculate critical regions for hypothesis tests.	What is the use of probability distribution
1241	A fast Fourier transform (FFT) is an algorithm that computes the discrete Fourier transform (DFT) of a sequence, or its inverse (IDFT). Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa.	What is FFT and its applications in DAA
1234	If the limit of |a[n+1]/a[n]| is less than 1, then the series (absolutely) converges. If the limit is larger than one, or infinite, then the series diverges.	How do you test for convergence and divergence in a series
4170	Five Common Types of Sampling ErrorsPopulation Specification Error—This error occurs when the researcher does not understand who they should survey.  Sample Frame Error—A frame error occurs when the wrong sub-population is used to select a sample.More items	What are the types of sampling errors
93	Yes, it can be used for both continuous and categorical target (dependent) variable. In random forest/decision tree, classification model refers to factor/categorical dependent variable and regression model refers to numeric or continuous dependent variable.	Can random forest predict continuous variable
4282	The term ''mixed model'' refers to the inclusion of both fixed effects, which are model components used to define systematic relationships such as overall changes over time and/ or experimentally induced group differences; and random effects, which account for variability among subjects around the systematic	What is mixed model analysis
1092	Outliers may be plotted as individual points. Box plots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution (though Tukey's boxplot assumes symmetry for the whiskers and normality for their length).	Why is box plot non parametric
135	The expected value is simply a way to describe the average of a discrete set of variables based on their associated probabilities. This is also known as a probability-weighted average.	What is expected value of a random variable
1515	A Correlation of 0 means that there is no linear relationship between the two variables. We already know that if two random variables are independent, the Covariance is 0. We can see that if we plug in 0 for the Covariance to the equation for Correlation, we will get a 0 for the Correlation.	What does a covariance of 0 mean
186	Definition: Given data the maximum likelihood estimate (MLE) for the parameter p is the value of p that maximizes the likelihood P(data |p). That is, the MLE is the value of p for which the data is most likely. 100 P(55 heads|p) = ( 55 ) p55(1 − p)45.	How do you find the maximum likelihood estimator
1165	A false positive is an outcome where the model incorrectly predicts the positive class. And a false negative is an outcome where the model incorrectly predicts the negative class. In the following sections, we'll look at how to evaluate classification models using metrics derived from these four outcomes.	What is false positive and false negative in machine learning
1956	2:1422:33Suggested clip · 114 secondsRegression Trees, Clearly Explained!!! - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you read a regression tree
6199	Convergence in probability implies convergence in distribution. In the opposite direction, convergence in distribution implies convergence in probability when the limiting random variable X is a constant. Convergence in probability does not imply almost sure convergence.	Does convergence in distribution imply convergence in probability
6328	AS a general thumb rule if adjusted R 2 increases when a new variables is added to the model, the variable should remain in the model. If the adjusted R2 decreases when the new variable is added then the variable should not remain in the model.	How do you decide whether to add or drop a predictor variable in a multiple regression model
6449	4 Answers. Linear SVMs and logistic regression generally perform comparably in practice. Use SVM with a nonlinear kernel if you have reason to believe your data won't be linearly separable (or you need to be more robust to outliers than LR will normally tolerate).	Which algorithm is better at handling outliers logistic regression or SVM
2553	Signal detection assumes that there is “noise” in any system. In this example, if we have an old car, we may hear clunks even when the car is operating effectively, or even tinnitus in our ear, or something rustling in the trunk. The signal is what you are trying to detect.	What is an example of the signal detection theory
2497	In probability theory and statistics, the exponential distribution is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate.	What does it mean to be exponentially distributed
6511	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is difference between supervised and unsupervised learning
1839	Test-retest reliability example You administer the test two months apart to the same group of people, but the results are significantly different, so the test-retest reliability of the IQ questionnaire is low.	What is an example of test retest reliability
7050	Reduce Variance of an Estimate If we want to reduce the amount of variance in a prediction, we must add bias. Consider the case of a simple statistical estimate of a population parameter, such as estimating the mean from a small random sample of data. A single estimate of the mean will have high variance and low bias.	How can machine learning reduce bias and variance
4608	A null hypothesis is a type of hypothesis used in statistics that proposes that there is no difference between certain characteristics of a population (or data-generating process).	What is a null hypothesis in laymans terms
771	Data Parallelism means concurrent execution of the same task on each multiple computing core. Let's take an example, summing the contents of an array of size N. For a single-core system, one thread would simply sum the elements [0] . . .  So the Two threads would be running in parallel on separate computing cores.	What is parallelism and its types
7569	Overview of stacking. Stacking mainly differ from bagging and boosting on two points.  Second, stacking learns to combine the base models using a meta-model whereas bagging and boosting combine weak learners following deterministic algorithms.	How the idea of stacking is different from bagging
977	Scatter plots' primary uses are to observe and show relationships between two numeric variables. The dots in a scatter plot not only report the values of individual data points, but also patterns when the data are taken as a whole.  A scatter plot can also be useful for identifying other patterns in data.	What is a scatter plot and why is it useful
271	1 Answer. 0 is a rational, whole, integer and real number. Some definitions include it as a natural number and some don't (starting at 1 instead).	What type of number is 0
521	In mathematics, a Fourier transform (FT) is a mathematical transform that decomposes a function (often a function of time, or a signal) into its constituent frequencies, such as the expression of a musical chord in terms of the volumes and frequencies of its constituent notes.	What does Fourier mean
6471	Deep reinforcement learning is a category of machine learning and artificial intelligence where intelligent machines can learn from their actions similar to the way humans learn from experience.  Actions that get them to the target outcome are rewarded (reinforced).	What does deep reinforcement learning do
4074	FDR is a very simple concept. It is the number of false discoveries in an experiment divided by total number of discoveries in that experiment.  (You calculate one P-value for each sample or test in your experiment.)	How is FDR calculated
6556	Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data. Boosting is an iterative technique which adjusts the weight of an observation based on the last classification.	When to Use bagging vs boosting
7414	Cluster sampling is typically used in market research. It's used when a researcher can't get information about the population as a whole, but they can get information about the clusters. For example, a researcher may be interested in data about city taxes in Florida.	What is the purpose of cluster sampling
4759	Covariance is the measure of how much two sets of data vary. The Covariance determines the degree to which the two variables are related or how they vary together. The Covariance is the average of the product of deviations of data points from their respective means, based on the following formula.	What is covariance in data analysis
7558	The availability heuristic is a mental shortcut that helps us make a decision based on how easy it is to bring something to mind.  The representativeness heuristic is a mental shortcut that helps us make a decision by comparing information to our mental prototypes.	What is the difference between representative and availability heuristics
3771	There is a clear difference between variables and parameters. A variable represents a model state, and may change during simulation. A parameter is commonly used to describe objects statically. A parameter is normally a constant in a single simulation, and is changed only when you need to adjust your model behavior.	What is the difference between parameter and variable 1
2109	There is no direct evidence that the brain uses a backprop-like algorithm for learning. Past work has shown, however, that backprop-trained models can account for observed neural responses, such as the response properties of neurons in the posterior parietal cortex68 and primary motor cortex69.	Does the brain do backpropagation
3850	"Predictive modeling is the process of using known results to create, process, and validate a model that can be used to forecast future outcomes. It is a tool used in predictive analytics, a data mining technique that attempts to answer the question ""what might possibly happen in the future?"""	What is predictive Modelling used for
612	Continuous variables are numeric variables that have an infinite number of values between any two values. A continuous variable can be numeric or date/time. For example, the length of a part or the date and time a payment is received.	What is an example of a continuous variable
3975	Conditional Random Fields (CRF) CRF is a discriminant model for sequences data similar to MEMM. It models the dependency between each state and the entire input sequences. Unlike MEMM, CRF overcomes the label bias issue by using global normalizer.	What is CRF NLP
5292	Experimental probability is the actual result of an experiment, which may be different from the theoretical probability. Example: you conduct an experiment where you flip a coin 100 times. The theoretical probability is 50% heads, 50% tails. The actual outcome of your experiment may be 47 heads, 53 tails.	What are some examples of experimental probability
811	Artificial Intelligence is the broader concept of machines being able to carry out tasks in a way that we would consider “smart”. And, Machine Learning is a current application of AI based around the idea that we should really just be able to give machines access to data and let them learn for themselves.	What is the relation between AI and machine learning
303	The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution.	What is the difference between at distribution and a normal distribution
1303	Examples of dependence without correlation are uncorrelated. are not independent.	Does uncorrelated mean independent
8016	Loss functions in neural networks The loss function is what SGD is attempting to minimize by iteratively updating the weights in the network. At the end of each epoch during the training process, the loss will be calculated using the network's output predictions and the true labels for the respective input.	What is loss in neural network training
8353	Second quartile (Q2) which is more commonly known as median splits the data in half (50%). Median divides the data into a lower half and an upper half. Third quartile (Q3), also known as upper quartile, splits lowest 75% (or highest 25%) of data. It is the middle value of the upper half.	What is common about the median and the second quartile
8011	The Fourier amplitude spectrum of strong earthquake acceleration is one of the most direct and common. functions used to describe the frequency content of strong earthquake shaking.' It is used in source. mechanism studies, where its amplitudes and the parameters describing its shape can be related to the slip on.	What is Fourier amplitude spectrum
8255	The 1×1 filter can be used to create a linear projection of a stack of feature maps. The projection created by a 1×1 can act like channel-wise pooling and be used for dimensionality reduction. The projection created by a 1×1 can also be used directly or be used to increase the number of feature maps in a model.	How are 1x1 convolutions used for dimensionality reduction
6020	Binning or discretization is the process of transforming numerical variables into categorical counterparts. An example is to bin values for Age into categories such as 20-39, 40-59, and 60-79. Numerical variables are usually discretized in the modeling methods based on frequency tables (e.g., decision trees).	What is binning method
214	A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem. — Practical recommendations for gradient-based training of deep architectures, 2012.	What should be the learning rate
7039	The universality property of Turing machines states that there exists a Turing machine, which can simulate the behaviour of any other Turing machine.  It says that a Turing machine can be adapted to different tasks by programming; from the viewpoint of computability it is not necessary to build special-purpose machines.	Does a universal Turing machine exist
7555	Let's establish a very basic fact, one of the simplest methods for calculating the correctness of a model is to use the error between predicted value and actual value.The metrics we want to look at are:Mean Absolute Error (MAE)Root Mean Squared Error (RMSE)Mean Absolute Percentage Error (MAPE)R-Squared Score.	How do you find the accuracy of a regression
5444	Handling overfittingReduce the network's capacity by removing layers or reducing the number of elements in the hidden layers.Apply regularization , which comes down to adding a cost to the loss function for large weights.Use Dropout layers, which will randomly remove certain features by setting them to zero.	How can we prevent Overfitting in deep learning
7240	"Reward functions describe how the agent ""ought"" to behave. In other words, they have ""normative"" content, stipulating what you want the agent to accomplish. For example, some rewarding state s might represent the taste of food. Or perhaps, (s,a) might represent the act of tasting the food."	What is reward function in reinforcement learning
150	A z-score measures exactly how many standard deviations above or below the mean a data point is.  A negative z-score says the data point is below average. A z-score close to 0 says the data point is close to average. A data point can be considered unusual if its z-score is above 3 or below −3 .	What is a z score in math
7537	FDR is a very simple concept. It is the number of false discoveries in an experiment divided by total number of discoveries in that experiment. (You calculate one P-value for each sample or test in your experiment.)	How is false discovery rate calculated
248	Goodness-of-fit tests are almost always right-tailed. This is because if, say, the observed frequencies were exactly the same as the expected, would be always zero, as would and . The more different the observed frequencies are from the expected, the bigger the .	Why is goodness of fit test right tailed
5575	The F-value is the test statistic used to determine whether the term is associated with the response. F-value for the lack-of-fit test. The F-value is the test statistic used to determine whether the model is missing higher-order terms that include the predictors in the current model.	What is F Manova
1365	A Bayesian network is a directed graphical model. (A Markov random field is a undirected graphical model.) A graphical model captures the conditional independence, which can be different from the Markovian property.	What is the difference between Markov networks and Bayesian networks
8165	Probability is the chance of an event occurring. A probability distribution is a table or an equation that links each outcome of a statistical experiment with its probability of occurrence.	What is the difference between probability and probability distribution
132	Definition: Given data the maximum likelihood estimate (MLE) for the parameter p is the value of p that maximizes the likelihood P(data |p). That is, the MLE is the value of p for which the data is most likely. 100 P(55 heads|p) = ( 55 ) p55(1 − p)45.	How do you calculate maximum likelihood estimation
7582	The variance estimated as the average squared difference from the sample mean will always be less than the variance estimated as the average squared difference from the population mean unless the sample mean equals the population mean in which case they will be the same.	What is the estimated variance
3588	In the extended Kalman filter, the state transition and observation models don't need to be linear functions of the state but may instead be differentiable functions.  These matrices can be used in the Kalman filter equations. This process essentially linearizes the non-linear function around the current estimate.	How does extended Kalman filter work
1847	How big data analytics worksdata mining, which sift through data sets in search of patterns and relationships;predictive analytics, which build models to forecast customer behavior and other future developments;machine learning, which taps algorithms to analyze large data sets; and.More items	What role does analytics play big data
1116	Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.	What is normalization in neural network
1210	To me an intuitive explanation is that minimizing the log loss equals minimizing the Kullback-Leibler divergence (Kullback–Leibler divergence - Wikipedia ) between the function you want to optimize (for example a neural network) and the true function that generates the data (from which you have samples in the form of a	What is an intuitive explanation for the log loss function
736	Imputation is a term that denotes a procedure that replaces the missing values in a data set by some plausible values. Our anal- ysis indicates that missing data imputation based on the k-nearest neighbour algorithm can outperform the internal methods used by C4. 5 and CN2 to treat missing data.	What is KNN imputation method
974	2- Key characteristics of machine learning2.1- The ability to perform automated data visualization.  2.2- Automation at its best.  2.3- Customer engagement like never before.  2.4- The ability to take efficiency to the next level when merged with IoT.  2.5- The ability to change the mortgage market.  2.6- Accurate data analysis.More items	What are the features of machine learning
765	In statistics, a type of probability distribution in which all outcomes are equally likely.  A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same.	What does it mean to have a uniform distribution
2566	GPU: RTX 2070 or RTX 2080 Ti. GTX 1070, GTX 1080, GTX 1070 Ti, and GTX 1080 Ti from eBay are good too! CPU: 1-2 cores per GPU depending how you preprocess data. > 2GHz; CPU should support the number of GPUs that you want to run.	How many GPU do you need for deep learning
5687	A statistical model is autoregressive if it predicts future values based on past values. For example, an autoregressive model might seek to predict a stock's future prices based on its past performance.	What is the meaning of autoregressive model
485	Image Processing or Digital Image Processing is technique to improve image quality by applying mathematical operations. Image Processing Projects involves modifying images by identification of its two dimensional signal and enhancing it by comparing with standard signal.	What is image processing projects
2018	Cross-validation is a standard tool in analytics and is an important feature for helping you develop and fine-tune data mining models.  Cross-validation has the following applications: Validating the robustness of a particular mining model. Evaluating multiple models from a single statement.	What is cross validation in data mining
3290	Bayesian Optimization is an approach that uses Bayes Theorem to direct the search in order to find the minimum or maximum of an objective function. It is an approach that is most useful for objective functions that are complex, noisy, and/or expensive to evaluate.	How does Bayesian optimization work
7592	If the weights are zero, complexity of the whole deep net would be the same as that of a single neuron and the predictions would be nothing better than random. Nodes that are side-by-side in a hidden layer connected to the same inputs must have different weights for the learning algorithm to update the weights.	Is random weight assignment better than assigning weights to the units in the hidden layer
8350	Physical scientists often use the term root-mean-square as a synonym for standard deviation when they refer to the square root of the mean squared deviation of a signal from a given baseline or fit.	Is root mean square standard deviation
251	Another view however is that the parameter value used to generate the data that are obtained in your study is just one drawn parameter value, where the draw is from some distribution (the prior).  as parameters, but rather as random or latent effects.	Are parameters random
340	Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population.	What are the different types of statistical methods
651	Let A and G be the Arithmetic Means and Geometric Means respectively of two positive numbers a and b. Then, As, a and b are positive numbers, it is obvious that A > G when G = -√ab.  This proves that the Arithmetic Mean of two positive numbers can never be less than their Geometric Means.	What is the relationship between arithmetic mean and geometric mean is
8197	Pearson correlation (r) is used to measure strength and direction of a linear relationship between two variables. Mathematically this can be done by dividing the covariance of the two variables by the product of their standard deviations. The value of r ranges between -1 and 1.	How do you prove correlation between two variables
3884	To calculate a z-score, subtract the mean from the raw score and divide that answer by the standard deviation. (i.e., raw score =15, mean = 10, standard deviation = 4. Therefore 15 minus 10 equals 5. 5 divided by 4 equals 1.25.	How do you find the standardized z score
1190	M = mean( A ) returns the mean of the elements of A along the first array dimension whose size does not equal 1.If A is a vector, then mean(A) returns the mean of the elements.If A is a matrix, then mean(A) returns a row vector containing the mean of each column.More items	How do you average a matrix in Matlab
4686	Multimodal machine learning is a vibrant multi-disciplinary research field which addresses some of the original goals of artificial intelligence by integrating and modeling multiple communicative modalities, including linguistic, acoustic and visual messages.	What is multimodal machine learning
1938	When someone talks about AR, they are referring to technology that overlays information and virtual objects on real-world scenes in real-time. It uses the existing environment and adds information to it to make a new artificial environment.	What is augmented reality used for
5189	The purpose of such selection is to determine a set of variables that will provide the best fit for the model so that accurate predictions can be made. Variable selection is one of the most difficult aspects of model building.	Why variable selection is important
403	When we control for variables that have a postive correlation with both the independent and the dependent variable, the original relationship will be pushed down, and become more negative. The same is true if we control for a variable that has a negative correlation with both independent and dependent.	What happens when you control for a variable
8235	Binomial counts successes in a fixed number of trials, while Negative binomial counts failures until a fixed number successes. The Bernoulli and Geometric distributions are the simplest cases of the Binomial and Negative Binomial distributions.	What is the difference between binomial and negative binomial distribution
742	Paradigm: a framework containing the basic assumptions, ways of thinking, and methodology that are commonly accepted by members of a scientific community. 12.	What is a paradigm outliers
6778	A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers.	What is Resnet neural network
3009	An experimental group, also known as a treatment group, receives the treatment whose effect researchers wish to study, whereas a control group does not. They should be identical in all other ways.	What is the difference between a control group and an experimental group
5864	Basically, there are three methods to solve a multi-label classification problem, namely: Problem Transformation. Adapted Algorithm.1 Binary Relevance. This is the simplest technique, which basically treats each label as a separate single class classification problem.  2 Classifier Chains.  3 Label Powerset.	How do you handle multi label classification
2481	6 Answers. Yes, this is the only difference. On-policy SARSA learns action values relative to the policy it follows, while off-policy Q-Learning does it relative to the greedy policy. Under some common conditions, they both converge to the real value function, but at different rates.	Is sarsa better than Q learning
7348	The primary goal of EDA is to maximize the analyst's insight into a data set and into the underlying structure of a data set, while providing all of the specific items that an analyst would want to extract from a data set, such as: a good-fitting, parsimonious model. a list of outliers.	What are the two goals of exploratory data analysis
643	The Altman Z-score is based on five financial ratios that can calculate from data found on a company's annual 10-K report. It uses profitability, leverage, liquidity, solvency, and activity to predict whether a company has a high probability of becoming insolvent.	How is Altman's Z score calculated
2851	2:1422:33Suggested clip · 114 secondsRegression Trees, Clearly Explained!!! - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret a regression tree
903	Standard deviation (represented by the symbol sigma, σ ) shows how much variation or dispersion exists from the average (mean), or expected value. More precisely, it is a measure of the average distance between the values of the data in the set and the mean.	How do you explain standard deviation in statistics
6831	In simple random sampling, each data point has an equal probability of being chosen. Meanwhile, systematic sampling chooses a data point per each predetermined interval. While systematic sampling is easier to execute than simple random sampling, it can produce skewed results if the data set exhibits patterns.	Why is a systematic sample not a random sample
6019	Multinomial logistic regression is used when you have a categorical dependent variable with two or more unordered levels (i.e. two or more discrete outcomes). It is practically identical to logistic regression, except that you have multiple possible outcomes instead of just one.	In laymans terms what is the multinomial logit regression
1689	Natural Language Processing (NLP) is what happens when computers read language. NLP processes turn text into structured data. Natural Language Generation (NLG) is what happens when computers write language. NLG processes turn structured data into text.	What is NLP and NLG
1158	Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron's input is relevant for the model's prediction.	Why are activation functions used in neural networks
5686	A very popular technique that is used along with SGD is called Momentum. Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go.	What is momentum deep learning
7308	Regularization is a technique used for tuning the function by adding an additional penalty term in the error function. The additional term controls the excessively fluctuating function such that the coefficients don't take extreme values.	What is the use of regularization
1193	Both data mining and machine learning draw from the same foundation, but in different ways.  Machine learning can look at patterns and learn from them to adapt behavior for future incidents, while data mining is typically used as an information source for machine learning to pull from.	Is machine learning the same as data mining
446	High Dimensional means that the number of dimensions are staggeringly high — so high that calculations become extremely difficult. With high dimensional data, the number of features can exceed the number of observations. For example, microarrays, which measure gene expression, can contain tens of hundreds of samples.	What is high dimensional data in machine learning
7186	Generally, ensemble learning involves training more than one network on the same dataset, then using each of the trained models to make a prediction before combining the predictions in some way to make a final outcome or prediction.	How do you ensemble a deep learning model
6210	Paired means that both samples consist of the same test subjects. A paired t-test is equivalent to a one-sample t-test. Unpaired means that both samples consist of distinct test subjects. An unpaired t-test is equivalent to a two-sample t-test.	How do I know if my data is paired or unpaired
1264	Neural Networks are complex structures made of artificial neurons that can take in multiple inputs to produce a single output. Usually, a Neural Network consists of an input and output layer with one or multiple hidden layers within.	What is neural network architecture
6866	UCB is a deterministic algorithm for Reinforcement Learning that focuses on exploration and exploitation based on a confidence boundary that the algorithm assigns to each machine on each round of exploration. ( A round is when a player pulls the arm of a machine)	What is UCB algorithm
175	LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate's activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.	How does Lstm solve the vanishing gradient problem
5894	A histogram shows bars representing numerical values by range of value. A bar chart shows categories, not numbers, with bars indicating the amount of each category. Histogram example: student's ages, with a bar showing the number of students in each year.	What can you tell from a histogram
1888	Connectionism presents a cognitive theory based on simultaneously occurring, distributed signal activity via connections that can be represented numerically, where learning occurs by modifying connection strengths based on experience.	Which method of learning is advocated by connectionism
8131	For example RSA Encryption padding is randomized, ensuring that the same message encrypted multiple times looks different each time. It also avoids other weaknesses, such as encrypting the same message using different RSA keys leaking the message, or an attacker creating messages derived from some other ciphertexts.	What is padding in RSA encryption
4552	Automated machine learning (AutoML) is the process of automating the process of applying machine learning to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model.	What does AutoML do
6345	The most intuitive way to increase the frequency resolution of an FFT is to increase the size while keeping the sampling frequency constant. Doing this will increase the number of frequency bins that are created, decreasing the frequency difference between each.	How can frequency resolution be improved
4209	If the order doesn't matter then we have a combination, if the order do matter then we have a permutation. One could say that a permutation is an ordered combination. The number of permutations of n objects taken r at a time is determined by the following formula: P(n,r)=n!	How do you do permutations and combinations in probability
4438	On each iteration, we update the parameters in the opposite direction of the gradient of the objective function J(w) w.r.t the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate α.	How are the parameters updates during the gradient descent process
1378	Multivariate analysis is conceptualized by tradition as the statistical study of experiments in which multiple measurements are made on each experimental unit and for which the relationship among multivariate measurements and their structure are important to the experiment's understanding.	What does a multivariate analysis mean
1480	Ridge regression uses regularization with L2 norm, while Bayesian regression, is a regression model defined in probabilistic terms, with explicit priors on the parameters. The choice of priors can have the regularizing effect, e.g. using Laplace priors for coefficients is equivalent to L1 regularization.	How is ridge regression related to Bayesian linear regression
3114	Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.	How does tokenization work in NLP
5038	Examples of Discrete Distribution The most common discrete probability distributions include binomial, Poisson, Bernoulli, and multinomial.	Which is an example of a discrete distribution
7163	Numeric data types are numbers kept in database columns. Numerical data is data that is quantifiable, such as time, height, weight, amount, and so on. A non- numeric data bring up to categorical data.	What is the difference between numeric data and non numeric data
4972	The classic machine learning procedure follows the scientific paradigm of induction and deduction. In the inductive step we learn the model from raw data (so called training set), and in the deductive step the model is applied to predict the behaviour of new data.	Is machine learning inductive or deductive
6357	In ideal conditions, facial recognition systems can have near-perfect accuracy. Verification algorithms used to match subjects to clear reference images (like a passport photo or mugshot) can achieve accuracy scores as high as 99.97% on standard assessments like NIST's Facial Recognition Vendor Test (FRVT).	How good is facial recognition
971	The input nodes take in information, in the form which can be numerically expressed. The information is presented as activation values, where each node is given a number, the higher the number, the greater the activation.  The output nodes then reflect the input in a meaningful way to the outside world.	What is activation value
764	Variance of estimator: Variance is one of the most popularly used measures of spread. It is taken into consideration for quantification of the amount of dispersion with respect to set of data values. Variance is defined as the average of the squared deviation of each observation from its mean.	What is the variance of the estimator
3067	Continuous probability distribution: A probability distribution in which the random variable X can take on any value (is continuous). Because there are infinite values that X could assume, the probability of X taking on any one specific value is zero.  The normal distribution is one example of a continuous distribution.	What is continuous probability distribution
6050	Cluster analysis, in statistics, set of tools and algorithms that is used to classify different objects into groups in such a way that the similarity between two objects is maximal if they belong to the same group and minimal otherwise.	What is a cluster analysis in statistics
6805	The joint behavior of two random variables X and Y is determined by the. joint cumulative distribution function (cdf):(1.1) FXY (x, y) = P(X ≤ x, Y ≤ y),where X and Y are continuous or discrete. For example, the probability.  P(x1 ≤ X ≤ x2,y1 ≤ Y ≤ y2) = F(x2,y2) − F(x2,y1) − F(x1,y2) + F(x1,y1).	How do you find the joint pdf of two independent random variables
511	Introducing Social Analytics: The Easiest Way To Understand Your Social Media EngagementMeasure the success of every social message so you can re-share your most engaging content (and improve future messages).Use real data to prove the ROI of the work you do.Identify trends and understand what your audience wants.More items•	How do you use social media analytics
5510	I.e multicollinearity describes a linear relationship between whereas autocorrelation describes correlation of a variable with itself given a time lag.	What is the difference between autocorrelation and multicolinearity
7825	Cluster analysis tries to maximize in-group homogeneity and maximize between group heterogeneity. Multiple discriminant analysis is different. It starts with a discrete DV and tries to determine how much the levels of the IV's distinguish the members of the groups.	What is the difference between cluster analysis and discriminant analysis
1070	The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word). Thus the model tries to predict the context_window words based on the target_word.	What is the skip gram approach
6735	5 | Problems and Issues of Linear RegressionSpecification.Proxy Variables and Measurement Error.Selection Bias.Multicollinearity.Autocorrelation.Heteroskedasticity.Simultaneous Equations.Limited Dependent Variables.More items	What are some problems that are encountered when creating a regression model
6255	There are two main reasons to use logarithmic scales in charts and graphs. The first is to respond to skewness towards large values; i.e., cases in which one or a few points are much larger than the bulk of the data. The second is to show percent change or multiplicative factors.	Why would you use a logarithmic scale
3120	An biased estimator is one which delivers an estimate which is consistently different from the parameter to be estimated. In a more formal definition we can define that the expectation E of a biased estimator is not equal to the parameter of a population.	Whats the concept of biased estimators
329	A global thresholding technique is one which makes use of a single threshold value for the whole image, whereas local thresholding technique makes use of unique threshold values for the partitioned subimages obtained from the whole image.	What is global thresholding in image processing
5752	Midrange determines the number that is halfway between the minimum and maximum numbers of a data set. It is a statistical tool that identifies a measure of center like median, mean or mode.	What is the mid range in statistics
432	There is a good reason why accuracy is not an appropriate measure for information retrieval problems. In almost all circumstances, the data is extremely skewed: normally over 99.9% of the documents are in the nonrelevant category.	Why accuracy is not used as a preferred method for real world IR system evaluation
714	Swarm intelligence is the discipline that deals with natural and artificial systems composed of many individuals that coordinate using decentralized control and self-organization.	What is meant by swarm intelligence
4441	To run the bivariate Pearson Correlation, click Analyze > Correlate > Bivariate. Select the variables Height and Weight and move them to the Variables box. In the Correlation Coefficients area, select Pearson. In the Test of Significance area, select your desired significance test, two-tailed or one-tailed.	How do you analyze bivariate correlation
240	Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.	How can algorithms be biased
562	How to Avoid the Type II Error?Increase the sample size. One of the simplest methods to increase the power of the test is to increase the sample size used in a test.  Increase the significance level. Another method is to choose a higher level of significance.	How can you minimize the risk of both type I and type II errors
5321	Moment generating functions have great practical relevance not only because they can be used to easily derive moments, but also because a probability distribution is uniquely determined by its mgf, a fact that, coupled with the analytical tractability of mgfs, makes them a handy tool to solve several problems, such as	What are the two advantages of a moment generating function
3147	Panel data usually contain more degrees of freedom and more sample variability than cross-sectional data which may be viewed as a panel with T = 1, or time series data which is a panel with N = 1, hence improving the efficiency of econometric estimates (e.g. Hsiao et al., 1995).	Why is panel data better than cross sectional data
3220	Remember that the decision to reject the null hypothesis (H 0) or fail to reject it can be based on the p-value and your chosen significance level (also called α). If the p-value is less than or equal to α, you reject H 0; if it is greater than α, you fail to reject H 0.	How do you decide whether to reject the null hypothesis
716	The prior probability of an event will be revised as new data or information becomes available, to produce a more accurate measure of a potential outcome. That revised probability becomes the posterior probability and is calculated using Bayes' theorem.	What is the prior probability in Bayes theorem
3274	There are three basic concepts in reinforcement learning: state, action, and reward. The state describes the current situation. For a robot that is learning to walk, the state is the position of its two legs. For a Go program, the state is the positions of all the pieces on the board.	What are states in reinforcement learning
885	Unstructured data is data that doesn't fit in a spreadsheet with rows and columns. It isn't in a database.  Examples of unstructured data includes things like video, audio or image files, as well as log files, sensor or social media posts.	What is an example of unstructured data
1066	Explanation: K-means requires a number of clusters. 9.	Which of the following is required by K means clustering
3872	In probability theory and statistics, the Bernoulli distribution, named after Swiss mathematician Jacob Bernoulli, is the discrete probability distribution of a random variable which takes the value 1 with probability and the value 0 with probability , and is sometimes denoted as .	What is Bernoulli distribution in probability
2274	A regression tree is built through a process known as binary recursive partitioning, which is an iterative process that splits the data into partitions or branches, and then continues splitting each partition into smaller groups as the method moves up each branch.	How does a regression tree work
4024	Analysis of variance (ANOVA) is an analysis tool used in statistics that splits an observed aggregate variability found inside a data set into two parts: systematic factors and random factors. The systematic factors have a statistical influence on the given data set, while the random factors do not.	What is the meaning of analysis of variance
7679	Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes.	What is the use of logistic regression in machine learning
8394	Normal distribution describes continuous data which have a symmetric distribution, with a characteristic 'bell' shape. Binomial distribution describes the distribution of binary data from a finite sample. Thus it gives the probability of getting r events out of n trials.	What are the three major differences between a normal distribution and a binomial distribution
60	The probability that a standard normal random variables lies between two values is also easy to find. The P(a < Z < b) = P(Z < b) - P(Z < a). For example, suppose we want to know the probability that a z-score will be greater than -1.40 and less than -1.20.	How do you find the standard normal distribution
8621	Precision and recall at k: Definition Precision at k is the proportion of recommended items in the top-k set that are relevant. Its interpretation is as follows. Suppose that my precision at 10 in a top-10 recommendation problem is 80%. This means that 80% of the recommendation I make are relevant to the user.	What is precision at K
798	Because the coefficient of determination is the result of squaring the correlation coefficient, the coefficient of determination cannot be negative. (Even if the correlation is negative, squaring it will result in a positive number.)	Can the coefficient of determination be negative
3225	The Canny filter is a multi-stage edge detector. It uses a filter based on the derivative of a Gaussian in order to compute the intensity of the gradients. The Gaussian reduces the effect of noise present in the image.	What type of filter is used for edge detection
4887	Longitudinal data (also known as panel data) arises when you measure a response variable of interest repeatedly through time for multiple subjects. The response variables in longitudinal studies can be either continuous or discrete.	What is longitudinal analysis statistics
7926	You can use the ArffViewer:(Tools -> ArffViewer or Ctrl+A). Then open your CSV file.Next go to File -> Save as and select Arff data files (should be selected by default.	How do I convert text files to arff format weka
1678	Backward chaining executes declare expression rules when a value is needed for a property, as opposed to when inputs change.  Backward chaining applies to declare expressions rules with the Calculate Value field set to one of the following: When used if no value present. When used, if property is missing. Whenever used.	What is backward chaining in PEGA
1843	Class Boundaries. Separate one class in a grouped frequency distribution from another. The boundaries have one more decimal place than the raw data and therefore do not appear in the data. There is no gap between the upper boundary of one class and the lower boundary of the next class.	What is class boundary in frequency distribution
1783	This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. A simple relation for linear regression looks like this.	What is regularization and what kind of problems does regularization solve
8303	In summary, nominal variables are used to “name,” or label a series of values. Ordinal scales provide good information about the order of choices, such as in a customer satisfaction survey. Interval scales give us the order of values + the ability to quantify the difference between each one.	What difference does it make whether we measure in terms of a nominal ordinal interval or ratio scale
113	When faced with a choice, we often lack the time or resources to investigate in greater depth. Faced with the need for an immediate decision, the availability heuristic allows people to quickly arrive at a conclusion. This can be helpful when you are trying to make a decision or judgment about the world around you.	How does availability heuristic impact data interpretation
5959	N-grams are simply all combinations of adjacent words or letters of length n that you can find in your source text. For example, given the word fox , all 2-grams (or “bigrams”) are fo and ox .  The longer the n-gram (the higher the n), the more context you have to work with.	How does n gram work
390	Artificial Intelligence is the broader concept of machines being able to carry out tasks in a way that we would consider “smart”. And, Machine Learning is a current application of AI based around the idea that we should really just be able to give machines access to data and let them learn for themselves.	What are the main differences between artificial intelligence and machine learning Is machine learning a part of artificial intelligence
5610	Mean deviation from median =∑fi∑fi∣xi−M∣=1001428. 6=14.	How do you find the mean median deviation
317	Poisson regression – Poisson regression is often used for modeling count data. Poisson regression has a number of extensions useful for count models. Negative binomial regression – Negative binomial regression can be used for over-dispersed count data, that is when the conditional variance exceeds the conditional mean.	When should we use Poisson regression
3874	adjective. apt or liable to vary or change; changeable: variable weather;variable moods. capable of being varied or changed; alterable: a variable time limit for completion of a book.	What does it mean for something to be variable
569	The Bernoulli distribution represents the success or failure of a single Bernoulli trial. The Binomial Distribution represents the number of successes and failures in n independent Bernoulli trials for some given value of n.  Another example is the number of heads obtained in tossing a coin n times.	What is the difference between Bernoulli distribution and binomial distribution
6715	Explanation: Weight adjustment is proportional to negative gradient of error with respect to weight. 10.	Does backpropagation learning is based on gradient descent along error surface
8421	Some examples of situations in which standard deviation might help to understand the value of the data:A class of students took a math test.  A dog walker wants to determine if the dogs on his route are close in weight or not close in weight.  A market researcher is analyzing the results of a recent customer survey.More items	How do we use standard deviation in the real world
285	Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are: Continuous Bag-of-Words, or CBOW model. Continuous Skip-Gram Model.	Which is the best model used in word2vec algorithm for word embedding
5334	The smaller the residual standard deviation, the closer is the fit of the estimate to the actual data. In effect, the smaller the residual standard deviation is compared to the sample standard deviation, the more predictive, or useful, the model is.	How do you interpret the standard deviation of the residuals
2525	By Paul King on Ap in Probability. A random walk refers to any process in which there is no observable pattern or trend; that is, where the movements of an object, or the values taken by a certain variable, are completely random.	What is random walk in probability
862	Calculate (1 - the reliability) - that is, subtract the reliability from 1. Take the square root of the amount calculated in step 3. Multiply the amount calculated in step 4 by the standard deviation found in step 1. This is the standard error of measurement.	How do you calculate measurement error
5352	In probability theory and related fields, a stochastic or random process is a mathematical object usually defined as a family of random variables.  Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner.	What is random process in probability
77	Generalization refers to your model's ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model. Estimated Time: 5 minutes Learning Objectives.	What is generalization in deep learning
584	A one-way ANOVA only involves one factor or independent variable, whereas there are two independent variables in a two-way ANOVA.  In a one-way ANOVA, the one factor or independent variable analyzed has three or more categorical groups. A two-way ANOVA instead compares multiple groups of two factors. 4.	What is the difference between Anova and one way Anova
5794	A statistic is a characteristic of a sample. Generally, a statistic is used to estimate the value of a population parameter. For instance, suppose we selected a random sample of 100 students from a school with 1000 students. The average height of the sampled students would be an example of a statistic.	What is an example of a statistic in the study
1076	A variance-covariance matrix is a square matrix that contains the variances and covariances associated with several variables. The diagonal elements of the matrix contain the variances of the variables and the off-diagonal elements contain the covariances between all possible pairs of variables.	What does a variance covariance matrix tell you
6417	"In statistics, a unimodal probability distribution or unimodal distribution is a probability distribution which has a single peak. The term ""mode"" in this context refers to any peak of the distribution, not just to the strict definition of mode which is usual in statistics."	What does a unimodal distribution mean
4362	Common tools for performing an assessment of the internal and external factors impacting on strategic decisions are SWOT, and PEST or PESTEL analysis.	What are the different methods and tools used for strategic planning and evaluation
7890	Covariance provides insight into how two variables are related to one another. More precisely, covariance refers to the measure of how two random variables in a data set will change together. A positive covariance means that the two variables at hand are positively related, and they move in the same direction.	What does covariance tell us about a set of data
730	Estimation, in statistics, any of numerous procedures used to calculate the value of some property of a population from observations of a sample drawn from the population.  A point estimate, for example, is the single number most likely to express the value of the property.	What is meant by estimation in statistics
2696	We demonstrated that convolutional neural networks are primarily utilized for text classification tasks while recurrent neural networks are commonly used for natural language generation or machine translation.	Does NLP use neural networks
936	ADVANTAGES OF DIMENSIONAL ANALYSIS : it helps in conversion of one system of units into the other . it is useful in checking the correctness of the given physical relation . it helps in deriving relationship between various physical quantities	What are the advantages of dimensional analysis
1379	The most common threshold is p < 0.05, which means that the data is likely to occur less than 5% of the time under the null hypothesis. When the p-value falls below the chosen alpha value, then we say the result of the test is statistically significant.	How do you interpret a test statistic
4087	Parametric alternatives. Another approach to robust estimation of regression models is to replace the normal distribution with a heavy-tailed distribution. A t-distribution with 4–6 degrees of freedom has been reported to be a good choice in various practical situations.	What approach is used in robust regression
833	You can tell if two random variables are independent by looking at their individual probabilities. If those probabilities don't change when the events meet, then those variables are independent. Another way of saying this is that if the two variables are correlated, then they are not independent.	How do you prove that two random variables are independent
6798	Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.	What is Adam algorithm
8632	Expected Population Error Rate (EPER) is the expected rate of error in the population. The rate is usually estimated based on past operating history, previous test results, process observation or walk-through.	What is expected error rate
5761	Deep Neural Networks struggle with the vanishing gradient problem because of the way back propagation is done by calculating an error value for each neuron, starting with the output layer working it's way back to the input layer. Back-propagation then uses the chain rule to calculate the gradient for each neuron.	How does the deep belief network DBN solve the vanishing gradient Select all that apply
7624	To calculate the true p-value, we just need to multiply 0.0968 by two, or 0.1936. This would be a p-value of 19.36%. The second method is using a graphing calculator. This can give us a more exact number because we will not have to cut off the z-score at the hundredths place.	Is there a formula to find the p value from a given z score
872	"Eigenvalues and eigenvectors allow us to ""reduce"" a linear operation to separate, simpler, problems. For example, if a stress is applied to a ""plastic"" solid, the deformation can be dissected into ""principle directions""- those directions in which the deformation is greatest."	Why do we use eigenvalues and eigenvectors
4933	A. Disparate Treatment DiscriminationThe employee is a member of a protected class;  The discriminator knew of the employee's protected class;  Acts of harm occurred;  Others who were similarly situated were either treated more favorably or not subjected to the same or similar adverse treatment.	How do you prove disparate treatment
5355	How to calculate margin of errorGet the population standard deviation (σ) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:	How do you calculate the margin of error
4979	While a frequency distribution gives the exact frequency or the number of times a data point occurs, a probability distribution gives the probability of occurrence of the given data point.	What is the difference between probability distribution and relative frequency distribution
2776	noun. the act of turning out; production: the factory's output of cars; artistic output. the quantity or amount produced, as in a given time: to increase one's daily output. the material produced or yield; product.	What do you mean by output
1080	1. The Gaussian Graphical Model.  Notably, in the Gaussian graphical model, these lines capture partial correlations, that is, the correlation between two items or variables when controlling for all other items or variables included in the data set.	What is a Gaussian graphical model
3801	"The random variable then takes values which are real numbers from the interval [0, 360), with all parts of the range being ""equally likely"".  Any real number has probability zero of being selected, but a positive probability can be assigned to any range of values."	Is there a real random variable
151	For omitted variable bias to occur, the omitted variable ”Z” must satisfy two conditions: The omitted variable is correlated with the included regressor (i.e. The omitted variable is a determinant of the dependent variable (i.e. expensive and the alternative funding is loan or scholarship which is harder to acquire.	What are the two conditions for omitted variable bias
1698	Perceptron Learning Rule states that the algorithm would automatically learn the optimal weight coefficients. The input features are then multiplied with these weights to determine if a neuron fires or not.	What is the Perceptron learning rule
1579	The biggest flaw in this machine learning technique, according to Mittu, is that there is a large amount of art to building these networks, which means there are few scientific methods to help understand when they will fail.	What's wrong with deep learning
1162	CNNs are used for image classification and recognition because of its high accuracy.  The CNN follows a hierarchical model which works on building a network, like a funnel, and finally gives out a fully-connected layer where all the neurons are connected to each other and the output is processed.	Why we use CNN for image classification
507	Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.  Bayesian updating is particularly important in the dynamic analysis of a sequence of data.	How does Bayesian inference work
7689	The natural logarithm, or logarithm to base e, is the inverse function to the natural exponential function. The natural logarithm of a number k > 1 can be defined directly as the area under the curve y = 1/x between x = 1 and x = k, in which case e is the value of k for which this area equals one (see image).	Why e is the base of natural logarithm
3545	PDF according to input X being discrete or continuous generates probability mass functions and CDF does the same but generates cumulative mass function. That means, PDF is derivative of CDF and CDF can be applied at any point where PDF has been applied.  The cumulative function is the integral of the density function.	What is the difference between a probability distribution function PDF and a cumulative distribution function CDF )
7295	A machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data.	What is machine learning model
1043	Featuretools is a framework to perform automated feature engineering. It excels at transforming temporal and relational datasets into feature matrices for machine learning.	What is Featuretools
500	Systematic vs. Random errors are (like the name suggests) completely random. They are unpredictable and can't be replicated by repeating the experiment again. Systematic Errors produce consistent errors, either a fixed amount (like 1 lb) or a proportion (like 105% of the true value).	What is the difference between systematic error and random error
5915	A moving average is a technique that calculates the overall trend in a data set. In operations management, the data set is sales volume from historical data of the company. This technique is very useful for forecasting short-term trends. It is simply the average of a select set of time periods.	What is moving average method of forecasting
8629	Bayesian inference is a machine learning model not as widely used as deep learning or regression models.	Is Bayesian inference machine learning
12	A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0).	How does regression be calculated
5588	Some applications of unsupervised machine learning techniques include: Clustering allows you to automatically split the dataset into groups according to similarity. Often, however, cluster analysis overestimates the similarity between groups and doesn't treat data points as individuals.	What are the application of unsupervised learning
2070	4:5510:35Suggested clip · 104 secondsSetting Up a Markov Chain - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you set up a Markov chain
407	A t-test tests a null hypothesis about two means; most often, it tests the hypothesis that two means are equal, or that the difference between them is zero.  A chi-square test tests a null hypothesis about the relationship between two variables.	What is the difference between T distribution and chi square distribution
1361	Factor loading is basically the correlation coefficient for the variable and factor. Factor loading shows the variance explained by the variable on that particular factor. In the SEM approach, as a rule of thumb, 0.7 or higher factor loading represents that the factor extracts sufficient variance from that variable.	What is factor loading SEM
7243	The loss is calculated on training and validation and its interpretation is based on how well the model is doing in these two sets. It is the sum of errors made for each example in training or validation sets. Loss value implies how poorly or well a model behaves after each iteration of optimization.	What is validation loss in machine learning
8585	Summary. Probably approximately correct (PAC) learning is a theoretical framework for analyzing the generalization error of a learning algorithm in terms of its error on a training set and some measure of complexity. The goal is typically to show that an algorithm achieves low generalization error with high probability	What is PAC model in machine learning
5293	As you have seen, in order to perform a likelihood ratio test, one must estimate both of the models one wishes to compare. The advantage of the Wald and Lagrange multiplier (or score) tests is that they approximate the LR test, but require that only one model be estimated.	How do you calculate the likelihood ratio
272	Log-likelihood values cannot be used alone as an index of fit because they are a function of sample size but can be used to compare the fit of different coefficients. Because you want to maximize the log-likelihood, the higher value is better. For example, a log-likelihood value of -3 is better than -7.	Is lower log likelihood better
991	A vector error correction (VEC) model is a restricted VAR designed for use with nonstationary series that are known to be cointegrated.  The cointegration term is known as the error correction term since the deviation from long-run equilibrium is corrected gradually through a series of partial short-run adjustments.	What is vector error correction model Vecm
5278	A squashing function is essentially defined as a function that squashes the input to one of the ends of a small interval. In Neural Networks, these can be used at nodes in a hidden layer to squash the input. This introduces non-linearity to the NN and allows the NN to be effective.	Why squashing function is important in neural network
1271	inverse error	What does denying the antecedent mean
45	To give you an idea of how drastically CAC can vary, here's a quick look at the average CAC in a variety of industries: Travel: $7. Retail: $10. Consumer Goods: $22.	What is the average CAC
4112	Yes, you should check normality of errors AFTER modeling. In linear regression, errors are assumed to follow a normal distribution with a mean of zero. Let's do some simulations and see how normality influences analysis results and see what could be consequences of normality violation.	Does linear regression assume normal distribution
394	Two events are said to be mutually exclusive when the two events cannot occur at the same time. For instance, when you throw a coin the event that a head appears and the event that a tail appears are mutually exclusive because they cannot occur at the same time, it's either a head appears or a tail appears.	How do you determine if an event is mutually exclusive
3499	3:537:13Suggested clip · 71 secondsStatistics With R - 4.4.3C - Bayesian model averaging - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do Bayesian model averaging in R
125	The formula for a simple linear regression is:y is the predicted value of the dependent variable (y) for any given value of the independent variable (x).B0 is the intercept, the predicted value of y when the x is 0.B1 is the regression coefficient – how much we expect y to change as x increases.More items•	How do you do a simple linear regression
6	In such a sequence of trials, the geometric distribution is useful to model the number of failures before the first success. The distribution gives the probability that there are zero failures before the first success, one failure before the first success, two failures before the first success, and so on.	What is geometric distribution used for
1775	An algorithm is considered efficient if its resource consumption, also known as computational cost, is at or below some acceptable level. Roughly speaking, 'acceptable' means: it will run in a reasonable amount of time or space on an available computer, typically as a function of the size of the input.	What do you mean by efficiency of an algorithm
886	Recursive neural network models	Which model is best suited for recursive data
4675	A linear model communication is one-way talking process But the disadvantage is that there is no feedback of the message by the receiver.	What are the limitations of the linear model of communication
7156	The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice. The function is differentiable.	Why do we use sigmoid function
1082	The function fX(x) gives us the probability density at point x. It is the limit of the probability of the interval (x,x+Δ] divided by the length of the interval as the length of the interval goes to 0. Remember that P(x<X≤x+Δ)=FX(x+Δ)−FX(x). =dFX(x)dx=F′X(x),if FX(x) is differentiable at x.	How do you find the probability density function
6722	The attention mechanism is a part of a neural architecture that enables to dynamically highlight relevant features of the input data, which, in NLP, is typically a sequence of textual elements. It can be applied directly to the raw input or to its higher level representation.	What is attention mechanism in NLP
68	At its core, a loss function is incredibly simple: it's a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they're pretty good, it'll output a lower number.	What does the equation for the loss function do conceptually
5263	In General, A Discriminative model ‌models the decision boundary between the classes. A Generative Model ‌explicitly models the actual distribution of each class.  A Discriminative model ‌learns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems.	What is difference between generative and discriminative model
2574	If you want to become a better decision-maker, incorporate these nine daily habits into your life.Take Note of Your Overconfidence.  Identify the Risks You Take.  Frame Your Problems In a Different Way.  Stop Thinking About the Problem.  Set Aside Time to Reflect on Your Mistakes.  Acknowledge Your Shortcuts.More items	How do I make better decisions
4272	Whereas AI is preprogrammed to carry out a task that a human can but more efficiently, artificial general intelligence (AGI) expects the machine to be just as smart as a human.  A machine that was able to do this would be considered a fine example of AGI.	What is the difference between general artificial intelligence and artificial intelligence
476	When one takes half of the difference or variance between the 3rd and the 1st quartiles of a simple distribution or frequency distribution it is quartile deviation. The quartile deviation formula is. Q.D. = Q3-Q1/ 2. Example – Quartiles are values that divide a list of numbers into quarters.	What is quartile deviation with example
747	"Ground truth is a term used in statistics and machine learning that means checking the results of machine learning for accuracy against the real world. The term is borrowed from meteorology, where ""ground truth"" refers to information obtained on site."	What is ground truth in AI
6353	The ReLu (Rectified Linear Unit) Layer ReLu refers to the Rectifier Unit, the most commonly deployed activation function for the outputs of the CNN neurons. Mathematically, it's described as: Unfortunately, the ReLu function is not differentiable at the origin, which makes it hard to use with backpropagation training.	What is ReLu layer in CNN
6701	"That is, it entails comparing the observed test statistic to some cutoff value, called the ""critical value."" If the test statistic is more extreme than the critical value, then the null hypothesis is rejected in favor of the alternative hypothesis."	What is the rejection rule using the critical value
388	The quantile-quantile (q-q) plot is a graphical technique for determining if two data sets come from populations with a common distribution. A q-q plot is a plot of the quantiles of the first data set against the quantiles of the second data set.  A 45-degree reference line is also plotted.	What does a QQ plot tell you
2035	A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease. The coefficient value signifies how much the mean of the dependent variable changes given a one-unit shift in the independent variable while holding other variables in the model constant.	What do negative coefficients mean in regression
4005	The Taguchi loss function is graphical depiction of loss developed by the Japanese business statistician Genichi Taguchi to describe a phenomenon affecting the value of products produced by a company.  This means that if the product dimension goes out of the tolerance limit the quality of the product drops suddenly.	What does the Taguchi loss function indicate
3698	A baseline is a method that uses heuristics, simple summary statistics, randomness, or machine learning to create predictions for a dataset. You can use these predictions to measure the baseline's performance (e.g., accuracy)-- this metric will then become what you compare any other machine learning algorithm against.	What is baseline model in machine learning
1439	Example: Finding customer segments Clustering is an unsupervised technique where the goal is to find natural groups or clusters in a feature space and interpret the input data. There are many different clustering algorithms.	What is an example of unsupervised learning
4198	The mean (average) of a data set is found by adding all numbers in the data set and then dividing by the number of values in the set. The median is the middle value when a data set is ordered from least to greatest. The mode is the number that occurs most often in a data set.	What is use of mean mode and median in statistics
106	If we know the joint CDF of X and Y, we can find the marginal CDFs, FX(x) and FY(y). Specifically, for any x∈R, we have FXY(x,∞)=P(X≤x,Y≤∞)=P(X≤x)=FX(x). Here, by FXY(x,∞), we mean limy→∞FXY(x,y). Similarly, for any y∈R, we have FY(y)=FXY(∞,y).	How do you find marginal CDF from joint PDF
1768	The prior probability of an event will be revised as new data or information becomes available, to produce a more accurate measure of a potential outcome. That revised probability becomes the posterior probability and is calculated using Bayes' theorem.	What is prior probability in Bayes Theorem
4369	Gibbs Sampling is based on sampling from condi- tional distributions of the variables of the posterior.  For LDA, we are interested in the latent document-topic portions θd, the topic-word distributions φ(z), and the topic index assignments for each word zi.	What is Gibbs sampling in LDA
3179	An ROC (Receiver Operating Characteristic) curve is a useful graphical tool to evaluate the performance of a binary classifier as its discrimination threshold is varied.  In binary classification, a collection of objects is given, and the task is to classify the objects into two groups based on their features.	What is ROC curve in data science
4758	The different types of regression in machine learning techniques are explained below in detail:Linear Regression. Linear regression is one of the most basic types of regression in machine learning.  Logistic Regression.  Ridge Regression.  Lasso Regression.  Polynomial Regression.  Bayesian Linear Regression.	What are the different types of regression
107	A unit of measurement is some specific quantity that has been chosen as the standard against which other measurements of the same kind are made.  The term standard refers to the physical object on which the unit of measurement is based.	What is unit and standard unit
3140	AUC (Area under the ROC Curve). AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.	What does area under ROC curve mean
3882	The term random refers to any collection of data or information that has no determined order, or is chosen in a way that is unknown beforehand. For example, 5, 8, 2, 9, and 0 are single-digit numbers listed in random order.  Data can be randomly selected, or random numbers can be generated using a random seed.	What is random data
7323	Particular distributions are associated with hypothesis testing. Perform tests of a population mean using a normal distribution or a Student's t-distribution. (Remember, use a Student's t-distribution when the population standard deviation is unknown and the distribution of the sample mean is approximately normal.)	Which distribution is used for hypothesis testing
2509	Functions are usually represented by a function rule where you express the dependent variable, y, in terms of the independent variable, x. A pair of an input value and its corresponding output value is called an ordered pair and can be written as (a, b).	What is the input variable of a function
671	In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points.	Which of the following is the definition of distance between two clusters in a complete linkage clustering
5440	In General, A Discriminative model ‌models the decision boundary between the classes. A Generative Model ‌explicitly models the actual distribution of each class.  A Discriminative model ‌learns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems.	What is the difference between a generative model and a discriminative model
7659	A correlation close to -1 or 1 tells us that there is a strong relationship between the variables. It is useful to know this. Strictly speaking, it applies to a linear relationship, but the correlation can be high even for an obviously curvilinear relationship.	What is the advantage of a correlation coefficient
1089	Forecast bias is distinct from the forecast error and one of the most important keys to improving forecast accuracy. Reducing bias means reducing the forecast input from biased sources. A test case study of how bias was accounted for at the UK Department of Transportation.	What is the difference between forecast accuracy and bias
8411	Hypothesis Tests of the Mean and MedianParametric tests (means)Nonparametric tests (medians)1-sample t test1-sample Sign, 1-sample Wilcoxon2-sample t testMann-Whitney testOne-Way ANOVAKruskal-Wallis, Mood's median testFactorial DOE with one factor and one blocking variableFriedman test	What are the types of parametric tests
3276	The P value, or calculated probability, is the probability of finding the observed, or more extreme, results when the null hypothesis (H 0) of a study question is true – the definition of 'extreme' depends on how the hypothesis is being tested.	Is P value the same as probability
1363	MedianArrange your numbers in numerical order.Count how many numbers you have.If you have an odd number, divide by 2 and round up to get the position of the median number.If you have an even number, divide by 2. Go to the number in that position and average it with the number in the next higher position to get the median.	How do you calculate the median
6143	The Dirichlet distribution is a conjugate prior for the multinomial distribution. This means that if the prior distribution of the multinomial parameters is Dirichlet then the posterior distribution is also a Dirichlet distribution (with parameters different from those of the prior).	What does the Dirichlet distribution is the conjugate prior of the categorical distribution mean
1787	Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.	What does transfer learning mean
1386	Rather, the swarm of humans uses software to input their opinions in real time, thus making micro-changes to the rest of the swarm and the inputs of other members. Studies show that swarm intelligence consistently outperforms individuals and crowds working without the algorithms.	How does swarm intelligence work
1561	All Redis data resides in-memory, in contrast to databases that store data on disk or SSDs. By eliminating the need to access disks, in-memory data stores such as Redis avoid seek time delays and can access data in microseconds.	How is data stored in Redis
7711	Data wrangling is the process of cleaning, structuring and enriching raw data into a desired format for better decision making in less time.  This self-service model with data wrangling tools allows analysts to tackle more complex data more quickly, produce more accurate results, and make better decisions.	Why is data wrangling
3478	An interpolated string is a string literal that might contain interpolation expressions. When an interpolated string is resolved to a result string, items with interpolation expressions are replaced by the string representations of the expression results.	What does string interpolation mean
1206	Different types of classifiersPerceptron.Naive Bayes.Decision Tree.Logistic Regression.K-Nearest Neighbor.Artificial Neural Networks/Deep Learning.Support Vector Machine.	What are the different types of classifiers
3680	In the case where events A and B are independent (where event A has no effect on the probability of event B), the conditional probability of event B given event A is simply the probability of event B, that is P(B). P(A and B) = P(A)P(B|A).	How do you find the probability of something given
5446	The least squares principle states that the SRF should be constructed (with the constant and slope values) so that the sum of the squared distance between the observed values of your dependent variable and the values estimated from your SRF is minimized (the smallest possible value).	What is the principle of least squares
8152	Pointwise mutual information (PMI), or point mutual information, is a measure of association used in information theory and statistics. In contrast to mutual information (MI) which builds upon PMI, it refers to single events, whereas MI refers to the average of all possible events.	What is the difference between Mutual information and Pointwise mutual information
4212	"It is technically defined as ""the nth root product of n numbers."" The geometric mean must be used when working with percentages, which are derived from values, while the standard arithmetic mean works with the values themselves. The harmonic mean is best used for fractions such as rates or multiples."	What are the uses of geometric mean and harmonic mean
3685	The Pearson's correlation coefficient is calculated as the covariance of the two variables divided by the product of the standard deviation of each data sample. It is the normalization of the covariance between the two variables to give an interpretable score.	How do you establish a correlation between variables
4750	AI works by combining large amounts of data with fast, iterative processing and intelligent algorithms, allowing the software to learn automatically from patterns or features in the data.  Cognitive computing is a subfield of AI that strives for a natural, human-like interaction with machines.	How is artificial intelligence made
1017	A non parametric test (sometimes called a distribution free test) does not assume anything about the underlying distribution (for example, that the data comes from a normal distribution).  It usually means that you know the population data does not have a normal distribution.	What is non parametric t test
3928	noun Mathematics. a mathematical operator with the property that applying it to a linear combination of two objects yields the same linear combination as the result of applying it to the objects separately.	What does it mean by a linear operator
4085	Pure serial correlation: occurs when the error terms are correlated and the regression equation is correctly specified. The most commonly assumed form of serial correlation is first-order serial correlation, in which one error term is a function of a previous error term.	What is pure serial correlation
1154	Clustering analysis is broadly used in many applications such as market research, pattern recognition, data analysis, and image processing. Clustering can also help marketers discover distinct groups in their customer base. And they can characterize their customer groups based on the purchasing patterns.	Where is clustering used
7800	0:496:46Suggested clip · 116 secondsUnderstanding Statistical Inference - statistics help - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you write an inference in statistics
164	IN preposition/subordinating conjunction. JJ adjective 'big' JJR adjective, comparative 'bigger' JJS adjective, superlative 'biggest'	What is JJ in POS tagging
857	Semantic similarity is calculated based on two semantic vectors. An order vector is formed for each sentence which considers the syntactic similarity between the sentences. Finally, semantic similarity is calculated based on semantic vectors and order vectors.	How do you find the semantic similarity between two words
1844	Within an artificial neural network, a neuron is a mathematical function that model the functioning of a biological neuron. Typically, a neuron compute the weighted average of its input, and this sum is passed through a nonlinear function, often called activation function, such as the sigmoid.	What does a neuron compute in deep learning
195	Supervised Learning Algorithms: A classification model might look at the input data and try to predict labels like “sick” or “healthy.” Regression is used to predict the outcome of a given sample when the output variable is in the form of real values.	What type of machine learning algorithm makes predictions when you have a set of input data and you know the possible responses
2596	Adjusting minor values in algorithms: This in turn would increase the bias of the model. Whereas, in the SVM algorithm, the trade-off can be changed by an increase in the C parameter that would influence the violations of the margin allowed in the training data. This will increase the bias but decrease the variance.	How do you handle bias variance trade offs
8587	The exponential distribution is the only continuous distribution that is memoryless (or with a constant failure rate). Geometric distribution, its discrete counterpart, is the only discrete distribution that is memoryless.	Is the exponential distribution discrete or continuous
5590	The coefficient of variation is a better risk measure than the standard deviation alone because the CV adjusts for the size of the project. The CV measures the standard deviation divided by the mean and therefore puts the standard deviation into context.	Why is the coefficient of variation a better risk measure to
728	To convert this distance metric into the similarity metric, we can divide the distances of objects with the max distance, and then subtract it by 1 to score the similarity between 0 and 1. We will look at the example after discussing the cosine metric.	How do you measure similarity
374	Therefore, a Random Forest model does not scale very well for time-series data and might need to be constantly updated in Production or trained with some Random Data that lies outside our range of Training set.	Can random forest be used for time series
788	The language of computer science in general, and software development in particular, is laced with metaphor. Indurkhya [5] characterizes metaphor as “a description of an object or event, real or imagined, using concepts that cannot be applied to the object or event in a conventional way” (p. 18).	How are metaphors used in computing
1996	There is a huge difference between classifiers and regressors. Classifiers predict one class from a predetermined list or probabilities of belonging to a class. Regressors predict some value, which could be almost anything. Differeng metrics are used for classification and regression.	What is the difference between random forest classifier and Regressor
635	The agent during its course of learning experience various different situations in the environment it is in.  These are called states. The agent while being in that state may choose from a set of allowable actions which may fetch different rewards(or penalties).	What do you call the set environment in Q learning
863	the condition or quality of being true, correct, or exact; freedom from error or defect; precision or exactness; correctness. Chemistry, Physics. the extent to which a given measurement agrees with the standard value for that measurement.	How do you define accuracy
4922	Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.	What is an acceptable level of Cohen's kappa
8201	Branches of Artificial Intelligence As AI CapabilitiesMachine learning.  Neural Network.  Robotics.  Expert Systems.Fuzzy Logic.  Natural Language Processing.	What are the branches of AI give example
73	Formally, a statistic T(X1,···,Xn) is said to be sufficient for θ if the conditional distribution of X1,···,Xn, given T = t, does not depend on θ for any value of t. In other words, given the value of T, we can gain no more knowledge about θ from knowing more about the probability distribution of X1,···,Xn.	How do you prove a statistic is sufficient
6193	Difference between rule-based AI and machine learning Machine learning systems are probabilistic and rule-based AI models are deterministic.  Machine learning systems require more data as compared to rule-based models. Rule-based AI models can operate with simple basic information and data.	What is the difference between a rule based system and a machine learning system
204	How To Develop Your Artificial Intelligence (AI) Strategy – With Handy TemplateStart with your AI strategic use cases.  Identifying the cross-cutting issues for your AI use cases.  Data strategy.  Ethical and legal issues.  Technology and infrastructure.  Skills and capacity.  Implementation.  Change management.More items	How do you develop an AI strategy
281	The major difference between using a Z score and a T statistic is that you have to estimate the population standard deviation. The T test is also used if you have a small sample size (less than 30).	What is Z statistics and t statistics
7710	A relative frequency distribution shows the proportion of the total number of observations associated with each value or class of values and is related to a probability distribution, which is extensively used in statistics.	Is relative frequency the same as probability distribution
853	Canonical correlation analysis (CCA) is very important in MVL, whose main idea is to map data from different views onto a common space with the maximum correlation. The traditional CCA can only be used to calculate the linear correlation between two views.	What is CCA in machine learning
1593	While Kalman filter can be used for linear or linearized processes and measurement system, the particle filter can be used for nonlinear systems. Also, the uncertainty of Kalman filter is restricted to Gaussian distribution, while the particle filter can deal with non-Gaussian noise distribution.	What is the difference between a particle filter and a Kalman filter
339	Definition: Stratified sampling is a type of sampling method in which the total population is divided into smaller groups or strata to complete the sampling process. The strata is formed based on some common characteristics in the population data.	What is a stratified sample in statistics
7269	This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration. The sample is randomly shuffled and selected for performing the iteration.	How many samples at a time do you use for stochastic gradient descent
330	By design, linear regression is, in some way, scale-invariant.  Some authors have developed rank-regression techniques to handle non-linear re-scaling, using the same approach as in the previous section on clustering.	Is linear regression invariant to scaling
7023	A discrete random variable has a countable number of possible values. The probability of each value of a discrete random variable is between 0 and 1, and the sum of all the probabilities is equal to 1. A continuous random variable takes on all the values in some interval of numbers.	What are discrete random variables
4125	Adaptive resonance theory is a type of neural network technique developed by Stephen Grossberg and Gail Carpenter in 1987. The basic ART uses unsupervised learning technique.	What type of learning is involved in Adaptive Resonance Theory
1663	"Variables that can only take on a finite number of values are called ""discrete variables."" All qualitative variables are discrete. Some quantitative variables are discrete, such as performance rated as 1,2,3,4, or 5, or temperature rounded to the nearest degree."	Is a discrete variable
1224	How to calculate margin of errorGet the population standard deviation (σ) and sample size (n).Take the square root of your sample size and divide it into your population standard deviation.Multiply the result by the z-score consistent with your desired confidence interval according to the following table:	How do you determine margin of error
854	A pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling.	What is the pooling layer used in a convolution neural network
6749	Bayesian networks are a type of Probabilistic Graphical Model that can be used to build models from data and/or expert opinion. They can be used for a wide range of tasks including prediction, anomaly detection, diagnostics, automated insight, reasoning, time series prediction and decision making under uncertainty.	What are Bayesian networks used for
4600	The R-squared of the regression is the fraction of the variation in your dependent variable that is accounted for (or predicted by) your independent variables. (In regression with a single independent variable, it is the same as the square of the correlation between your dependent and independent variable.)	What is the output of regression model
1199	Sample moments are those that are utilized to approximate the unknown population moments. Sample moments are calculated from the sample data. Such moments include mean, variance, skewness, and kurtosis.	What are sample moments
774	Entry level positions require at least a bachelor's degree while positions entailing supervision, leadership or administrative roles frequently require master's or doctoral degrees. Typical coursework involves study of: Various level of math, including probability, statistics, algebra, calculus, logic and algorithms.	What should I study to work with artificial intelligence
3840	Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.  Like univariate analysis, bivariate analysis can be descriptive or inferential.	What is bivariate data analysis
8123	A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss).	What is an adversarial neural network
5773	The lognormal distribution is commonly used to model the lives of units whose failure modes are of a fatigue-stress nature. Since this includes most, if not all, mechanical systems, the lognormal distribution can have widespread application.	What is the lognormal distribution used for
1379	Confusion matrices are used to visualize important predictive analytics like recall, specificity, accuracy, and precision. Confusion matrices are useful because they give direct comparisons of values like True Positives, False Positives, True Negatives and False Negatives.	Why do we need Confusion Matrix
5774	Joint entropy: H ( X , Y ) : = − Σ x ∈ J X Σ y ∈ J Y p ( x , y ) log p ( x , y ) . .	How do you calculate joint entropy
435	It is calculated in the same way - by running the network forward over inputs xi and comparing the network outputs ˆyi with the ground truth values yi using a loss function e.g. J=1N∑Ni=1L(ˆyi,yi) where L is the individual loss function based somehow on the difference between predicted value and target.	How is validation loss calculated
757	Normal distributions are symmetric around their mean. The mean, median, and mode of a normal distribution are equal. The area under the normal curve is equal to 1.0.  Approximately 95% of the area of a normal distribution is within two standard deviations of the mean.	What is approximately normal distribution
3984	Surface must be closed But unlike, say, Stokes' theorem, the divergence theorem only applies to closed surfaces, meaning surfaces without a boundary. For example, a hemisphere is not a closed surface, it has a circle as its boundary, so you cannot apply the divergence theorem.	What are divergence theorem limitations
6872	A linear classifier that the perceptron is categorized as is a classification algorithm, which relies on a linear predictor function to make predictions. Its predictions are based on a combination that includes weights and feature vector.	What type of algorithm is Perceptron
5893	Interpreting the ROC curve Classifiers that give curves closer to the top-left corner indicate a better performance. As a baseline, a random classifier is expected to give points lying along the diagonal (FPR = TPR). The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.	How do you read ROC curve results
3936	Flow Rate Calibration – Improve Print Accuracy3.1 1. Measure the Filament Diameter.3.2 2. Print a Hollow Test Cube.3.3 3. Measure the Cube Walls.3.4 4. Enter the new Flow Rate value in your slicer.	How do you calibrate a flow rate
213	"In relation to out-group, a social group toward which a person feels a sense of competition or opposition. They both affect the opinions and behavior of individuals because In-groups and Out- groups are based on the idea that ""we"" have valued traits that ""they"" lack."	What is the meaning of in group and out group how do they both affect the opinions and behavior of individuals
1113	Binomial counts successes in a fixed number of trials, while Negative binomial counts failures until a fixed number successes. The Bernoulli and Geometric distributions are the simplest cases of the Binomial and Negative Binomial distributions.	What is difference between binomial and negative binomial distribution
7483	Now, three variable case it is less clear for me. An intuitive definition for covariance function would be Cov(X,Y,Z)=E[(x−E[X])(y−E[Y])(z−E[Z])], but instead the literature suggests using covariance matrix that is defined as two variable covariance for each pair of variables.	How do you find the covariance of three variables
1387	Content-based filtering, makes recommendations based on user preferences for product features. Collaborative filtering mimics user-to-user recommendations. It predicts users preferences as a linear, weighted combination of other user preferences. Both methods have limitations.	What is the difference between content based filtering and collaborative filtering
6326	Tensorflow is the more popular of the two. Tensorflow is typically used more in Deep Learning and Neural Networks. SciKit learn is more general Machine Learning.	Which is better Tensorflow or Scikit
8	Similar to the distinction in philosophy between a priori and a posteriori, in Bayesian inference a priori denotes general knowledge about the data distribution before making an inference, while a posteriori denotes knowledge that incorporates the results of making an inference.	What is the difference between a priori and a posteriori probability
7341	Pruning reduces the size of decision trees by removing parts of the tree that do not provide power to classify instances. Decision trees are the most susceptible out of all the machine learning algorithms to overfitting and effective pruning can reduce this likelihood.	What is the purpose of pruning a decision tree
1495	Chi-square Test. The Pearson's χ2 test (after Karl Pearson, 1900) is the most commonly used test for the difference in distribution of categorical variables between two or more independent groups.	How do you find the difference between two categorical variables
973	"Statistical data binning is a way to group numbers of more or less continuous values into a smaller number of ""bins"". For example, if you have data about a group of people, you might want to arrange their ages into a smaller number of age intervals (for example, grouping every five years together)."	What is data binning in statistics
80	Agents can be grouped into four classes based on their degree of perceived intelligence and capability :Simple Reflex Agents.Model-Based Reflex Agents.Goal-Based Agents.Utility-Based Agents.Learning Agent.	What are the types of agent in AI
5	Two approaches to avoiding overfitting are distinguished: pre-pruning (generating a tree with fewer branches than would otherwise be the case) and post-pruning (generating a tree in full and then removing parts of it). Results are given for pre-pruning using either a size or a maximum depth cutoff.	How do you avoid overfitting in decision trees
4401	LDA is a parametric model, and the parameter is number of topics.	Is Latent Dirichlet Allocation a parametic model or nonparametric
7304	A generative model includes the distribution of the data itself, and tells you how likely a given example is. For example, models that predict the next word in a sequence are typically generative models (usually much simpler than GANs) because they can assign a probability to a sequence of words.	What is a generative model in machine learning
1377	Classic linear regression is one form of general linear model. But with a general linear model you can have any number of continuous or nominal independent variables and their interactions.	Can nominal variables be used in regression analysis
6966	Single-pattern algorithmsAlgorithmPreprocessing timeMatching timeKnuth–Morris–Pratt algorithmΘ(m)Θ(n)Boyer–Moore string-search algorithmΘ(m + k)best Ω(n/m), worst O(mn)Bitap algorithm (shift-or, shift-and, Baeza–Yates–Gonnet; fuzzy; agrep)Θ(m + k)O(mn)Two-way string-matching algorithm (glibc memmem/strstr)Θ(m)O(n+m)6 more rows	Which algorithm is used for pattern matching
5825	Hypothesis Space (H): Hypothesis space is the set of all the possible legal hypothesis. This is the set from which the machine learning algorithm would determine the best possible (only one) which would best describe the target function or the outputs.	What is hypothesis space in ML
267	The formula for calculating a z-score is is z = (x-μ)/σ, where x is the raw score, μ is the population mean, and σ is the population standard deviation. As the formula shows, the z-score is simply the raw score minus the population mean, divided by the population standard deviation.	How do you calculate the Z score
7053	Coef. A regression coefficient describes the size and direction of the relationship between a predictor and the response variable. Coefficients are the numbers by which the values of the term are multiplied in a regression equation.	What do the coefficients in logistic regression mean
193	A high-pass filter (HPF) is an electronic filter that passes signals with a frequency higher than a certain cutoff frequency and attenuates signals with frequencies lower than the cutoff frequency. The amount of attenuation for each frequency depends on the filter design.	How does a high pass RC filter work
6237	A Markov chain is ergodic if it is both irreducible and aperiodic. This condition is equivalent to the transition matrix being a primitive nonnegative matrix.	How can you tell if a Markov chain is ergodic
8539	Three of the most common applications of exponential and logarithmic functions have to do with interest earned on an investment, population growth, and carbon dating.	What application has the logarithmic function
1420	A good maximum sample size is usually 10% as long as it does not exceed 1000. A good maximum sample size is usually around 10% of the population, as long as this does not exceed 1000. For example, in a population of 5000, 10% would be 500. In a population of 200,000, 10% would be 20,000.	What is a good representative sample size
7586	A deep neural net is a single independent model, whereas ensemble models are ensembles of many independent models. The primary connection between the two is dropout, a particular method of training deep neural nets that's inspired by ensemble methods.	Do deep learning algorithms represent ensemble based methods
7127	Second-Order/Pseudo-Second-Order Reaction For a Pseudo-Second-Order Reaction, the reaction rate constant k is replaced by the apparent reaction rate constant k'. If the reaction is not written out specifically to show a value of νA, the value is assumed to be 1 and is not shown in these equations.	What is a pseudo second order reaction
1283	An intercept or offset from an origin. Bias (also known as the bias term) is referred to as b or w0 in machine learning models.	What is W in machine learning
7702	In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.	What are the measures of skewness
310	The need for a CNN with variable input dimensions FCN is a network that does not contain any “Dense” layers (as in traditional CNNs) instead it contains 1x1 convolutions that perform the task of fully connected layers (Dense layers).	How is Fully Convolutional Network FCN different from the original Convolutional Neural Network CNN
8617	Bayesian OptimizationBuild a surrogate probability model of the objective function.Find the hyperparameters that perform best on the surrogate.Apply these hyperparameters to the true objective function.Update the surrogate model incorporating the new results.Repeat steps 2–4 until max iterations or time is reached.	How do I use Bayesian optimization
6656	The Normal Distribution is a distribution that has most of the data in the center with decreasing amounts evenly distributed to the left and the right. Skewed Distribution is distribution with data clumped up on one side or the other with decreasing amounts trailing off to the left or the right.	What is the difference between a normal distribution and a skewed distribution
3347	Inter-Rater Reliability MethodsCount the number of ratings in agreement. In the above table, that's 3.Count the total number of ratings. For this example, that's 5.Divide the total by the number in agreement to get a fraction: 3/5.Convert to a percentage: 3/5 = 60%.	How do you calculate kappa inter rater reliability
4405	The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is an error from sensitivity to small fluctuations in the training set.	What is bias in machine learning algorithms
870	Quota sampling means to take a very tailored sample that's in proportion to some characteristic or trait of a population.  Care is taken to maintain the correct proportions representative of the population. For example, if your population consists of 45% female and 55% male, your sample should reflect those percentages.	What is an example of quota sampling
8571	The main limitation of random forest is that a large number of trees can make the algorithm too slow and ineffective for real-time predictions. In general, these algorithms are fast to train, but quite slow to create predictions once they are trained.	Ensemble Learning What are some shortcomings of random forests
2144	A statistical model is a family of probability distributions, the central problem of statistical inference being to identify which member of the family generated the data currently of interest.	What does model mean in statistics
2700	A subquery is a select statement that is embedded in a clause of another select statement.  A Correlated subquery is a subquery that is evaluated once for each row processed by the outer query or main query.	What is the difference between subquery and correlated query
1661	A variable xj is said to be endogenous within the causal model M if its value is determined or influenced by one or more of the independent variables X (excluding itself). A purely endogenous variable is a factor that is entirely determined by the states of other variables in the system.	How do you identify endogenous variables
4148	One should always conduct a residual analysis to verify that the conditions for drawing inferences about the coefficients in a linear model have been met. Recall that, if a linear model makes sense, the residuals will: have a constant variance.	What is residual analysis used for in multiple regression
286	Answer: An independent variable is exactly what it sounds like. It is a variable that stands alone and isn't changed by the other variables you are trying to measure. For example, someone's age might be an independent variable.	How do you identify an independent variable
1477	Fewer than 1,000 steps a day is sedentary. 1,000 to 10,000 steps or about 4 miles a day is Lightly Active. 10,000 to 23,000 steps or 4 to 10 miles a day is considered Active. More than 23,000 steps or 10 miles a day is Highly active.	What is considered active activity level
660	There's no difference. They are two names for the same thing. They tend to be used in different contexts, though. You talk about the expected value of a random variable and the mean of a sample, population or probability distribution.	Is expected value the same as the mean
138	(Note that how a support vector machine classifies points that fall on a boundary line is implementation dependent. In our discussions, we have said that points falling on the line will be considered negative examples, so the classification equation is w . u + b ≤ 0.)	What equations are used for Classificationion in a support vector machine
244	2 AnswersUse weight regularization. It tries to keep weights low which very often leads to better generalization.  Corrupt your input (e.g., randomly substitute some pixels with black or white).  Expand your training set.  Pre-train your layers with denoising critera.  Experiment with network architecture.	How do you improve validation
720	Control Charts: A discrete distribution is one in which the data can only take on certain values, for example integers. A continuous distribution is one in which data can take on any value within a specified range (which may be infinite).	What is discrete and continuous distribution
7910	Whereas the normal distribution is the sum/difference of lots of things, the lognormal (because it is the log transform) is the product/quotient of lots of things. So if you are multiplying a bunch of variables together, the resultant distribution approaches lognormal as the number of variables gets large.	What is intuition explanation of log normal distribution
1286	Coefficient of correlation is “R” value which is given in the summary table in the Regression output.  In other words Coefficient of Determination is the square of Coefficeint of Correlation. R square or coeff. of determination shows percentage variation in y which is explained by all the x variables together.	Correlation coefficient vs coefficient of determination whats the difference in simple terms
340	This binary classifier for multiclass can be used with one-vs-all or all-vs-all reduction method. Here you can go with logistic regression, decision tree algorithms. You can go with algorithms like Naive Bayes, Neural Networks and SVM to solve multi class problem.	Which algorithm is used for multiclass classification
948	The population mean of the distribution of sample means is the same as the population mean of the distribution being sampled from.  Thus as the sample size increases, the standard deviation of the means decreases; and as the sample size decreases, the standard deviation of the sample means increases.	What happens when the sample size decreases
114	To reiterate parameter sharing occurs when a feature map is generated from the result of the convolution between a filter and input data from a unit within a plane in the conv layer. All units within this layer plane share the same weights; hence it is called weight/parameter sharing.	Why are RNN's and CNN's called weight shareable layers
5328	A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.	What is Bayesian posterior probabilities
1123	Triplet loss is a loss function for machine learning algorithms where a baseline (anchor) input is compared to a positive (truthy) input and a negative (falsy) input.  This can be avoided by posing the problem as a similarity learning problem instead of a classification problem.	What is triplet loss function
1634	Convolutional neural networks (CNNs, or ConvNets) are essential tools for deep learning, and are especially suited for analyzing image data. For example, you can use CNNs to classify images. To predict continuous data, such as angles and distances, you can include a regression layer at the end of the network.	Can CNN be used for regression
2506	A probability histogram is a graph that shows the probability of each outcome on the y -axis.	What is a probability distribution histogram
307	Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are. Instead, they need to be recoded into a series of variables which can then be entered into the regression model.	Can you do regression with categorical variables
3878	Natural numbers are a part of the number system which includes all the positive integers from 1 till infinity and are also used for counting purpose. It does not include zero (0). In fact, 1,2,3,4,5,6,7,8,9…., are also called counting numbers.	Is 0 part of the natural numbers
112	0:002:42Suggested clip · 110 secondsNormal distribution moment generating function - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you derive the MGF of a normal distribution
797	Difference Between Cross Correlation and Autocorrelation Cross correlation happens when two different sequences are correlated. Autocorrelation is the correlation between two of the same sequences. In other words, you correlate a signal with itself.	What is the difference between autocorrelation cross correlation
7452	Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron.	What is activation function in deep learning
1771	To estimate the oriented bounding box, you need to train the network with objects and their oriented bounding boxes. For that, you need to modify the bounding box regression head of the network. Frustum PointNet[2] employs such regression but for the 3D bounding boxes. It can easily be extended for the 2D use cases.	How can I get oriented tight bounding boxes for object detection
1317	Convolutional layers in a convolutional neural network systematically apply learned filters to input images in order to create feature maps that summarize the presence of those features in the input. A pooling layer is a new layer added after the convolutional layer.	What is pooling in convolutional neural network
6585	The values of the kernel filters are learned automatically by the neural network through the training process, and the filters kernels which results in the features that are most efficient for the particular classification or the detection are automatically learned.	How are filters chosen in CNN
1004	"""Correlation is not causation"" means that just because two things correlate does not necessarily mean that one causes the other.  Correlations between two things can be caused by a third factor that affects both of them."	What does it mean when a researcher says Correlation is not causation
5762	Mathematical expectation, also known as the expected value, is the summation or integration of a possible values from a random variable. It is also known as the product of the probability of an event occurring, denoted P(x), and the value corresponding with the actual observed occurrence of the event.	What is mathematical expectation of a random variable
606	YOUR preferred learning style is the way in which YOU learn best. Three learning styles that are often identified in students are the Auditory Learning Style, the Visual Learning Style, and theTactile/Kinesthetic Learning Style. Read about each of these learning styles to identify YOUR preferred learning style.	What is your preferred way of learning
3573	Discriminant analysis is statistical technique used to classify observations into non-overlapping groups, based on scores on one or more quantitative predictor variables. For example, a doctor could perform a discriminant analysis to identify patients at high or low risk for stroke.	What do you mean by discriminant analysis
2176	In machine learning, scoring is the process of applying an algorithmic model built from a historical dataset to a new dataset in order to uncover practical insights that will help solve a business problem.  The second stage is scoring, in which you apply the trained model to a new dataset.	What is model scoring in machine learning
5057	Qualities of a Good Sampling Frame Include all individuals in the target population. Exclude all individuals not in the target population. Includes accurate information that can be used to contact selected individuals.	What is a good sampling frame
5025	Iterable is an object, which one can iterate over. It generates an Iterator when passed to iter() method. Iterator is an object, which is used to iterate over an iterable object using __next__() method.  Note that every iterator is also an iterable, but not every iterable is an iterator.	What are iterators and Iterables in Python
3614	Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.	Does AI stand for Artificial Intelligence
3425	In two-dimensional signals like digital images, frequencies are rate of change of grey scale value (intensity of pixel) with respect to space. This is also called Spatial frequency .  Convert the cosine values represented by the red dots into greyscale (0-255), such that -1 maps to 0 and 1 maps to 255.	What is frequency of an image
8050	H (hypothesis set): A space of possible hypotheses for mapping inputs to outputs that can be searched, often constrained by the choice of the framing of the problem, the choice of model and the choice of model configuration.	What is hypothesis set in machine learning
1348	The neuron is the basic working unit of the brain, a specialized cell designed to transmit information to other nerve cells, muscle, or gland cells. Neurons are cells within the nervous system that transmit information to other nerve cells, muscle, or gland cells. Most neurons have a cell body, an axon, and dendrites.	What is neurons and its function
4742	Proof: The integers Z are countable because the function f : Z → N given by f(n) = 2n if n is non-negative and f(n) = 3− n if n is negative, is an injective function.	How do you prove that Z is countable
7272	In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable.	What is the use of maximum likelihood estimation
7728	The standard deviation (SD) measures the amount of variability, or dispersion, from the individual data values to the mean, while the standard error of the mean (SEM) measures how far the sample mean of the data is likely to be from the true population mean.  SD is the dispersion of individual data values.	What does standard deviation of the mean represent
8227	If you want to control for the effects of some variables on some dependent variable, you just include them into the model. Say, you make a regression with a dependent variable y and independent variable x. You think that z has also influence on y too and you want to control for this influence.	How do you control variables in a linear regression
659	The consequences of making a type I error mean that changes or interventions are made which are unnecessary, and thus waste time, resources, etc. Type II errors typically lead to the preservation of the status quo (i.e. interventions remain the same) when change is needed.	Why are type I and type II errors used
6453	1:2713:04Suggested clip · 104 secondsEstimated Mean Average and Cumulative Frequency Graphs 10A2 YouTubeStart of suggested clipEnd of suggested clip	How do you find the mean from a cumulative frequency table
1566	To calculate the standard deviation of those numbers:Work out the Mean (the simple average of the numbers)Then for each number: subtract the Mean and square the result.Then work out the mean of those squared differences.Take the square root of that and we are done!	How do you calculate SD from the mean
1340	Typically, a regression analysis is done for one of two purposes: In order to predict the value of the dependent variable for individuals for whom some information concerning the explanatory variables is available, or in order to estimate the effect of some explanatory variable on the dependent variable.	What is the purpose of regression
1900	It is possible to find the correlation between a categorical variable and a continuous variable using the analysis of covariance technique.	Can you correlate a categorical variable with a continuous variable
5517	According to this link LDA is a generative classifier. Also, the motto of LDA is to model a discriminant function to classify.	Is linear discriminant analysis a generative model
4901	Connected components, in a 2D image, are clusters of pixels with the same value, which are connected to each other through either 4-pixel, or 8-pixel connectivity.  We offer several user-friendly ways to segment, and then rapidly calculate and display the connected components of 2D and 3D segmentations.	What is connected component in image processing
1003	Examples of multivariate regression Example 1. A researcher has collected data on three psychological variables, four academic variables (standardized test scores), and the type of educational program the student is in for 600 high school students.  A doctor has collected data on cholesterol, blood pressure, and weight.	What is an example of multivariate analysis
655	K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups).  The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.	What type of algorithm is K means
2755	100	What is the minimum sample size for statistical analysis
8489	The method involves asking individuals to state their preference over hypothetical alternative scenarios, goods or services. Each alternative is described by several attributes and the responses are used to determine whether preferences are significantly influenced by the attributes and also their relative importance.	What is choice experiment method
4026	Steps in Data Exploration and Preprocessing:Identification of variables and data types.Analyzing the basic metrics.Non-Graphical Univariate Analysis.Graphical Univariate Analysis.Bivariate Analysis.Variable transformations.Missing value treatment.Outlier treatment.More items•	How do you do exploratory data analysis
4453	General steps to calculate the mean squared error from a set of X and Y values:Find the regression line.Insert your X values into the linear regression equation to find the new Y values (Y').Subtract the new Y value from the original to get the error.Square the errors.Add up the errors.Find the mean.	How do you find the mean squared error
2442	5) In general, practice, choosing the value of k is k = sqrt(N) where N stands for the number of samples in your training dataset .	How do you find K in nearest neighbor
8153	But the size of the input image of a Convolutional network should not be less than the input, so padding is done. To calculate padding, input_size + 2 * padding_size-(filter_size-1). For above case, (50+(2*1)-(3–1) = 52–2 = 50) which gives as a same input size.	How is CNN padding calculated
1265	EPA's Positive Matrix Factorization (PMF) Model is a mathematical receptor model developed by EPA scientists that provides scientific support for the development and review of air and water quality standards, exposure research and environmental forensics.	What is positive matrix factorization
7629	LDA tends to be a better than QDA when you have a small training set. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major issue, or if the assumption of a common covariance matrix for the K classes is clearly untenable.	What is the difference between a discriminant function and a hypothesis function in machine learning
5401	Random numbers are sets of digits (i.e., 0, 1, 2, 3, 4, 5, 6, 7, 8, 9) arranged in random order. Because they are randomly ordered, no individual digit can be predicted from knowledge of any other digit or group of digits.	What is random number example
5694	Concept shift is closely related to concept drift. This occurs when a model learned from data sampled from one distribution needs to be applied to data drawn from another.	What is Concept shift
2743	Linear regression is a way to model the relationship between two variables.  The equation has the form Y= a + bX, where Y is the dependent variable (that's the variable that goes on the Y axis), X is the independent variable (i.e. it is plotted on the X axis), b is the slope of the line and a is the y-intercept.	What is the formula for linear regression
1273	Make a new calculated column based on the mathematical form (shape) of your data. Plot a new graph using your new calculated column of data on one of your axes. If the new graph (using the calculated column) is straight, you have succeeded in linearizing your data. Draw a best fit line USING A RULER!	How do you linearize a curve
692	Generative adversarial networks (GANs) have been widely used and have achieved competitive results in semi-supervised learning.  We first prove that, given a fixed generator, optimizing the discriminator of GAN-SSL is equivalent to optimizing that of supervised learning.	Can GANs solve semi supervised learning
6372	Conditional probabilities can be reversed using Bayes' theorem. Conditional probabilities can be displayed in a conditional probability table.	How do you reverse conditional probability
2768	"More generally, survival analysis involves the modelling of time to event data; in this context, death or failure is considered an ""event"" in the survival analysis literature – traditionally only a single event occurs for each subject, after which the organism or mechanism is dead or broken."	How does Survival analysis work
138	Cohen's d is an effect size used to indicate the standardised difference between two means. It can be used, for example, to accompany reporting of t-test and ANOVA results. It is also widely used in meta-analysis. Cohen's d is an appropriate effect size for the comparison between two means.	What is Cohen's d in statistics
1193	"Motivation involves the biological, emotional, social, and cognitive forces that activate behavior. In everyday usage, the term ""motivation"" is frequently used to describe why a person does something."	What are the forces of motivation
8484	Statistical Methods for Finding the Best Regression ModelAdjusted R-squared and Predicted R-squared: Generally, you choose the models that have higher adjusted and predicted R-squared values.  P-values for the predictors: In regression, low p-values indicate terms that are statistically significant.More items•	How do you know which regression model to use
5466	The Spearman's rank-order correlation is the nonparametric version of the Pearson product-moment correlation. Spearman's correlation coefficient, (ρ, also signified by rs) measures the strength and direction of association between two ranked variables.	What does Spearman's rank correlation coefficient show
1113	False-negative results on the QFT-GIT test for patients with latent and active TB disease have been reported with a frequency of 4–38% [22].	How accurate is QuantiFERON gold test
2350	Machine learning can be automated when it involves the same activity again and again. However, the fundamental nature of machine learning deals with the opposite: variable conditions. In this regard, machine learning needs to be able to function independently and with different solutions to match different demands.	Can machine learning be automated
904	When a document needs modelling by LDA, the following steps are carried out initially:The number of words in the document are determined.A topic mixture for the document over a fixed set of topics is chosen.A topic is selected based on the document's multinomial distribution.More items•	How LDA works step by step
1841	Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error.	What is an intuitive explanation of Gradient Boosting
2788	Linear filtering is the filtering method in which the value of output pixel is linear combinations of the neighbouring input pixels. it can be done with convolution. For examples, mean/average filters or Gaussian filtering. A non-linear filtering is one that cannot be done with convolution or Fourier multiplication.	What is difference between linear filtersand nonlinear filters
6147	Quite simply, an insignificant coefficient means that the independent variable has no effect on the dependent variable, that is, its effect is statistically equal to zero (according to the results).  The effect of independent variable is too little to actually affect the dependent variable most of the time.	What does it mean when a variable is not significant
484	The Laplacian is a 2-D isotropic measure of the 2nd spatial derivative of an image.  The Laplacian is often applied to an image that has first been smoothed with something approximating a Gaussian smoothing filter in order to reduce its sensitivity to noise, and hence the two variants will be described together here.	What is Laplacian of Gaussian filter
3242	Posterior probability is a conditional probability, but more specifically implies the probability of a particular parameter value(s) from a given parameter space when a given set of observations (say Xi) have been observed.	What is the difference between conditional probability and posterior probability
1675	Laws that concern data are highly relevant for AI, since those laws can impact the use and growth of AI systems.  However, no countries yet have specific laws in place around ethical and responsible AI. Time will tell whether or not companies will self-monitor or if governments will step in to more formally regulate.	Are there any laws about artificial intelligence
1584	Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron.	What does activation function do
5051	PCA finds a lower dimensional representation of the data that minimizes the squared reconstruction error. If you have irrelevant features (often the case in text classification), PCA counts errors in those with equal importance as errors in words that are important for your classification.	Why is PCA bad for classification
136	ARMA stands for “Autoregressive Moving Average” and ARIMA stands for “Autoregressive Integrated Moving Average.” The only difference, then, is the “integrated” part. Integrated refers to the number of times needed to difference a series in order to achieve stationarity, which is required for ARMA models to be valid.	What is difference between ARMA and Arima model
8206	Use simple logistic regression when you have one nominal variable and one measurement variable, and you want to know whether variation in the measurement variable causes variation in the nominal variable.	When should you use logistic regression
7790	Overfitting occurs when a model tries to predict a trend in data that is too noisy. This is the caused due to an overly complex model with too many parameters. A model that is overfitted is inaccurate because the trend does not reflect the reality present in the data.	How does overfitting happen in a neural network
6536	The determinant of a matrix is a special value that is calculated from a square matrix. It can help you determine whether a matrix has an inverse, find the area of a triangle, and let you know if the system of equations has a unique solution. Determinants are also used in calculus and linear algebra.	What is the purpose of the determinant of a matrix
5759	A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results.	What is the difference between decision tree and random forest
8424	It means that there is no absolute good or bad threshold, however you can define it based on your DV. For a datum which ranges from 0 to 1000, an RMSE of 0.7 is small, but if the range goes from 0 to 1, it is not that small anymore.	What is a good RMSE score
3056	Normal distributions are symmetric, unimodal, and asymptotic, and the mean, median, and mode are all equal. A normal distribution is perfectly symmetrical around its center. That is, the right side of the center is a mirror image of the left side. There is also only one mode, or peak, in a normal distribution.	What are the characteristics of a normal distribution
5214	The normal distribution is a probability function that describes how the values of a variable are distributed. It is a symmetric distribution where most of the observations cluster around the central peak and the probabilities for values further away from the mean taper off equally in both directions.	Which distribution is a normal distribution
8680	Definition: In simple words, data mining is defined as a process used to extract usable data from a larger set of any raw data. It implies analysing data patterns in large batches of data using one or more software.  Data mining is also known as Knowledge Discovery in Data (KDD).	What you mean by data mining
6205	There are three types of layers in a convolutional neural network: convolutional layer, pooling layer, and fully connected layer. Each of these layers has different parameters that can be optimized and performs a different task on the input data. Features of a convolutional layer.	What are layers in CNN
234	For a 2x2 table, the null hypothesis may equivalently be written in terms of the probabilities themselves, or the risk difference, the relative risk, or the odds ratio. In each case, the null hypothesis states that there is no difference between the two groups.	What is the null hypothesis for a cross tabulation
1347	"A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents.  Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning."	What are multi agents in artificial intelligence
8135	"The beginnings of modern AI can be traced to classical philosophers' attempts to describe human thinking as a symbolic system. But the field of AI wasn't formally founded until 1956, at a conference at Dartmouth College, in Hanover, New Hampshire, where the term ""artificial intelligence"" was coined."	What is the history of artificial intelligence
7951	Association between two variables means the values of one variable relate in some way to the values of the other. Association is usually measured by correlation for two continuous variables and by cross tabulation and a Chi-square test for two categorical variables.	What is an association between two variables
6177	Classical statistics uses techniques such as Ordinary Least Squares and Maximum Likelihood – this is the conventional type of statistics that you see in most textbooks covering estimation, regression, hypothesis testing, confidence intervals, etc.  In fact Bayesian statistics is all about probability calculations!	What is the difference between Bayesian and regular statistics
853	Optimal control focuses on a subset of problems, but solves these problems very well, and has a rich history. RL can be thought of as a way of generalizing or extending ideas from optimal control to non-traditional control problems.	What is the difference between optimal control theory and reinforcement learning
475	Use the hypergeometric distribution with populations that are so small that the outcome of a trial has a large effect on the probability that the next outcome is an event or non-event.	What is the hypergeometric distribution used for
7958	A time series is a dataset whose unit of analysis is a time period, rather than a person. Regression is an analytic tool that attempts to predict one variable, y as a function of one or more x variables. It can be used to analyze both time-series and static data.	What is the difference between regression and time series
1399	A new study suggests that the placebo effect may work in reverse. A new study suggests that the placebo effect may work in reverse. In the past, placebos have been given to participants in studies to detect whether the participant would still feel the effects of the “drug” they thought they were being given.	Does the placebo effect work in reverse
278	Mathematics Behind PCATake the whole dataset consisting of d+1 dimensions and ignore the labels such that our new dataset becomes d dimensional.Compute the mean for every dimension of the whole dataset.Compute the covariance matrix of the whole dataset.Compute eigenvectors and the corresponding eigenvalues.More items	How do you calculate principal component analysis
425	1. A Simple Way of Solving an Object Detection Task (using Deep Learning)First, we take an image as input:Then we divide the image into various regions:We will then consider each region as a separate image.Pass all these regions (images) to the CNN and classify them into various classes.More items•	How do you learn object detection
147	To prevent selection bias, investigators should anticipate and analyze all the confounders important for the outcome studied. They should use an adequate method of randomization and allocation concealment and they should report these methods in their trial.	How do you reduce selection bias in RCT
6057	The term 'univariate' implies that forecasting is based on a sample of time series observations of the exchange rate without taking into account the effect of the other variables such as prices and interest rates.	What is univariate time series forecasting
1012	The normal distribution is a probability distribution. It is also called Gaussian distribution because it was first discovered by Carl Friedrich Gauss.  It is often called the bell curve, because the graph of its probability density looks like a bell. Many values follow a normal distribution.	Why normal distribution is called normal
6850	Use the hypergeometric distribution with populations that are so small that the outcome of a trial has a large effect on the probability that the next outcome is an event or non-event. For example, in a population of 10 people, 7 people have O+ blood.	When would you use a hypergeometric distribution
7901	A ranked variable is an ordinal variable; a variable where every data point can be put in order (1st, 2nd, 3rd, etc.). You may not know an exact value of any of your points, but you know which comes after the other.	What is the rank of a variable
8116	among the constituent fields of anthropology. Physical anthropology has made the most use of statistics, while archeology, linguistics, and cultural anthropology have employed them much less frequently.	Does anthropology consist of statistics
7600	CRF is a discriminant model. MEMM is not a generative model, but a model with finite states based on state classification. HMM and MEMM are a directed graph, while CRF is an undirected graph. HMM directly models the transition probability and the phenotype probability, and calculates the probability of co-occurrence.	What is the major difference between CRF Conditional Random Field and HMM hidden Markov model )
7897	Network analytics, in its simplest definition, involves the analysis of network data and statistics to identify trends and patterns. Once identified, operators take the next step of 'acting' on this data—which typically involves a network operation or a set of operations.	What is network data analysis
1146	Imputation is a procedure for entering a value for a specific data item where the response is missing or unusable. Context: Imputation is the process used to determine and assign replacement values for missing, invalid or inconsistent data that have failed edits.	What does imputation mean in statistics
4222	Description. VGG-19 is a convolutional neural network that is 19 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database [1]. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals.	What is Vgg in deep learning
54	So there are exactly n vectors in every basis for Rn . By definition, the four column vectors of A span the column space of A. The third and fourth column vectors are dependent on the first and second, and the first two columns are independent. Therefore, the first two column vectors are the pivot columns.	How many vectors are in a basis
8111	The main difference between these two techniques is that regression analysis deals with a continuous dependent variable, while discriminant analysis must have a discrete dependent variable. The methodology used to complete a discriminant analysis is similar to regression analysis.	What is the difference between regression analysis and discriminant analysis
4761	Machine Learning AlgorithmsLinear Regression. To understand the working functionality of this algorithm, imagine how you would arrange random logs of wood in increasing order of their weight.  Logistic Regression.  Decision Tree.  SVM (Support Vector Machine)  Naive Bayes.  KNN (K- Nearest Neighbors)  K-Means.  Random Forest.More items•	What algorithms are used in machine learning
6386	Definition: Given data the maximum likelihood estimate (MLE) for the parameter p is the value of p that maximizes the likelihood P(data |p). That is, the MLE is the value of p for which the data is most likely. 100 P(55 heads|p) = ( 55 ) p55(1 − p)45.	How do you find maximum likelihood estimation
4859	Since this impulse response in infinitely long, recursive filters are often called infinite impulse response (IIR) filters. In effect, recursive filters convolve the input signal with a very long filter kernel, although only a few coefficients are involved.	Is an IIR filter recursive
436	Explanation: Simple reflex agent is based on the present condition and so it is condition action rule. 5. What are the composition for agents in artificial intelligence? Explanation: An agent program will implement function mapping percepts to actions.	What are the composition for agents in artificial intelligence
8618	Outlier detection is the process of detecting and subsequently excluding outliers from a given set of data. An outlier may be defined as a piece of data or observation that deviates drastically from the given norm or average of the data set.	What is data outlier detection
7627	Step 1 — Deciding on the network topology (not really considered optimization but is obviously very important)  Step 2 — Adjusting the learning rate.  Step 3 — Choosing an optimizer and a loss function.  Step 4 — Deciding on the batch size and number of epochs.  Step 5 — Random restarts.	How do I tune Hyperparameters in neural network
8043	"""AI is a computer system able to perform tasks that ordinarily require human intelligence Many of these artificial intelligence systems are powered by machine learning, some of them are powered by deep learning and some of them are powered by very boring things like rules."""	What is a artificial intelligence system
529	It is closely related to prior probability, which is the probability an event will happen before you taken any new evidence into account. You can think of posterior probability as an adjustment on prior probability: Posterior probability = prior probability + new evidence (called likelihood).	How do you calculate posterior and prior probability
1329	Clustering starts by computing a distance between every pair of units that you want to cluster. A distance matrix will be symmetric (because the distance between x and y is the same as the distance between y and x) and will have zeroes on the diagonal (because every item is distance zero from itself).	What is a distance matrix in clustering
505	Thus, if the random variable X is log-normally distributed, then Y = ln(X) has a normal distribution. Equivalently, if Y has a normal distribution, then the exponential function of Y, X = exp(Y), has a log-normal distribution. A random variable which is log-normally distributed takes only positive real values.	How do you find the log of a normal distribution
2784	Calculating Disparity Map First, squared difference or absolute difference is calcluated for each pixel and then all the values are summed over a window W. For each shift value of the right image, there is an SSD/SAD map equal to the size of the image. The disparity map is a 2D map reduced from 3D space.	How is Map disparity calculated
376	A deck of cards has within it uniform distributions because the likelihood of drawing a heart, a club, a diamond or a spade is equally likely. A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same.	What is the example of uniform distribution
4627	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution.	Why does data follow the normal curve
421	The “trick” is that kernel methods represent the data only through a set of pairwise similarity comparisons between the original data observations x (with the original coordinates in the lower dimensional space), instead of explicitly applying the transformations ϕ(x) and representing the data by these transformed	What is the kernel trick in SVM
4639	Thus, a double-blind, placebo-controlled clinical trial is a medical study involving human participants in which neither side knows who's getting what treatment and placebo are given to a control group.	What is a randomized double blind placebo controlled study
201	Classification is the process of classifying the data with the help of class labels whereas, in clustering, there are no predefined class labels. 2. Classification is supervised learning, while clustering is unsupervised learning.	What is machine learning name its types and discuss the difference between classification and clustering
3475	Inductive Reasoning Tips and TricksLearn the most common patterns. There are a set of extremely common patterns that the test providers will re-use.  Use the elimination method. The optimal method of solving these logical problems is to use what we call the elimination method.  Lock onto one sub pattern at a time and follow that through.	How can inductive reasoning be improved
5465	Typical well-designed randomized controlled trials set at 0.10 or 0.20. Related to is the statistical power (), the probability of declaring the two treatments different when the true difference is exactly .	What is a good sample size for a randomized control trial
2701	Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.	Why is Max pooling CNN
7877	So standard deviation gives you more deviation than mean deviation whem there are certain data points that are too far from its mean.	Which is larger average deviation or standard deviation
4050	14) A deep thinker doesn't care for small talk They'd rather talk about the universe and what the meaning of life is. The good thing about a deep thinker is that they'll only speak when they have something important to say so everyone around them knows to listen. This is why they don't see silence as awkward.	How do you know if you are a deep thinker
5550	The intercept of the regression line is just the predicted value for y, when x is 0. Any line has an equation, in terms of its slope and intercept: y = slope x x + intercept.	What does the intercept of the regression line represent
103	"""Recognize that any frequentist statistical test has a random chance of indicating significance when it is not really present. Running multiple tests on the same data set at the same stage of an analysis increases the chance of obtaining at least one invalid result."	What is the purpose of multiple testing in statistical inference
2771	Factorials (!) are products of every whole number from 1 to n. For example: If n is 3, then 3! is 3 x 2 x 1 = 6. If n is 5, then 5! is 5 x 4 x 3 x 2 x 1 = 120.	What does factorial mean in probability
3213	The chi-square distribution has the following properties: The mean of the distribution is equal to the number of degrees of freedom: μ = v. The variance is equal to two times the number of degrees of freedom: σ2 = 2 * v.	What are the parameters for the chi square distribution
130	Platt scaling works well for SVMs(Support Vector Machine) as well as other types of classification models, including boosted models and even naive Bayes classifiers, which produce distorted probability distributions.	Can Platt Scaling to calibrate probabilities be used for classifiers other than SVM
787	Augmented reality holds the promise of creating direct, automatic, and actionable links between the physical world and electronic information. It provides a simple and immediate user interface to an electronically enhanced physical world.	What is the scope of augmented reality
1550	Unsupervised or undirected data science uncovers hidden patterns in unlabeled data. In unsupervised data science, there are no output variables to predict. The objective of this class of data science techniques, is to find patterns in data based on the relationship between data points themselves.	What is unsupervised data
8042	KNN algorithm is one of the simplest classification algorithm. Even with such simplicity, it can give highly competitive results. KNN algorithm can also be used for regression problems.	Is Knn a classification algorithm
7556	0:559:25Suggested clip · 84 secondsHow To Calculate Pearson's Correlation Coefficient (r) by Hand YouTubeStart of suggested clipEnd of suggested clip	How is Pearson correlation calculated
5985	Matrix theory is a branch of mathematics which is focused on study of matrices. Initially, it was a sub-branch of linear algebra, but soon it grew to cover subjects related to graph theory, algebra, combinatorics and statistics as well.	What is the Matrix theory
411	“Kernel” is used due to set of mathematical functions used in Support Vector Machine provides the window to manipulate the data. So, Kernel Function generally transforms the training set of data so that a non-linear decision surface is able to transformed to a linear equation in a higher number of dimension spaces.	Why kernel is used in SVM
1357	68% of the data is within 1 standard deviation (σ) of the mean (μ), 95% of the data is within 2 standard deviations (σ) of the mean (μ), and 99.7% of the data is within 3 standard deviations (σ) of the mean (μ).	What is 2 standard deviations from the mean
7884	The difference between a ratio scale and an interval scale is that the zero point on an interval scale is some arbitrarily agreed value, whereas on a ratio scale it is a true zero.	What is the difference between a ratio scale of measurement and an interval scale of measurement quizlet
433	An expert system (ES) is a knowledge-based system that employs knowledge about its application domain and uses an inferencing (reason) procedure to solve problems that would otherwise require human competence or expertise.	How do Expert Systems work
8473	Positive feedback may be controlled by signals in the system being filtered, damped, or limited, or it can be cancelled or reduced by adding negative feedback. Positive feedback is used in digital electronics to force voltages away from intermediate voltages into '0' and '1' states.	What is positive feedback controlled by
1373	The correlation coefficient is a number that summarizes the direction and degree (closeness) of linear relations between two variables. The correlation coefficient is also known as the Pearson Product-Moment Correlation Coefficient. The sample value is called r, and the population value is called r (rho).	What is Pearson product moment correlation coefficient of sample
3852	Bivariate analysis investigates the relationship between two data sets, with a pair of observations taken from a single sample or individual. However, each sample is independent. You analyze the data using tools such as t-tests and chi-squared tests, to see if the two groups of data correlate with each other.	Is at test bivariate
8067	In statistics, a positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer.	What does it mean when data is positively skewed
3284	Variance (σ2) in statistics is a measurement of the spread between numbers in a data set. That is, it measures how far each number in the set is from the mean and therefore from every other number in the set.	What is the meaning of variance
342	C and Gamma are the parameters for a nonlinear support vector machine (SVM) with a Gaussian radial basis function kernel. A standard SVM seeks to find a margin that separates all positive and negative examples.  Gamma is the free parameter of the Gaussian radial basis function.	What is C and gamma SVM
5450	4:551:11:29Suggested clip · 112 secondsRodrigo Agundez: Building a live face recognition system | Pydata YouTubeStart of suggested clipEnd of suggested clip	How do you create a face recognition system
5177	The Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent.	What is the Fourier transform and why do we use it
657	An A/B test, also known as a split test, is an experiment for determining which of different variations of an online experience performs better by presenting each version to users at random and analyzing the results.  A/B testing can do a lot more than prove how changes can impact your conversions in the short-term.	Why do we do AB testing
194	1 Biasedness - The bias of on estimator is defined as: Bias( ˆθ) = E( ˆ θ ) - θ, where ˆ θ is an estimator of θ, an unknown population parameter. If E( ˆ θ ) = θ, then the estimator is unbiased.	How do you find the bias of an estimator
8592	If there are other predictor variables, all coefficients will be changed.  All the coefficients are jointly estimated, so every new variable changes all the other coefficients already in the model. This is one reason we do multiple regression, to estimate coefficient B1 net of the effect of variable Xm.	Why do coefficients change in multiple regression
1329	A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example.	What are latent variables in research
5268	Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.  As the predicted probability decreases, however, the log loss increases rapidly.	What is categorical cross entropy loss
322	Advertisements. An algorithm is designed to achieve optimum solution for a given problem. In greedy algorithm approach, decisions are made from the given solution domain. As being greedy, the closest solution that seems to provide an optimum solution is chosen.	What is greedy algorithm in data structure
1425	Natural language processing (NLP) is one of the most important technologies of the information age.  The course provides a deep excursion into cutting-edge research in deep learning applied to NLP. The final project will involve training a complex recurrent neural network and applying it to a large scale NLP problem.	What is deep NLP
1530	Important!The Coin Flipping Example.Steps of Bayesian Inference. Step 1: Identify the Observed Data. Step 2: Construct a Probabilistic Model to Represent the Data. Step 3: Specify Prior Distributions. Step 4: Collect Data and Application of Bayes' Rule.Conclusions.R Session.	How do you do a Bayesian analysis
6526	For example, if the distribution of raw scores if normally distributed, so is the distribution of z-scores. The mean of any SND always = 0. The standard deviation of any SND always = 1. Therefore, one standard deviation of the raw score (whatever raw value this is) converts into 1 z-score unit.	Why does az score have a mean of 0 and standard deviation of 1
953	Both skew and kurtosis can be analyzed through descriptive statistics. Acceptable values of skewness fall between − 3 and + 3, and kurtosis is appropriate from a range of − 10 to + 10 when utilizing SEM (Brown, 2006).	What are acceptable values for skewness and kurtosis
2611	According to Investopedia, a model is considered to be robust if its output dependent variable (label) is consistently accurate even if one or more of the input independent variables (features) or assumptions are drastically changed due to unforeseen circumstances.	How can you make sure that a model is robust
4915	Standard units are common units of measurement such as centimetres, grams and litres. Non-standard units of measurement might include cups, cubes or sweets.	What is standard and non standard unit
741	Feature detection is a low-level image processing operation. That is, it is usually performed as the first operation on an image, and examines every pixel to see if there is a feature present at that pixel.	What is feature detection in image processing
279	Continuous probability distribution: A probability distribution in which the random variable X can take on any value (is continuous). Because there are infinite values that X could assume, the probability of X taking on any one specific value is zero. Therefore we often speak in ranges of values (p(X>0) = . 50).	What are the continuous probability distributions
2370	Machine vision systems rely on digital sensors protected inside industrial cameras with specialized optics to acquire images, so that computer hardware and software can process, analyze, and measure various characteristics for decision making.	How does a vision system work
971	In implementing most of the machine learning algorithms, we represent each data point with a feature vector as the input. A vector is basically an array of numerics, or in physics, an object with magnitude and direction.	What is data representation in machine learning
3156	CNNs can be used in tons of applications from image and video recognition, image classification, and recommender systems to natural language processing and medical image analysis.  This is the way that a CNN works! Image by NatWhitePhotography on Pixabay. CNNs have an input layer, and output layer, and hidden layers.	Can CNN be used for classification
6559	The number of examples that belong to each class may be referred to as the class distribution. Imbalanced classification refers to a classification predictive modeling problem where the number of examples in the training dataset for each class label is not balanced.	What is class distribution in machine learning
6382	In statistics, a frequency distribution is a list, table or graph that displays the frequency of various outcomes in a sample. Each entry in the table contains the frequency or count of the occurrences of values within a particular group or interval.	What is frequency distribution in statistics with example
1133	7 Techniques to Handle Imbalanced DataUse the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	How do you deal with an imbalanced data set
8371	Logistic Regression in R: A Classification Technique to Predict Credit Card Default. Logistic regression is one of the statistical techniques in machine learning used to form prediction models.  In short, Logistic Regression is used when the dependent variable(target) is categorical.	What is the statistical method used in R to predict a classification variable
4126	The goal of observational research is to describe a variable or set of variables.  The data that are collected in observational research studies are often qualitative in nature but they may also be quantitative or both (mixed-methods).	Is an observational study quantitative
4578	0:434:29Suggested clip · 113 secondsPROBABILITY HISTOGRAM WITH EXCEL SIMPLE - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you make a probability histogram in Excel
1281	There are multiple ways to select a good starting point for the learning rate. A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. We might start with a large value like 0.1, then try exponentially lower values: 0.01, 0.001, etc.	How does neural network determine learning rate
1980	Six quick tips to improve your regression modelingA.1. Fit many models.  A.2. Do a little work to make your computations faster and more reliable.  A.3. Graphing the relevant and not the irrelevant.  A.4. Transformations.  A.5. Consider all coefficients as potentially varying.  A.6. Estimate causal inferences in a targeted way, not as a byproduct of a large regression.	How do you improve regression model
7993	Deviation means change or distance. But change is always followed by the word 'from'. Hence standard deviation is a measure of change or the distance from a measure of central tendency - which is normally the mean. Hence, standard deviation is different from a measure of central tendency.	Is standard deviation a central tendency
1130	Description. Probability & Statistics introduces students to the basic concepts and logic of statistical reasoning and gives the students introductory-level practical ability to choose, generate, and properly interpret appropriate descriptive and inferential methods.	What is a probability and statistics class
968	Statistical Validity is the extent to which the conclusions drawn from a statistical test are accurate and reliable. To achieve statistical validity, researchers must have an adequate sample size and pick the right statistical test to analyze the data.	What is statistical validity in research
3809	Cluster analysis is a multivariate method which aims to classify a sample of subjects (or ob- jects) on the basis of a set of measured variables into a number of different groups such that similar subjects are placed in the same group.  – Agglomerative methods, in which subjects start in their own separate cluster.	How does cluster analysis work
1456	Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data. Boosting is an iterative technique which adjusts the weight of an observation based on the last classification.	Whats the difference between boosting and bagging
4124	Blind Search - searching without information.  Heuristic Seach- searching with information. For example : A* Algorithm. We choose our next state based on cost and 'heuristic information' with heuristic function.	What is the difference between heuristic and non heuristic searching technique
1081	This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction.	How do you know when to use a one tailed or two tailed test
1470	We use factorials when we look at permutations and combinations. Permutations tell us how many different ways we can arrange things if their order matters. Combinations tells us how many ways we can choose k item from n items if their order does not matter.	When would you use a factorial
2086	Greedy is an algorithmic paradigm that builds up a solution piece by piece, always choosing the next piece that offers the most obvious and immediate benefit. So the problems where choosing locally optimal also leads to global solution are best fit for Greedy. For example consider the Fractional Knapsack Problem.	What is greedy approach of algorithm designing
1227	This is because a two-tailed test uses both the positive and negative tails of the distribution. In other words, it tests for the possibility of positive or negative differences. A one-tailed test is appropriate if you only want to determine if there is a difference between groups in a specific direction.	When to use a one tailed and two tailed test
792	This term is used in statistics in its ordinary sense, but most frequently occurs in connection with samples from different populations which may or may not be identical. If the populations are identical they are said to be homogeneous, and by extension, the sample data are also said to be homogeneous.	What does homogeneous mean in statistics
6743	Seq2seq is a family of machine learning approaches used for language processing. Applications include language translation, image captioning, conversational models and text summarization.	What is seq2seq in machine learning
2234	First consider the case when X and Y are both discrete. Then the marginal pdf's (or pmf's = probability mass functions, if you prefer this terminology for discrete random variables) are defined by fY(y) = P(Y = y) and fX(x) = P(X = x). The joint pdf is, similarly, fX,Y(x,y) = P(X = x and Y = y).	How do you find the marginal pdf of X and Y
1027	The confidence of an association rule is the support of (X U Y) divided by the support of X. Therefore, the confidence of the association rule is in this case the support of (2,5,3) divided by the support of (2,5).	How do you calculate confidence in association rule
43	So to summarize, the basic principles that guide the use of the AIC are:Lower indicates a more parsimonious model, relative to a model fit.  It is a relative measure of model parsimony, so it only has.  We can compare non-nested models.  The comparisons are only valid for models that are fit to the same response.More items•	How do you interpret Akaike information criterion
104	"The binomial distribution model allows us to compute the probability of observing a specified number of ""successes"" when the process is repeated a specific number of times (e.g., in a set of patients) and the outcome for a given patient is either a success or a failure."	What are binomial distributions used for
7979	The central limit theorem states that the CDF of Zn converges to the standard normal CDF. converges in distribution to the standard normal random variable as n goes to infinity, that is limn→∞P(Zn≤x)=Φ(x), for all x∈R,  The Xi's can be discrete, continuous, or mixed random variables.	Does the central limit theorem apply to discrete random variables
203	As we saw above, KNN algorithm can be used for both classification and regression problems. The KNN algorithm uses 'feature similarity' to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set.	How do you predict using Knn
645	Convolutional neural networks (CNNs, or ConvNets) are essential tools for deep learning, and are especially suited for analyzing image data. For example, you can use CNNs to classify images. To predict continuous data, such as angles and distances, you can include a regression layer at the end of the network.	Can we use CNN for regression
2253	A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.	What does a convolutional neural network do
7297	The 2-sample t-test takes your sample data from two groups and boils it down to the t-value. The process is very similar to the 1-sample t-test, and you can still use the analogy of the signal-to-noise ratio. Unlike the paired t-test, the 2-sample t-test requires independent groups for each sample.	What is the difference between one sample and two sample t test
1341	Input means to provide the program with some data to be used in the program and Output means to display data on screen or write the data to a printer or a file. C programming language provides many built-in functions to read any given input and to display data on screen when there is a need to output the result.	What is input and output function
6859	Simple Random Sampling	What kind of sampling avoids bias
5163	Moment generating functions are a way to find moments like the mean(μ) and the variance(σ2). They are an alternative way to represent a probability distribution with a simple one-variable function.	What is the purpose of a moment generating function
7987	Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.  Explicit regression gradient boosting algorithms were subsequently developed by Jerome H.	What is gradient boosting machine learning
7371	Decision trees provide an effective method of Decision Making because they: Clearly lay out the problem so that all options can be challenged. Allow us to analyze fully the possible consequences of a decision. Provide a framework to quantify the values of outcomes and the probabilities of achieving them.	How is the decision tree useful
1153	A representative sample is a subset of a population that seeks to accurately reflect the characteristics of the larger group. For example, a classroom of 30 students with 15 males and 15 females could generate a representative sample that might include six students: three males and three females.	What is considered a representative sample
7533	A t score is one form of a standardized test statistic (the other you'll come across in elementary statistics is the z-score). The t score formula enables you to take an individual score and transform it into a standardized form>one which helps you to compare scores.	What is the T score in statistics
4687	Recall is the number of relevant documents retrieved by a search divided by the total number of existing relevant documents, while precision is the number of relevant documents retrieved by a search divided by the total number of documents retrieved by that search.	What is the difference between precision and recall
5848	Let X be a discrete random variable with a geometric distribution with parameter p for some 0<p≤1. Then the moment generating function MX of X is given by: MX(t)=p1−(1−p)et.	How do you find the moment generating function for a geometric distribution
599	Authors sometimes calculate the difference between the highest and the lowest range value and report it as one estimate of the spread, most commonly for interquartile range (4). For example, instead reporting values of 34 (30–39) for median and interquartile range, one can report 34 (9).	How do you report interquartile range
435	(Example: a test with 90% specificity will correctly return a negative result for 90% of people who don't have the disease, but will return a positive result — a false-positive — for 10% of the people who don't have the disease and should have tested negative.)	What is a good false positive rate
4384	Collective intelligence (CI) is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals and appears in consensus decision making.	What does collective intelligence mean
6496	Machine Learning on Code (MLonCode) is a new interdisciplinary field of research related to Natural Language Processing, Programming Language Structure, and Social and History analysis such contributions graphs and commit time series.	What is machine learning code
6565	Stacking, also known as stacked generalization, is an ensemble method where the models are combined using another machine learning algorithm. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models.	Which algorithm works by ensemble method
1141	A good property of conditional entropy is that if we know H(Y|X)=0, then Y=f(X) for a function f. To see another interest behind the conditional entropy, suppose that Y is an estimation of X and we are interested in probability of error Pe. If for Y=y, we can estimate X without error then H(Y|Y=y)=0.	What is good way to understand conditional entropy H y x
4882	It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.  The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better.	Why rectified linear unit is a good activation function
8384	Neural Networks are essentially a part of Deep Learning, which in turn is a subset of Machine Learning. So, Neural Networks are nothing but a highly advanced application of Machine Learning that is now finding applications in many fields of interest.	Are neural networks part of machine learning
6675	The Kruskal Wallis H test uses ranks instead of actual data.  It is sometimes called the one-way ANOVA on ranks, as the ranks of the data values are used in the test rather than the actual data points. The test determines whether the medians of two or more groups are different.	How does the Kruskal Wallis test work
2853	The regression slope intercept formula, b0 = y – b1 * x is really just an algebraic variation of the regression equation, y' = b0 + b1x where “b0” is the y-intercept and b1x is the slope. Once you've found the linear regression equation, all that's required is a little algebra to find the y-intercept (or the slope).	How do you find the Y intercept of a least squares regression line
2864	Tokens are the smallest elements of a program, which are meaningful to the compiler. The following are the types of tokens: Keywords, Identifiers, Constant, Strings, Operators, etc.	What is token and its types
5061	The first postulate of statistical mechanics � This postulate is often called the principle of equal a priori probabilities. It says that if the microstates have the same energy, volume, and number of particles, then they occur with equal frequency in the ensemble.	What is equal a priori probability
3633	7 Top Linear Algebra Resources For Machine Learning BeginnersEssence Of Linear Algebra By 3Blue1Brown.Linear Algebra By Khan Academy.Basic Linear Algebra for Deep Learning By Niklas Donges.Computational Linear Algebra for Coders By fast.ai.Deep Learning Book By Ian Goodfellow and Yoshua Bengio and Aaron Courville.Linear Algebra for Machine Learning By AppliedAICourse.More items•	How do I learn linear algebra for machine learning
398	Restricted Boltzmann Machines are shallow, two-layer neural nets that constitute the building blocks of deep-belief networks. The first layer of the RBM is called the visible, or input layer, and the second is the hidden layer. Each circle represents a neuron-like unit called a node.	What are the two layers of a restricted Boltzmann machine called in deep learning
4256	Hierarchical Task AnalysisDEFINE TASK BEING ANALYZED, as well as the purpose of the task analysis.CONDUCT DATA COLLECTION.  DETERMINE THE OVERALL GOAL OF THE TASK.  DETERMINE TASK SUB-GOALS.  PERFORM SUB-GOAL DECOMPOSITION.  DEVELOP PLANS ANALYSIS.	How do you do a hierarchical task analysis
5102	Deep Learning is a part of Machine Learning which is applied to larger data-sets and based on ANN (Artificial Neural Networks). The main technology used in NLP (Natural Language Processing) which mainly focuses on teaching natural/human language to computers.  NLP is a part of AI which overlaps with ML & DL.	Does NLP come under deep learning
518	The sign test is a statistical method to test for consistent differences between pairs of observations, such as the weight of subjects before and after treatment.  The sign test can also test if the median of a collection of numbers is significantly greater than or less than a specified value.	What is a sign test in statistics
7682	The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1. The standard normal distribution is centered at zero and the degree to which a given measurement deviates from the mean is given by the standard deviation.	What is the mean of a standard normal curve
8513	Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples.	What is meant by supervised machine learning
5476	Factor analysis is a technique that is used to reduce a large number of variables into fewer numbers of factors. This technique extracts maximum common variance from all variables and puts them into a common score. As an index of all variables, we can use this score for further analysis.	How do you explain factor analysis
2223	Another way to describe the imbalance of classes in a dataset is to summarize the class distribution as percentages of the training dataset. For example, an imbalanced multiclass classification problem may have 80 percent examples in the first class, 18 percent in the second class, and 2 percent in a third class.	How do you find a dataset imbalance
3886	Definition of the loss The goal of the triplet loss is to make sure that: Two examples with the same label have their embeddings close together in the embedding space. Two examples with different labels have their embeddings far away.	What is the importance of the triplet loss in Machine Learning
2294	At equilibrium, the change in entropy is zero, i.e., ΔS=0 (at equilibrium).	What is the value of entropy at equilibrium
8099	Max Pooling is a convolution process where the Kernel extracts the maximum value of the area it convolves. Max Pooling simply says to the Convolutional Neural Network that we will carry forward only that information, if that is the largest information available amplitude wise.	What is Max pooling in neural networks
3416	If two events have no elements in common (Their intersection is the empty set.), the events are called mutually exclusive. Thus, P(A∩B)=0 . This means that the probability of event A and event B happening is zero. They cannot both happen.	When two events are mutually exclusive Why is P A and B )= 0
4429	The normal approximation gives us a very poor result without the continuity correction. We make a continuity correction when p is > 0.5.	When we use a normal distribution to approximate a binomial distribution Why do we make a continuity correction
1069	Python is easy to learn and work with, and provides convenient ways to express how high-level abstractions can be coupled together. Nodes and tensors in TensorFlow are Python objects, and TensorFlow applications are themselves Python applications. The actual math operations, however, are not performed in Python.	What algorithm does TensorFlow use
275	and the definition of unbiased estimator corresponds to the fact that the above integral should be equal to the parameter θ of the underlying distribution.  The sample mean and variance are consistent and unbiased esti- mators of the mean and variance of the underlying distribution.	Is the sample variance a consistent estimator
4590	A goodness-of-fit test, in general, refers to measuring how well do the observed data correspond to the fitted (assumed) model.  Like in a linear regression, in essence, the goodness-of-fit test compares the observed values to the expected (fitted or predicted) values.	What is goodness of fit in linear regression
124	1 Answer. For binary classification, it should give the same results, because softmax is a generalization of sigmoid for a larger number of classes.	Can we use Softmax for binary classification
2466	A Likert Scale is a type of rating scale used to measure attitudes or opinions. With this scale, respondents are asked to rate items on a level of agreement. For example: Strongly agree. Agree.	What is Likert scale analysis
830	Abnormal BRCA1 and BRCA2 genes are found in 5% to 10% of all breast cancer cases in the United States. A study found that women with an abnormal BRCA1 gene had a worse prognosis than women with an abnormal BRCA2 gene 5 years after diagnosis.	Which is worse brca1 or brca2
3066	Stochastic Gradient Descent (SGD): Hence, in Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration.  This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration.	What is correct about stochastic gradient descent
6771	Definition. A score that is derived from an individual's raw score within a distribution of scores. The standard score describes the difference of the raw score from a sample mean, expressed in standard deviations. Standard scores preserve the absolute differences between scores.	What does a standard score measure
624	Any study that attempts to predict human behavior will tend to have R-squared values less than 50%. However, if you analyze a physical process and have very good measurements, you might expect R-squared values over 90%. There is no one-size fits all best answer for how high R-squared should be.	What is a good adjusted R squared
7988	Negative coefficients indicate that the event is less likely at that level of the predictor than at the reference level. The coefficient is the estimated change in the natural log of the odds when you change from the reference level to the level of the coefficient.	How do you interpret negative coefficients in logistic regression
1139	Example 1: Fair Dice Roll The number of desired outcomes is 3 (rolling a 2, 4, or 6), and there are 6 outcomes in total. The a priori probability for this example is calculated as follows: A priori probability = 3 / 6 = 50%. Therefore, the a priori probability of rolling a 2, 4, or 6 is 50%.	How do you calculate a priori probability
3125	List of Common Machine Learning AlgorithmsLinear Regression.Logistic Regression.Decision Tree.SVM.Naive Bayes.kNN.K-Means.Random Forest.More items•	What are the different machine learning models
1190	This issue calls for the need of {Large-scale Machine Learning} (LML), which aims to learn patterns from big data with comparable performance efficiently.	What is Large Scale Machine Learning
8209	Statistical analysts test a hypothesis by measuring and examining a random sample of the population being analyzed. All analysts use a random population sample to test two different hypotheses: the null hypothesis and the alternative hypothesis.	How are statistical tests used in hypothesis testing
432	geometrical product specifications	What does GD&T mean
1265	Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.	What is binary cross entropy loss
7665	The t-distribution describes the standardized distances of sample means to the population mean when the population standard deviation is not known, and the observations come from a normally distributed population.	What is the mean of the T distribution
6857	D refers to the number of differencing transformations required by the time series to get stationary.  Differencing is a method of transforming a non-stationary time series into a stationary one. This is an important step in preparing data to be used in an ARIMA model.	What is D in Arima model
7168	Simple Linear RegressionLinearity: The relationship between X and the mean of Y is linear.Homoscedasticity: The variance of residual is the same for any value of X.Independence: Observations are independent of each other.Normality: For any fixed value of X, Y is normally distributed.	What are the three conditions for linear regression models
490	"The ""Linear-by-Linear"" test is for ordinal (ordered) categories and assumes equal and ordered intervals. The Linear-by-Linear Association test is a test for trends in a larger-than-2x2 table. Its value is shown to be significant and indicates that income tends to rise with values of ""male"" (i.e., from 0 to 1)."	What is linear by linear association chi square test
6650	Not all machine learning algorithms make the iid assumption (for example, decision tree based approaches do not).  So, common learning algorithms can be used to learn time series data.	Does the i i d assumption in machine learning imply that we can not use common machine learning techniques to analyse time series data
2173	In statistics, the likelihood function (often simply called the likelihood) measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters.	What does the likelihood function mean
3806	A Bayesian Neural Network (BNN) can then be defined as any stochastic artificial neural network. trained using Bayesian inference [54]. To design a BNN, the first step is the choice of a deep neural. network architecture, i.e., of a functional model.	What is a Bayesian neural network
2407	Student's t-distribution and Snedecor-Fisher's F- distribution. These are two distributions used in statistical tests. The first one is commonly used to estimate the mean µ of a normal distribution when the variance σ2 is not known, a common situation.	What is t distribution and F
1188	A sampling distribution is the theoretical distribution of a sample statistic that would be obtained from a large number of random samples of equal size from a population. Consequently, the sampling distribution serves as a statistical “bridge” between a known sample and the unknown population.	What is the relation between population sampling and sampling distributions
8326	"The idea behind importance sampling is that certain values of the input random variables in a simulation have more impact on the parameter being estimated than others. If these ""important"" values are emphasized by sampling more frequently, then the estimator variance can be reduced."	Why do we do importance sampling
701	If the sample being tested falls into either of the critical areas, the alternative hypothesis is accepted instead of the null hypothesis. The two-tailed test gets its name from testing the area under both tails of a normal distribution, although the test can be used in other non-normal distributions.	What is the alternative hypothesis for a two tailed test
3985	The gradients are the partial derivatives of the loss with respect to each of the six variables. TensorFlow presents the gradient and the variable of which it is the gradient, as members of a tuple inside a list. We display the shapes of each of the gradients and variables to check that is actually the case.	What is gradient TensorFlow
2965	Classification model: A classification model tries to draw some conclusion from the input values given for training. It will predict the class labels/categories for the new data. Feature: A feature is an individual measurable property of a phenomenon being observed.	What are the classification of model
5413	TensorFlow is more of a low-level library.  Scikit-Learn is a higher-level library that includes implementations of several machine learning algorithms, so you can define a model object in a single line or a few lines of code, then use it to fit a set of points or predict a value.	What is the difference between TensorFlow and Scikit learn
5170	The standard deviation of the sample mean ˉX that we have just computed is the standard deviation of the population divided by the square root of the sample size: √10=√20/√2.	What is the formula for the standard deviation of the sampling distribution of the sample mean X
3893	If the sequence of estimates can be mathematically shown to converge in probability to the true value θ0, it is called a consistent estimator; otherwise the estimator is said to be inconsistent.	How do you prove an estimator is consistent
3512	In null hypothesis testing, this criterion is called α (alpha) and is almost always set to . 05. If there is less than a 5% chance of a result as extreme as the sample result if the null hypothesis were true, then the null hypothesis is rejected. When this happens, the result is said to be statistically significant .	What is the implication if the null hypothesis is rejected
333	Skip connections are extra connections between nodes in different layers of a neural network that skip one or more layers of nonlinear processing.	What are Skip connections
130	“A priori” and “a posteriori” refer primarily to how, or on what basis, a proposition might be known. In general terms, a proposition is knowable a priori if it is knowable independently of experience, while a proposition knowable a posteriori is knowable on the basis of experience.	What is the difference between a posteriori and a priori
7622	"Algorithm. As of 2016, AlphaGo's algorithm uses a combination of machine learning and tree search techniques, combined with extensive training, both from human and computer play. It uses Monte Carlo tree search, guided by a ""value network"" and a ""policy network,"" both implemented using deep neural network technology."	What method does AlphaGo use
6026	Definition: The Population Distribution is a form of probability distribution that measures the frequency with which the items or variables that make up the population are drawn or expected to be drawn for a given research study.	What is a population distribution in statistics
6812	Time series analysis can be useful to see how a given asset, security, or economic variable changes over time. It can also be used to examine how the changes associated with the chosen data point compare to shifts in other variables over the same time period.	What are the uses of time series analysis
4726	The law of large numbers, in probability and statistics, states that as a sample size grows, its mean gets closer to the average of the whole population. In the 16th century, mathematician Gerolama Cardano recognized the Law of Large Numbers but never proved it.	What does the law of large numbers mean
1062	Discriminant analysis is statistical technique used to classify observations into non-overlapping groups, based on scores on one or more quantitative predictor variables. For example, a doctor could perform a discriminant analysis to identify patients at high or low risk for stroke.	Why do we use the discriminant
3008	An easy way to define the difference between frequency and relative frequency is that frequency relies on the actual values of each class in a statistical data set while relative frequency compares these individual values to the overall totals of all classes concerned in a data set.	Why do we use relative frequency instead of frequency
860	Perhaps the most famous case ever of misleading statistics in the news is the case of Sally Clark, who was convicted of murdering her children. She was freed after it was found the statistics used in her murder trial were completely wrong.	What is an example of using statistics to mislead
1201	Spark is capable of handling large-scale batch and streaming data to figure out when to cache data in memory and processing them up to 100 times faster than Hadoop-based MapReduce.  First, you will learn how to install Spark with all new features from the latest Spark 2.0 release.	Why is Spark efficient for large scale machine learning
210	66.5%	What is the probability of getting at least one six when rolling six fair dice
6775	Quartile deviation is the difference between “first and third quartiles” in any distribution. Standard deviation measures the “dispersion of the data set” that is relative to its mean.	What is the difference between standard deviation and quartile deviation
65	Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.	What is meant by reinforcement learning
564	A precision-recall point is a point with a pair of x and y values in the precision-recall space where x is recall and y is precision. A precision-recall curve is created by connecting all precision-recall points of a classifier. Two adjacent precision-recall points can be connected by a straight line.	How do you construct a precision recall curve
2551	14:3826:41Suggested clip · 115 secondsCanonical correlation using SPSS - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret canonical correlation in SPSS
5628	Summary: Population variance refers to the value of variance that is calculated from population data, and sample variance is the variance calculated from sample data.  As a result both variance and standard deviation derived from sample data are more than those found out from population data.	What is the difference between population variance and sample variance
934	The potential solutions include the following: Remove some of the highly correlated independent variables. Linearly combine the independent variables, such as adding them together. Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression.	How can multicollinearity be solved
7602	If slope = 0, as you increase one variable, the other variable doesn't change at all. This means no relationship.	What is the correlation coefficient for a slope of 0
4835	For example, if we want to measure current obesity levels in a population, we could draw a sample of 1,000 people randomly from that population (also known as a cross section of that population), measure their weight and height, and calculate what percentage of that sample is categorized as obese.	What is cross sectional data examples
5220	Linear algebra is used in almost all compute-intensive tasks. It can efficiently be used to solve any linear or non-linear set of equations.	What is linear algebra used for in real life
427	Each of the steps should take about 4–6 weeks' time. And in about 26 weeks since the time you started, and if you followed all of the above religiously, you will have a solid foundation in deep learning.	How long will it take to learn deep learning
687	The Lorenz Curve is a graph that illustrates the distribution of income in the economy. It suggests that the distribution of income in the United States is unequal.	What is the Lorenz curve and what does it suggest
2629	"The difference is pretty simple: in squared error, you are penalizing large deviations more.  The mean absolute error is a common measure of forecast error in time [2]series analysis, where the terms ""mean absolute deviation"" is sometimes used in confusion with the more standard definition of mean absolute deviation."	What is the difference between mean absolute deviation and mean squared error
94	The cumulative distribution function of the standard normal distribution is, up to constant factors, the error function, erf ( x ) ≡ 2 π ∫ 0 x exp ( − y 2 ) d y , (Exercise 7).	What is the cumulative distribution function of the standard normal distribution
2015	Introduction. Categorical Data is the data that generally takes a limited number of possible values. Also, the data in the category need not be numerical, it can be textual in nature. All machine learning models are some kind of mathematical model that need numbers to work with.	What are categorical features in machine learning
8470	Serial 7s (ie, serial subtraction of 7 from 100 to 65) has been proposed as a measure of attention and concentration. Spelling the word WORLD backwards is commonly used as a substitute for patients who cannot perform the serial 7s. Digit span is also used to measure attention and concentration.	How do you assess attention and concentration
1193	In Convolutional Neural Networks, Filters detect spatial patterns such as edges in an image by detecting the changes in intensity values of the image.  High pass filters are used to enhance the high-frequency parts of an image.	What is filters in convolutional neural networks
293	Performance Testing is a software testing process used for testing the speed, response time, stability, reliability, scalability and resource usage of a software application under particular workload.  It is a subset of performance engineering and also known as “Perf Testing”.	What is performance testing and its types
6815	Correlation in the error terms suggests that there is additional information in the data that has not been exploited in the current model. When the observations have a natural sequential order, the correlation is referred to as autocorrelation. Autocorrelation may occur for several reasons.	What happens if the error terms are correlated
447	To reduce variability we perform multiple rounds of cross-validation with different subsets from the same data. We combine the validation results from these multiple rounds to come up with an estimate of the model's predictive performance. Cross-validation will give us a more accurate estimate of a model's performance.	What does cross validation reduce
181	Linear regression is the next step up after correlation. It is used when we want to predict the value of a variable based on the value of another variable. The variable we want to predict is called the dependent variable (or sometimes, the outcome variable).	Why would you use linear regression
3429	Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model.	What are boosting techniques
712	"In statistics, self-selection bias arises in any situation in which individuals select themselves into a group, causing a biased sample with nonprobability sampling.  In such fields, a poll suffering from such bias is termed a self-selected listener opinion poll or ""SLOP""."	What does self selection bias mean
7160	The general linear model requires that the response variable follows the normal distribution whilst the generalized linear model is an extension of the general linear model that allows the specification of models whose response variable follows different distributions.	What is the difference between general linear model and generalized linear model
4214	Dense CNN is a type of Deep CNN in which each layer is connected with another layer deeper than itself.	What is dense CNN
5870	Covariances have significant applications in finance and modern portfolio theory. For example, in the capital asset pricing model (CAPM), which is used to calculate the expected return of an asset, the covariance between a security and the market is used in the formula for one of the model's key variables, beta.	What are the applications of covariance
2845	The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map.	What is meant by convolution in CNN
412	Unlike the independent-samples t-test, the Mann-Whitney U test allows you to draw different conclusions about your data depending on the assumptions you make about your data's distribution.  These different conclusions hinge on the shape of the distributions of your data, which we explain more about later.	What is the difference between t test and Mann Whitney test
1005	AlphaGo surprised the world with its so-called “move 37,” which human experts initially thought was a mistake, but which proved decisive in game two. Lee made his own impact with his “hand of God” play (move 78), which flummoxed the AI program and allowed Lee to win a single game.	How did Lee sedol beat AlphaGo
754	Multinomial logistic regression is a form of logistic regression used to predict a target variable have more than 2 classes.  Now, there are two common methods to perform multi-class classification using the binary classification logistic regression algorithm: one-vs-all and one-vs-one.	Can you apply logistic regression for more than two classes
6491	Pattern recognition is the process of classifying input data into objects or classes based on key features.  Pattern recognition has applications in computer vision, radar processing, speech recognition, and text classification.	What is pattern recognition in image processing
952	SummaryUse the function cor. test(x,y) to analyze the correlation coefficient between two variables and to get significance level of the correlation.Three possible correlation methods using the function cor.test(x,y): pearson, kendall, spearman.	How do you show the relationship between two variables in R
1705	AI assistants, like Alexa and Siri, are examples of intelligent agents as they use sensors to perceive a request made by the user and the automatically collect data from the internet without the user's help. They can be used to gather information about its perceived environment such as weather and time.	What are the intelligent agents of AI
893	Top 10 Machine Learning ApplicationsTraffic Alerts.Social Media.Transportation and Commuting.Products Recommendations.Virtual Personal Assistants.Self Driving Cars.Dynamic Pricing.Google Translate.More items•	What are the applications of machine learning
5857	Skewness refers to distortion or asymmetry in a symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed. Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution.	What does it mean when data is skewed
4486	Disproportional vs. The main difference between the two sampling techniques is the proportion given to each stratum with respect to other strata. In proportional sampling, each stratum has the same sampling fraction while in disproportional sampling technique; the sampling fraction of each stratum varies.	What is proportionate and disproportionate sampling
1823	The above equation tells us that the value of a particular state is determined by the immediate reward plus the value of successor states when we are following a certain policy(π).	What does π represent in Bellman equation
275	The cumulative distribution function (CDF) of random variable X is defined as FX(x)=P(X≤x), for all x∈R.SolutionTo find the CDF, note that.  To find P(2<X≤5), we can write P(2<X≤5)=FX(5)−FX(2)=3132−34=732.  To find P(X>4), we can write P(X>4)=1−P(X≤4)=1−FX(4)=1−1516=116.	How do you find the CDF of a random variable
4504	In pattern recognition, information retrieval and classification (machine learning), precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of the total amount of relevant instances that were	What is the difference between a true positive rate and recall
6327	The negative binomial distribution is a probability distribution that is used with discrete random variables. This type of distribution concerns the number of trials that must occur in order to have a predetermined number of successes.	What is a negative binomial distribution used for
6107	Boosting is used to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analysing data for errors.  This process converts weak learners into better performing model.	Why do we use boosting
982	A Markov logic network is a first-order knowledge base with a weight attached to each formula, and can be viewed as a template for constructing Markov networks.  Experiments with a real-world database and knowledge base illustrate the benefits of using MLNs over purely logical and purely probabilistic ap- proaches.	What do you think about Markov Logic Networks
210	If a variable can take on any value between its minimum value and its maximum value, it is called a continuous variable; otherwise, it is called a discrete variable. The number of heads could be any integer value between 0 and plus infinity.	What is a continuous response variable
6563	In particular, three datasets are commonly used in different stages of the creation of the model. The model is initially fit on a training dataset, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model.	What is training data in neural network
2697	Quota sampling is different from stratified sampling, because in a stratified sample individuals within each stratum are selected at random. Quota sampling achieves a representative age distribution, but it isn't a random sample, because the sampling frame is unknown.	Is quota sampling random
1449	Six Fundamental Methods to Generate a Random VariablePhysical sources.Empirical resampling.Pseudo random generators.Simulation/Game-play.Rejection Sampling.Transform methods.	How do you create a random variable
3335	If a and b are two non-zero numbers, then the harmonic mean of a and b is a number H such that the numbers a, H, b are in H.P. We have H = 1/H = 1/2 (1/a + 1/b) ⇒ H = 2ab/a+b.	What is the harmonic mean of A and B
2151	Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.	What does gradient mean in Machine Learning
3903	Kalman filters combine two sources of information, the predicted states and noisy measurements, to produce optimal, unbiased estimates of system states. The filter is optimal in the sense that it minimizes the variance in the estimated states.	Is Kalman filter optimal
131	In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines.	What is meant by Hyperplane
5150	Reinforcement Learning : Simple reward feedback is required for the agent to learn its behavior; this is known as the reinforcement signal.  In the problem, an agent is supposed to decide the best action to select based on his current state. When this step is repeated, the problem is known as a Markov Decision Process.	What is Markov decision process in reinforcement learning
6814	It is well known that maximum likelihood estimators are often biased, and it is of use to estimate the expected bias so that we can reduce the mean square errors of our parameter estimates.  In both problems, the first-order bias is found to be linear in the parameter and the sample size.	Is maximum likelihood estimator biased
5976	In statistics, an estimator is a rule for calculating an estimate of a given quantity based on observed data: thus the rule (the estimator), the quantity of interest (the estimand) and its result (the estimate) are distinguished. There are point and interval estimators.	What do you mean by estimator
2346	Introduction to K-means Clustering. K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K.	What is the purpose of K means clustering
2375	Interpret the key results for CovarianceIf both variables tend to increase or decrease together, the coefficient is positive.If one variable tends to increase as the other decreases, the coefficient is negative.	How do you interpret a covariance matrix
6275	An activation function is a node that you add to the output layer or between two layers of any neural network. It is also known as the transfer function. It is used to determine the output of neural network layer in between 0 to 1 or -1 to 1 etc.	How do you choose activation function in deep learning
5923	The accuracy of location determination is improved because WiFi radio signals are one of the best ways to determine where you are.  This technique works more reliably than GPS in urban environments, indoors, and other places where GPS signals get distorted by radio interference.	Why is location accuracy improved when wi fi is enabled
6985	Robust regression is an alternative to least squares regression when data is contaminated with outliers or influential observations and it can also be used for the purpose of detecting influential observations. Please note: The purpose of this page is to show how to use various data analysis commands.	When should I use robust regression
782	The unmixed second-order partial derivatives, fxx f x x and fyy, f y y , tell us about the concavity of the traces. The mixed second-order partial derivatives, fxy f x y and fyx, f y x , tell us how the graph of f twists.	What does the second partial derivative tell us
7641	Definition. A Turing Machine (TM) is a mathematical model which consists of an infinite length tape divided into cells on which input is given.  After reading an input symbol, it is replaced with another symbol, its internal state is changed, and it moves from one cell to the right or left.	What is Turing machine in automata
3868	The discriminator in a GAN is simply a classifier. It tries to distinguish real data from the data created by the generator. It could use any network architecture appropriate to the type of data it's classifying. Figure 1: Backpropagation in discriminator training.	What is discriminator in Gan
6269	A studentized residual is calculated by dividing the residual by an estimate of its standard deviation. The standard deviation for each residual is computed with the observation excluded. For this reason, studentized residuals are sometimes referred to as externally studentized residuals.	How do you find the Studentized residual
2761	In data mining, association rules are useful for analyzing and predicting customer behavior. They play an important part in customer analytics, market basket analysis, product clustering, catalog design and store layout. Programmers use association rules to build programs capable of machine learning.	What is the importance of association rule mining
1002	If f (x, y) > T then f (x, y) = 0 else f (x, y) = 255 where f (x, y) = Coordinate Pixel Value T = Threshold Value. In OpenCV with Python, the function cv2. threshold is used for thresholding.	How do you find the threshold value of an image in OpenCV
3315	1 Answer. No! There is no limit whatsoever on the size of the output relative to the size of the input. But in most cases, a higher number of outputs is not necessary at all.	Can the output layer size be bigger than input layer size in a neural network
3501	General method. Optimal control deals with the problem of finding a control law for a given system such that a certain optimality criterion is achieved.  An optimal control is a set of differential equations describing the paths of the control variables that minimize the cost function.	What is optimal control system
8242	In probability theory and statistics, Bayes's theorem (alternatively Bayes's law or Bayes's rule), named after Reverend Thomas Bayes, describes the probability of an event, based on prior knowledge of conditions that might be related to the event.  Bayesian inference is fundamental to Bayesian statistics.	What is Bayes theorem statistics
482	The decision boundary Let's suppose we define a line that is equal to zero along this decision boundary.  For example, in the following graph, z=6−x1 represents a decision boundary for which any values of x1>6 will return a negative value for z and any values of x1<6 will return a positive value for z.	What is decision boundary in logistic regression
6544	Definition: Probability sampling is defined as a sampling technique in which the researcher chooses samples from a larger population using a method based on the theory of probability. For a participant to be considered as a probability sample, he/she must be selected using a random selection. Select your respondents.	What is probability sampling or unbiased sampling
1977	Computational photography is a digital image processing technique that uses algorithms to replace optical processes, and it seeks to improve image quality by using machine vision to identify the content of an image.  “For example, we use AI to train algorithms about the features of people's faces.”	What is AI in photography
7266	A stratified sample is one that ensures that subgroups (strata) of a given population are each adequately represented within the whole sample population of a research study. For example, one might divide a sample of adults into subgroups by age, like 18–29, 30–39, 40–49, 50–59, and 60 and above.	What is an example of a stratified sample
8562	Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.	What type of data would you use with logistic regression
2068	Linear regressions are among the simplest types of predictive models.  Other more complex predictive models include decision trees, k-means clustering and Bayesian inference, to name just a few potential methods. The most complex area of predictive modeling is the neural network.	What are the different predictive models
788	Similar to Correlation Coefficient, the range of values of MCC lie between -1 to +1. A model with a score of +1 is a perfect model and -1 is a poor model.	What is a good MCC score
1493	Gaussian RBF(Radial Basis Function) is another popular Kernel method used in SVM models for more. RBF kernel is a function whose value depends on the distance from the origin or from some point. Gaussian Kernel is of the following format; ||X1 — X2 || = Euclidean distance between X1 & X2.	What is gaussian kernel in SVM
5066	A pooling layer is another building block of a CNN. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling.	Why Max pooling is used in CNN
534	Understanding the Correlation Coefficient A value of exactly 1.0 means there is a perfect positive relationship between the two variables. For a positive increase in one variable, there is also a positive increase in the second variable.	What does a correlation coefficient of 1.0 mean
5699	the expected rate of occurrences	What does λ mean in statistics
5829	At a higher level, the chief difference between the L1 and the L2 terms is that the L2 term is proportional to the square of the β values, while the L1 norm is proportional the absolute value of the values in β.	What is the difference between l1 and l2 norm
3325	How to Handle Imbalanced DatasetChange the evaluation matrix. If we apply the wrong evaluation matrix on the imbalanced dataset, it can give us misleading results.  Resample the dataset. Resample means to change the distribution of the imbalance classes in the dataset.  Change the algorithm and approach to the problem.	How do you handle an imbalanced data set
5247	POS tags make it possible for automatic text processing tools to take into account which part of speech each word is. This facilitates the use of linguistic criteria in addition to statistics.	Why is POS tagging useful
6189	Perceptron Learning Rule The Perceptron receives multiple input signals, and if the sum of the input signals exceeds a certain threshold, it either outputs a signal or does not return an output. In the context of supervised learning and classification, this can then be used to predict the class of a sample.	How does a perceptron algorithm work
6560	In other words, as long as each sample contains a very large number of observations, the sampling distribution of the mean must be normal. So if we're going to assume one thing for all situations, it has to be a normal, because the normal is always correct for large samples.	Is a normal distribution an appropriate assumption for the sampling distribution of the mean
3470	Statistics Definitions > A random walk is a sequence of discrete, fixed-length steps in random directions. Random walks may be 1-dimensional, 2-dimensional, or n-dimensional for any n. A random walk can also be confined to a lattice.	What is a random walk in statistics
4202	Five tips to prevent confirmation bias Encourage and carefully consider critical views on the working hypothesis. Ensure that all stakeholders examine the primary data. Do not rely on analysis and summary from a single individual. Design experiments to actually test the hypothesis.	How do you avoid confirmation bias in research
787	3:295:25Suggested clip · 42 secondsExcel Statistics 55.5: Bayes Theorem Posterior Probabilities - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the probability of posterior in Excel
5700	joint entropy is the amount of information in two (or more) random variables; conditional entropy is the amount of information in one random variable given we already know the other.	What is joint and conditional entropy
2508	Recurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed as input to the current step.  Thus RNN came into existence, which solved this issue with the help of a Hidden Layer.	What is hidden layer in RNN
629	Backward elimination is a feature selection technique while building a machine learning model. It is used to remove those features that do not have a significant effect on the dependent variable or prediction of output.	What is backward elimination in machine learning
838	The cumulative distribution function (CDF) calculates the cumulative probability for a given x-value. Use the CDF to determine the probability that a random observation that is taken from the population will be less than or equal to a certain value.	What is the use of cumulative distribution function
1885	there are three general categories of learning that artificial intelligence (AI)/machine learning utilizes to actually learn. They are Supervised Learning, Unsupervised Learning and Reinforcement learning.  The machine then maps the inputs and the outputs.	What is learning and types of learning in artificial intelligence
3493	The gradients carry information used in the RNN parameter update and when the gradient becomes smaller and smaller, the parameter updates become insignificant which means no real learning is done. Let's have a short reminder of how RNNs look like.	What is gradient RNN
6440	"Predictive modeling is the process of using known results to create, process, and validate a model that can be used to forecast future outcomes. It is a tool used in predictive analytics, a data mining technique that attempts to answer the question ""what might possibly happen in the future?"""	What is predictive modeling in data mining
1866	Statistical Significance Definition Statistical significance is the likelihood that the difference in conversion rates between a given variation and the baseline is not due to random chance.  It also means that there is a 5% chance that you could be wrong.	How do you explain statistical significance
4912	Firstly, while the sample variance (using Bessel's correction) is an unbiased estimator of the population variance, its square root, the sample standard deviation, is a biased estimate of the population standard deviation; because the square root is a concave function, the bias is downward, by Jensen's inequality.	Why is the sample standard deviation biased
1219	The generator is a convolutional neural network and the discriminator is a deconvolutional neural network. The goal of the generator is to artificially manufacture outputs that could easily be mistaken for real data. The goal of the discriminator is to identify which outputs it receives have been artificially created.	What is the goal of a generative adversarial network GAN )
477	"In artificial intelligence research, commonsense knowledge consists of facts about the everyday world, such as ""Lemons are sour"", that all humans are expected to know.  Common sense knowledge also helps to solve problems in the face of incomplete information."	What is common sense in artificial intelligence
5646	Linear regression is used to find the best fitting line between all the points of your dataset (by computing the minimum of a given distance), it does not, in itself, reduce the dimensionality of your data.	Is dimensionality reduction applicable in simple linear regression
6728	An ROC curve shows the relationship between clinical sensitivity and specificity for every possible cut-off. The ROC curve is a graph with: The x-axis showing 1 – specificity (= false positive fraction = FP/(FP+TN)) The y-axis showing sensitivity (= true positive fraction = TP/(TP+FN))	What does ROC curve tell you
5505	Systematic sampling is a type of probability sampling method in which sample members from a larger population are selected according to a random starting point but with a fixed, periodic interval. This interval, called the sampling interval, is calculated by dividing the population size by the desired sample size.	How does systematic sampling work
4739	Yes, it is ok to run a Pearson r correlation using two binary coded variables*. Pearson r has a special name in that situation (phi coefficient). There are some special issues when you look at correlations between binary or dichotomous variables.	Can Pearson's correlation coefficient be used with binary variables
2749	Coefficient of variation is a measure used to assess the total risk per unit of return of an investment. It is calculated by dividing the standard deviation of an investment by its expected rate of return.  Coefficient of variation provides a standardized measure of comparing risk and return of different investments.	Does coefficient of variation measure risk
1828	Dimensional analysis provides you with an alternative approach to problem solving. Problems in which a measurement with one unit is converted to an equivalent measurement with another unit are easily solved using dimensional analysis. They form cations with positive charges equal to their group number.	Why is dimensional analysis useful
5434	"An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is ""spam"" or ""non-spam"").  This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns."	What is an example of pattern recognition
1274	Definition: Stratified sampling is a type of sampling method in which the total population is divided into smaller groups or strata to complete the sampling process. The strata is formed based on some common characteristics in the population data.	What is a stratified sampling method
1036	The sobel operator is very similar to Prewitt operator. It is also a derivate mask and is used for edge detection. Like Prewitt operator sobel operator is also used to detect two kinds of edges in an image: Vertical direction.	Which of the following is used by Sobel edge detection
1262	Filters typically are applied to data in the data processing stage or the preprocessing stage. Filters enhance the clarity of the signal that's used for machine learning.	What is filtering in machine learning
5529	Clipping path is the Photoshop technique — used with the Pen Tool — to remove the background from an image. Clipping path is generally used when the subject of the image has sharp, smooth edges. This allows the clipping path to stay straight.	What is the purpose of a clipping path
448	In deep multilayer Perceptron networks, exploding gradients can result in an unstable network that at best cannot learn from the training data and at worst results in NaN weight values that can no longer be updated. … exploding gradients can make learning unstable.	Do ReLU networks suffer from the exploding gradient problem
581	ReLU is important because it does not saturate; the gradient is always high (equal to 1) if the neuron activates. As long as it is not a dead neuron, successive updates are fairly effective. ReLU is also very quick to evaluate.	Why do we use ReLU in CNN
3087	A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease.	How do you know if a regression line is positive or negative
7100	The item response theory (IRT), also known as the latent response theory refers to a family of mathematical models that attempt to explain the relationship between latent traits (unobservable characteristic or attribute) and their manifestations (i.e. observed outcomes, responses or performance).	What is IRT model
5035	Sometimes we want to know the probability of getting one result or another. When events are mutually exclusive and we want to know the probability of getting one event OR another, then we can use the OR rule.  P(A or B) = P(A) + P(B) for mutually exclusive events.	What is the OR rule in probability
639	All Answers (8) A matrix is a two dimensional array of numbers (or values from some field or ring). A 2-rank tensor is a linear map from two vector spaces, over some field such as the real numbers, to that field.	What is the difference between a matrix and a tensor
1364	Whenever we train our own Neural Networks, we need to take care of something called the generalization of the Neural Network. This essentially means how good our model is at learning from the given data and applying the learnt information elsewhere.	What is generalization in the training and testing of neural networks
1632	A fancy name for training: the selection of parameter values, which are optimal in some desired sense (eg. minimize an objective function you choose over a dataset you choose). The parameters are the weights and biases of the network.	What is parameter optimization
3988	Choosing the Best Algorithm for your Classification Model.•Read the Data.• Create Dependent and Independent Datasets based on our Dependent and Independent features.•Split the Data into Training and Testing sets.• Train our Model for different Classification Algorithms namely XGB Classifier, Decision Tree, SVM Classifier, Random Forest Classifier.•Select the Best Algorithm.	How do you choose the best classification model
1301	Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting.	What is the use of linear regression in machine learning
7130	Structured data is highly specific and is stored in a predefined format, where unstructured data is a conglomeration of many varied types of data that are stored in their native formats.  Structured data is commonly stored in data warehouses and unstructured data is stored in data lakes.	What is structured and unstructured data in big data
8533	Wikipedia defines Taguchi loss function as the graphical depiction of loss to describe a phenomenon affecting the value of products produced by a company. It emphasizes the need for incorporating quality and reliability at the design stage, prior to production.	What is meant by Taguchi's loss function
5088	In probability theory, an event is an outcome or defined collection of outcomes of a random experiment. Since the collection of all possible outcomes to a random experiment is called the sample space, another definiton of event is any subset of a sample space.	What is the definition of an event in statistics
255	The term convolution refers to the mathematical combination of two functions to produce a third function. It merges two sets of information. In the case of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map.	What is CNN convolution operation
2448	Finally, the test dataset is a dataset used to provide an unbiased evaluation of a final model fit on the training dataset. If the data in the test dataset has never been used in training (for example in cross-validation), the test dataset is also called a holdout dataset.	Why is test data set used
812	You can perform statistical analysis with the help of Excel.  If it is not there, go to Excel → File → Options → Add-in and enable the Analysis ToolPak by selecting the Excel Add-ins option in manage tab and then, click GO. This will open a small window; select the Analysis ToolPak option and enable it.	Can Excel be used for statistical analysis
7737	An adaptive filter is a system with a linear filter that has a transfer function controlled by variable parameters and a means to adjust those parameters according to an optimization algorithm.  The closed loop adaptive filter uses feedback in the form of an error signal to refine its transfer function.	What is adaptive linear filtering
1137	Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models.	What is data pre processing as used in machine learning
2707	Here are a few examples where unstructured data is being used in analytics today. Classifying image and sound. Using deep learning, a system can be trained to recognize images and sounds. The systems learn from labeled examples in order to accurately classify new images or sounds.	When would you use unstructured data
4116	The significance level, also denoted as alpha or α, is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference.	What does alpha level of .05 mean
3342	A perceptron is a simple model of a biological neuron in an artificial neural network. Perceptron is also the name of an early algorithm for supervised learning of binary classifiers.	How would you best define a Perceptron
1298	IBM has been a leader in the field of artificial intelligence since the 1950s. Its efforts in recent years are around IBM Watson, including an a AI-based cognitive service, AI software as a service, and scale-out systems designed for delivering cloud-based analytics and AI services.	Who is leading in artificial intelligence
3123	Here are 7 examples of clustering algorithms in action.Identifying Fake News. Fake news is not a new phenomenon, but it is one that is becoming prolific.  Spam filter.  Marketing and Sales.  Classifying network traffic.  Identifying fraudulent or criminal activity.  Document analysis.  Fantasy Football and Sports.	Where can we apply clustering algorithm in real life
709	1.1 The Role of Logic in Artificial Intelligence Logic, for instance, can provide a specification for a programming language by characterizing a mapping from programs to the computations that they license.	What is the role of logic in artificial intelligence
1586	The k-means problem is finding the least-squares assignment to centroids. There are multiple algorithms for finding a solution. There is an obvious approach to find the global optimum: enumerating all k^n possible assignments - that will yield a global minimum, but in exponential runtime.	How do you get global minima in K means algorithm
6405	A unimodal distribution only has one peak in the distribution, a bimodal distribution has two peaks, and a multimodal distribution has three or more peaks. Another way to describe the shape of histograms is by describing whether the data is skewed or symmetric.	How many peaks does a multimodal distribution have
38	Divide the number of events by the number of possible outcomes.Determine a single event with a single outcome.  Identify the total number of outcomes that can occur.  Divide the number of events by the number of possible outcomes.  Determine each event you will calculate.  Calculate the probability of each event.More items•	How do you find the probability of an outcome
744	An easy guide to choose the right Machine Learning algorithmSize of the training data. It is usually recommended to gather a good amount of data to get reliable predictions.  Accuracy and/or Interpretability of the output.  Speed or Training time.  Linearity.  Number of features.	How do I know which algorithm to use in machine learning
4204	Nonresponse in sample surveys (see Survey Sampling ) may be defined as the failure to make measurements or obtain observations on some of the listing units selected for inclusion in a sample.	What is non response in sample survey
5284	It has been successfully used for many purposes, but it works particularly well with natural language processing (NLP) problems. Naive Bayes is a family of probabilistic algorithms that take advantage of probability theory and Bayes' Theorem to predict the tag of a text (like a piece of news or a customer review).	What is naive Bayes used for
6745	The example of reinforcement learning is your cat is an agent that is exposed to the environment. The biggest characteristic of this method is that there is no supervisor, only a real number or reward signal. Two types of reinforcement learning are 1) Positive 2) Negative.	What is reinforcement learning example
1197	Each feature, or column, represents a measurable piece of data that can be used for analysis: Name, Age, Sex, Fare, and so on. Features are also sometimes referred to as “variables” or “attributes.” Depending on what you're trying to analyze, the features you include in your dataset can vary widely.	What are feature variables
1382	Artificial intelligence is imparting a cognitive ability to a machine.  The idea behind machine learning is that the machine can learn without human intervention. The machine needs to find a way to learn how to solve a task given the data. Deep learning is the breakthrough in the field of artificial intelligence.	What is artificial intelligence machine learning and deep learning
1726	Definition LT Linear Transformation A linear transformation, T:U→V T : U → V , is a function that carries elements of the vector space U (called the domain) to the vector space V (called the codomain), and which has two additional properties. T(u1+u2)=T(u1)+T(u2) T ( u 1 + u 2 ) = T ( u 1 ) + T ( u 2 ) for all u1,u2∈U.	What is linear transformation in linear algebra
372	If two random variables X and Y are independent, then they are uncorrelated. Proof. Uncorrelated means that their correlation is 0, or, equivalently, that the covariance between them is 0.	How do you prove two variables are uncorrelated
4235	A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.	What is posterior probability in machine learning
57	The degrees of freedom in a multiple regression equals N-k-1, where k is the number of variables. The more variables you add, the more you erode your ability to test the model (e.g. your statistical power goes down).	How do you calculate degrees of freedom in multiple regression
32	MATLABL-shaped membrane logoMATLAB R2015b running on Windows 10Initial release1984Stable releaseR2020b / September 17, 2020Written inC/C++, MATLAB8 more rows	What is Matlab written in
6863	You now know that: Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance.	What's the trade off between bias and variance
5712	Predictive analytics uses historical data to predict future events. Typically, historical data is used to build a mathematical model that captures important trends. That predictive model is then used on current data to predict what will happen next, or to suggest actions to take for optimal outcomes.	How do predictive analytics work
1706	Unsupervised learning is a machine learning technique, where you do not need to supervise the model. Supervised learning allows you to collect data or produce a data output from the previous experience. Unsupervised machine learning helps you to finds all kind of unknown patterns in data.	Is Machine Learning supervised or unsupervised
1319	For a binary classification like our example, the typical loss function is the binary cross-entropy / log loss.	Which loss function is used for binary classification
8435	Omitted variable bias occurs when a regression model leaves out relevant independent variables, which are known as confounding variables. This condition forces the model to attribute the effects of omitted variables to variables that are in the model, which biases the coefficient estimates.	How do you explain omitted variable bias
183	Mini-batch gradient descent is a variation of the gradient descent algorithm that splits the training dataset into small batches that are used to calculate model error and update model coefficients. Implementations may choose to sum the gradient over the mini-batch which further reduces the variance of the gradient.	What is mini batch stochastic gradient descent
1805	The law of averages is not a mathematical principle, whereas the law of large numbers is. In probability theory, the law of large numbers is a theorem that describes the result of performing the same experiment a large number of times.	Is the Law of Averages mathematical or scientific
4856	In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  In unsupervised feature learning, features are learned with unlabeled input data.	What is representation learning and how does it relate to machine learning and deep learning
1178	The first four are: 1) The mean, which indicates the central tendency of a distribution. 2) The second moment is the variance, which indicates the width or deviation. 3) The third moment is the skewness, which indicates any asymmetric 'leaning' to either left or right.	What are the four moments of statistics
7046	If you see a lowercase x or y, that's the kind of variable you're used to in algebra. It refers to an unknown quantity or quantities. If you see an uppercase X or Y, that's a random variable and it usually refers to the probability of getting a certain outcome.	How do you identify a random variable
2505	12:2824:57Suggested clip · 118 secondsPoisson versus negative binomial regression in SPSS - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you do a negative binomial regression in SPSS
8063	Odds ratios are one of those concepts in statistics that are just really hard to wrap your head around.  For example, in logistic regression the odds ratio represents the constant effect of a predictor X, on the likelihood that one outcome will occur. The key phrase here is constant effect.	What are odds ratios in logistic regression
1559	At its most basic, machine learning uses programmed algorithms that receive and analyse input data to predict output values within an acceptable range. As new data is fed to these algorithms, they learn and optimise their operations to improve performance, developing 'intelligence' over time.	What is machine learning algorithm
8194	"""Mean"" usually refers to the population mean. This is the mean of the entire population of a set.  It's more practical to measure a smaller sample from the set. The mean of the sample group is called the sample mean."	What is the difference between mean and sample mean
1319	Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves.	What is the function of machine learning
6003	Give an example in which binning is useful. The purpose of binning is to analyze the frequency of quantitative data grouped into categories that cover a range of possible values. A useful example is grouping quiz scores with a maximum score of 40 points with​ 10-point bins.	What is the purpose of binning give an example in which binning is useful
466	Exponential moving averages, or EMA, give more weighting to recent prices. They reduce the effect of the lag that comes from using previous price data and can help you identify a trend earlier, so it's a useful indicator for trading short-term contracts.	In trading why would we use the exponential moving average over the simple moving average
404	Random search (RS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized, and RS can hence be used on functions that are not continuous or differentiable. Such optimization methods are also known as direct-search, derivative-free, or black-box methods.	What is randomized search
2787	Convolutional neural networks work because it's a good extension from the standard deep-learning algorithm. Given unlimited resources and money, there is no need for convolutional because the standard algorithm will also work. However, convolutional is more efficient because it reduces the number of parameters.	Why are convolutional neural networks better
6964	An indicator random variable is a special kind of random variable associated with the occurence of an event. The indicator random variable IA associated with event A has value 1 if event A occurs and has value 0 otherwise. In other words, IA maps all outcomes in the set A to 1 and all outcomes outside A to 0.	What is an indicator random variable
3059	How to Use K-means Cluster Algorithms in Predictive AnalysisPick k random items from the dataset and label them as cluster representatives.Associate each remaining item in the dataset with the nearest cluster representative, using a Euclidean distance calculated by a similarity function.Recalculate the new clusters' representatives.More items	How do you predict using clustering
1896	A factorial distribution happens when a set of variables are independent events. In other words, the variables don't interact at all; Given two events x and y, the probability of x doesn't change when you factor in y.	What is factorial distribution
7313	0:005:03Suggested clip · 117 secondsPCA 5: finding eigenvalues and eigenvectors - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the eigenvectors of a covariance matrix
7708	As the formula shows, the standard score is simply the score, minus the mean score, divided by the standard deviation.	How do you calculate a standard score
6588	Examples of unstructured data are:Rich media. Media and entertainment data, surveillance data, geo-spatial data, audio, weather data.Document collections. Invoices, records, emails, productivity applications.Internet of Things (IoT). Sensor data, ticker data.Analytics. Machine learning, artificial intelligence (AI)	What are examples of unstructured data
5796	A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex. Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers.	What is ResNet in deep learning
1126	Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for—tasks that involve creativity and empathy among others.	What is the role of artificial intelligence in the shaping modern society
4531	Specifically, you learned: That a key approach is to use word embeddings and convolutional neural networks for text classification. That a single layer model can do well on moderate-sized problems, and ideas on how to configure it.	Which neural network is best for text classification
942	A probability sampling method is any method of sampling that utilizes some form of random selection. In order to have a random selection method, you must set up some process or procedure that assures that the different units in your population have equal probabilities of being chosen.	What is probability sampling statistics
733	gamma is a parameter for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set gammas = [0.1, 1, 10, 100]for gamma in gammas: svc = svm.SVC(kernel='rbf', gamma=gamma).fit(X, y)	What is Gamma in SVC
151	A collinearity is a special case when two or more variables are exactly correlated. This means the regression coefficients are not uniquely determined. In turn it hurts the interpretability of the model as then the regression coefficients are not unique and have influences from other features.	What is Collinearity in machine learning
35	Exponential smoothing is a way to smooth out data for presentations or to make forecasts. It's usually used for finance and economics. If you have a time series with a clear pattern, you could use moving averages — but if you don't have a clear pattern you can use exponential smoothing to forecast.	When would you use exponential smoothing
8277	Discriminant analysis is statistical technique used to classify observations into non-overlapping groups, based on scores on one or more quantitative predictor variables. For example, a doctor could perform a discriminant analysis to identify patients at high or low risk for stroke.	What is discriminant analysis example
1342	"The ""least squares"" method is a form of mathematical regression analysis used to determine the line of best fit for a set of data, providing a visual demonstration of the relationship between the data points."	What is least square regression analysis
298	In statistics, the likelihood function (often simply called the likelihood) measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters.	What does likelihood mean in statistics
6845	An AR(1) autoregressive process is one in which the current value is based on the immediately preceding value, while an AR(2) process is one in which the current value is based on the previous two values. An AR(0) process is used for white noise and has no dependence between the terms.	What does autoregressive mean
315	Associations Software: commercial IBM SPSS Modeler Suite, includes market basket analysis. LPA Data Mining Toolkit supports the discovery of association rules within relational database. Magnum Opus, flexible tool for finding associations in data, including statistical support for avoiding spurious discoveries.	What are the various softwares used for association rule mining
792	The Minimax algorithm helps find the best move, by working backwards from the end of the game. At each step it assumes that player A is trying to maximize the chances of A winning, while on the next turn player B is trying to minimize the chances of A winning (i.e., to maximize B's own chances of winning).	How does the minimax algorithm work
1459	Relative Frequency Of A Class Is The Percentage Of The Data That Falls In That Class, While Cumulative Frequency Of A Class Is The Sum Of The Frequencies Of That Class And All Previous Classes.	What is the difference between relative and cumulative frequency distribution
1538	Compute the Total without disease by subtraction. Multiply the Total with disease by the Sensitivity to get the number of True positives. Multiply the Total without disease by the Specificity to get the number of True Negatives.	How do you calculate true negative rate
3704	An estimator attempts to approximate the unknown parameters using the measurements. In estimation theory, two approaches are generally considered.	What does the term “estimator” mean in estimation theory
1053	0:243:37Suggested clip · 110 secondsFinding and Interpreting the Coefficient of Determination - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you calculate the coefficient of determination for the data and interpret the value in context
4883	Variables such as heart rate, platelet count and respiration rate are in fact discrete yet are considered continuous because of large number of possible values. Only those variables which can take a small number of values, say, less than 10, are generally considered discrete.	Is heart rate a continuous or discrete variable
6354	An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set.  Outliers can occur by chance in any distribution, but they often indicate either measurement error or that the population has a heavy-tailed distribution.	What causes outliers in data
1447	Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it. Filter methods are much faster compared to wrapper methods as they do not involve training the models.	What is filter method in feature selection
101	An image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image. It plots the number of pixels for each tonal value. By looking at the histogram for a specific image a viewer will be able to judge the entire tonal distribution at a glance.	How and why do you use an image histogram
160	When all the points on a scatterplot lie on a straight line, you have what is called a perfect correlation between the two variables (see below). A scatterplot in which the points do not have a linear trend (either positive or negative) is called a zero correlation or a near-zero correlation (see below).	Is it possible for a scatter plot to have a positive or negative association that is not linear
4517	So, for 10% error, you need 100 hash functions. For 1% error, you need 10,000 hash functions. Yick. That's friggin expensive, and if that's all there were to MinHash, I'd simply go with the O(n log(n)) algorithm.	How many hash functions are required in a minhash algorithm
1180	The pdf represents the relative frequency of failure times as a function of time. The cdf is a function, F(x)\,\!, of a random variable X\,\!, and is defined for a number x\,\!	What is a PDF vs CDF
8366	To perform principal component analysis using the correlation matrix using the prcomp() function, set the scale argument to TRUE . Plot the first two PCs of the correlation matrix using the autoplot() function.	How do you do principal component analysis in R
448	Binary Cross-Entropy Loss Also called Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss. Unlike Softmax loss it is independent for each vector component (class), meaning that the loss computed for every CNN output vector component is not affected by other component values.	What is binary Crossentropy
2941	How To Develop a Machine Learning Model From ScratchDefine adequately our problem (objective, desired outputs…).Gather data.Choose a measure of success.Set an evaluation protocol and the different protocols available.Prepare the data (dealing with missing values, with categorial values…).Spilit correctly the data.More items	How do you apply machine learning algorithms on a dataset
166	Supervised: Use the target variable (e.g. remove irrelevant variables).Wrapper: Search for well-performing subsets of features. RFE.Filter: Select subsets of features based on their relationship with the target. Feature Importance Methods.Intrinsic: Algorithms that perform automatic feature selection during training.	How do you select best features in regression
602	You simply measure the number of correct decisions your classifier makes, divide by the total number of test examples, and the result is the accuracy of your classifier. It's that simple. The vast majority of research results report accuracy, and many practical projects do too.	How do you determine the accuracy of a classifier
1449	There are two forms of statistical inference:Hypothesis testing.Confidence interval estimation.	What are the two types of statistical inference
5861	In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression.	How does machine learning define features
724	Gradient boosting is a greedy algorithm and can overfit a training dataset quickly. It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting.	Why is gradient boosting good
3931	Backpropagation through time is the method to overcome decay in information through RNN. BPTT helps a practitioner to solve the sequence prediction problems for recurrent neural networks. It is used as a training algorithm which can update its weight in RNN.	What is the method to overcome the decay of information through time in RNN known as
1324	The regression effect causes an individual's expected post-test measurement to fall somewhere between her pre-test measurement and the mean pre-test measurement.  Consider those subjects whose pre-test measurements are less than the overall mean (filled circles).	What is the regression effect
486	In logistic regression, an odds ratio of 2 means that the event is 2 time more probable given a one-unit increase in the predictor. In Cox regression, a hazard ratio of 2 means the event will occur twice as often at each time point given a one-unit increase in the predictor.	What is the difference between an odds ratio and a hazard ratio
7184	Maximum likelihood estimation is a method that will find the values of μ and σ that result in the curve that best fits the data.  The goal of maximum likelihood is to find the parameter values that give the distribution that maximise the probability of observing the data.	How does Maximum Likelihood work
8648	Geometrical meaning of integration is a statement so it must be true. Another way of analysing this statement is area of a curve or the volume of a curve of revolution or area of an implicit equation of x and y.  The geometrical meaning of integration is to find the area under the corresponding curve.	What is the geometrical meaning of integration
6303	A probability distribution may be either discrete or continuous. A discrete distribution means that X can assume one of a countable (usually finite) number of values, while a continuous distribution means that X can assume one of an infinite (uncountable) number of different values.	What is continuous and discrete probability distribution
5499	The traditional method of training AI models involves setting up servers where models are trained on data, often through the use of a cloud-based computing platform.  Federated learning brings machine learning models to the data source, rather than bringing the data to the model.	What is Federated AI
6448	The main use of F-distribution is to test whether two independent samples have been drawn for the normal populations with the same variance, or if two independent estimates of the population variance are homogeneous or not, since it is often desirable to compare two variances rather than two averages.	What is an F distribution used for
360	Two main types of fuzzy inference systems can be implemented: Mamdani-type (1977) and Sugeno-type (1985). These two types of inference systems vary somewhat in the way outputs are determined. Mamdani-type inference expects the output membership functions to be fuzzy sets.	What are the two types of fuzzy inference systems
946	Exploratory Data Analysis is one of the important steps in the data analysis process.  Exploratory Data Analysis is a crucial step before you jump to machine learning or modeling of your data. It provides the context needed to develop an appropriate model – and interpret the results correctly.	Why do we need to perform exploratory data analysis
3969	The Z-distribution is a normal distribution with mean zero and standard deviation 1; its graph is shown here.  Values on the Z-distribution are called z-values, z-scores, or standard scores. A z-value represents the number of standard deviations that a particular value lies above or below the mean.	What is Z distribution
2421	Rather than using the past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model.  While, the autoregressive model(AR) uses the past forecasts to predict future values.	What are the differences between autoregressive and moving average models
40	To calculate the mean of grouped data, the first step is to determine the midpoint (also called a class mark) of each interval, or class. These midpoints must then be multiplied by the frequencies of the corresponding classes. The sum of the products divided by the total number of values will be the value of the mean.	How do you find the mean of a frequency table with intervals
1604	2:1510:12Suggested clip · 108 secondsHistograms In Photography - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How are histograms used in photography
1177	Sigma /ˈsɪɡmə/ (uppercase Σ, lowercase σ, lowercase in word-final position ς; Greek: σίγμα) is the eighteenth letter of the Greek alphabet. In the system of Greek numerals, it has a value of 200. In general mathematics, uppercase ∑ is used as an operator for summation.	What does sigma mean
65	An estimator of a given parameter is said to be unbiased if its expected value is equal to the true value of the parameter. In other words, an estimator is unbiased if it produces parameter estimates that are on average correct.	How do you show an estimator is unbiased
1809	Parametric tests assume a normal distribution of values, or a “bell-shaped curve.” For example, height is roughly a normal distribution in that if you were to graph height from a group of people, one would see a typical bell-shaped curve.	What is an example of parametric statistics
2918	"The term ""running median"" is typically used to refer to the median of a subset of data."	What is a running median
1532	For example, the Hamiltonian represents the energy of a system. The eigen functions represent stationary states of the system i.e. the system can achieve that state under certain conditions and eigenvalues represent the value of that property of the system in that stationary state.	What do eigenvalues represent
2094	You can use the Lasso or elastic net regularization for generalized linear model regression which can be used for classification problems. Here data is the data matrix with rows as observations and columns as features. group is the labels.	Can Lasso be used for classification
3387	Regression analysis is used when you want to predict a continuous dependent variable from a number of independent variables. If the dependent variable is dichotomous, then logistic regression should be used.	When would you use regression
469	SVM tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data, while logistic regression does not, instead it can have different decision boundaries with different weights that are near the optimal point.	Why is SVM more accurate than logistic regression
48	As mentioned in the context of the gradient theorem, a vector field F is conservative if and only if it has a potential function f with F=∇f. Therefore, if you are given a potential function f or if you can find one, and that potential function is defined everywhere, then there is nothing more to do.	How do you know if F is conservative vector field
2446	stratified k-fold cross-validation	Which type of cross validation is used for imbalanced dataset K fold
2104	Let X = fft(x) . Both x and X have length N . Suppose X has two peaks at n0 and N-n0 . Then the sinusoid frequency is f0 = fs*n0/N Hertz.Replace all coefficients of the FFT with their square value (real^2+imag^2).  Take the iFFT.Find the largest peak in the iFFT.	How do you use FFT to find frequency
6601	“The advantages of bootstrapping are that it is a straightforward way to derive the estimates of standard errors and confidence intervals, and it is convenient since it avoids the cost of repeating the experiment to get other groups of sampled data.	Why does bootstrapping in statistics work
1936	Intersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.  The ground-truth bounding boxes (i.e., the hand labeled bounding boxes from the testing set that specify where in the image our object is). The predicted bounding boxes from our model.	What is ground truth bounding box
5282	AI is designed to draw conclusions on data, understand concepts, become self-learning and even interact with humans. Data analytics refers to technologies that study data and draw patterns.  Furthermore, when it comes to data analytics, it is not a single product.	Is Data Analytics part of AI
7482	Seven Techniques for Data Dimensionality ReductionDimensionality ReductionReduction RateAuCMissing Values Ratio71%82%Low Variance Filter73%82%High Correlation Filter74%82%PCA62%72%4 more rows	Which method would you choose for dimensionality reduction
790	Yes, although 'linear regression' refers to any approach to model the relationship between one or more variables, OLS is the method used to find the simple linear regression of a set of data.	What is the difference between OLS and linear regression
3674	In computer programming, an iterator is an object that enables a programmer to traverse a container, particularly lists. Various types of iterators are often provided via a container's interface.  An iterator is behaviorally similar to a database cursor.	What is meant by iterator
306	In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input–output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually.	What is back propagation in neural networks
5277	mAP (mean average precision) is the average of AP. In some context, we compute the AP for each class and average them. But in some context, they mean the same thing. For example, under the COCO context, there is no difference between AP and mAP.	What is mAP mean average precision
4729	Average pooling method smooths out the image and hence the sharp features may not be identified when this pooling method is used. Max pooling selects the brighter pixels from the image. It is useful when the background of the image is dark and we are interested in only the lighter pixels of the image.	What is difference between average pooling layer and Max pooling
530	Data visualization refers to the techniques used to communicate data or information by encoding it as visual objects (e.g., points, lines or bars) contained in graphics. The goal is to communicate information clearly and efficiently to users. It is one of the steps in data analysis or data science.	What is data visualization and its techniques
8203	- Categorical Variable Transformation: is turning a categorical variable to a numeric variable. Categorical variable transformation is mandatory for most of the machine learning models because they can handle only numeric values.	Can you transform categorical variables
2226	The (statistical) design of experiments (DOE) is an efficient procedure for planning experiments so that the data obtained can be analyzed to yield valid and objective conclusions. DOE begins with determining the objectives of an experiment and selecting the process factors for the study.	What is statistical design of experiments
3298	If your learning rate is set too low, training will progress very slowly as you are making very tiny updates to the weights in your network. However, if your learning rate is set too high, it can cause undesirable divergent behavior in your loss function.	What happens if learning rate is too high
7738	If adjacent residuals are correlated, one residual can predict the next residual. In statistics, this is known as autocorrelation. This correlation represents explanatory information that the independent variables do not describe. Models that use time-series data are susceptible to this problem.	What if residuals are correlated
5681	Spatiotemporal, or spatial temporal, is used in data analysis when data is collected across both space and time. It describes a phenomenon in a certain location and time — for example, shipping movements across a geographic area over time (see above example image).	What is spatiotemporal analysis
2444	In simple terms, a quantile is where a sample is divided into equal-sized, adjacent, subgroups (that's why it's sometimes called a “fractile“).  The median cuts a distribution into two equal areas and so it is sometimes called 2-quantile. Quartiles are also quantiles; they divide the distribution into four equal parts.	What is quantile example
5841	Essentially, the process goes as follows:Select k centroids. These will be the center point for each segment.Assign data points to nearest centroid.Reassign centroid value to be the calculated mean value for each cluster.Reassign data points to nearest centroid.Repeat until data points stay in the same cluster.	How do you find the centroid in K means clustering
8486	A t-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related in certain features. The t-test is one of many tests used for the purpose of hypothesis testing in statistics. Calculating a t-test requires three key data values.	What is the t test used for
217	Simple linear regression is commonly used in forecasting and financial analysis—for a company to tell how a change in the GDP could affect sales, for example.	Can regression be used for forecasting
51	Not only are nose strips bad for those with sensitive skin, they also worsen other skin conditions. Pore strips exacerbate rosacea-prone skin , especially if they contain irritating ingredients like alcohol and astringents. They also aggravate extremely dry skin, eczema and psoriasis .	Is pore strip bad
1869	A Boltzmann Machine is a network of symmetrically connected, neuron- like units that make stochastic decisions about whether to be on or off. Boltz- mann machines have a simple learning algorithm that allows them to discover interesting features in datasets composed of binary vectors.	What is Boltzmann machine used for
4004	It is not appropriate because the regression line models the trend of the given​ data, and it is not known if the trend continues beyond the range of those data.	Is it appropriate to use a regression line to predict y values for x values that are not in or close to the range of x values found in the data
8198	Named Entity Recognition can automatically scan entire articles and reveal which are the major people, organizations, and places discussed in them. Knowing the relevant tags for each article help in automatically categorizing the articles in defined hierarchies and enable smooth content discovery.	How do you use a named entity recognition
2733	Active learning: Reinforces important material, concepts, and skills. Provides more frequent and immediate feedback to students. Provides students with an opportunity to think about, talk about, and process course material.	What is the purpose of active learning
7581	So, the total number of parameters are “(n*m*l+1)*k”. Pooling Layer: There are no parameters you could learn in pooling layer. This layer is just used to reduce the image dimension size. Fully-connected Layer: In this layer, all inputs units have a separable weight to each output unit.	What is the number of parameters in a Max pooling layer
1128	Deep learning is an artificial intelligence (AI) function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.  Also known as deep neural learning or deep neural network.	What is considered deep learning
1075	Cluster analysis divides data into groups (clusters) that are meaningful, useful, or both. If meaningful groups are the goal, then the clusters should capture the natural structure of the data. In some cases, however, cluster analysis is only a useful starting point for other purposes, such as data summarization.	How do you read cluster analysis
35	Systematic random sampling is the random sampling method that requires selecting samples based on a system of intervals in a numbered population. For example, Lucas can give a survey to every fourth customer that comes in to the movie theater.	What is systematic random sampling with example
263	K-NN is a lazy learner because it doesn't learn a discriminative function from the training data but “memorizes” the training dataset instead. For example, the logistic regression algorithm learns its model weights (parameters) during training time.  A lazy learner does not have a training phase.	Why K Nearest Neighbor algorithm is lazy learning algorithm
6755	To put put it bluntly, Artificial intelligence (AI) relies on machines, whereas Collective Intelligence (CI) relies on people. AI stands for the simulation of human intelligence by machines, computers or software systems.  In fact, artificial and collective intelligence can -and should – reinforce each other.	Is collective intelligence artificial intelligence
8161	To see the accuracy of clustering process by using K-Means clustering method then calculated the square error value (SE) of each data in cluster 2. The value of square error is calculated by squaring the difference of the quality score or GPA of each student with the value of centroid cluster 2.	How do you find the accuracy of K means clustering
7040	They are defined as follows: Bias: Bias describes how well a model matches the training set. A model with high bias won't match the data set closely, while a model with low bias will match the data set very closely.  Typically models with high bias have low variance, and models with high variance have low bias.	What is biased model
6676	"To give you two ideas:A Kolmogorov-Smirnov test is a non-parametric test, that measures the ""distance"" between two cumulative/empirical distribution functions.The Kullback-Leibler divergence measures the ""distance"" between two distributions in the language of information theory as a change in entropy."	How do you find the difference between two distributions
7694	"Consistency refers to logical and numerical coherence. Context: An estimator is called consistent if it converges in probability to its estimand as sample increases (The International Statistical Institute, ""The Oxford Dictionary of Statistical Terms"", edited by Yadolah Dodge, Oxford University Press, 2003)."	What does consistent mean in statistics
1152	Neural networks generally perform supervised learning tasks, building knowledge from data sets where the right answer is provided in advance. The networks then learn by tuning themselves to find the right answer on their own, increasing the accuracy of their predictions.	How do artificial neural networks learn
6418	- Chad Orzel - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the Uncertainty Principle
5078	Input means to provide the program with some data to be used in the program and Output means to display data on screen or write the data to a printer or a file. C programming language provides many built-in functions to read any given input and to display data on screen when there is a need to output the result.	What is input function and output function explain
1464	Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes.  Mathematically, a logistic regression model predicts P(Y=1) as a function of X.	How does logistic regression algorithm work
5580	A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text. So you're working on a text classification problem.	What is support vector algorithm
8457	In probability theory, an experiment or trial (see below) is any procedure that can be infinitely repeated and has a well-defined set of possible outcomes, known as the sample space. An experiment is said to be random if it has more than one possible outcome, and deterministic if it has only one.	What is the difference between trial and random experiment
7855	Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification. ConvNets have been successful in identifying faces, objects and traffic signs apart from powering vision in robots and self driving cars.	Why do we need convolutional neural network
4205	Principal component analysis aims at reducing a large set of variables to a small set that still contains most of the information in the large set. The technique of principal component analysis enables us to create and use a reduced set of variables, which are called principal factors.	What is the purpose of principal component analysis
6516	A pooling or subsampling layer often immediately follows a convolution layer in CNN. Its role is to downsample the output of a convolution layer along both the spatial dimensions of height and width.	What is subsampling in CNN
1477	Statistical machine learning merges statistics with the computational sciences---computer science, systems science and optimization.  Moreover, by its interdisciplinary nature, statistical machine learning helps to forge new links among these fields.	What is statistical machine learning
205	LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs.	What is Lstm used for
52	"The classic example of experimenter bias is that of ""Clever Hans"", an Orlov Trotter horse claimed by his owner von Osten to be able to do arithmetic and other tasks."	What is an example of experimenter bias
7420	When you want to learn about the probability of two events occurring together, you're multiplying because it means “expanding the possibilities.” Because: Now, the possibilities are four, not two. It means it's harder to hit two heads twice, which is intuitively true.	Why do we multiply the probability of independent events
3071	Training loss is the error on the training set of data. Validation loss is the error after running the validation set of data through the trained network. Train/valid is the ratio between the two. Unexpectedly, as the epochs increase both validation and training error drop.	What does training loss mean
1806	It is well known that correlation does not prove causation. What is less well known is that causation can exist when correlation is zero. The upshot of these two facts is that, in general and without additional information, correlation reveals literally nothing about causation.	Is it possible for two things to have a causal relationship but not be correlated
5258	A data set is bimodal if it has two modes. This means that there is not a single data value that occurs with the highest frequency. Instead, there are two data values that tie for having the highest frequency.	How could we tell if data are bimodal
2628	Define the population.  Choose the relevant stratification.  List the population.  List the population according to the chosen stratification.  Choose your sample size.  Calculate a proportionate stratification.  Use a simple random or systematic sample to select your sample.	How do you collect a stratified random sample
6375	The linear relationship between exposure (either continuous or categorical) and a continuous outcome can be assessed by using linear regression analysis.	Can linear regression be used to predict continuous outcomes
1829	The regular regression coefficients that you see in your statistical output describe the relationship between the independent variables and the dependent variable.  After all, a larger coefficient signifies a greater change in the mean of the independent variable.	How can I tell which independent variable has more effect from other independent variables on a dependent variable in regression analysis
1469	With the LassoCV, RidgeCV, and Linear Regression machine learning algorithms.Define the problem.Gather the data.Clean & Explore the data.Model the data.Evaluate the model.Answer the problem.	How do you predict in machine learning
1852	0:0012:40Suggested clip · 82 secondsCommon Source Amplifiers - Gain Equation - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you calculate gain of common source amplifier
5561	Formally, the Quartile Deviation is equal to the half of the Inter-Quartile Range and thus we can write it as – Q d = Q 3 – Q 1 2 Q_d = \frac{Q_3 – Q_1}{2} Qd=2Q3–Q1 Therefore, we also call it the Semi Inter-Quartile Range. The Quartile Deviation doesn't take into account the extreme points of the distribution.	What is the formula of quartile deviation
2183	Statistical researchers often use a linear relationship to predict the (average) numerical value of Y for a given value of X using a straight line (called the regression line). If you know the slope and the y-intercept of that regression line, then you can plug in a value for X and predict the average value for Y.	How do you use linear regression to predict
927	Compare r to the appropriate critical value in the table. If r is not between the positive and negative critical values, then the correlation coefficient is significant. If r is significant, then you may want to use the line for prediction. Suppose you computed r=0.801 using n=10 data points.	How do you know if a correlation coefficient is statistically significant
5560	"Each 'particle' is in fact a guess about the initial location of the robot. But as the filter gathers more detail, it can eliminate some guesses.  The robot will then ""refine"" its initial guess, by generating additional guesses: it will also guess that its initial location may have been (2.1,3.2), or (1.9,3)."	What is the Particle Filter intuition
1412	Listen to pronunciation. (NOR-mul raynj) In medicine, a set of values that a doctor uses to interpret a patient's test results. The normal range for a given test is based on the results that are seen in 95% of the healthy population.	What does normal range mean
8534	Optimization Toolbox™ provides functions for finding parameters that minimize or maximize objectives while satisfying constraints.  The toolbox lets you perform design optimization tasks, including parameter estimation, component selection, and parameter tuning.	What is Matlab optimization
3279	The ratio scale of measurement is the most informative scale.  However, zero on the Kelvin scale is absolute zero. This makes the Kelvin scale a ratio scale. For example, if one temperature is twice as high as another as measured on the Kelvin scale, then it has twice the kinetic energy of the other temperature.	Is kelvin a ratio scale
6625	Numerical data is a data type expressed in numbers, rather than natural language description. Sometimes called quantitative data,numerical data is always collected in number form.  This characteristic is one of the major ways of identifying numerical data.	What is a numeric data
8000	Different types of the convolution layersSimple Convolution.1x1 Convolutions.Flattened Convolutions.Spatial and Cross-Channel convolutions.Depthwise Separable Convolutions.Grouped Convolutions.Shuffled Grouped Convolutions.	What are the types of convolution
703	The scale-invariant feature transform (SIFT) is a feature detection algorithm in computer vision to detect and describe local features in images.  Each cluster of 3 or more features that agree on an object and its pose is then subject to further detailed model verification and subsequently outliers are discarded.	What is sift in image processing
5778	Credit card tokenization substitutes sensitive customer data with a one-time alphanumeric ID that has no value or connection to the account's owner. This randomly generated token is used to access, pass, transmit and retrieve customer's credit card information safely.	What is tokenization and how does it work
7646	If correlation =1, then it shows there exists a directly proportional relationship between the two variables and if the same is - 1 then it denotes that there exists a inversely proportional relation between the two two variables and if we fit a regression line for the same then we'll get a straight line having	Why is the correlation coefficient between 1 and 1
1091	The One Sample t Test compares a sample mean to a hypothesized population mean to determine whether the two means are significantly different.	What is a one sample t test
6330	When to use the sample or population standard deviation Therefore, if all you have is a sample, but you wish to make a statement about the population standard deviation from which the sample is drawn, you need to use the sample standard deviation.	Should I use standard deviation of population or sample
1058	Data Structure - Depth First TraversalRule 1 − Visit the adjacent unvisited vertex. Mark it as visited. Display it. Push it in a stack.Rule 2 − If no adjacent vertex is found, pop up a vertex from the stack. (It will pop up all the vertices from the stack, which do not have adjacent vertices.)Rule 3 − Repeat Rule 1 and Rule 2 until the stack is empty.	How do you use depth first search
2309	1 Answer. For binary classification, it should give the same results, because softmax is a generalization of sigmoid for a larger number of classes.	Can Softmax be used for binary classification
6134	"DEEP LEARNING"" document.  It is a short State of the Art on two kinds of interesting neural network algorithms: Recurrent Neural Networks and Long Short-Term Memory. It also describes a set of open source tools for this deep learning approach."	What is SOTA in machine learning
2852	0:278:54Suggested clip · 121 secondsDeriving Engineering Equations Using Dimensional Analysis YouTubeStart of suggested clipEnd of suggested clip	How do you derive equations using dimensional analysis
666	The chi-square goodness of fit test is appropriate when the following conditions are met: The sampling method is simple random sampling. The variable under study is categorical. The expected value of the number of sample observations in each level of the variable is at least 5.	What are the conditions for conducting a chi square goodness of fit test
4166	In the development of the probability function for a discrete random variable, two conditions must be satisfied: (1) f(x) must be nonnegative for each value of the random variable, and (2) the sum of the probabilities for each value of the random variable must equal one.	What are the requirements of a probability distribution
7024	"Q-learning is a model-free reinforcement learning algorithm to learn quality of actions telling an agent what action to take under what circumstances.  ""Q"" names the function that the algorithm computes with the maximum expected rewards for an action taken in a given state."	What is Q in reinforcement learning
903	Keras is a neural networks library written in Python that is high-level in nature – which makes it extremely simple and intuitive to use. It works as a wrapper to low-level libraries like TensorFlow or Theano high-level neural networks library, written in Python that works as a wrapper to TensorFlow or Theano.	What is keras and TensorFlow
991	Centering predictor variables is one of those simple but extremely useful practices that is easily overlooked. It's almost too simple. Centering simply means subtracting a constant from every value of a variable.  The effect is that the slope between that predictor and the response variable doesn't change at all.	What does centering a variable mean
229	Transfer learning is useful when you have insufficient data for a new domain you want handled by a neural network and there is a big pre-existing data pool that can be transferred to your problem.	Why is transfer learning useful
592	Question: 1. When A Value Of Y Is Calculated Using The Regression Equation (Y_hat), It Is Called: -the Fitted Value -the Estimated Value -the Predicted Value -all Of The Above 2.	When a value of y is calculated using the regression equation it is called
370	The log likelihood This means that if the value on the x-axis increases, the value on the y-axis also increases (see figure below). This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function.	What is the meaning of log likelihood
273	"Information entropy is a concept from information theory. It tells how much information there is in an event. In general, the more certain or deterministic the event is, the less information it will contain.  The ""average ambiguity"" or Hy(x) meaning uncertainty or entropy. H(x) represents information."	What do you understand by information and entropy
15	Classification is a data mining function that assigns items in a collection to target categories or classes. The goal of classification is to accurately predict the target class for each case in the data. For example, a classification model could be used to identify loan applicants as low, medium, or high credit risks.	What is data mining classification
7075	Statistical knowledge helps you use the proper methods to collect the data, employ the correct analyses, and effectively present the results. Statistics is a crucial process behind how we make discoveries in science, make decisions based on data, and make predictions.	What are the advantages of statistics
879	When most people hear the term artificial intelligence, the first thing they usually think of is robots.  Artificial intelligence is based on the principle that human intelligence can be defined in a way that a machine can easily mimic it and execute tasks, from the most simple to those that are even more complex.	What is intelligence in artificial intelligence terms
1314	The sample correlation coefficient, denoted r,  For example, a correlation of r = 0.9 suggests a strong, positive association between two variables, whereas a correlation of r = -0.2 suggest a weak, negative association. A correlation close to zero suggests no linear association between two continuous variables.	What is an example of a correlation coefficient
5391	"The range containing values that are consistent with the null hypothesis is the ""acceptance region""; the other range, in which the null hypothesis is rejected, is the rejection region (or critical region)."	What is the difference between critical region and acceptance region
1259	Marginal probability effects are the partial effects of each explanatory variable on. the probability that the observed dependent variable Yi = 1, where in probit. models.	What is marginal effects in probit model
115	Continuous learning Another way to keep your models up-to-date is to have an automated system to continuously evaluate and retrain your models. This type of system is often referred to as continuous learning, and may look something like this: Save new training data as you receive it.	Can machine learning models be continuously trained
925	In a dataset, a training set is implemented to build up a model, while a test (or validation) set is to validate the model built. Data points in the training set are excluded from the test (validation) set.	What is the difference between a training set and a test set
4003	Now we'll check out the proven way to improve the performance(Speed and Accuracy both) of neural network models:Increase hidden Layers.  Change Activation function.  Change Activation function in Output layer.  Increase number of neurons.  Weight initialization.  More data.  Normalizing/Scaling data.More items•	How do you improve the accuracy of a neural network
179	As a general rule of thumb: If skewness is less than -1 or greater than 1, the distribution is highly skewed. If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed. If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.	What is the acceptable value of skewness
417	Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis.	What is prior and posterior
3606	Usually a pattern recognition system uses training samples from known categories to form a decision rule for unknown patterns.  Clustering methods simply try to group similar patterns into clusters whose members are more similar to each other (according to some distance measure) than to members of other clusters.	What is clustering in pattern recognition
1448	There is only one way to roll two 6's on a pair of dice: the first die must be a 6 and the second die must be a 6. The probability is 1/6 × 1/6 = 1/36. There are 3 ways in which to get at least one 6 in the roll of two dice. The first is to roll 6 on both dice, which we already determined has a probability of 1/36.	What is the probability of rolling two six sided dice and obtaining a 6 on each die
7786	Just like the mean value, the median also represents the location of a set of numerical data by means of a single number. Roughly speaking, the median is the value that splits the individual data into two halves: the (approximately) 50% largest and 50% lowest data in the collective.	What is the use of median in statistics
8393	Predictive modeling is a form of artificial intelligence that uses data mining and probability to forecast or estimate more granular, specific outcomes. For example, predictive modeling could help identify customers who are likely to purchase our new One AI software over the next 90 days.	Is predictive modeling AI
4301	Example 1: Fair Dice Roll The number of desired outcomes is 3 (rolling a 2, 4, or 6), and there are 6 outcomes in total. The a priori probability for this example is calculated as follows: A priori probability = 3 / 6 = 50%. Therefore, the a priori probability of rolling a 2, 4, or 6 is 50%.	How do you calculate a priori probability
1231	Pre-Interview PreparationDevelop a deep knowledge of data structures. You should understand and be able to talk about different data structures and their strengths, weaknesses, and how they compare to each other.  Understand Big O notation.  Know the major sorting algorithms.	How do I start preparing for data structures and algorithms
7917	Additive smoothing plays an important role in Naive Bayes classification, as long as not all events were observed at least ones. In this case of having at least one event with no observation, the probability for this event is absolut zero.  To prevent this problem, addative smoothing is used.	How does additive smoothing pseudo counts play a role in the context of Naive Bayes classifiers
4029	SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs.	How SVM is used for classification
1696	The steps are:Clean the data by removing outliers and treating missing data.Identify a parametric or nonparametric predictive modeling approach to use.Preprocess the data into a form suitable for the chosen modeling algorithm.Specify a subset of the data to be used for training the model.More items	How do you make a predictive model
8528	Non negative matrix factorization only takes positive values as input while SVD can take both positive and negative values.  SVD and NMF are both matrix decomposition techniques but they are very different and are generally used for different purposes. SVD helps in giving Eigen vectors of the input matrix.	What is the difference between non negative matrix factorization and singular value decomposition
3938	It's more of an approach than a process. Predictive analytics and machine learning go hand-in-hand, as predictive models typically include a machine learning algorithm. These models can be trained over time to respond to new data or values, delivering the results the business needs.	Is predictive modeling machine learning
338	Both phrases are grammatically correct and meaningful, so the difference is a matter of style. “In” is more specific than “and”. A glove with a hand in it functions in close conformance with the hand, while a glove on a shelf does not.	Is it working hand in glove or hand and glove
3150	There are ways, however, to try to maintain objectivity and avoid bias with qualitative data analysis:Use multiple people to code the data.  Have participants review your results.  Verify with more data sources.  Check for alternative explanations.  Review findings with peers.	How can bias be reduced in a study
5922	The Subfields of Artificial IntelligenceMachine Learning. Machine learning refers to the ability of a computer system to use data to learn automatically, predict, act, and explain the decisions it makes.  Deduction and Reasoning Systems.  Robotics and Motion.  Knowledge Representation.  Image and Voice Recognition.  Other Fields.	What are the fields in artificial intelligence
8065	Standard deviation is an important measure of spread or dispersion. It tells us how far, on average the results are from the mean. Therefore if the standard deviation is small, then this tells us that the results are close to the mean, whereas if the standard deviation is large, then the results are more spread out.	How do you compare mean and standard deviation
7714	The perceptron is a mathematical model of a biological neuron. While in actual neurons the dendrite receives electrical signals from the axons of other neurons, in the perceptron these electrical signals are represented as numerical values.	What is the difference between neuron and Perceptron
544	Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data.  Deep learning allows machines to solve complex problems even when using a data set that is very diverse, unstructured and inter-connected.	What exactly is deep learning
280	Binning method is used to smoothing data or to handle noisy data. In this method, the data is first sorted and then the sorted values are distributed into a number of buckets or bins. As binning methods consult the neighborhood of values, they perform local smoothing.	How binning can handle noisy data
945	In AI, the study on perception is mostly focused on the reproduction of human perception, especially on the perception of aural and visual signals. However, this is not necessarily the case since the perception mechanism of a computer system does not have to be identical to that of a human being.	What is the perception of AI
1894	Naive bayes is a Generative model whereas Logistic Regression is a Discriminative model . Generative model is based on the joint probability, p( x, y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(y | x), and then picking the most likely label y.	Is logistic regression a discriminative or generative model Why
427	Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable).	What is stochastic gradient descent used for
7918	Sample size measures the number of individual samples measured or observations used in a survey or experiment. For example, if you test 100 samples of soil for evidence of acid rain, your sample size is 100. If an online survey returned 30,500 completed questionnaires, your sample size is 30,500.	What is sample size example
4141	To convert a transfer function into state equations in phase variable form, we first convert the transfer function to a differential equation by cross-multiplying and taking the inverse Laplace transform, assuming zero initial conditions.	How do you get the state space model from transfer function
999	It allows researchers to determine the strength and direction of a relationship so that later studies can narrow the findings down and, if possible, determine causation experimentally. Correlation research only uncovers a relationship; it cannot provide a conclusive reason for why there's a relationship.	What are the advantages of correlation
731	Network representation learning has been recently proposed as a new learning paradigm to embed network vertices into a low-dimensional vector space, by preserving network topology structure, vertex content, and other side information.	What is network representation learning
1635	Conclusion. Human intelligence revolves around adapting to the environment using a combination of several cognitive processes. The field of Artificial intelligence focuses on designing machines that can mimic human behavior. However, AI researchers are able to go as far as implementing Weak AI, but not the Strong AI.	What is the difference between human intelligence and computer intelligence
7882	Segmentation methods divide a digital image into (usually small) groups of connected pixels. Each group (aka segment, or image-object) has a unique numeric ID (e.g., 67897) in the resulting raster (aka partition).  In contrast, classification methods assign a class to each element, be it individual pixels or segments.	What is the difference between classification and segmentation
4971	In Short: If your Father and Mother are working in Government as a class 2 employee or above, then you belong to OBC Creamy layer. If only one of your parent is class 2 employee and other is below class 2 or unemployed then you fall under OBC Non Creamy Layer.	Do you belong to non creamy layer ?*
147	The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.  Due to this reason, during the backpropogation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated.	Why is ReLU a good activation function
6090	Q-Learning is a basic form of Reinforcement Learning which uses Q-values (also called action values) to iteratively improve the behavior of the learning agent. Q-Values or Action-Values: Q-values are defined for states and actions. is an estimation of how good is it to take the action at the state .	What is Q value in reinforcement learning
6083	The defining characteristic of fifth generation computers (FGC) is that they are to be knowledge processing machines in contrast to being merely data processing machines as is the case with most of the present generation computers.	Which generation uses the concept of artificial intelligence
825	Although many types of neural network models have been developed to solve different problems, the most widely used model by far for time series forecasting has been the feedforward neural network.	Which type of neural networks can be used for time series data
3611	An SLI (service level indicator) measures compliance with an SLO (service level objective). So, for example, if your SLA specifies that your systems will be available 99.95% of the time, your SLO is likely 99.95% uptime and your SLI is the actual measurement of your uptime. Maybe it's 99.96%. Maybe 99.99%.	How is SLI calculated
829	A cross-sectional study involves looking at data from a population at one specific point in time.  Cross-sectional studies are observational in nature and are known as descriptive research, not causal or relational, meaning that you can't use them to determine the cause of something, such as a disease.	What is a cross sectional study in statistics
8429	Normality is not the only “usual” assumption. We also usually assume that the residuals have the same distribution for all values of the explanatory variables. A linear regression requires residuals to be normally distributed.  You need assumptions about the distribution of the residuals in order to make inferences.	Why linear model needs to assume the residual is normally distributed
4290	According to SAS, predictive analytics is “the use of data, statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data.  In short, predictive intelligence drives marketing decisions.”	Do predictive analytics drive more informed decisions
260	In a histogram, the total range of data set (i.e from minimum value to maximum value) is divided into 8 to 15 equal parts. These equal parts are known as bins or class intervals. Each and every observation (or value) in the data set is placed in the appropriate bin.	What is binning in histogram
6810	Advantages of Recurrent Neural Network It is useful in time series prediction only because of the feature to remember previous inputs as well. This is called Long Short Term Memory. Recurrent neural network are even used with convolutional layers to extend the effective pixel neighborhood.	What are recurrent neural networks good for
4361	Evaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model.	What is evaluation metrics in machine learning
3994	Two types of statistical methods are used in analyzing data: descriptive statistics and inferential statistics. Descriptive statistics are used to synopsize data from a sample exercising the mean or standard deviation. Inferential statistics are used when data is viewed as a subclass of a specific population.	What are the different types of statistics used in research
7276	Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subtopic of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.	What is NLU in Machine Learning
4221	A clinical trial is a randomized controlled trial only when participants are randomly allocated to the group receiving the treatment and a control group. What participants are allocated among groups receiving different treatments the clinical trial is simply called a randomized trial.	What is the difference between a randomized controlled trial and a randomized clinical trial
1325	Here are some important ones used in deep learning architectures:Multilayer Perceptron Neural Network (MLPNN)  Backpropagation.  Convolutional Neural Network (CNN)  Recurrent Neural Network (RNN)  Long Short-Term Memory (LSTM)  Generative Adversarial Network (GAN)  Restricted Boltzmann Machine (RBM)  Deep Belief Network (DBN)	What are the algorithms used in deep learning
460	The 'd' means a Δ in the limit approaching zero. Basically the slope is approximately Δy/Δx but if you let Δx approach zero, you reach the exactly slope which is then dy/dx.	What does the D stand for in differentiation
2691	Naive Bayes is a linear classifier Naive Bayes leads to a linear decision boundary in many common cases.  The red decision line indicates the decision boundary where P(y=1|x)=P(y=2|x).	Is naive Bayes a linear classifier
8058	The notation for the uniform distribution is X ~ U(a, b) where a = the lowest value of x and b = the highest value of x. The probability density function is f(x)=1b−a f ( x ) = 1 b − a for a ≤ x ≤ b. For this example, X ~ U(0, 23) and f(x)=123−0 f ( x ) = 1 23 − 0 for 0 ≤ X ≤ 23.	How do you calculate uniform distribution
675	The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. The inverted file may be the database file itself, rather than its index.	What are the uses of an inverted index
153	In statistics, latent variables (from Latin: present participle of lateo (“lie hidden”), as opposed to observable variables) are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured).	What is latent variable analysis
3800	More precisely, it is a measure of the average distance between the values of the data in the set and the mean. A low standard deviation indicates that the data points tend to be very close to the mean; a high standard deviation indicates that the data points are spread out over a large range of values.	How do you interpret the standard deviation
6763	Ordinal logistic regression (often just called 'ordinal regression') is used to predict an ordinal dependent variable given one or more independent variables.  As with other types of regression, ordinal regression can also use interactions between independent variables to predict the dependent variable.	Can regression be used for ordinal data
7603	In game theory, minimax is a decision rule used to minimize the worst-case potential loss; in other words, a player considers all of the best opponent responses to his strategies, and selects the strategy such that the opponent's best strategy gives a payoff as large as possible.	What is Minimax rule
362	The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. The lasso does this by imposing a constraint on the model parameters that causes regression coefficients for some variables to shrink toward zero.	What is the purpose of Lasso regression
4986	In statistics: Numerical measures. The range, the difference between the largest value and the smallest value, is the simplest measure of variability in the data. The range is determined by only the two extreme data values.	What is range in statistics used for
5719	The Empirical Rule states that 99.7% of data observed following a normal distribution lies within 3 standard deviations of the mean. Under this rule, 68% of the data falls within one standard deviation, 95% percent within two standard deviations, and 99.7% within three standard deviations from the mean.	What does it mean to be 3 standard deviations away from the mean
5304	Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable).	When we should use multiple linear regression
6412	Data that can only take certain values. For example: the number of students in a class (you can't have half a student). Discrete Data is not Continuous Data. See: Continuous Data.	What is discrete data with example
231	Data drift is the sum of data changes — think mobile interactions, sensor logs and web clickstreams — that started life as well-meaning business tweaks or system updates, as CMSWire contributor, Girish Pancha, explains in greater detail here.	What is the drift in data collection
3734	Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.	What is a Word2Vec model
66	"StepsStep 1: For each (x,y) point calculate x2 and xy.Step 2: Sum all x, y, x2 and xy, which gives us Σx, Σy, Σx2 and Σxy (Σ means ""sum up"")Step 3: Calculate Slope m:m = N Σ(xy) − Σx Σy N Σ(x2) − (Σx)2Step 4: Calculate Intercept b:b = Σy − m Σx N.Step 5: Assemble the equation of a line."	How do you determine the least squares regression line
4919	1 plural also banditti\ ban-​ˈdi-​tē \ : an outlaw who lives by plunder especially : a member of a band of marauders. 2 : robber. 3 : an enemy plane.	What is the meaning of bandits
8237	Likelihood is the chance that the reality you've hypothesized could have produced the particular data you got. Likelihood: The probability of data given a hypothesis. However Probability is the chance that the reality you're considering is true, given the data you have.	Why likelihood is not a probability
910	0:404:05Suggested clip · 108 secondsHow to interpret a survival plot - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret survival analysis results
1137	The covariance is defined as the mean value of this product, calculated using each pair of data points xi and yi.  If the covariance is zero, then the cases in which the product was positive were offset by those in which it was negative, and there is no linear relationship between the two random variables.	What does it mean when the covariance is 0
5365	The various methods used for dimensionality reduction include: Principal Component Analysis (PCA) Linear Discriminant Analysis (LDA) Generalized Discriminant Analysis (GDA)	What are some linear methods for dimensionality reduction
6762	Multiple regression models forecast a variable using a linear combination of predictors, whereas autoregressive models use a combination of past values of the variable.  These concepts and techniques are used by technical analysts to forecast security prices.	What is difference between linear regression and autoregressive model in time series analysis
1665	Analysis of variance (ANOVA) is a statistical technique that is used to check if the means of two or more groups are significantly different from each other. ANOVA checks the impact of one or more factors by comparing the means of different samples.  Another measure to compare the samples is called a t-test.	How does Anova work in statistics
119	Predictive modeling, also called predictive analytics, is a mathematical process that seeks to predict future events or outcomes by analyzing patterns that are likely to forecast future results.	What is predictive data modeling
2978	The idea behind the chi-square goodness-of-fit test is to see if the sample comes from the population with the claimed distribution.  Only when the sum is large is the a reason to question the distribution. Therefore, the chi-square goodness-of-fit test is always a right tail test.	Why is goodness of fit test for Chi Square distribution always a right tailed test
6911	Gradient descent is an optimization algorithm that finds the optimal weights (a,b) that reduces prediction error. Step 2: Calculate the gradient i.e. change in SSE when the weights (a & b) are changed by a very small value from their original randomly initialized value.	What are the steps for using a gradient descent algorithm
156	Sequence classification methods can be organized into three categories: (1) feature-based classification, which transforms a sequence into a feature vector and then applies conventional classification methods; (2) sequence distance–based classification, where the distance function that measures the similarity between	What are the methods of classification
713	Mentor: Well, if the line is a good fit for the data then the residual plot will be random. However, if the line is a bad fit for the data then the plot of the residuals will have a pattern.	Do residual plots determine if a function is a good fit
3504	Why is the derivative of the LSTM cell state w.r.t. to the previous cell state equal to the forget gate?  The chain rule would extend for longer and we'd have more derivative terms in there.	Why is the derivative of the LSTM cell state w r t to the previous cell state equal to the forget gate
8087	Connected components labeling scans an image and groups its pixels into components based on pixel connectivity, i.e. all pixels in a connected component share similar pixel intensity values and are in some way connected with each other.	What is labeling in image processing
40	Regularization is a technique used for tuning the function by adding an additional penalty term in the error function. The additional term controls the excessively fluctuating function such that the coefficients don't take extreme values.	Why is regularization used
2590	Multinomial logistic regression is used to predict categorical placement in or the probability of category membership on a dependent variable based on multiple independent variables. The independent variables can be either dichotomous (i.e., binary) or continuous (i.e., interval or ratio in scale).	What is meant by multinomial logistic regression
4745	A sampling frame is a list of all the items in your population. It's a complete list of everyone or everything you want to study.  For example, the population could be “People who live in Jacksonville, Florida.” The frame would name ALL of those people, from Adrian Abba to Felicity Zappa.	What is sampling frame and examples
2141	Some of the popular techniques are: Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept). Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data.	What are anomaly detection methods
780	Loss is nothing but a prediction error of Neural Net. And the method to calculate the loss is called Loss Function. In simple words, the Loss is used to calculate the gradients. And gradients are used to update the weights of the Neural Net. This is how a Neural Net is trained.	What is training loss in neural network
1246	Inductive Learning is where we are given examples of a function in the form of data (x) and the output of the function (f(x)). The goal of inductive learning is to learn the function for new data (x). Classification: when the function being learned is discrete. Regression: when the function being learned is continuous.	What is inductive learning in machine learning
418	The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean.	What is the mean of standard normal distribution
1796	"The consequent of a conditional statement is the part that usually follows ""then"". The part that usually follows ""if"" is called the ""antecedent"".  To affirm the consequent is, of course, to claim that the consequent is true. Thus, affirming the consequent in the example would be to claim that I have logic class."	What is the difference between affirming the antecedent and denying the consequent
3593	After completing Calculus I and II, you may continue to Calculus III, Linear Algebra, and Differential Equations. These three may be taken in any order that fits your schedule, but the listed order is most common.	Is Linear Algebra After calculus
456	Syllabus:Basic Data Structures: Arrays, Strings, Stacks, Queues.Asymptotic analysis (Big-O notation)Basic math operations (addition, subtraction, multiplication, division, exponentiation)Sqrt(n) primality testing.Euclid's GCD Algorithm.Basic Recursion.Greedy Algorithms.Basic Dynamic Programming.More items	What are the topics to be covered in algorithms and data structures
287	In order to assist the kernel with processing the image, padding is added to the frame of the image to allow for more space for the kernel to cover the image. Adding padding to an image processed by a CNN allows for more accurate analysis of images.	Why is padding needed in image processing
1629	A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated).	What are scatter plot examples
6299	These are all examples of what are called Continuous Time Markov Chains, and they can each be modeled by representing the passage from one state to another as a Poisson Process . On it's own, a Poisson distribution describes the probability of an event occurring after a given amount of time, t.	What is the relationship between Markov chains and Poisson processes
699	Do you know how to choose the right machine learning algorithm among 7 different types?1-Categorize the problem.  2-Understand Your Data.  Analyze the Data.  Process the data.  Transform the data.  3-Find the available algorithms.  4-Implement machine learning algorithms.  5-Optimize hyperparameters.More items	How do I know which ML model to use
2341	5 Techniques to Prevent Overfitting in Neural NetworksSimplifying The Model. The first step when dealing with overfitting is to decrease the complexity of the model.  Early Stopping. Early stopping is a form of regularization while training a model with an iterative method, such as gradient descent.  Use Data Augmentation.  Use Regularization.  Use Dropouts.	What steps can we take to prevent Overfitting in a neural network
2099	Time-series data is a set of observations collected at usually discrete and equally spaced time intervals.  Cross-sectional data are observations that come from different individuals or groups at a single point in time.	What is the difference between cross sectional data and time series data
1082	It is easier to reject the null hypothesis with a one-tailed than with a two-tailed test as long as the effect is in the specified direction. Therefore, one-tailed tests have lower Type II error rates and more power than do two-tailed tests.	Is it easier to reject the null hypothesis with a one tailed or two tailed test
4262	A posterior probability, in Bayesian statistics, is the revised or updated probability of an event occurring after taking into consideration new information.  In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.	What is a posterior probability in statistics
2675	Deep Learning tries to find out the optimal set of features on your own and generate the output based on those features. So, in a nutshell, we can say that deep learning does not require feature selection. It will automatically find out the optimal set of features.	Is feature selection necessary for deep learning
1354	Multiple linear regression requires at least two independent variables, which can be nominal, ordinal, or interval/ratio level variables. A rule of thumb for the sample size is that regression analysis requires at least 20 cases per independent variable in the analysis.	What type of variable must the criterion be in a multiple linear regression
1628	This ability to access offline learning means employees can still read a how-to or watch a video explaining a task without a connection. That prevents learners from having to wait to get back to their office or home before they can find the information they need.	What is the importance of offline learning
3745	Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression.	Can logistic regression be used for multiclass classification problems a yes b no
3048	Statistical independence is a concept in probability theory. Two events A and B are statistical independent if and only if their joint probability can be factorized into their marginal probabilities, i.e., P(A ∩ B) = P(A)P(B).  The concept can be generalized to more than two events.	What does statistically independent mean
7087	A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model.  The rows represent the predicted values of the target variable.	What is confusion matrix in machine learning
5657	Descriptive statistics describe what is going on in a population or data set. Inferential statistics, by contrast, allow scientists to take findings from a sample group and generalize them to a larger population. The two types of statistics have some important differences.	When should you use descriptive and inferential statistics
2185	In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms.	What is evolutionary computing in artificial intelligence
1212	A Gentle Introduction to the Rectified Linear Unit (ReLU) In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input.	What is the use of ReLU in CNN
930	List of applicationsOptical character recognition.Handwriting recognition.Speech recognition.Face recognition.Artificial creativity.Computer vision.Virtual reality.Image processing.More items	What are some applications of artificial intelligence
2215	Inferential statistics are often used to compare the differences between the treatment groups. Inferential statistics use measurements from the sample of subjects in the experiment to compare the treatment groups and make generalizations about the larger population of subjects.	Why are inferential statistical tests useful
2619	SEM uses latent variables to account for measurement error. Latent Variables. A latent variable is a hypothetical construct that is invoked to explain observed covariation in behavior. Examples in psychology include intelligence (a.k.a. cognitive ability), Type A personality, and depression.	What is a latent variable in SEM
5143	A moving average term in a time series model is a past error (multiplied by a coefficient). Let w t ∼ i i d N ( 0 , σ w 2 ) , meaning that the wt are identically, independently distributed, each with a normal distribution having mean 0 and the same variance.	What is moving average model in time series
4516	8 Methods to Boost the Accuracy of a ModelAdd more data. Having more data is always a good idea.  Treat missing and Outlier values.  Feature Engineering.  Feature Selection.  Multiple algorithms.  Algorithm Tuning.  Ensemble methods.	How can you increase the accuracy of a data set
587	Some common types of sampling bias include self-selection, non-response, undercoverage, survivorship, pre-screening or advertising, and healthy user bias.	What are the types of sampling bias
18	The Implicit Association Test (IAT) measures the strength of associations between concepts (e.g., black people, gay people) and evaluations (e.g., good, bad) or stereotypes (e.g., athletic, clumsy). The main idea is that making a response is easier when closely related items share the same response key.	How does the implicit bias test work
173	1 Answer. The card token is valid for a few minutes (usually up to 10). What Stripe recommends in that case is to use the token now to create a customer via the API first to save its card and then let your background job handle the charge part after the fact.	How long do stripe tokens last
4238	Factor Analysis (FA) is an exploratory technique applied to a set of outcome variables that seeks to find the underlying factors (or subsets of variables) from which the observed variables were generated.	What is factor analysis in multivariate analysis
273	How to read a stock chartIdentify the trend line. This is that blue line you see every time you hear about a stock—it's either going up or down right?  Look for lines of support and resistance.  Know when dividends and stock splits occur.  Understand historic trading volumes.	How do you Analyse a stock chart
5892	Saying that the sample mean is an unbiased estimate of the population mean simply means that there is no systematic distortion that will tend to make it either overestimate or underestimate the population parameter. We run into a problem when we work with the variance, although it is a problem that is easily fixed.	What is an unbiased estimate of the population variance
187	To calculate the variance follow these steps: Work out the Mean (the simple average of the numbers) Then for each number: subtract the Mean and square the result (the squared difference). Then work out the average of those squared differences.	How do you find the mean and variance of a normal distribution
1375	The coefficient of variation (CV) is a measure of relative variability. It is the ratio of the standard deviation to the mean (average). For example, the expression “The standard deviation is 15% of the mean” is a CV.	What is the coefficient of variation in a normal distribution
558	If we have an irreducible Markov chain, this means that the chain is aperiodic. Since the number 1 is co-prime to every integer, any state with a self-transition is aperiodic. Consider a finite irreducible Markov chain Xn: If there is a self-transition in the chain (pii>0 for some i), then the chain is aperiodic.	What is aperiodic in Markov chain
2153	The Loss Function is one of the important components of Neural Networks. Loss is nothing but a prediction error of Neural Net. And the method to calculate the loss is called Loss Function. In simple words, the Loss is used to calculate the gradients. And gradients are used to update the weights of the Neural Net.	What is loss in a neural network
3889	One way to measure multicollinearity is the variance inflation factor (VIF), which assesses how much the variance of an estimated regression coefficient increases if your predictors are correlated. If no factors are correlated, the VIFs will all be 1.	How do you calculate Multicollinearity
1686	11 Applications of Artificial Intelligence in Business:Chatbots:  Artificial Intelligence in eCommerce:  AI to Improve Workplace Communication:  Human Resource Management:  AI in Healthcare:  Intelligent Cybersecurity:  Artificial Intelligence in Logistics and Supply Chain:  Sports betting Industry:More items•	What are applications of artificial intelligence that are in use today
5831	It repetitively leverages the patterns in residuals, strengthens the model with weak predictions, and make it better. By combining the advantages from both random forest and gradient boosting, XGBoost gave the a prediction error ten times lower than boosting or random forest in my case.	Why is XGBoost better than random forest
583	An example of a nonlinear classifier is kNN.  If a problem is nonlinear and its class boundaries cannot be approximated well with linear hyperplanes, then nonlinear classifiers are often more accurate than linear classifiers. If a problem is linear, it is best to use a simpler linear classifier.	What is a non linear classifier
678	Linear mixed models (sometimes called “multilevel models” or “hierarchical models”, depending on the context) are a type of regression model that take into account both (1) variation that is explained by the independent variables of interest (like lm() ) – fixed effects, and (2) variation that is not explained by the	How do mixed models work
4432	Linear regression is called 'Linear regression' not because the x's or the dependent variables are linear with respect to the y or the independent variable but because the parameters or the thetas are.	Why is it called linear regression
601	Use simple logistic regression when you have one nominal variable and one measurement variable, and you want to know whether variation in the measurement variable causes variation in the nominal variable.	When should I use logistic regression
1298	"Predictive modeling is the process of using known results to create, process, and validate a model that can be used to forecast future outcomes. It is a tool used in predictive analytics, a data mining technique that attempts to answer the question ""what might possibly happen in the future?"""	How does predictive modeling work
7169	Probability limits are used when the parameter is considered as the realization of a random variable with given prior distribution.	What is a probability limit
4045	Advertisements. Interpolation search is an improved variant of binary search. This search algorithm works on the probing position of the required value. For this algorithm to work properly, the data collection should be in a sorted form and equally distributed.	What is interpolation search in data structure
276	K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970's as a non-parametric technique.	What is the use of KNN algorithm
5901	We use many algorithms such as Naïve Bayes, Decision trees, SVM, Random forest classifier, KNN, and logistic regression for classification.	Which algorithm is used for multinomial classification
6653	The Likelihood-Ratio test (sometimes called the likelihood-ratio chi-squared test) is a hypothesis test that helps you choose the “best” model between two nested models.  Model Two has two predictor variables (age,sex). It is “nested” within model one because it has just two of the predictor variables (age, sex).	What is likelihood ratio in Chi Square
1616	Serial dependence refers to the notion that returns evolve nonrandomly; that is, they are correlated with their prior values. One variation of serial dependence is called mean reversion. With mean reversion, returns revert to an average value or asset prices revert to an equilibrium value.	What does serial dependence mean
2966	One of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). ResNets refer to neural networks where skip connections or residual connections are part of the network architecture.	How vanishing gradient problem can be solved
914	In probability theory, an experiment or trial (see below) is any procedure that can be infinitely repeated and has a well-defined set of possible outcomes, known as the sample space. An experiment is said to be random if it has more than one possible outcome, and deterministic if it has only one.	What is experiment in statistics and probability
1244	7:0814:24Suggested clip · 84 secondsBasic Inference in Bayesian Networks - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you solve Bayesian networks
908	Performance bottlenecks can lead an otherwise functional computer or server to slow down to a crawl. The term “bottleneck” refers to both an overloaded network and the state of a computing device in which one component is unable to keep pace with the rest of the system, thus slowing overall performance.	What is bottleneck in performance testing
5941	n essence, the kappa statistic is a measure of how closely the instances classified by the machine learning classifier matched the data labeled as ground truth, controlling for the accuracy of a random classifier as measured by the expected accuracy.	What is Kappa in classification
2828	Automatic Document Classification Techniques Include:Expectation maximization (EM)Naive Bayes classifier.Instantaneously trained neural networks.Latent semantic indexing.Support vector machines (SVM)Artificial neural network.K-nearest neighbour algorithms.Decision trees such as ID3 or C4.More items•	How do you classify a document in machine learning
2510	The log likelihood This is important because it ensures that the maximum value of the log of the probability occurs at the same point as the original probability function. Therefore we can work with the simpler log-likelihood instead of the original likelihood.	Why do we take log of likelihood
4439	False confidence in stepwise results The standard errors of the coefficient estimates are underestimated, which makes the confidence intervals too narrow, the t statistics too high, and the p values too low—which leads to overfitting and creates a false confidence in the final model.	Why is stepwise selection bad
6846	Below are the methods to convert a categorical (string) input to numerical nature:Label Encoder: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables).  Convert numeric bins to number: Let's say, bins of a continuous variable are available in the data set (shown below).	What are the methods for transforming categorical variables
6295	Hierarchical SVMs refer to those methods that decompose the training tasks according to the structure of the taxonomy [4][5][6][10][17][19]. That is, an SVM model is trained to distinguish only among those categories with the same parent node in the taxonomy tree.	What is a Hierarchical Support vector Machine SVM
1322	Heterogeneity in statistics means that your populations, samples or results are different. It is the opposite of homogeneity, which means that the population/data/results are the same. A heterogeneous population or sample is one where every member has a different value for the characteristic you're interested in.	What is homogeneous and heterogeneous in statistics
200	= e−(λ+µ) z!  The above computation establishes that the sum of two independent Poisson distributed random variables, with mean values λ and µ, also has Poisson distribution of mean λ + µ. We can easily extend the same derivation to the case of a finite sum of independent Poisson distributed random variables.	Is the sum of Poisson random variables Poisson
5487	Time Complexity and Space Complexity are two factors which determine which algorithm is better than the other. Time Complexity in a simple way means the amount of time an algorithm takes to run. Space complexity means the amount of space required by the algorithm.	On what basis will you consider algorithm A is better than Algorithm B
1462	R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.  For instance, small R-squared values are not always a problem, and high R-squared values are not necessarily good!	Is R squared goodness of fit
8560	For a normal distribution, the average deviation is somewhat less efficient than the standard deviation as a measure of scale, but this advantage quickly reverses for distributions with heavier tails.	What is the advantage of the standard deviation over the average deviation
5430	Interpolation is a statistical method by which related known values are used to estimate an unknown price or potential yield of a security. Interpolation is achieved by using other established values that are located in sequence with the unknown value. Interpolation is at root a simple mathematical concept.	What does interpolation mean
5404	An Artificial Neural Network is an information processing model that is inspired by the way biological nervous systems, such as the brain, process information. They are loosely modeled after the neuronal structure of the mamalian cerebral cortex but on much smaller scales.	What are artificial neural networks explain with a real example
3513	Any LTI filter with output and input can be represented by a difference equation in the form: If at least one of the is not null, the filter is recursive. If the are all zero, it is a non recursive filter usually called FIR (Finite Input Response) filter.  This happens both to recursive and non recursive filters.	Are recursive filters LTI system Why
1076	Advantages of Offline Training Faculty can easily judge the performance of each student during the class and can work on problem areas. Students who are trained offline usually tend to perform better than online training, if the course content remains the same. One of the reasons is peer's pressure and competition.	Why are offline classes better
4182	: the ratio of the frequency of a particular event in a statistical experiment to the total frequency.	What does the relative frequency mean
5426	Now, for the differences… The Mann-Whitney U is a very simple test that makes almost no assumptions about any underlying distribution.  Because the K-S test can assume interval or higher level data, it is a more powerful statistical test than the MW-U, assuming that assumption is valid.	What are the differences between the Kolmogorov Smirnov test and the Mann Whitney U test
1095	When you are controlling for a variable x1 in regression, you are trying to determine how the dependent variable (say, y) moves as a function of the other (independent) variables x2, …, xp in the regression model while holding your variable x1 constant.	What specifically are you doing when you control for a variable in a regression
979	Jaccard similarity is good for cases where duplication does not matter, cosine similarity is good for cases where duplication matters while analyzing text similarity. For two product descriptions, it will be better to use Jaccard similarity as repetition of a word does not reduce their similarity.	What is the advantage of using cosine similarity over Jaccard coefficient
1169	Overall, Sentiment analysis may involve the following types of classification algorithms:Linear Regression.Naive Bayes.Support Vector Machines.RNN derivatives LSTM and GRU.	Which algorithm is best for sentiment analysis
7561	Artificial intelligence (AI) is wide-ranging branch of computer science concerned with building smart machines capable of performing tasks that typically require human intelligence.	What is the branch of artificial intelligence
5697	Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).	What does regression explain
3609	Regression analysis is a powerful statistical method that allows you to examine the relationship between two or more variables of interest. While there are many types of regression analysis, at their core they all examine the influence of one or more independent variables on a dependent variable.	What is regression effect in data collection
141	The primary difference between classification and regression decision trees is that, the classification decision trees are built with unordered values with dependent variables. The regression decision trees take ordered values with continuous values.	What is the difference between classification tree and regression tree
662	“Kernel” is used due to set of mathematical functions used in Support Vector Machine provides the window to manipulate the data. So, Kernel Function generally transforms the training set of data so that a non-linear decision surface is able to transformed to a linear equation in a higher number of dimension spaces.	Why are kernels used in SVM
3706	Additive error is the error that is added to the true value and does not depend on the true value itself. In other words, the result of the measurement is considered as a sum of the true value and the additive error: where.	What is additive error
466	"The ith order statistic of a set of n elements is the ith smallest element. For example, the minimum of a set of elements is the first order statistic (i = 1), and the maximum is the nth order statistic (i = n). A median, informally, is the ""halfway point"" of the set."	What is the ith order statistic
6234	A noncorrelated (simple) subquery obtains its results independently of its containing (outer) statement. A correlated subquery requires values from its outer query in order to execute.	SQL What is the difference between a correlated and an uncorrelated subquery
280	To recap, Logistic regression is a binary classification method. It can be modelled as a function that can take in any number of inputs and constrain the output to be between 0 and 1. This means, we can think of Logistic Regression as a one-layer neural network.	Is logistic regression a neural network
126	Sentiment analysis is extremely useful in social media monitoring as it allows us to gain an overview of the wider public opinion behind certain topics. Social media monitoring tools like Brandwatch Analytics make that process quicker and easier than ever before, thanks to real-time monitoring capabilities.	Where can we use the sentiment analysis
4144	What you want is multi-label classification, so you will use Binary Cross-Entropy Loss or Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss.	What loss function will you use to measure multi label problems
7860	In statistics, linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression.	What type of variables are used in linear regression
7963	Now we'll check out the proven way to improve the performance(Speed and Accuracy both) of neural network models:Increase hidden Layers.  Change Activation function.  Change Activation function in Output layer.  Increase number of neurons.  Weight initialization.  More data.  Normalizing/Scaling data.More items•	How can neural networks be improved
1650	For example, Q-learning is an off-policy learner. On-policy methods attempt to evaluate or improve the policy that is used to make decisions. In contrast, off-policy methods evaluate or improve a policy different from that used to generate the data.11‏/04‏/2020	What is the difference between on policy and off policy
5221	7 Practical Guidelines for Accurate Statistical Model BuildingRemember that regression coefficients are marginal results.  Start with univariate descriptives and graphs.  Next, run bivariate descriptives, again including graphs.  Think about predictors in sets.  Model building and interpreting results go hand-in-hand.More items	How do you develop a statistical model
2520	A weather reporter is analyzing the high temperature forecasted for a series of dates versus the actual high temperature recorded on each date. A low standard deviation would show a reliable weather forecast. A class of students took a test in Language Arts.	What is standard deviation used for in real life
8456	It is often pointed out that when ANOVA is applied to just two groups, and when therefore one can calculate both a t-statistic and an F-statistic from the same data, it happens that the two are related by the simple formula: t2 = F.	What is the relationship between F statistic and T statistic
4519	Bayes' theorem, named after 18th-century British mathematician Thomas Bayes, is a mathematical formula for determining conditional probability. Conditional probability is the likelihood of an outcome occurring, based on a previous outcome occurring.	What is Bayes theorem in probability
163	Find all of your absolute errors, xi – x. Add them all up. Divide by the number of errors. For example, if you had 10 measurements, divide by 10.Mean Absolute Errorn = the number of errors,Σ = summation symbol (which means “add them all up”),|xi – x| = the absolute errors.	How do you calculate the mean absolute error
611	Used to test if different populations have the same proportion of individuals with some characteristic. Used to test whether a frequency distribution fits an expected distribution.	What is the purpose of the chi square test for homogeneity of proportions
607	Here is a step-by-step plan to improve your data structure and algorithm skills:Step 1: Understand Depth vs.  Step 2: Start the Depth-First Approach—make a list of core questions.  Step 3: Master each data structure.  Step 4: Spaced Repetition.  Step 5: Isolate techniques that are reused.  Step 6: Now, it's time for Breadth.More items•	How do you start learning data structures and algorithms
3047	2 Answers. By definition the probability density function is the derivative of the distribution function. But distribution function is an increasing function on R thus its derivative is always positive. Assume that probability density of X is -ve in the interval (a, b).	Is a probability density function always positive
2725	Each is essentially a component of the prior term. That is, machine learning is a subfield of artificial intelligence. Deep learning is a subfield of machine learning, and neural networks make up the backbone of deep learning algorithms.	Is neural network a part of machine learning
2310	Accuracy: The error between the real and measured value. Precision: The random spread of measured values around the average measured values. Resolution: The smallest to be distinguished magnitude from the measured value.	What is the difference between accuracy precision and resolution
1999	RMS stands for Root Mean Square and TRMS (True RMS) for True Root Mean Square. The TRMS instruments are much more accurate than the RMS when measuring AC current. This is why all the multimeters in PROMAX catalog have True RMS measurement capabilities.	What is the difference between RMS and True RMS meter
961	Class boundaries are the data values which separate classes. They are not part of the classes or the dataset. The lower class boundary of a class is defined as the average of the lower limit of the class in question and the upper limit of the previous class.	What are class boundaries in statistics
774	Accuracy: The number of correct predictions made divided by the total number of predictions made. We're going to predict the majority class associated with a particular node as True. i.e. use the larger value attribute from each node.	What is accuracy in decision tree
7637	Simply stated: the R2 value is simply the square of the correlation coefficient R . The correlation coefficient ( R ) of a model (say with variables x and y ) takes values between −1 and 1 . It describes how x and y are correlated.	What is the difference between r2 and correlation coefficient
1959	Discriminative learning refers to any classification learning process that classifies by using a model or estimate of the probability P(y\,\vert x) without reference to an explicit estimate of any of P(x), P(y, x), or P(x \vert \,y), where y is a class and x is a description of an object to be classified.	What is discriminative learning
8313	Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.	What is feature engineering in machine learning
7680	Maximizing the log likelihood is equivalent to minimizing the distance between two distributions, thus is equivalent to minimizing KL divergence, and then the cross entropy.  It's not just because optimizers are built to minimize functions, since you can easily minimize -likelihood.	Why is maximizing the log likelihood the same as minimizing the negative log likelihood
4391	The Normal Distribution has:mean = median = mode.symmetry about the center.50% of values less than the mean. and 50% greater than the mean.	How do you find the mean of a standard normal distribution
3858	For example, medical diagnosis, image processing, prediction, classification, learning association, regression etc. The intelligent systems built on machine learning algorithms have the capability to learn from past experience or historical data.	What is machine learning examples
6566	First, correlation measures the degree of relationship between two variables. Regression analysis is about how one variable affects another or what changes it triggers in the other. For more on variables and regression, check out our tutorial How to Include Dummy Variables into a Regression.	What is the difference between correlation analysis and regression analysis
4770	The median is usually preferred in these situations because the value of the mean can be distorted by the outliers. However, it will depend on how influential the outliers are. If they do not significantly distort the mean, using the mean as the measure of central tendency will usually be preferred.	Why is median preferred over mean
648	Rule-based machine learning approaches include learning classifier systems, association rule learning, artificial immune systems, and any other method that relies on a set of rules, each covering contextual knowledge.	Is machine learning rule based
7576	Serial correlation causes OLS to no longer be a minimum variance estimator. 3. Serial correlation causes the estimated variances of the regression coefficients to be biased, leading to unreliable hypothesis testing. The t-statistics will actually appear to be more significant than they really are.	What is the problem with serial correlation
6673	3.1. Coreference resolution (or anaphora) is an expression, the interpretation of which depends on another word or phrase presented earlier in the text (antecedent). For example, “Tom has a backache. He was injured.” Here the words “Tom” and “He” refer to the same entity.	How do coreference resolution anaphora resolution algorithms work
4364	The moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term.  The moving-average model should not be confused with the moving average, a distinct concept despite some similarities.	How does moving average model work
6229	One way that we calculate the predicted probability of such binary events (drop out or not drop out) is using logistic regression. Unlike regular regression, the outcome calculates the predicted probability of mutually exclusive event occuring based on multiple external factors.	What is predicted probability in logistic regression
331	The Linear Regression Equation The equation has the form Y= a + bX, where Y is the dependent variable (that's the variable that goes on the Y axis), X is the independent variable (i.e. it is plotted on the X axis), b is the slope of the line and a is the y-intercept.	How do you find the line of regression
1581	It enables private IP networks that use unregistered IP addresses to connect to the Internet. NAT operates on a router, usually connecting two networks together, and translates the private (not globally unique) addresses in the internal network into legal addresses, before packets are forwarded to another network.	What is NAT and why is it used
3043	The mean, or average, IQ is 100. Standard deviations, in most cases, are 15 points. The majority of the population, 68.26%, falls within one standard deviation of the mean (IQ 85-115).	What is IQ standard deviation
4329	where Ua is size m × n, Ub is size m × (m - n), and Σa is of size n × n. Then A = UaΣaVH is called the reduced SVD of the matrix A. In this context the SVD defined in Equation (1) is sometimes referred to as the full SVD for contrast. Notice that Ua is not unitary, but it does have orthogonal columns.	What is reduced SVD
1563	"""Bias"" in K-Pop is basically someone's most favorite member of an idol group. It is derived from the original way the word is used, to have a bias towards someone. So for example, if someone asks you ""Who is your bias?"", they're basically asking who your favorite K-Pop idol is of all time.  My bias from BTS is Yoongi!"	Who is your BTS bias meaning
8045	Chapter 1 introduced the dictionary and the inverted index as the central data structures in information retrieval (IR). The second more subtle advantage of compression is faster transfer of data from disk to memory.	What is index compression in information retrieval
579	An example of Multiple stage sampling by clusters – An organization intends to survey to analyze the performance of smartphones across Germany. They can divide the entire country's population into cities (clusters) and select cities with the highest population and also filter those using mobile devices.	What is cluster sampling example
172	Centroid is generally defined for a two dimensional object and pertains basically to the geometric centre of a body. It is more of shape dependent. Whereas, centre of mass is a point where the entire mass of a body can be assumed to be concentrated.  For 2d objects, Centroid and COM will be the same point.	What is the difference between a centroid and a centre of mass
5913	Principle Component Analysis (PCA) is a common feature extraction method in data science. Technically, PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues and then uses those to project the data into a new subspace of equal or less dimensions.	How is PCA used in feature extraction
7440	Quasi-experiments usually select only a certain range of values of an independent variable, while a typical correlational study measures all available values of an independent variable.	How does a correlational experiment differ from a quasi experiment
8603	Classification Error. The classification error Ei of an individual program i depends on the number of samples incorrectly classified (false positives plus false negatives) and is evaluated by the formula: where f is the number of sample cases incorrectly classified, and n is the total number of sample cases.	What is classification error
4889	All medical tests can be resulted in false positive and false negative errors.  A false positive can lead to unnecessary treatment and a false negative can lead to a false diagnostic, which is very serious since a disease has been ignored.	Which is more dangerous false positive and false negative
7823	Artificial general intelligence (AGI) is the representation of generalized human cognitive abilities in software so that, faced with an unfamiliar task, the AI system could find a solution.  IBM's Watson supercomputer, expert systems and the self-driving car are all examples of weak or narrow AI.	What is Artificial General Intelligence examples
560	Let's return to our example comparing the mean of a sample to a given value x using a t-test. Our null hypothesis is that the mean is equal to x. A one-tailed test will test either if the mean is significantly greater than x or if the mean is significantly less than x, but not both.	What is the difference between null hypothesis and alternative hypothesis one tailed tests and two tailed test
8673	"Time efficiency - a measure of amount of time for an algorithm to execute. Space efficiency - a measure of the amount of memory needed for an algorithm to execute. Asymptotic dominance - comparison of cost functions when n is large. That is, g asymptotically dominates f if g dominates f for all ""large"" values of n."	What is the asymptotic efficiency of an algorithm
4343	Moment generating functions are a way to find moments like the mean(μ) and the variance(σ2). They are an alternative way to represent a probability distribution with a simple one-variable function.	What is the meaning of moment generating function
4320	fastText is another word embedding method that is an extension of the word2vec model. Instead of learning vectors for words directly, fastText represents each word as an n-gram of characters.  This helps capture the meaning of shorter words and allows the embeddings to understand suffixes and prefixes.	What is FastText embedding
8022	ReLU is linear (identity) for all positive values, and zero for all negative values. This means that:  Since ReLU is zero for all negative inputs, it's likely for any given unit to not activate at all. This is often desirable (see below).	Why is ReLu activation so popular in deep learning
3195	Returns the inverse, or critical value, of the cumulative standard normal distribution. This function computes the critical value so that the cumulative distribution is greater than or equal to a pre-specified value.	What is the inverse of the standard normal cumulative distribution
695	N-grams of texts are extensively used in text mining and natural language processing tasks. They are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios).	What is ngram in NLP
5291	Leonard Savage's decision theory, as presented in his (1954) The Foundations of Statistics, is without a doubt the best-known normative theory of choice under uncertainty, in particular within economics and the decision sciences.	Who proposed the decision theory
7793	Last Updated on Decem. Cross-entropy is commonly used in machine learning as a loss function. Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions.	What is entropy and cross entropy
8180	The fundamental difference between the two correlation coefficients is that the Pearson coefficient works with a linear relationship between the two variables whereas the Spearman Coefficient works with monotonic relationships as well.	Why does rank correlation coefficient differ from Pearsonian correlation coefficient
838	There are two types of probability distribution which are used for different purposes and various types of the data generation process. Let us discuss now both the types along with its definition, formula and examples.	How many types of distribution are there
7942	Step 1: Learn the fundamental data structures and algorithms. First, pick a favorite language to focus on and stick with it.  Step 2: Learn advanced concepts, data structures, and algorithms.  Step 1+2: Practice.  Step 3: Lots of reading + writing.  Step 4: Contribute to open-source projects.  Step 5: Take a break.	What is the best way to learn algorithms
5095	Facebook Trending is a feature of the social network designed to show each user a list of topics that are spiking in popularity in updates, posts, and comments. Facebook Trending appears as a short list of keywords and phrases in a small module at the top right of the user's News Feed.	What does trending on Facebook mean
6747	Table 1Type of BiasHow to AvoidSelection bias• Select patients using rigorous criteria to avoid confounding results. Patients should originate from the same general population. Well designed, prospective studies help to avoid selection bias as outcome is unknown at time of enrollment.17 more rows	What is selection bias and how can you avoid it
6781	It is very much like the exponential distribution, with λ corresponding to 1/p, except that the geometric distribution is discrete while the exponential distribution is continuous.	Is exponential distribution discrete or continuous
6881	In the graph, the tangent line at c (derivative at c) is equal to the slope of [a,b] where a <>. The Mean Value Theorem is an extension of the Intermediate Value Theorem, stating that between the continuous interval [a,b], there must exist a point c where the tangent at f(c) is equal to the slope of the interval.	How do you find the value of C in mean value theorem
436	Page Content. ​Many texts are multimodal, where meaning is communicated through combinations of two or more modes. Modes include written language, spoken language, and patterns of meaning that are visual, audio, gestural, tactile and spatial.	What does it mean if you are multimodal
254	Sets can be used in calculated fields Sets can be used in calculated fields as if they were a field.  Or you can have the calculation return a specific value, or return another field instead, the main point is that they are not very different than normal dimensions in this respect.	Can we use sets in calculated fields
7372	Ordinal YouTubeStart of suggested clipEnd of suggested clip	How do you do regression on Likert scale data
139	ARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. This is one of the easiest and effective machine learning algorithm to performing time series forecasting.  In simple words, it performs regression in previous time step t-1 to predict t.	What is Arima in machine learning
7391	Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.	Why do we do batch normalization
146	There are many practical measures of randomness for a binary sequence.Specific tests for randomnessLinear congruential generator and Linear-feedback shift register.Generalized Fibonacci generator.Cryptographic generators.Quadratic congruential generator.Cellular automaton generators.Pseudorandom binary sequence.	How do you measure randomness
3289	Hidden Markov models (HMMs) have been extensively used in biological sequence analysis.  We especially focus on three types of HMMs: the profile-HMMs, pair-HMMs, and context-sensitive HMMs.	What are different Hidden Markov Models
25	A one-way ANOVA uses one independent variable, while a two-way ANOVA uses two independent variables. One-way ANOVA example As a crop researcher, you want to test the effect of three different fertilizer mixtures on crop yield.	What is a one way Anova example
6115	Algorithms can be difficult for some people. But I think if you learn a couple of basic ones, it will gradually get easier. But you just gotta do them. For some people, they are a little easier in the beginning.	Is algorithm hard to learn
2093	Types of testing strategiesAnalytical strategy.Model based strategy.Methodical strategy.Standards compliant or Process compliant strategy.Reactive strategy.Consultative strategy.Regression averse strategy.	What are the different testing strategies
3671	Hyperparameters are crucial as they control the overall behaviour of a machine learning model. The ultimate goal is to find an optimal combination of hyperparameters that minimizes a predefined loss function to give better results.	Why Hyperparameter tuning is important
3296	The critical region is the area that lies to the left of -1.645. If the z-value is less than -1.645 there we will reject the null hypothesis and accept the alternative hypothesis. If it is greater than -1.645, we will fail to reject the null hypothesis and say that the test was not statistically significant.	If z statistic is less than than z critical value then we fail to reject the null hypothesis
6371	In a nutshell, hierarchical linear modeling is used when you have nested data; hierarchical regression is used to add or remove variables from your model in multiple steps. Knowing the difference between these two seemingly similar terms can help you determine the most appropriate analysis for your study.	When would you use a hierarchical model
2116	Similarity is a numerical measure of how alike two data objects are, and dissimilarity is a numerical measure of how different two data objects are.  We go into more data mining in our data science bootcamp, have a look.	What is similarity and dissimilarity in data mining
8622	It is the task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups.	How is inter cluster similarity defined
1687	A series converges uniformly on if the sequence of partial sums defined by. (2) converges uniformly on . To test for uniform convergence, use Abel's uniform convergence test or the Weierstrass M-test.	What is uniform convergence series
139	The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.	What kind of activation function is ReLU
4371	In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a probability distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable.	What is meant by maximum likelihood estimation
1405	Randomization as a method of experimental control has been extensively used in human clinical trials and other biological experiments. It prevents the selection bias and insures against the accidental bias. It produces the comparable groups and eliminates the source of bias in treatment assignments.	Why is it important to randomise participants in a study
1099	The central limit theorem tells us that no matter what the distribution of the population is, the shape of the sampling distribution will approach normality as the sample size (N) increases.  Thus, as the sample size (N) increases the sampling error will decrease.	What happens to the sampling error as the sample size increases
199	Mixed models explicitly account for the correlations between repeated measurements within each patient.  Mixed models are called “mixed” because they generally contain both fixed and random effects.	What is mixed model repeated measures analysis
1022	The logit model uses something called the cumulative distribution function of the logistic distribution. The probit model uses something called the cumulative distribution function of the standard normal distribution to define f(∗). Both functions will take any number and rescale it to fall between 0 and 1.	What is the difference between logit and probit regression
8305	Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.	Why do we use Adam Optimizer
1195	5 Successful ExamplesSentiment Analysis Examples.Reputation Management - Social Media Monitoring - Brand Monitoring.Market Research, Competitor Analysis.Product Analytics.Customer Analysis.Customer Support.	What are the most popular application areas for sentiment analysis
2772	For a random variable yt, the unconditional mean is simply the expected value, E ( y t ) . In contrast, the conditional mean of yt is the expected value of yt given a conditioning set of variables, Ωt. A conditional mean model specifies a functional form for E ( y t | Ω t ) . .	What is a conditional mean in regression
6425	SVM tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data, while logistic regression does not, instead it can have different decision boundaries with different weights that are near the optimal point.	What are the advantages of support vector machines SVM compared with linear regression or logistic regression
7949	Bivariate analysis looks at two paired data sets, studying whether a relationship exists between them. Multivariate analysis uses two or more variables and analyzes which, if any, are correlated with a specific outcome. The goal in the latter case is to determine which variables influence or cause the outcome.	Which analysis is done when you have two dependent variables
879	Showing a transformation is linear using the definitionT(c→u+d→v)=cT(→u)+dT(→v)Overall, since our goal is to show that T(c→u+d→v)=cT(→u)+dT(→v), we will calculate one side of this equation and then the other, finally showing that they are equal.T(c→u+d→v)=cT(→u)+dT(→v)=we have shown that T(c→u+d→v)=cT(→u)+dT(→v). Thus, by definition, the transformation is linear. ◼	How do you prove a linear transformation is linear
1746	"2 Answers. Simply put because one level of your categorical feature (here location) become the reference group during dummy encoding for regression and is redundant. I am quoting form here ""A categorical variable of K categories, or levels, usually enters a regression as a sequence of K-1 dummy variables."	Why do we drop one dummy variable
249	Random forest is a supervised learning algorithm which is used for both classification as well as regression.  Similarly, random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting.	How does random forest predict
1413	A neural network is either a system software or hardware that works similar to the tasks performed by neurons of human brain. Neural networks include various technologies like deep learning, and machine learning as a part of Artificial Intelligence (AI).	Are neural networks intelligent
493	In cost-sensitive learning instead of each instance being either correctly or incorrectly classified, each class (or instance) is given a misclassification cost.	What is misclassification cost
2312	Use. Cluster sampling is typically used in market research. It's used when a researcher can't get information about the population as a whole, but they can get information about the clusters.  Cluster sampling is often more economical or more practical than stratified sampling or simple random sampling.	Why would you use cluster sampling
3972	Measures of Dispersion A measure of dispersion is a statistic that tells you how dispersed, or spread out, data values are. One simple measure of dispersion is the range, which is the difference between the greatest and least data values.	How we determine which statistical measures of central tendency and measures of dispersion is suitable
7319	The decision of which statistical test to use depends on the research design, the distribution of the data, and the type of variable.  In general, if the data is normally distributed, parametric tests should be used. If the data is non-normal, non-parametric tests should be used.	Which statistical test should I use
1122	ROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests.  In addition, the area under the ROC curve gives an idea about the benefit of using the test(s) in question.	When would you use a ROC curve
2001	Within an artificial neural network, a neuron is a mathematical function that model the functioning of a biological neuron. Typically, a neuron compute the weighted average of its input, and this sum is passed through a nonlinear function, often called activation function, such as the sigmoid.	What does a neuron compute in neural network
5344	A sample survey can be broadly defined as an exercise that involves collecting standardised data from a sample of study units (e.g., persons, households, businesses) designed to represent a larger population of units, in order to make quantitative inferences about the population.	What is sample survey method
7623	In Convolutional Neural Networks, Filters detect spatial patterns such as edges in an image by detecting the changes in intensity values of the image.	What are filters in neural networks
133	"In mathematics, proof by contrapositive, or proof by contraposition, is a rule of inference used in proofs, where one infers a conditional statement from its contrapositive. In other words, the conclusion ""if A, then B"" is inferred by constructing a proof of the claim ""if not B, then not A"" instead."	How do you prove Contrapositive
2636	How to calculate percentileRank the values in the data set in order from smallest to largest.Multiply k (percent) by n (total number of values in the data set).  If the index is not a round number, round it up (or down, if it's closer to the lower number) to the nearest whole number.Use your ranked data set to find your percentile.	How do you find the percentile of a data point
546	We use two well-known trained CNNs, GoogLeNet (Szegedy et al.  GoogLeNet has Inception Modules, which perform different sizes of convolutions and concatenate the filters for the next layer. AlexNet, on the other hand, has layers input provided by one previous layer instead of a filter concatenation.	What is AlexNet and GoogLeNet
7492	Median smoothers are widely used in image processing to clean images corrupted by noise. Median filters are particularly effective at removing outliers. Often referred to as “salt and pepper” noise, outliers are often present due to bit errors in transmission, or introduced during the signal acquisition stage.	What is the application of median filter
5349	For two numbers x and y, let x, a, y be a sequence of three numbers. If x, a, y is an arithmetic progression then 'a' is called arithmetic mean. If x, a, y is a geometric progression then 'a' is called geometric mean. If x, a, y form a harmonic progression then 'a' is called harmonic mean.	What is arithmetic mean geometric mean and harmonic mean
8448	Machine Learning: Reinforcement Learning — Markov Decision Processes.  A mathematical representation of a complex decision making process is “Markov Decision Processes” (MDP). MDP is defined by: A state S, which represents every state that one could be in, within a defined world.	What is MDP in machine learning
426	Bias allows you to shift the activation function by adding a constant (i.e. the given bias) to the input. Bias in Neural Networks can be thought of as analogous to the role of a constant in a linear function, whereby the line is effectively transposed by the constant value.	What is the importance of bias in neural network
4244	Model selection is the process of selecting one final machine learning model from among a collection of candidate machine learning models for a training dataset.  Model selection is the process of choosing one of the models as the final model that addresses the problem.	What is model selection in machine learning
889	"The mean of the random variable Y is also called the expected value or the expectation of Y. It is denoted E(Y). It is also called the population mean, often denoted µ. It is what we do not know in this example. A sample mean is typically denoted ȳ (read ""y-bar"")."	What does Y mean in statistics
1392	The chi-square test is a hypothesis test designed to test for a statistically significant relationship between nominal and ordinal variables organized in a bivariate table. In other words, it tells us whether two variables are independent of one another.  The chi-square test is sensitive to sample size.	Is Chi square bivariate analysis
113	For machine learning, every dataset does not require normalization. It is required only when features have different ranges. For example, consider a data set containing two features, age, and income(x2). Where age ranges from 0–100, while income ranges from 0–100,000 and higher.	When should you not normalize data in machine learning
1766	Nonparametric tests have the following limitations: Nonparametric tests are usually less powerful than corresponding parametric test when the normality assumption holds. Thus, you are less likely to reject the null hypothesis when it is false if the data comes from the normal distribution.	What are the main limitations of non parametric test
5914	However, for a general population it is not true that the sample median is an unbiased estimator of the population median. The sample mean is a biased estimator of the population median when the population is not symmetric.  It only will be unbiased if the population is symmetric.	Is median a biased estimator
1077	Chi-square Test. The Pearson's χ2 test (after Karl Pearson, 1900) is the most commonly used test for the difference in distribution of categorical variables between two or more independent groups.	How do you test the significance between two categorical variables
1460	SVM can be used to optimize classification of images (or subimages, for segmentation). SVM does not provide image classification mechanisms.	Is SVM good for image classification
28	Ordinary Least Squares regression (OLS) is more commonly named linear regression (simple or multiple depending on the number of explanatory variables).  The OLS method corresponds to minimizing the sum of square differences between the observed and predicted values.	What is ordinary least squares regression analysis
1457	Essentially, the process goes as follows:Select k centroids. These will be the center point for each segment.Assign data points to nearest centroid.Reassign centroid value to be the calculated mean value for each cluster.Reassign data points to nearest centroid.Repeat until data points stay in the same cluster.	How do you select the centroid in K means clustering
3575	Linear filtering is the filtering method in which the value of output pixel is linear combinations of the neighbouring input pixels. it can be done with convolution. For examples, mean/average filters or Gaussian filtering. A non-linear filtering is one that cannot be done with convolution or Fourier multiplication.	How can I compare linear filtering and nonlinear filtering
1463	In mathematics, the geometric mean is a mean or average, which indicates the central tendency or typical value of a set of numbers by using the product of their values (as opposed to the arithmetic mean which uses their sum).	What does the geometric mean represent
129	Pooling Layers Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently. The most common approach used in pooling is max pooling.	Why is the pooling layer used in a convolution neural network
2222	Definition: A vector space is a set V on which two operations + and · are defined, called vector addition and scalar multiplication. The operation + (vector addition) must satisfy the following conditions: Closure: If u and v are any vectors in V, then the sum u + v belongs to V.	How do you define a vector space
4095	In general, an AUC of 0.5 suggests no discrimination (i.e., ability to diagnose patients with and without the disease or condition based on the test), 0.7 to 0.8 is considered acceptable, 0.8 to 0.9 is considered excellent, and more than 0.9 is considered outstanding.	What is a good AUC score
1152	One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.	Why is sparse categorical cross entropy
8015	Multimodal learning suggests that when a number of our senses - visual, auditory, kinaesthetic - are being engaged during learning, we understand and remember more. By combining these modes, learners experience learning in a variety of ways to create a diverse learning style.	What is multimodal learning style
6527	"At a bare minimum, collect around 1000 examples. For most ""average"" problems, you should have 10,000 - 100,000 examples. For “hard” problems like machine translation, high dimensional data generation, or anything requiring deep learning, you should try to get 100,000 - 1,000,000 examples."	How many observations do you need for machine learning
60	An algorithm X is said to be asymptotically better than Y if X takes smaller time than y for all input sizes n larger than a value n0 where n0 > 0.	What does it mean when we say algorithm X is asymptotically more efficient than Y
131	In General, A Discriminative model ‌models the decision boundary between the classes. A Generative Model ‌explicitly models the actual distribution of each class.  A Discriminative model ‌learns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems.	What are the differences between generative and discriminative machine learning
72	Cross Entropy is definitely a good loss function for Classification Problems, because it minimizes the distance between two probability distributions - predicted and actual.	Why is cross entropy loss good
7777	Minimax is a kind of backtracking algorithm that is used in decision making and game theory to find the optimal move for a player, assuming that your opponent also plays optimally. It is widely used in two player turn-based games such as Tic-Tac-Toe, Backgammon, Mancala, Chess, etc.	What is Minimax algorithm explain in detail
796	Mean, variance, and standard deviation The mean of the sampling distribution of the sample mean will always be the same as the mean of the original non-normal distribution. In other words, the sample mean is equal to the population mean. where σ is population standard deviation and n is sample size.	Is sample mean equal to population mean
1919	Running the ProcedureClick Transform > Recode into Different Variables.Double-click on variable CommuteTime to move it to the Input Variable -> Output Variable box. In the Output Variable area, give the new variable the name CommuteLength, then click Change.Click the Old and New Values button.  Click OK.	How do you convert continuous variables to categorical in SPSS
7153	The least-squares regression line always passes through the point (x, y). 3. The square of the correlation, r2, is the fraction of the variation in the values of y that is explained by the least- squares regression of y on x.	What point does the least squares regression line pass through
2699	To reach the best generalization, the dataset should be split into three parts: The training set is used to train a neural net. The error of this dataset is minimized during training. The validation set is used to determine the performance of a neural network on patterns that are not trained during learning.	How do you generalize a neural network
6667	A set is countable if: (1) it is finite, or (2) it has the same cardinality (size) as the set of natural numbers (i.e., denumerable). Equivalently, a set is countable if it has the same cardinality as some subset of the set of natural numbers. Otherwise, it is uncountable.	What is countable set in analysis
659	The higher the number of features, the harder it gets to visualize the training set and then work on it.  Dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.	How does dimensionality reduction work
887	“The major difference between machine learning and statistics is their purpose. Machine learning models are designed to make the most accurate predictions possible. Statistical models are designed for inference about the relationships between variables.”  You cannot do statistics unless you have data.	What are the differences among statistical inferences modeling and machine learning based analytics
3320	An additive effect refers to the role of a variable in an estimated model. A variable that has an additive effect can merely be added to the other terms in a model to determine its effect on the independent variable.	What is an additive effect in statistics
461	chromate ions	Which indicator is Mohr's method
4066	In Decision Trees, for predicting a class label for a record we start from the root of the tree. We compare the values of the root attribute with the record's attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.	How does Decision Tree predict
69	A good maximum sample size is usually 10% as long as it does not exceed 1000. A good maximum sample size is usually around 10% of the population, as long as this does not exceed 1000. For example, in a population of 5000, 10% would be 500. In a population of 200,000, 10% would be 20,000.	What is a good sample percentage
6889	The parameters of a neural network are typically the weights of the connections.  So, the algorithm itself (and the input data) tunes these parameters. The hyper parameters are typically the learning rate, the batch size or the number of epochs.	What are parameters in neural network
1761	An iteration is a term used in machine learning and indicates the number of times the algorithm's parameters are updated.  A typical example of a single iteration of training of a neural network would include the following steps: processing the training dataset batch.	What is iteration in machine learning
2261	hadoop is an open-source computer code framework used for distributed storage and process of very massive data sets. pig is a high-level platform for making programs that run on Apache Hadoop. The language for this platform is termed Pig Latin.	Which interface is used to translate unstructured data into structured data
7545	By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure. A random sampling of the memory bank breaks our sequence, how does that help when you are trying to back-fill a Q (reward) matrix?	What is the random sample policy and why is it so important in DQN
7189	Disentangled representation is an unsupervised learning technique that breaks down, or disentangles, each feature into narrowly defined variables and encodes them as separate dimensions. The goal is to mimic the quick intuition process of a human, using both “high” and “low” dimension reasoning.	What is disentangled representation
4875	Linear algebra is usually taken by sophomore math majors after they finish their calculus classes, but you don't need a lot of calculus in order to do it.	What level is linear algebra
1588	7 Answers. Gradient is covariant!  The components of a vector contravariant because they transform in the inverse (i.e. contra) way of the vector basis. It is customary to denote these components with an upper index.	Why is gradient covariant
1340	An agent is anything that can perceive its environment through sensors and acts upon that environment through effectors. A human agent has sensory organs such as eyes, ears, nose, tongue and skin parallel to the sensors, and other organs such as hands, legs, mouth, for effectors.	What is an agent and environment in artificial intelligence
480	Classification is a technique where we categorize data into a given number of classes. The main goal of a classification problem is to identify the category/class to which a new data will fall under.  Classifier: An algorithm that maps the input data to a specific category.	What are classification techniques
366	Positive and negative predictive values are influenced by the prevalence of disease in the population that is being tested. If we test in a high prevalence setting, it is more likely that persons who test positive truly have disease than if the test is performed in a population with low prevalence..	Why does positive predictive value depend on prevalence
7225	Data quality is important when applying Artificial Intelligence techniques, because the results of these solutions will be as good or bad as the quality of the data used.  The algorithms that feed systems based on Artificial Intelligence can only assume that the data to be analyzed are reliable.	Why is data important in AI
1243	Ensemble learning is a machine learning paradigm where multiple models (often called “weak learners”) are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.	What are weak learners and how are they used in ensemble methods
1862	How to train your Deep Neural NetworkTraining data.  Choose appropriate activation functions.  Number of Hidden Units and Layers.  Weight Initialization.  Learning Rates.  Hyperparameter Tuning: Shun Grid Search - Embrace Random Search.  Learning Methods.  Keep dimensions of weights in the exponential power of 2.More items•	How do I train deep neural network
3375	The Agglomerative Hierarchical Clustering is the most common type of hierarchical clustering used to group objects in clusters based on their similarity.	Which type of hierarchical clustering algorithm is more commonly used
13	In medical diagnosis, test sensitivity is the ability of a test to correctly identify those with the disease (true positive rate), whereas test specificity is the ability of the test to correctly identify those without the disease (true negative rate).	What is difference between sensitivity and specificity
877	communalities is calculated sum of square factor loadings. Generally, an item factor loading is recommended higher than 0.30 or 0.33 cut value. So if an item load only one factor its communality will be 0.30*0.30 = 0.09.	What is the cutoff for loading factors using factor analysis
1371	Means and Variances of Random Variables: The mean of a discrete random variable, X, is its weighted average. Each value of X is weighted by its probability. To find the mean of X, multiply each value of X by its probability, then add all the products. The mean of a random variable X is called the expected value of X.	How do you find the discrete random variable
416	Both disparate impact and disparate treatment refer to discriminatory practices.  Disparate treatment is intentional employment discrimination. For example, testing a particular skill of only certain minority applicants is disparate treatment.	What is the meaning of disparate treatment
833	If you're given the probability (percent) greater than x and you need to find x, you translate this as: Find b where p(X > b) = p (and p is given). Rewrite this as a percentile (less-than) problem: Find b where p(X < b) = 1 – p. This means find the (1 – p)th percentile for X.	How do you find the percentile under the normal curve
1026	Let's understand what the matrix of features is.  The matrix of features is a term used in machine learning to describe the list of columns that contain independent variables to be processed, including all lines in the dataset. These lines in the dataset are called lines of observation.	What is feature matrix machine learning
1063	random variable	What do you mean by variate
7249	The 1-proportion z test is used to test hypotheses regarding population proportions.  Before you can proceed with entering the data into your calculator, you will need to symbolize the null and alternative hypotheses. For this example, let's define p as the proportion of 1-Euro coins that land heads up.	What is a 1 Prop Z test
8292	Scientific uncertainty generally means that there is a range of possible values within which the true value of the measurement lies. Further research on a topic or theory may reduce the level of uncertainty or the range of possible values.	What is the science of uncertainty
6957	Sensitivity and specificity are inversely proportional, meaning that as the sensitivity increases, the specificity decreases and vice versa.	Are sensitivity and specificity inversely related
8634	Abstract. Hidden Markov Models (HMMs) provide a simple and effective frame- work for modelling time-varying spectral vector sequences. As a con- sequence, almost all present day large vocabulary continuous speech recognition (LVCSR) systems are based on HMMs.	What is hidden Markov in speech recognition
981	They all contain elements of random selection. They all measure every member of the population of interest.  They all contain elements of random selection.	Why are techniques like cluster sampling and systematic sampling just as externally valid as simple random sampling
6377	The data structure which is being used in DFS is stack. The process is similar to BFS algorithm. In DFS, the edges that leads to an unvisited node are called discovery edges while the edges that leads to an already visited node are called block edges.	Which data structure is used by depth first search algorithm
3834	A latent variable is a variable that cannot be observed. The presence of latent variables, however, can be detected by their effects on variables that are observable. Most constructs in research are latent variables. Consider the psychological construct of anxiety, for example.	What is meant by latent variable
52	Logit models are used for discrete outcome modeling. This can be for binary outcomes (0 and 1) or for three or more outcomes (multinomial logit).  It has nothing to do with binary or discrete outcomes. Tobit models are a form of linear regression.	What is the difference between tobit and logit regression
3231	Another common model for classification is the support vector machine (SVM). An SVM works by projecting the data into a higher dimensional space and separating it into different classes by using a single (or set of) hyperplanes. A single SVM does binary classification and can differentiate between two classes.	Which model is used for multiclass classification
608	Interpolation search is an algorithm for searching for a key in an array that has been ordered by numerical values assigned to the keys (key values). It was first described by W. W. Peterson in 1957.	What is interpolation search technique
357	There are two major problems while training deep learning models is overfitting and underfitting of the model. Those problems are solved by data augmentation is a regularization technique that makes slight modifications to the images and used to generate data.	Is data augmentation a regularization
6554	A (non-mathematical) definition I like by Miller (2017)3 is: Interpretability is the degree to which a human can understand the cause of a decision.  The higher the interpretability of a machine learning model, the easier it is for someone to comprehend why certain decisions or predictions have been made.	What is model interpretability
984	A Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and are used mainly for image processing, classification, segmentation and also for other auto correlated data. A convolution is essentially sliding a filter over the input.	What are convolutional neural networks used for
6081	Pattern Recognition is an engineering application of Machine Learning. Machine Learning deals with the construction and study of systems that can learn from data, rather than follow only explicitly programmed instructions whereas Pattern recognition is the recognition of patterns and regularities in data.	What is the difference between pattern recognition and machine learning
1052	A statistic is a number that represents a property of the sample. For example, if we consider one math class to be a sample of the population of all math classes, then the average number of points earned by students in that one math class at the end of the term is an example of a statistic.	What is an example of an statistic
987	A one-way analysis of variance (ANOVA) is used when you have a categorical independent variable (with two or more categories) and a normally distributed interval dependent variable and you wish to test for differences in the means of the dependent variable broken down by the levels of the independent variable.	What is the appropriate statistical test for comparing the data collected from two groups when the independent variable is categorical nominal and the dependent variable is continuous interval or ratio
390	Owing to the dependence of an IIR filter's result upon its previous results, an IIR filter is necessarily recursive. However, certain recursive filters have finite impulse response, so a recursive filter does not necessarily have infinite impulse response.	Why IIR filter and its realization is called recursive system
1102	Hidden layers, simply put, are layers of mathematical functions each designed to produce an output specific to an intended result.  Hidden layers allow for the function of a neural network to be broken down into specific transformations of the data. Each hidden layer function is specialized to produce a defined output.	What is the role of hidden layer
819	A p value is used in hypothesis testing to help you support or reject the null hypothesis. The p value is the evidence against a null hypothesis. The smaller the p-value, the stronger the evidence that you should reject the null hypothesis.  On the other hand, a large p-value of .	How is p value used in hypothesis testing
7051	bias(ˆθ) = Eθ(ˆθ) − θ. An estimator T(X) is unbiased for θ if EθT(X) = θ for all θ, otherwise it is biased.	What is the formula for bias
651	Definition: Quota sampling is a sampling methodology wherein data is collected from a homogeneous group. It involves a two-step process where two variables can be used to filter information from the population. It can easily be administered and helps in quick comparison.	What does quota sampling mean
811	Class boundaries are the numbers used to separate classes. The size of the gap between classes is the difference between the upper class limit of one class and the lower class limit of the next class. In this case, gap=21.83−21.82=0.01 gap = 21.83 - 21.82 = 0.01 .	How do I calculate class boundaries in statistics
3100	Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. One cycle through the entire training dataset is called a training epoch.	How does batch gradient descent work
6404	The scale-invariant feature transform (SIFT) is a feature detection algorithm in computer vision to detect and describe local features in images.  SIFT keypoints of objects are first extracted from a set of reference images and stored in a database.	What is scale invariant in image processing
6045	Your performance on the training data/the training error does not tell you how well your model is overall, but only how well it has learned the training data. The validation error tells you how well your learned model generalises, that means how well it fits to data that it has not been trained on.	What is training error and validation error
7980	Parallel stochastic gradient descent Parallel SGD, introduced by Zinkevich et al. [12] and shown in Algorithms 2 and 3, is one such technique and can be viewed as an improvement on model averaging. Model averaging convergence is dependent on the degree of convexity as a result of regularization.	Can stochastic gradient descent be parallelized
300	If the set has an even number of terms, the median is the average of the middle two terms. For example, in the set {10, 12, 15, 20}, the median is the average of 12 and 15: 13.5.  Since the numbers are consecutive, the mean and the median are the same.	Can the median and mean be the same
4602	Know the formula for the linear interpolation process. The formula is y = y1 + ((x – x1) / (x2 – x1)) * (y2 – y1), where x is the known value, y is the unknown value, x1 and y1 are the coordinates that are below the known x value, and x2 and y2 are the coordinates that are above the x value.	How do you calculate interpolation
3494	The rejection region is the interval, measured in the sampling distribution of the statistic under study, that leads to rejection of the null hypothesis H 0 in a hypothesis test.	What is the rejection region in statistics
327	A vector space is any set of objects with a notion of addition and scalar multiplication that behave like vectors in Rn.	What is a vector space in Matrix
1268	You can tell if two random variables are independent by looking at their individual probabilities. If those probabilities don't change when the events meet, then those variables are independent. Another way of saying this is that if the two variables are correlated, then they are not independent.	How do you prove that two distributions are independent
3346	Here it is in plain language. An OR of 1.2 means there is a 20% increase in the odds of an outcome with a given exposure. An OR of 2 means there is a 100% increase in the odds of an outcome with a given exposure. Or this could be stated that there is a doubling of the odds of the outcome.	What does an odds ratio of 2.0 mean
7989	One or two of the sections is the “rejection region“; if your test value falls into that region, then you reject the null hypothesis. A one tailed test with the rejection rejection in one tail. The critical value is the red line to the left of that region.	How do you find the critical value and rejection region
3770	The Sampling Distribution of the Sample Mean. If repeated random samples of a given size n are taken from a population of values for a quantitative variable, where the population mean is μ (mu) and the population standard deviation is σ (sigma) then the mean of all sample means (x-bars) is population mean μ (mu).	What is a sampling distribution for the sample mean x bar
1302	The law of averages typically assumes that unnatural short-term “balance” must occur. This can also be known as “Gambler's Fallacy” and is not a real mathematical principle.  The law of large numbers is important because it “guarantees” stable long-term results for the averages of random events.	Does the Law of Averages exist
4271	When the response categories are ordered, you could run a multinomial regression model. The disadvantage is that you are throwing away information about the ordering. An ordinal logistic regression model preserves that information, but it is slightly more involved.	What is the difference between multinomial and ordinal logistic regression
1332	In mathematics, the geometric–harmonic mean M(x, y) of two positive real numbers x and y is defined as follows: we form the geometric mean of g0 = x and h0 = y and call it g1, i.e. g1 is the square root of xy.  The geometric–harmonic mean is also designated as the harmonic–geometric mean. (cf. Wolfram MathWorld below.)	What is geometric mean and harmonic mean in statistics
7258	The difference is a matter of design. In the test of independence, observational units are collected at random from a population and two categorical variables are observed for each unit.  In the goodness-of-fit test there is only one observed variable.	What is the difference between the goodness of fit test and the test of independence
476	If a and b are two non-zero numbers, then the harmonic mean of a and b is a number H such that the numbers a, H, b are in H.P. We have H = 1/H = 1/2 (1/a + 1/b) ⇒ H = 2ab/a+b.	What is harmonic mean of A and B
3691	0:569:57Suggested clip · 118 secondsHow to Pass Reasoning Tests - Inductive Reasoning Sample YouTubeStart of suggested clipEnd of suggested clip	How do you master inductive reasoning test
5940	Correlation and Convolution are basic operations that we will perform to extract information from images. They are in some sense the simplest operations that we can perform on an image, but they are extremely useful.  Shift-invariant means that we perform the same operation at every point in the image.	What is convolution and correlation in image processing
2058	Unlike humans, artificial neural networks are fed with massive amount of data to learn.  Also, real neurons do not stay on until the inputs change and the outputs may encode information using complex pulse arrangements.	Is there a difference between how humans and artificial neural networks learn
3935	The degrees of freedom in a multiple regression equals N-k-1, where k is the number of variables. The more variables you add, the more you erode your ability to test the model (e.g. your statistical power goes down).	How do you calculate degrees of freedom for multiple regression
859	KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).	How does K nearest neighbor work
1402	Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.	When should logistic regression be used for data analysis
689	Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis.	Whats the difference between a prior probability and a posterior probability
459	Dealing with imbalanced datasets entails strategies such as improving classification algorithms or balancing classes in the training data (data preprocessing) before providing the data as input to the machine learning algorithm. The later technique is preferred as it has wider application.	How do you handle imbalanced data in classification problems
5956	Characteristics of a Poisson Distribution The probability that an event occurs in a given time, distance, area, or volume is the same. Each event is independent of all other events. For example, the number of people who arrive in the first hour is independent of the number who arrive in any other hour.	What are the properties of Poisson distribution
924	0:002:44Suggested clip · 118 secondsGeometric Distribution: Mean - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the mean of a geometric distribution
5297	The rate at which gases diffuse is inversely proportional to the square root of their densities.	What is Grahams law of diffusion
6674	Hinge Loss - This has been used in SVMs (Soft Margin). The aim of this loss function is to penalize miss-classification. Cross-Entropy Loss - Probably one of best loss functions being used in classification. Now-a-days this is being used in many advanced machine learning models like deep neural networks etc.	What are some alternatives to negative log likelihood loss for classification problems
1890	"It gives us tools that can be used to go forward. Then the community of scientists have defined what ""graphical models"" are. Tools have been developed that apply to models that match this definition. NN is one of them, it is a graphical model."	Are neural networks graphical models
1313	area under the curve	What does AUC stand for
632	2 Key Challenges of Streaming Data and How to Solve ThemStreaming Data is Very Complex. Streaming data is particularly challenging to handle because it is continuously generated by an array of sources and devices and is delivered in a wide variety of formats.  Business Wants Data, But IT Can't Keep Up.	What are the challenges of data stream processing
776	A distribution is skewed if one of its tails is longer than the other. The first distribution shown has a positive skew. This means that it has a long tail in the positive direction. The distribution below it has a negative skew since it has a long tail in the negative direction.	What does skewed distribution mean
1052	When I calculate population variance, I then divide the sum of squared deviations from the mean by the number of items in the population (in example 1 I was dividing by 12). When I calculate sample variance, I divide it by the number of items in the sample less one.	How do you convert sample variance to population variance
831	Hypergeometric Formula.. The hypergeometric distribution has the following properties: The mean of the distribution is equal to n * k / N . The variance is n * k * ( N - k ) * ( N - n ) / [ N2 * ( N - 1 ) ] .	What is the formula for hypergeometric distribution
3604	Introduction. The standard deviation is a measure of the spread of scores within a set of data. Usually, we are interested in the standard deviation of a population. However, as we are often presented with data from a sample only, we can estimate the population standard deviation from a sample standard deviation.	Do you use sample or population standard deviation
2219	Functions of Random Variables One law is called the “weak” law of large numbers, and the other is called the “strong” law of large numbers. The weak law describes how a sequence of probabilities converges, and the strong law describes how a sequence of random variables behaves in the limit.	What is the difference between weak law and strong law of large number in probability theory
6963	Gradient Descent runs iteratively to find the optimal values of the parameters corresponding to the minimum value of the given cost function, using calculus. Mathematically, the technique of the 'derivative' is extremely important to minimise the cost function because it helps get the minimum point.	What is the gradient descent algorithm for linear regression How does it help to reduce the cost function
6279	SVM Kernel Functions SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form.  For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid.	What are SVM kernels
250	Statistics is generally considered a prerequisite to the field of applied machine learning. We need statistics to help transform observations into information and to answer questions about samples of observations.	What is the use of statistics in machine learning
495	To identify a random error, the measurement must be repeated a small number of times. If the observed value changes apparently randomly with each repeated measurement, then there is probably a random error. The random error is often quantified by the standard deviation of the measurements.	How do you find the random error
7792	It is used to predict values of a continuous response variable using one or more explanatory variables and can also identify the strength of the relationships between these variables (these two goals of regression are often referred to as prediction and explanation).	What is OLS regression used for
4707	Steps 3/4: Test Statistic and p-Value. This is the heart of a hypothesis test.  Definition: The p-value is the probability of getting your sample, or a sample even further from H0, if H0 is true.	What is at the​ heart of hypothesis testing in​ statistics
7470	The larger the absolute value of the t-value, the smaller the p-value, and the greater the evidence against the null hypothesis.	How do you interpret the p value and T value
7928	To find the area between two positive z scores takes a couple of steps. First use the standard normal distribution table to look up the areas that go with the two z scores. Next subtract the smaller area from the larger area. For example, to find the area between z1 = .	How do you find the area of a corresponding z score
4488	30.4. Introduction. A matrix norm is a number defined in terms of the entries of the matrix. The norm is a useful quantity which can give important information about a matrix.	What is meant by Norm of a matrix
3991	Conclusion. Linear Regression is the process of finding a line that best fits the data points available on the plot, so that we can use it to predict output values for inputs that are not present in the data set we have, with the belief that those outputs would fall on the line.	How does a linear regression work
142	The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.	How do you read a precision recall curve
145	Nonlinear filters: Non-linear functions of signals. Examples: thresholding, image equalisation, or median filtering.	Is thresholding a linear filter
659	When comparing data samples from different populations, covariance is used to determine how much two random variables vary together, whereas correlation is used to determine when a change in one variable can result in a change in another. Both covariance and correlation measure linear relationships between variables.	What is the significance of covariance and correlation and in what cases can we not use correlation
1369	The Google Goggles app is an image-recognition mobile app that uses visual search technology to identify objects through a mobile device's camera. Users can take a photo of a physical object, and Google searches and retrieves information about the image.	How can I identify an object in a picture
307	You description is confusing, but it is totally possible to have test error both lower and higher than training error. A lower training error is expected when a method easily overfits to the training data, yet, poorly generalizes.	Is it possible to have a higher train error than a test error in machine learning
6977	A wide-column store (or extensible record stores) is a type of NoSQL database. It uses tables, rows, and columns, but unlike a relational database, the names and format of the columns can vary from row to row in the same table. A wide-column store can be interpreted as a two-dimensional key–value store.	What is the wide column model
7284	Qualitative Variables - Variables that are not measurement variables. Their values do not result from measuring or counting. Examples: hair color, religion, political party, profession. Designator - Values that are used to identify individuals in a table.	What is qualitative variable
342	​Cross-sectional studies are observational studies that collect information about individuals at a specific point in time or over a very short period of time.  For the lung cancer​ study, it could be that individuals develop cancer after the data are​ collected, so the study will not give the full picture.	What is a cross sectional study quizlet statistics
650	Implementing Deep Learning Methods and Feature Engineering for Text Data: FastText. Overall, FastText is a framework for learning word representations and also performing robust, fast and accurate text classification. The framework is open-sourced by Facebook on GitHub.	Is fastText deep learning
5808	The distinction between probability and likelihood is fundamentally important: Probability attaches to possible results; likelihood attaches to hypotheses. Explaining this distinction is the purpose of this first column. Possible results are mutually exclusive and exhaustive.	What is the difference between probability and likelihood 1
6282	The adjusted R-squared compensates for the addition of variables and only increases if the new predictor enhances the model above what would be obtained by probability. Conversely, it will decrease when a predictor improves the model less than what is predicted by chance.	What does the adjusted R squared value tell you
7831	Genetic Algorithms are a type of learning algorithm, that uses the idea that crossing over the weights of two good neural networks, would result in a better neural network.	What is genetic algorithm in neural network
994	Abstract. A memory-based learning system is an extended memory management system that decomposes the input space either statically or dynamically into subregions for the purpose of storing and retrieving functional information.	What is memory based learning
6194	A residual plot is typically used to find problems with regression. Some data sets are not good candidates for regression, including: Heteroscedastic data (points at widely varying distances from the line). Data that is non-linearly associated.	What is the purpose of doing residual plotting
95	The joint probability mass function is P(X = x and Y = y). Conditional distributions are P(X = x given Y = y), P(Y = y given X = x). Marginal distributions are P(X = x), P(Y = y).	How do you find the marginal probability distribution
3966	A logarithm is the power to which a number must be raised in order to get some other number (see Section 3 of this Math Review for more about exponents). For example, the base ten logarithm of 100 is 2, because ten raised to the power of two is 100: log 100 = 2. because.	What is a logarithm in simple terms
1707	Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis.	What is the difference between prior and posterior and likelihood probabilities
336	Linear discriminant analysis (LDA) is one of commonly used supervised subspace learning methods.  The objective optimization is in both the ratio trace and the trace ratio forms, forming a complete framework of a new approach to jointly clustering and unsupervised subspace learning.	Is linear discriminant analysis supervised or unsupervised
7113	Artificial Intelligence (AI) is a kind of simulation that involves a model intended to represent human intelligence or knowledge. An AI-based simulation model typically mimics human intelligence such as reasoning, learning, perception, planning, language comprehension, problem-solving, and decision making.	What is simulation artificial intelligence
3635	How to Calculate VarianceFind the mean of the data set. Add all data values and divide by the sample size n.Find the squared difference from the mean for each data value. Subtract the mean from each data value and square the result.Find the sum of all the squared differences.  Calculate the variance.	How do I calculate the variance
673	If the test is statistically significant (e.g., p<0.05), then data do not follow a normal distribution, and a nonparametric test is warranted.When to Use a Nonparametric Testwhen the outcome is an ordinal variable or a rank,when there are definite outliers or.when the outcome has clear limits of detection.	When should you use non parametric tests of statistical significance
867	A commonly used rule says that a data point is an outlier if it is more than 1.5 ⋅ IQR 1.5\cdot \text{IQR} 1. 5⋅IQR1, point, 5, dot, start text, I, Q, R, end text above the third quartile or below the first quartile. Said differently, low outliers are below Q 1 − 1.5 ⋅ IQR \text{Q}_1-1.5\cdot\text{IQR} Q1−1.	How do you calculate if there are outliers
7158	More precisely, the divergence theorem states that the surface integral of a vector field over a closed surface, which is called the flux through the surface, is equal to the volume integral of the divergence over the region inside the surface.	What does the divergence theorem tell us
2422	An LSTM has a similar control flow as a recurrent neural network. It processes data passing on information as it propagates forward. The differences are the operations within the LSTM's cells. These operations are used to allow the LSTM to keep or forget information.	How does Lstm model work
6604	In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment.	What is the purpose of probability distribution functions
7339	A parametric model is one where we assume the 'shape' of the data, and therefore only have to estimate the coefficients of the model. A non-parametric model is one where we do not assume the 'shape' of the data, and we have to estimate the most suitable form of the model, along with the coefficients.	What is the difference between parametric and non parametric models
5164	Gramin Dak Sevak- GDS. The minimum working hours of GDS Post Offices and GDS is increased to 4 hours from 3 hours.  The Level 1 GDS Post Offices/GDSs will have 4 hours as working hours and Level – 2 will have 5 hours as working hours. The Point System for assessment of workload of BPMs has been abolished.	What is GDS BPM Level 1 and Level 2
8583	The main differences between an RMS Voltage and an Average Voltage, is that the mean value of a periodic wave is the average of all the instantaneous areas taken under the curve over a given period of the waveform, and in the case of a sinusoidal quantity, this period is taken as one-half of the cycle of the wave.	Is RMS the same as average
3662	Statisticians often call this “statistical inference.” There are four main types of conclusions (inferences) that statisticians can draw from data: significance, estimation, generalization, and causation. In the remainder of this chapter we will focus on statistical significance.	What are the four pillars of statistical inference
1318	Machine learning algorithms can minimize forecasting error and do the forecast much faster and with the usage of more data. What's more, machine learning algorithms can analyze many alternative models at the same time, when in traditional econometrics you can analyze just one model at a time.	Economics How can machine learning be used for econometrics
1636	"The obvious difference between ANOVA and a ""Multivariate Analysis of Variance"" (MANOVA) is the “M”, which stands for multivariate. In basic terms, A MANOVA is an ANOVA with two or more continuous response variables. Like ANOVA, MANOVA has both a one-way flavor and a two-way flavor."	What is the difference between Anova and Manova
7675	Moments are used to find the central tendency(In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution), dispersion, skewness and kurtosis( the sharpness of the peak of a frequency-distribution curve)..	What are moments in statistics What do they provide to us about the data distribution
2716	Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis.	How do you find the similarity between two vectors
3174	There are mainly four ways of knowledge representation which are given as follows: Logical Representation. Semantic Network Representation. Frame Representation.	What are the different knowledge representation techniques
5952	Apriori is an algorithm for discovering itemsets (group of items) occurring frequently in a transaction database (frequent itemsets). A frequent itemset is an itemset appearing in at least minsup transactions from the transaction database, where minsup is a parameter given by the user.	What is frequent itemset in Apriori algorithm
4048	6 Steps To Write Any Machine Learning Algorithm From Scratch: Perceptron Case StudyGet a basic understanding of the algorithm.Find some different learning sources.Break the algorithm into chunks.Start with a simple example.Validate with a trusted implementation.Write up your process.	How do you create a learning algorithm
3343	Sample Space is a set of all possible outcomes. It is mainly denoted as 'S'. Infinite sample spaces may be discrete or continuous. The probability of sample space is 1.	What is the probability of sample space
303	Statistics is a mathematically-based field which seeks to collect and interpret quantitative data.  In contrast, data science is a multidisciplinary field which uses scientific methods, processes, and systems to extract knowledge from data in a range of forms.	Is data science the same as statistics
6212	A Neural Network has got non linear activation layers which is what gives the Neural Network a non linear element. The function for relating the input and the output is decided by the neural network and the amount of training it gets.  Similarly, a complex enough neural network can learn any function.	Which gives nonlinearity to a neural network
1401	Train the network. Initializing all the weights with zeros leads the neurons to learn the same features during training.  Thus, both neurons will evolve symmetrically throughout training, effectively preventing different neurons from learning different things.	What will happen if we initialize all the weights to 0 in neural networks check all that apply
8297	A bivariate distribution, whose marginals are Poisson is developed as a product of Poisson marginals with a multiplicative factor. The correlation between the two variates can be either positive or negative, depending on the value chosen for the parameter in the above multiplicative factor.	How would you explain a bivariate poisson distribution
5233	Probability and the Normal Curve The normal distribution is a continuous probability distribution. This has several implications for probability. The total area under the normal curve is equal to 1. The probability that a normal random variable X equals any particular value is 0.	What is the probability for a normal distribution
4533	Type II Error and Power Calculations. Recall that in hypothesis testing you can make two types of errors • Type I Error – rejecting the null when it is true. • Type II Error – failing to reject the null when it is false.  = ⎛ ⎞ −  − − = =  = ⎛ ⎞ −	How do you calculate Type 2 error in hypothesis testing
8314	Poisson regression assumes the response variable Y has a Poisson distribution, and assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters. A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables.	How does Poisson regression work
4154	The Sarsa algorithm is an On-Policy algorithm for TD-Learning. The major difference between it and Q-Learning, is that the maximum reward for the next state is not necessarily used for updating the Q-values.	What is the difference between Q learning and SARSA learning
1002	Starting at $99.00 USD per user per month. Single-user, desktop application for Windows and Macs. Includes 12 months of technical support. Pricing information for IBM SPSS Statistics is supplied by the software provider or retrieved from publicly accessible pricing materials.	How much is SPSS for Mac
97	In the case of a conditional probability, P(D|H), the hypothesis is fixed and the data are free to vary. Likelihood, however, is the opposite.  For conditional probability, the hypothesis is treated as a given and the data are free to vary. For likelihood, the data are a given and the hypotheses vary.	Is likelihood a conditional probability
3149	Regularization is a set of techniques that can prevent overfitting in neural networks and thus improve the accuracy of a Deep Learning model when facing completely new data from the problem domain.	What is regularization in deep learning
3820	The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.  The C parameter trades off correct classification of training examples against maximization of the decision function's margin.	What is C and gamma in SVM
676	Naive bayes is a Generative model whereas Logistic Regression is a Discriminative model . Generative model is based on the joint probability, p( x, y), of the inputs x and the label y, and make their predictions by using Bayes rules to calculate p(y | x), and then picking the most likely label y.	Is naive Bayes generative or discriminative
3788	The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler.	What does Singular Value Decomposition do
5868	Search friction [r]: The effect of obstacles to the matching the supply of a product with the demand for it that arise from the time and cost of the process of finding a match.	What are search frictions
754	Marginal probability: the probability of an event occurring (p(A)), it may be thought of as an unconditional probability. It is not conditioned on another event. Example: the probability that a card drawn is red (p(red) = 0.5). Another example: the probability that a card drawn is a 4 (p(four)=1/13).	What is marginal probability with example
7595	A normal distribution has a bell-shaped density curve described by its mean and standard deviation . The density curve is symmetrical, centered about its mean, with its spread determined by its standard deviation.	What is normal distribution density
614	Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable. A quantity called mutual information measures the amount of information one can obtain from one random variable given another.	What is mutual information in machine learning
3379	"""A discrete variable is one that can take on finitely many, or countably infinitely many values"", whereas a continuous random variable is one that is not discrete, i.e. ""can take on uncountably infinitely many values"", such as a spectrum of real numbers."	What is the difference between discrete and continuous variables
7437	Partitioning is a way of splitting numbers into smaller parts to make them easier to work with. Partitioning links closely to place value: a child will be taught to recognise that the number 54 represents 5 tens and 4 ones, which shows how the number can be partitioned into 50 and 4.	What is partition method
5216	Logistic regression is a powerful machine learning algorithm that utilizes a sigmoid function and works best on binary classification problems, although it can be used on multi-class classification problems through the “one vs. all” method.	Can logistic regression be used for classification
4596	Sentiment analysis is the automated process of analyzing text data and sorting it into sentiments positive, negative, or neutral. Using sentiment analysis tools to analyze opinions in Twitter data can help companies understand how people are talking about their brand.	Why is Twitter used for sentiment analysis
6062	Any study that attempts to predict human behavior will tend to have R-squared values less than 50%. However, if you analyze a physical process and have very good measurements, you might expect R-squared values over 90%.	What is a good r squared for a model
4387	As a hypothetical example of systematic sampling, assume that in a population of 10,000 people, a statistician selects every 100th person for sampling. The sampling intervals can also be systematic, such as choosing a new sample to draw from every 12 hours.	What is systematic sampling example
2880	Three keys to managing bias when building AIChoose the right learning model for the problem. There's a reason all AI models are unique: Each problem requires a different solution and provides varying data resources.  Choose a representative training data set.  Monitor performance using real data.	How do you handle bias in data
5041	To make an ROC curve from your data you start by ranking all the values and linking each value to the diagnosis – sick or healthy. In the example in TABLE II 159 healthy people and 81 sick people are tested. The results and the diagnosis (sick Y or N) are listed and ranked based on parameter concentration.	How do you create a receiver operating characteristic curve
683	Linear regression models are used to show or predict the relationship between two variables or factors. The factor that is being predicted (the factor that the equation solves for) is called the dependent variable.	What is linear regression model used for
8144	Methods are commonly divided into linear and non-linear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.	What is the dimensionality reduction used for
7261	Z Score is free of any scale, hence it is used as a transformation technique while we need to make any variable unit free in various statistical techniques. Also, it is used to identifying outliers in a univarite way.  Z-test is a statistical technique to test the Null Hypothesis against the Alternate Hypothesis.	What is the Difference between the Z test and Z score
6475	The hierarchical cluster analysis follows three basic steps: 1) calculate the distances, 2) link the clusters, and 3) choose a solution by selecting the right number of clusters. First, we have to select the variables upon which we base our clusters.	How do you do a cluster analysis
7656	Steps to follow while conducting non-parametric tests:The first step is to set up hypothesis and opt a level of significance. Now, let's look at what these two are.  Set a test statistic.  Set decision rule.  Calculate test statistic.  Compare the test statistic to the decision rule.	How do you Analyse non parametric data
8691	The t-distribution cannot be calculated without a known standard deviation, while the standard normal distribution can be.	Which of the following is a difference between the T distribution and the standard normal Z distribution group of answer choices
500	When the image goes through them, the important features are kept in the convolution layers, and thanks to the pooling layers, these features are intensified and kept over the network, while discarding all the information that doesn't make a difference for the task.	What is convolution and pooling
6017	SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.	Is SVM a linear classifier
2801	A data set provides statistical significance when the p-value is sufficiently small. When the p-value is large, then the results in the data are explainable by chance alone, and the data are deemed consistent with (while not proving) the null hypothesis.	How do you determine which variables are statistically significant
1049	First, logistic regression does not require a linear relationship between the dependent and independent variables. Second, the error terms (residuals) do not need to be normally distributed.  This means that the independent variables should not be too highly correlated with each other.	Should independent variables be normally distributed for ordered logit model
8105	Advantages of Linear Regression Linear regression has a considerably lower time complexity when compared to some of the other machine learning algorithms. The mathematical equations of Linear regression are also fairly easy to understand and interpret. Hence Linear regression is very easy to master.	What are advantages of different regression algorithms
1860	Tensors are simply mathematical objects that can be used to describe physical properties, just like scalars and vectors. In fact tensors are merely a generalisation of scalars and vectors; a scalar is a zero rank tensor, and a vector is a first rank tensor.	What is meant by a tensor
1055	There are 5 values above the median (upper half), the middle value is 77 which is the third quartile. The interquartile range is 77 – 64 = 13; the interquartile range is the range of the middle 50% of the data.  When the sample size is odd, the median and quartiles are determined in the same way.	Is median and interquartile range the same
6596	In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.	What is logit model used for
722	2 Model selection criteria. Akaike information criterion (AIC) (Akaike, 1974) is a fined technique based on in-sample fit to estimate the likelihood of a model to predict/estimate the future values. A good model is the one that has minimum AIC among all the other models.  A lower AIC or BIC value indicates a better fit.	How is the significance of the Akaike information criterion in model selection
909	The general formula for pointwise mutual information is given below; it is the binary logarithm of the joint probability of X = a and Y = b, divided by the product of the individual probabilities that X = a and Y = b.	How is Pointwise mutual information calculated
1540	The Backpropagation algorithm looks for the minimum value of the error function in weight space using a technique called the delta rule or gradient descent. The weights that minimize the error function is then considered to be a solution to the learning problem.	What is backpropagation learning algorithm
1035	Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the	Is Random Forest ensemble learning
1161	Systematic random sampling is the random sampling method that requires selecting samples based on a system of intervals in a numbered population. For example, Lucas can give a survey to every fourth customer that comes in to the movie theater.	What is an example of systematic random sample
7732	"In this view, associative networks are fundamentally unorganized lists of features. By specifying what attributes to include, a frame structure promises to provide the ""framework"" upon which to organize and hang what a consumer knows about a product."	What do you know about associative network and frames
7758	If a p-value is lower than our significance level, we reject the null hypothesis. If not, we fail to reject the null hypothesis.	What happens when the p value is lower than the level of significance
5254	The Regression Tree Algorithm can be used to find one model that results in good predictions for the new data.	What are regression trees used for
868	If X takes values in [a, b] and Y takes values in [c, d] then the pair (X, Y ) takes values in the product [a, b] × [c, d]. The joint probability density function (joint pdf) of X and Y is a function f(x, y) giving the probability density at (x, y).	How do you find the joint probability density function of X and Y
3752	Here are five ways to identify segments.Cross-Tab. Cross-tabbing is the process of examining more than one variable in the same table or chart (“crossing” them).  Cluster Analysis.  Factor Analysis.  Latent Class Analysis (LCA)  Multidimensional Scaling (MDS)	How do you find clusters in data
28	Creating the Regression LineDEFINITIONS:b1 - This is the SLOPE of the regression line.  b0 - This is the intercept of the regression line with the y-axis.  Y-hat = b0 + b1(x) - This is the sample regression line.	How do you find b0 and b1 in linear regression
4249	In Chi-Square goodness of fit test, the term goodness of fit is used to compare the observed sample distribution with the expected probability distribution. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution.	What is goodness of fit in chi square test
5961	The correlation is the covariance divided by the product of the standard deviations. Therefore the correlation is the gradient of the regression line multiplied by the ratio of the standard deviations. If these standard deviations are equal the correlation is equal to the gradient.	What is gradient correlation correlation statistics
3601	In convolutional networks, multiple filters are taken to slice through the image and map them one by one and learn different portions of an input image. Imagine a small filter sliding left to right across the image from top to bottom and that moving filter is looking for, say, a dark edge.	What are convolutional filters
7402	Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.	What are weights in deep learning
5605	Multivariate testing uses the same core mechanism as A/B testing, but compares a higher number of variables, and reveals more information about how these variables interact with one another. As in an A/B test, traffic to a page is split between different versions of the design.	What is the difference between AB testing and multivariate
6133	Rule of Multiplication The probability that Events A and B both occur is equal to the probability that Event A occurs times the probability that Event B occurs, given that A has occurred.	What is the multiplication rule in probability
7144	The world is fast evolving, with Artificial intelligence (AI) at the forefront in changing the world and the way we live.  This means that with AI, many of our everyday activities can now be carried out effectively by programmed machine technology.	How did artificial intelligence change the world
3163	Examples of such greedy algorithms are Kruskal's algorithm and Prim's algorithm for finding minimum spanning trees, and the algorithm for finding optimum Huffman trees. Greedy algorithms appear in network routing as well.	What is greedy algorithm example
5782	Median filtering is generally less sensitive to outliers than mean filtering. If you don't believe that the Gaussian assumption of the data will hold very accurately, then a median filter may be the better choice. However, if the Gaussian assumption holds pretty well, then the median filter may be less efficient.	Why median filter is not suitable for Gaussian noise
921	Given a probability density function, we define the cumulative distribution function (CDF) as follows. The cumulative distribution function (CDF) of a random variable X is denoted by F(x), and is defined as F(x) = Pr(X ≤ x). where xn is the largest possible value of X that is less than or equal to x.	How do you write a cumulative distribution function
3364	If your regression model contains independent variables that are statistically significant, a reasonably high R-squared value makes sense.  Correspondingly, the good R-squared value signifies that your model explains a good proportion of the variability in the dependent variable.	What do you report in a multiple regression to say whether the variables are significant or not
1165	First consider the case when X and Y are both discrete. Then the marginal pdf's (or pmf's = probability mass functions, if you prefer this terminology for discrete random variables) are defined by fY(y) = P(Y = y) and fX(x) = P(X = x). The joint pdf is, similarly, fX,Y(x,y) = P(X = x and Y = y).	How do you find the marginal PDF of a joint distribution
906	Weighted kNN is a modified version of k nearest neighbors.  The simplest method is to take the majority vote, but this can be a problem if the nearest neighbors vary widely in their distance and the closest neighbors more reliably indicate the class of the object.	What is distance weighted KNN
1067	A type I error (false-positive) occurs if an investigator rejects a null hypothesis that is actually true in the population; a type II error (false-negative) occurs if the investigator fails to reject a null hypothesis that is actually false in the population.	How do type I and type II errors relate to each other for testing the hypothesis
4495	Given any collection of pairs of numbers (except when all the x-values are the same) and the corresponding scatter diagram, there always exists exactly one straight line that fits the data better than any other, in the sense of minimizing the sum of the squared errors. It is called the least squares regression line.	What is special about a least squares regression line
890	Poisson distribution is used to model the # of events in the future, Exponential distribution is used to predict the wait time until the very first event, and Gamma distribution is used to predict the wait time until the k-th event.	Why is the gamma distribution important
1305	Content validity is different from face validity, which refers not to what the test actually measures, but to what it superficially appears to measure.  In clinical settings, content validity refers to the correspondence between test items and the symptom content of a syndrome.	What is the difference between face and content validity
1985	False Alarm Rate. A false alarm is “an erroneous radar target detection decision caused by noise or other interfering signals exceeding the detection threshold”. In general, it is an indication of the presence of radar target when there is no valid aim.	What is false alarm time
5730	Process of Calculating the Histogram of Oriented Gradients (HOG)Step 1: Preprocess the Data (64 x 128) This is a step most of you will be pretty familiar with.  Step 2: Calculating Gradients (direction x and y)  Step 3: Calculate the Magnitude and Orientation.	How do you calculate histogram of oriented gradients
3555	Under the hood, these RDDs are stored in partitions on different cluster nodes. Partition basically is a logical chunk of a large distributed data set. It provides the possibility to distribute the work across the cluster, divide the task into smaller parts, and reduce memory requirements for each node.	How is RDD partitioned
8264	TL;DR – The train_test_split function is for splitting a single dataset for two different purposes: training and testing.	Which of the following functions can be used to split the data into train and test
2589	The sample space of a random experiment is the collection of all possible outcomes. An event associated with a random experiment is a subset of the sample space. The probability of any outcome is a number between 0 and 1. The probabilities of all the outcomes add up to 1.	Can be the probability of an outcome in a sample space
1265	K-NN is a lazy learner because it doesn't learn a discriminative function from the training data but “memorizes” the training dataset instead. For example, the logistic regression algorithm learns its model weights (parameters) during training time.  A lazy learner does not have a training phase.	Why K nearest Neighbour algorithm is lazy learning algorithm
595	Weights and biases (commonly referred to as w and b) are the learnable parameters of a machine learning model.  When the inputs are transmitted between neurons, the weights are applied to the inputs along with the bias. A neuron. Weights control the signal (or the strength of the connection) between two neurons.	What is mean by weight in machine learning
7864	The More Formal Formula You can solve these types of problems using the steps above, or you can us the formula for finding the probability for a continuous uniform distribution: P(X) = d – c / b – a. This is also sometimes written as: P(X) = x2 – x1 / b – a.	How do you find the continuous probability of a uniform
1918	Predictive analytics are used to determine customer responses or purchases, as well as promote cross-sell opportunities. Predictive models help businesses attract, retain and grow their most profitable customers. Improving operations. Many companies use predictive models to forecast inventory and manage resources.	How are predictive analytics commonly used
1042	Based on recent research, we hypothesize that there is a neural network of consciousness in which the paraventricular nucleus formally serves as the control nucleus of arousal, which is closely related to the maintenance of consciousness, and the neurons in the posterior cerebral cortex.	Are neural networks conscious
1203	Five Advantages of Running Repeated Measures ANOVA as a Mixed Model. There are two ways to run a repeated measures analysis. The traditional way is to treat it as a multivariate test–each response is considered a separate variable. The other way is to it as a mixed model.	Is repeated measures Anova a mixed model
4294	Some researchers say that it is a good idea to mean center variables prior to computing a product term (to serve as a moderator term) because doing so will help reduce multicollinearity in a regression model. Other researchers say that mean centering has no effect on multicollinearity.	Do centering variables reduce multicollinearity
2363	If you increase your sample size you increase the precision of your estimates, which means that, for any given estimate / size of effect, the greater the sample size the more “statistically significant” the result will be.	How does sample size affect precision
995	The Frobenius Norm of a matrix is defined as the square root of the sum of the squares of the elements of the matrix. Approach: Find the sum of squares of the elements of the matrix and then print the square root of the calculated value.	How do you find the Frobenius norm of a matrix
893	Image annotation is the process of manually defining regions in an image and creating text-based descriptions of those regions.  You can use the following image annotation tools to quickly and accurately build the ground truth for your computer vision models.	What is image annotation tool
3648	Bias is the simplifying assumptions made by the model to make the target function easier to approximate. Variance is the amount that the estimate of the target function will change given different training data. Trade-off is tension between the error introduced by the bias and the variance.	What is bias and variance in machine learning
1424	The Neural Network is constructed from 3 type of layers: Input layer — initial data for the neural network. Hidden layers — intermediate layer between input and output layer and place where all the computation is done. Output layer — produce the result for given inputs.	Is the output layer a hidden layer
8090	In information theory, the graph entropy is a measure of the information rate achievable by communicating symbols over a channel in which certain pairs of values may be confused. This measure, first introduced by Körner in the 1970s, has since also proven itself useful in other settings, including combinatorics.	What is an entropy of Graph Is it related to concept of entropy in Information Theory
6018	A true positive is an outcome where the model correctly predicts the positive class. Similarly, a true negative is an outcome where the model correctly predicts the negative class. A false positive is an outcome where the model incorrectly predicts the positive class.	What is true positive false positive
7085	Solution: Double Q learning The solution involves using two separate Q-value estimators, each of which is used to update the other. Using these independent estimators, we can unbiased Q-value estimates of the actions selected using the opposite estimator [3].	What is Double Q learning
325	The standard temporal/spatial Gaussian is a low-pass filter. It replaces every element of the input signal with a weighted average of its neighborhood. This causes blurring in time/space, which is the same as attenuating high-frequency components in the frequency domain.	Is Gaussian filter a low pass filter
1729	Overview. This Master's course aims to respond to the demand for data scientists with the skills to develop innovative computational intelligence applications, capable of analysing large amounts of complex data to inform businesses decisions and market strategies.	What is data science and computational intelligence
1100	The straight line is a trend line, designed to come as close as possible to all the data points. The trend line has a positive slope, which shows a positive relationship between X and Y. The points in the graph are tightly clustered about the trend line due to the strength of the relationship between X and Y.	How can you use a trend line to determine the type of linear association for a scatter plot
245	Cost Function It is a function that measures the performance of a Machine Learning model for given data. Cost Function quantifies the error between predicted values and expected values and presents it in the form of a single real number. Depending on the problem Cost Function can be formed in many different ways.	What is a cost function in machine learning
5863	In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.	What is lasso in machine learning
333	With an appropriate kernel function, we can solve any complex problem. Unlike in neural networks, SVM is not solved for local optima. It scales relatively well to high dimensional data. SVM models have generalization in practice, the risk of over-fitting is less in SVM.	What are the advantages and disadvantages of support vector machines SVM
1436	Andrew Ng says that batch normalization should be applied immediately before the non-linearity of the current layer. The authors of the BN paper said that as well, but now according to François Chollet on the keras thread, the BN paper authors use BN after the activation layer.	Where do I put batch normalization
6120	Normalization usually means to scale a variable to have a values between 0 and 1, while standardization transforms data to have a mean of zero and a standard deviation of 1. This standardization is called a z-score, and data points can be standardized with the following formula: A z-score standardizes variables.	What does it mean to normalize a variable
313	A correlation between two variables does not imply causation. On the other hand, if there is a causal relationship between two variables, they must be correlated. Example: A study shows that there is a negative correlation between a student's anxiety before a test and the student's score on the test.	Are there ever any circumstances when a correlation can be interpreted as evidence for a causal connection between two variables
2820	MNIST Handwritten Digit Classification Dataset The MNIST dataset is an acronym that stands for the Modified National Institute of Standards and Technology dataset. It is a dataset of 60,000 small square 28×28 pixel grayscale images of handwritten single digits between 0 and 9.	What is Mnist classification
5636	Explainable AI (XAI) refers to methods and techniques in the application of artificial intelligence technology (AI) such that the results of the solution can be understood by humans.	What is explainable machine learning
1223	Bayes Theorem for Modeling Hypotheses. Bayes Theorem is a useful tool in applied machine learning. It provides a way of thinking about the relationship between data and a model. A machine learning algorithm or model is a specific way of thinking about the structured relationships in the data.	How Bayes theorem is applied in machine learning
6049	Voting and Averaging Based Ensemble Methods Voting and averaging are two of the easiest ensemble methods.  Voting is used for classification and averaging is used for regression. In both methods, the first step is to create multiple classification/regression models using some training dataset.	What are voting and averaging based ensemble methods in machine learning
6374	The three main metrics used to evaluate a classification model are accuracy, precision, and recall. Accuracy is defined as the percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.	How do you evaluate machine learning model performance
805	Artificial General Intelligence	What does AGI stand for in artificial intelligence
8651	Why use Random Forest Algorithm Random forest algorithm can be used for both classifications and regression task. It provides higher accuracy through cross validation. Random forest classifier will handle the missing values and maintain the accuracy of a large proportion of data.	Why do we use random forest
2826	In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data.  In supervised feature learning, features are learned using labeled input data.	Is the representation useful for machine learning
128	15:3248:19Suggested clip · 37 secondsMotion 5 | How to Use Motion Tracking, Analyze Motion, and Match YouTubeStart of suggested clipEnd of suggested clip	How do you analyze motion
4385	If the student does have multiple learning styles (multimodal), the advantages gained through multiple learning strategies include the ability to learn more quickly and at a deeper level so that recall at a later date will be more successful. Using various modes of learning also improves attention span.	Why is multimodal learning important
6926	TensorFlow is Google's open source AI framework for machine learning and high performance numerical computation. TensorFlow is a Python library that invokes C++ to construct and execute dataflow graphs. It supports many classification and regression algorithms, and more generally, deep learning and neural networks.	Is TensorFlow a framework
2353	Cluster analysis is the task of grouping a set of data points in such a way that they can be characterized by their relevancy to one another.  These types are Centroid Clustering, Density Clustering Distribution Clustering, and Connectivity Clustering.	What is cluster analysis and its types
1206	The LASSO method puts a constraint on the sum of the absolute values of the model parameters, the sum has to be less than a fixed value (upper bound). In order to do so the method apply a shrinking (regularization) process where it penalizes the coefficients of the regression variables shrinking some of them to zero.	How does Lasso regression perform model selection
4315	Whereas multiple regression predicts a single dependent variable from a set of multiple independent variables, canonical correlation simultaneously predicts multiple dependent variables from multiple independent variables.	When would someone use canonical correlation analysis versus multiple multiple regressions
7296	One disadvantage to this method is that outliers can cause less-than-optimal merging. Average Linkage, or group linkage: similarity is calculated between groups of objects, rather than individual objects. Centroid Method: each iteration merges the clusters with the most similar centroid.	What are the disadvantages of agglomerative hierarchical clustering
256	Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. Tim Salimans, Diederik P. Kingma. Download PDF. We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction.	What is called as weight normalization in machine learning
2303	Definition : A random experiment is an experiment or a process for which the outcome cannot be predicted with certainty. Definition : The sample space (denoted S) of a random experiment is the set of all possible outcomes.	What is random experiment with example
696	A frequency table is a chart that shows the popularity or mode of a certain type of data. When we look at frequency, we are looking at the number of times an event occurs within a given scenario.  You can find the relative frequency by simply dividing the frequency number by the total number of values in the data set.	What is the difference between a data table frequency table and relative frequency table
4448	Risk tolerance	What is the opposite of risk aversion
1722	But when we say multiple regression, we mean only one dependent variable with a single distribution or variance. The predictor variables are more than one. To summarise multiple refers to more than one predictor variables but multivariate refers to more than one dependent variables.	What is the difference between regression analysis and multi variate analysis
925	It is not rare that the results from a study that uses a convenience sample differ significantly with the results from the entire population.  Since the sample is not representative of the population, the results of the study cannot speak for the entire population. This results to a low external validity of the study.	How does convenience sampling affect results
3198	If you use import numpy , all sub-modules and functions in the numpy module can only be accesses in the numpy.  If you use from numpy import * , all functions will be loaded into the local namespace. For example array([1,2,3]) can then be used.	What is the difference between import numpy and from numpy import *
2476	Partitions: A collection of sets B1,B2,,Bn is said to partition the sample space if the sets (i) are mutually disjoint and (ii) have as union the entire sample space. A simple example of a partition is given by a set B, together with its complement B . 2.	What is a partition in probability
3992	Fei-Fei Li, computer vision is defined as “a subset of mainstream artificial intelligence that deals with the science of making computers or machines visually enabled, i.e., they can analyze and understand an image.” Human vision starts at the biological camera's “eyes,” which takes one picture about every 200	What is AI computer vision
5585	In regression analysis, those factors are called variables. You have your dependent variable — the main factor that you're trying to understand or predict.	What is the dependent variable in a regression
6545	Integral testIt is possible to prove that the harmonic series diverges by comparing its sum with an improper integral.  Additionally, the total area under the curve y = 1x from 1 to infinity is given by a divergent improper integral:More items	How do you prove that a harmonic series is divergent
6034	The law of large numbers states that the sample mean of independent and identically distributed observations converges to a certain value. The central limit theorem describes the distribution of the difference between the sample mean and that value.	Whats the difference between the law of large numbers and the central limit theorem
762	The test statistic is used to calculate the p-value. A test statistic measures the degree of agreement between a sample of data and the null hypothesis.  This Z-value corresponds to a p-value of 0.0124. Because this p-value is less than α, you declare statistical significance and reject the null hypothesis.	Is P value the same as test statistic
777	TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.	Is TensorFlow used for machine learning
1239	A sample refers to a smaller, manageable version of a larger group. It is a subset containing the characteristics of a larger population. Samples are used in statistical testing when population sizes are too large for the test to include all possible members or observations.	How do you describe a sample in statistics
4153	In a nutshell, hierarchical linear modeling is used when you have nested data; hierarchical regression is used to add or remove variables from your model in multiple steps. Knowing the difference between these two seemingly similar terms can help you determine the most appropriate analysis for your study.	When should we use hierarchical linear models
5637	A metric is a function that is used to judge the performance of your model. Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Note that you may use any loss function as a metric.	What is metrics in neural network
5836	Partial least squares discriminant analysis (PLS-DA) is a variant used when the Y is categorical. PLS is used to find the fundamental relations between two matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces.	What is partial least squares discriminant analysis
4283	It helps you to find which situation needs an action. Helps you to discover which action yields the highest reward over the longer period. Reinforcement Learning also provides the learning agent with a reward function. It also allows it to figure out the best method for obtaining large rewards.21‏/09‏/2020	What can reinforcement learning be used for
6639	Micro-level adaptive instruction: The main feature of this approach is to utilize on-task rather than pre-task measurement to diagnose the students' learning behaviors and performance so as to adapt the instruction at the micro-level. Typical examples include one-on-one tutoring and intelligent tutoring systems.	Which is an example of adaptive instruction
124	In statistics, multivariate analysis of variance (MANOVA) is a procedure for comparing multivariate sample means. As a multivariate procedure, it is used when there are two or more dependent variables, and is often followed by significance tests involving individual dependent variables separately.	What use does multivariate Analyses of variance have if any
7422	To sum up: to build a conditional random field, you just define a bunch of feature functions (which can depend on the entire sentence, a current position, and nearby labels), assign them weights, and add them all together, transforming at the end to a probability if necessary.	How does conditional random field work
7957	In simple random sampling, each member of a population has an equal chance of being included in the sample. Also, each combination of members of the population has an equal chance of composing the sample. Those two properties are what defines simple random sampling.	What is the probability of a simple random sample
3061	A dimension is a structure that categorizes facts and measures in order to enable users to answer business questions.  In a data warehouse, dimensions provide structured labeling information to otherwise unordered numeric measures. The dimension is a data set composed of individual, non-overlapping data elements.	What is the dimension of data
1825	If you have outliers, the best way is to use a clustering algorithm that can handle them. For example DBSCAN clustering is robust against outliers when you choose minpts large enough. Don't use k-means: the squared error approach is sensitive to outliers. But there are variants such as k-means-- for handling outliers.	How do clusters deal with outliers
5419	Dilution (also called Dropout) is a regularization technique for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. It is an efficient way of performing model averaging with neural networks. The term dilution refers to the thinning of the weights.	What does dropout do in neural network
352	ADALINE (Adaptive Linear Neuron or later Adaptive Linear Element) is an early single-layer artificial neural network and the name of the physical device that implemented this network. The network uses memistors.  It is based on the McCulloch–Pitts neuron. It consists of a weight, a bias and a summation function.	What is Adaline in neural network
899	The Logit Model, better known as Logistic Regression is a binomial regression model. Logistic Regression is used to associate with a vector of random variables to a binomial random variable. Logistic regression is a special case of a generalized linear model. It is widely used in machine learning.	What is a logit model in data science
1994	K-means is a least-squares optimization problem, so is PCA. k-means tries to find the least-squares partition of the data. PCA finds the least-squares cluster membership vector.	What's the relationship between PCA and K means clustering
233	The assumption of homogeneity of variance means that the level of variance for a particular variable is constant across the sample.  In ANOVA, when homogeneity of variance is violated there is a greater probability of falsely rejecting the null hypothesis.	What does it mean when homogeneity of variance is violated
4100	Sampling bias occurs when some members of a population are systematically more likely to be selected in a sample than others. It is also called ascertainment bias in medical fields.  In other words, findings from biased samples can only be generalized to populations that share characteristics with the sample.	What is sampling bias in research
891	An object detector that uses anchor boxes can process an entire image at once, making real-time object detection systems possible. Because a convolutional neural network (CNN) can process an input image in a convolutional manner, a spatial location in the input can be related to a spatial location in the output.	How do anchor boxes in object detection really work
689	The normal distribution is a continuous probability distribution that is symmetrical on both sides of the mean, so the right side of the center is a mirror image of the left side. The area under the normal distribution curve represents probability and the total area under the curve sums to one.	How is the normal probability distribution related to the normal curve
1040	Best Way to Analyze Likert Item Data: Two Sample T-Test versus Mann-WhitneyParametric tests, such as the 2-sample t-test, assume a normal, continuous distribution.  Nonparametric tests, such as the Mann-Whitney test, do not assume a normal or a continuous distribution.	Which statistical analysis should I use to analyze a Likert scale survey
7021	The null hypothesis is the one to be tested and the alternative is everything else. In our example, The null hypothesis would be: The mean data scientist salary is 113,000 dollars. While the alternative: The mean data scientist salary is not 113,000 dollars.	What is null hypothesis and alternative hypothesis with examples
429	"Electronic apparatus which generates random numbers, used as targets in a psi test. A basic form of REG is an electronic coin-tossing machine, generating a series of ""heads and tails"" outputs. Other REGs have more complex outputs."	What is a random event generator
544	For many continuous random variables, we can define an extremely useful function with which to calculate probabilities of events associated to the random variable. In short, the PDF of a continuous random variable is the derivative of its CDF.	Is the derivative of the probability distribution function PDF just a cumulative distribution function cdf
8349	The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of three or more independent (unrelated) groups.	When is analysis of variance the appropriate statistical procedure to use
6086	How to Compare Data SetsCenter. Graphically, the center of a distribution is the point where about half of the observations are on either side.Spread. The spread of a distribution refers to the variability of the data.  Shape. The shape of a distribution is described by symmetry, skewness, number of peaks, etc.Unusual features.	How do you compare datasets
626	Deep learning is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data.  Deep learning allows machines to solve complex problems even when using a data set that is very diverse, unstructured and inter-connected.	What is deep learning in AI
90	An estimator of a given parameter is said to be unbiased if its expected value is equal to the true value of the parameter. In other words, an estimator is unbiased if it produces parameter estimates that are on average correct.	What makes an estimator unbiased
212	Here are just some of the many uses of eigenvectors and eigenvalues:Using singular value decomposition for image compression.  Deriving Special Relativity is more natural in the language of linear algebra.  Spectral Clustering.  Dimensionality Reduction/PCA.  Low rank factorization for collaborative prediction.More items	What are some applications of Eigenvalues and Eigenvectors
3907	How to Calculate VarianceFind the mean of the data set. Add all data values and divide by the sample size n.Find the squared difference from the mean for each data value. Subtract the mean from each data value and square the result.Find the sum of all the squared differences.  Calculate the variance.	How do you calculate variance
407	Events A and B are independent if the equation P(A∩B) = P(A) · P(B) holds true. You can use the equation to check if events are independent; multiply the probabilities of the two events together to see if they equal the probability of them both happening together.	How do you calculate independent and dependent probability
8523	In probability theory and statistics, a covariance matrix (also known as auto-covariance matrix, dispersion matrix, variance matrix, or variance–covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector.	What does covariance matrix mean
8404	A Classification and Regression Tree(CART) is a predictive algorithm used in machine learning. It explains how a target variable's values can be predicted based on other values. It is a decision tree where each fork is a split in a predictor variable and each node at the end has a prediction for the target variable.	What is classification and regression tree
4948	The mean of the log-normal distribution is m = e μ + σ 2 2 , m = e^{\mu+\frac{\sigma^2}{2}}, m=eμ+2σ2​, which also means that μ \mu μ can be calculated from m m m: μ = ln ⁡ m − 1 2 σ 2 .	How do you find the mean of a lognormal distribution
4701	Factor analysis aims to find independent latent variables.  The theory behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset.	What is latent factor analysis
3307	A relatively new method of dimensionality reduction is the autoencoder. Autoencoders are a branch of neural network which attempt to compress the information of the input variables into a reduced dimensional space and then recreate the input data set.  This is where the information from the input has been compressed.	Can Autoencoders be used for dimensionality reduction
732	The geometric mean differs from the arithmetic average, or arithmetic mean, in how it is calculated because it takes into account the compounding that occurs from period to period. Because of this, investors usually consider the geometric mean a more accurate measure of returns than the arithmetic mean.	Why is geometric mean better than arithmetic
6691	Prior probability, in Bayesian statistical inference, is the probability of an event before new data is collected. This is the best rational assessment of the probability of an outcome based on the current knowledge before an experiment is performed.	What is prior probability in machine learning
8503	In mathematics, a Fourier series (/ˈfʊrieɪ, -iər/) is a periodic function composed of harmonically related sinusoids, combined by a weighted summation.  The discrete-time Fourier transform is an example of Fourier series. The process of deriving the weights that describe a given function is a form of Fourier analysis.	What do you mean by Fourier series
6228	Level of significance (alpha error): 0.05. The test is run, and the p value obtained was 0.02 (p=0.02). What does the p value indicate? It tells us that if the null hypothesis were true, the probability of obtaining such a difference (or more extreme difference) in timing between the two fighters is 2 in 100, or 0.02.	What does P value of 0.02 mean
6443	In General, A Discriminative model ‌models the decision boundary between the classes. A Generative Model ‌explicitly models the actual distribution of each class.  A Discriminative model ‌learns the conditional probability distribution p(y|x). Both of these models were generally used in supervised learning problems.	What is the difference between generative and discriminative models in machine learning pattern recognition
1535	The correlation coefficient is a measure of the degree of linear association between two continuous variables, i.e. when plotted together, how close to a straight line is the scatter of points.  Both x and y must be continuous random variables (and Normally distributed if the hypothesis test is to be valid).	For correlation coefficient between two random variables to be a meaningful measure of their linear association do the variables need to be normally distributed
318	The output unit is used to present soft and hardcopy of information. The VDU (Visual Display Unit or Monitor) and printer are common output units. There are many categories of display units available for computer.	What are output units used for
929	Exploratory Data Analysis tools (EDA) are a diverse mix of tools that are mainly used to explore data, to find trends, exception, rules, correlation and other statistical feedback. These tools are something fairly technical (R | SPSS) or the fairly visual (Visual Intelligence | Tableau Software) stack.	What are the tools we can use for exploratory data analysis
3804	In Grid Search, the data scientist sets up a grid of hyperparameter values and for each combination, trains a model and scores on the testing data.  By contrast, Random Search sets up a grid of hyperparameter values and selects random combinations to train the model and score.	What is the difference between random search and grid search for hyperparameter optimization
1015	Best Image Processing Projects CollectionLicense plate recognition.Face Emotion recognition.Face recognition.Cancer detection.Object detection.Pedestrian detection.Lane detection for ADAS.Blind assistance systems.More items	Whatt are best image processing ideas
4651	Mean filtering is a simple, intuitive and easy to implement method of smoothing images, i.e. reducing the amount of intensity variation between one pixel and the next. It is often used to reduce noise in images.	What is mean filter in image processing
1350	Population change, defined generally, is the difference in the size of a population between the end and the beginning of a given time period (usually one year).  Population change has two components: natural population change (the number of live births minus the number of deaths);	What does population change mean
1027	Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.	Which method analyzes the document based on semantic words
5932	Yes it can be the same. In fact, If you don't write a meta description, Google will take portion of your site's content, which it sees are relevant, and make it your meta description.  A meta description is important because it is one of the first things that users will see in the SERPs.	Can meta description be the same as content
983	In a normal distribution, the mean and the median are the same number while the mean and median in a skewed distribution become different numbers: A left-skewed, negative distribution will have the mean to the left of the median. A right-skewed distribution will have the mean to the right of the median.	Is skewed distribution a normal distribution
2071	The Minkowski distance defines a distance between two points in a normed vector space. Minkowski Distance. When p=1 , the distance is known as the Manhattan distance. When p=2 , the distance is known as the Euclidean distance. In the limit that p --> +infinity , the distance is known as the Chebyshev distance.	What is P in Minkowski distance
4133	The AUC value lies between 0.5 to 1 where 0.5 denotes a bad classifer and 1 denotes an excellent classifier.	What is a good ROC score
4839	In short, fourier series is for periodic signals and fourier transform is for aperiodic signals. Fourier series is used to decompose signals into basis elements (complex exponentials) while fourier transforms are used to analyze signal in another domain (e.g. from time to frequency, or vice versa).	What is the difference between Fourier series and Fourier transform
4763	Grid-searching is the process of scanning the data to configure optimal parameters for a given model.  Grid-Search will build a model on each parameter combination possible. It iterates through every parameter combination and stores a model for each combination.	What is grid search in ML
1055	A-squared is the test statistic for the Anderson-Darling Normality test. It is a measure of how closely a dataset follows the normal distribution.  So if you get an A-squared that is fairly large, then you will get a small p-value and thus reject the null hypothesis.	What is a squared in Anderson Darling normality test
4737	The sample mean is a consistent estimator for the population mean. A consistent estimate has insignificant errors (variations) as sample sizes grow larger.  In other words, the more data you collect, a consistent estimator will be close to the real population parameter you're trying to measure.	Why is the definition of a consistent estimator the way it is
423	Efficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter. Simplicity: ReLu is simple.	What are the advantages of relu and leaky relu over sigmoid activation functions
5026	The main motivation is to aggregate multiple low-level features in the neighborhood to gain invariance mainly in object recognition. Why do we use pooling layers in CNN?	What is the motivation for pooling in convolutional neural networks CNN
8504	The RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close the observed data points are to the model's predicted values. Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit.  Lower values of RMSE indicate better fit.	What is the difference between RMSE and R squared in statistics
8033	Machine Learning This phenomenon states that with a fixed number of training samples, the average (expected) predictive power of a classifier or regressor first increases as number of dimensions or features used is increased but beyond a certain dimensionality it starts deteriorating instead of improving steadily.	What is the curse of dimensionality in machine learning
435	Mean Absolute Error (MAE) is another loss function used for regression models. MAE is the sum of absolute differences between our target and predicted variables. So it measures the average magnitude of errors in a set of predictions, without considering their directions.	What is mean absolute error in regression
5119	The sum of a square matrix and its conjugate transpose. is Hermitian. The difference of a square matrix and its conjugate transpose. is skew-Hermitian.	What are Hermitian and skew Hermitian matrix
1944	Advantages of Dimensionality Reduction It helps in data compression, and hence reduced storage space. It reduces computation time. It also helps remove redundant features, if any.	Why dimensionality reduction is important step in machine learning
6517	Swarm-intelligence principles inspired by the collective insect societies are used for developing computer algorithms and motion control principles for robotics. The basic idea is that a swarm of individuals can coordinate and behave as a single entity that performs better than the individuals.	Where is swarm intelligence used
4804	A support vector machine is a machine learning model that is able to generalise between two different classes if the set of labelled data is provided in the training set to the algorithm. The main function of the SVM is to check for that hyperplane that is able to distinguish between the two classes.	How does a support vector machine work
1457	You can pass data between view controllers in Swift in 6 ways:By using an instance property (A → B)By using segues (for Storyboards)By using instance properties and functions (A ← B)By using the delegation pattern.By using a closure or completion handler.By using NotificationCenter and the Observer pattern.	How do you pass data between view controllers
1904	Non-hierarchical clustering is frequently referred to as k-means clustering. This type of clustering does not require all possible distances to be computed in a large data set. This technique is primarily used for the analysis of clusters in data mining.	Is frequently referred to as K means clustering
8008	If you have n numbers in a group, the median is the (n + 1)/2 th value. For example, there are 7 numbers in the example above, so replace n by 7 and the median is the (7 + 1)/2 th value = 4th value. The 4th value is 6. On a histogram, the median value occurs where the whole histogram is divided into two equal parts.	How do you find the median in a histogram
846	"The Kruskal-Wallis H test (sometimes also called the ""one-way ANOVA on ranks"") is a rank-based nonparametric test that can be used to determine if there are statistically significant differences between two or more groups of an independent variable on a continuous or ordinal dependent variable."	What is Kruskal Wallis test used for
2671	Most machine learning roles will require the use of Python or C/C++ (though Python is often preferred). Background in the theory behind machine learning algorithms and an understanding of how they can be efficiently implemented in terms of both space and time is critical.	What background do you need for machine learning
1086	Here are the most common examples of multitasking in personal and professional settings:Responding to emails while listening to a podcast.Taking notes during a lecture.Completing paperwork while reading the fine print.Driving a vehicle while talking to someone.Talking on the phone while greeting someone.More items•	What are some examples of multitasking
5462	The distributional hypothesis suggests that the more semantically similar two words are, the more distributionally similar they will be in turn, and thus the more that they will tend to occur in similar linguistic contexts.	What are distributional properties
935	Yes. For a 1D signal, shift invariance of a filter implies the following.  The following example illustrates the shift invariance (for all the signals, the sample at the origin is in bold, and zero padding is assumed).	Is median filter shift invariant
5286	Tabular in this context simply means that we will store the Q function in a lookup table. I.e. we create a table where we store the Q value for each possible State and Move.	What is tabular Q learning
521	recursion	Which search method is used in Minimax algorithm
988	The simple moving average (SMA) is the average price of a security over a specific period.  The exponential moving average (EMA) provides more weight to the most recent prices in an attempt to better reflect new market data. The difference between the two is noticeable when comparing long-term averages.	What is the difference between exponential moving average and simple moving average
1073	It is believed that Facebook's new algorithm is based on the Vickrey-Clarke-Groves algorithm, which “operates as a closed auction.” Facebook's algorithm for ranking content on your News Feed is based on four factors: The Inventory of all posts available to display. Signals that tell Facebook what each post is.	What algorithm is used in Facebook
3624	Machine learning uses algorithms to parse data, learn from that data, and make informed decisions based on what it has learned.  Deep learning is a subfield of machine learning. While both fall under the broad category of artificial intelligence, deep learning is what powers the most human-like artificial intelligence.	Is machine learning and deep learning same
4697	"The relative efficiency of two procedures is the ratio of their efficiencies, although often this concept is used where the comparison is made between a given procedure and a notional ""best possible"" procedure."	What do you mean by relative efficiency
814	Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures.  Descriptive statistics are typically distinguished from inferential statistics. With descriptive statistics you are simply describing what is or what the data shows.	What do you mean by descriptive statistics
292	Data skewed to the right is usually a result of a lower boundary in a data set (whereas data skewed to the left is a result of a higher boundary). So if the data set's lower bounds are extremely low relative to the rest of the data, this will cause the data to skew right. Another cause of skewness is start-up effects.	What causes a skewed distribution
5924	These are some of the most popular examples of artificial intelligence that's being used today. Everyone is familiar with Apple's personal assistant, Siri. She's the friendly voice-activated computer that we interact with on a daily basis.	What is an example of artificial intelligence
1060	The loss function is used to optimize your model. This is the function that will get minimized by the optimizer. A metric is used to judge the performance of your model.	What is the difference between a loss function and a metric
7813	To conclude, the important thing to remember about the odds ratio is that an odds ratio greater than 1 is a positive association (i.e., higher number for the predictor means group 1 in the outcome), and an odds ratio less than 1 is negative association (i.e., higher number for the predictor means group 0 in the outcome	How do you interpret odds ratio in logistic regression
760	Quota sampling achieves a representative age distribution, but it isn't a random sample, because the sampling frame is unknown. Therefore, the sample may not be representative of the population.	Is quota a sampling representative
8085	A neural network is either a system software or hardware that works similar to the tasks performed by neurons of human brain. Neural networks include various technologies like deep learning, and machine learning as a part of Artificial Intelligence (AI).	What are neural networks and how do they relate to AI
8425	"A convolution is an integral that expresses the amount of overlap of one function as it is shifted over another function. . It therefore ""blends"" one function with another."	What is a convolution integral
2251	Decision tree	Which machine learning algorithm is more applicable for continuous data
1283	The model can only make recommendations based on existing interests of the user. In other words, the model has limited ability to expand on the users' existing interests.	What is the shortcoming of content based recommender systems
709	Mann-Whitney test	Which non parametric test is the equivalent of an independent samples t test
2567	Attention Mechanism in Neural Networks - 1. Introduction. Attention is arguably one of the most powerful concepts in the deep learning field nowadays. It is based on a common-sensical intuition that we “attend to” a certain part when processing a large amount of information.	What is attention mechanism in neural networks
4324	Decision theory is an interdisciplinary approach to arrive at the decisions that are the most advantageous given an uncertain environment. Decision theory brings together psychology, statistics, philosophy, and mathematics to analyze the decision-making process.	What is decision theory approach
4481	The least squares method is a statistical procedure to find the best fit for a set of data points by minimizing the sum of the offsets or residuals of points from the plotted curve. Least squares regression is used to predict the behavior of dependent variables.	What does the principle of least square mean in regression
1393	Today, neural networks are used for solving many business problems such as sales forecasting, customer research, data validation, and risk management. For example, at Statsbot we apply neural networks for time-series predictions, anomaly detection in data, and natural language understanding.	What kind of problems can neural networks solve
451	A dummy variable (aka, an indicator variable) is a numeric variable that represents categorical data, such as gender, race, political affiliation, etc.  For example, suppose we are interested in political affiliation, a categorical variable that might assume three values - Republican, Democrat, or Independent.	What is dummy variable given an example
785	A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable.  The slope of the line is b, and a is the intercept (the value of y when x = 0).	What does a represent in linear regression
525	The use of computer algorithms plays an essential role in space search programs.  We are in the age of algorithms because they solve our everyday tasks and we won't be able to live with them. They make our life more comfortable and, in the future, they will be able to predict our behavior.	Why are algorithms so important
3996	Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).  Cluster analysis itself is not one specific algorithm, but the general task to be solved.	What is meant by cluster analysis
6555	Advantages. The main advantage of multivariate analysis is that since it considers more than one factor of independent variables that influence the variability of dependent variables, the conclusion drawn is more accurate.	What are the benefits of multivariate data analysis techniques
3442	The beta distribution is a continuous probability distribution that can be used to represent proportion or probability outcomes. For example, the beta distribution might be used to find how likely it is that your preferred candidate for mayor will receive 70% of the vote.	Why do we use beta distribution
1467	EdgeRank	What is the name for Facebook's ranking algorithm
3781	PCA attempts to find uncorrelated sources, where as ICA attempts to find independent sources. Both techniques try to obtain new sources by linearly combining the original sources.	What is the difference between ICA and PCA
4854	Output is defined as the act of producing something, the amount of something that is produced or the process in which something is delivered. An example of output is the electricity produced by a power plant. An example of output is producing 1,000 cases of a product.	What is output and examples
6410	In our categorical case we would use a simple regression equation for each group to investigate the simple slopes. It is common practice to standardize or center variables to make the data more interpretable in simple slopes analysis; however, categorical variables should never be standardized or centered.	Do we need to standardize categorical variables
5695	“Bayesian statistics is a mathematical procedure that applies probabilities to statistical problems. It provides people the tools to update their beliefs in the evidence of new data.”	What is Bayesian statistics used for
790	Multiple Linear Regression Analysis consists of more than just fitting a linear line through a cloud of data points. It consists of three stages: 1) analyzing the correlation and directionality of the data, 2) estimating the model, i.e., fitting the line, and 3) evaluating the validity and usefulness of the model.	How do you do a multiple regression analysis
3810	The posterior probability is one of the quantities involved in Bayes' rule. It is the conditional probability of a given event, computed after observing a second event whose conditional and unconditional probabilities were known in advance.	Is posterior conditional probability
1004	There are two major different types of uncertainty in deep learning: epistemic uncertainty and aleatoric uncertainty.  Epistemic uncertainty describes what the model does not know because training data was not appropriate. Epistemic uncertainty is due to limited data and knowledge.	What is uncertainty in deep learning
1483	Every parametric test has the assumption that the sample means are following a normal distribution. This is the case if the sample itself is normal distributed or if approximately if the sample size is big enough.	Why does data need to be normally distributed in parametric tests
5386	RMSLE, or the Root Mean Square Logarithmic Error, is the ratio (the log) between the actual values in your data and predicted values in the model.	What is root mean squared logarithmic error
6204	Approach –Load dataset from source.Split the dataset into “training” and “test” data.Train Decision tree, SVM, and KNN classifiers on the training data.Use the above classifiers to predict labels for the test data.Measure accuracy and visualise classification.	How do you do multi class classification
644	The mean (average) of a data set is found by adding all numbers in the data set and then dividing by the number of values in the set. The median is the middle value when a data set is ordered from least to greatest. The mode is the number that occurs most often in a data set.	What is the mean value in statistics
911	The k-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems.	Can Knn be used for classification
7427	Chi Square distributions are positively skewed, with the degree of skew decreasing with increasing degrees of freedom. As the degrees of freedom increases, the Chi Square distribution approaches a normal distribution. Figure 1 shows density functions for three Chi Square distributions.	What is the skewness of a chi square distribution
7856	In statistics, the likelihood function (often simply called the likelihood) measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters.	What is the meaning of likelihood in statistics
5103	Use the hypergeometric distribution with populations that are so small that the outcome of a trial has a large effect on the probability that the next outcome is an event or non-event. For example, in a population of 10 people, 7 people have O+ blood.	When would you use a hypergeometric distribution
137	For values of x > 0, the gamma function is defined using an integral formula as Γ(x) = Integral on the interval [0, ∞ ] of ∫ 0∞t x −1 e−t dt. The probability density function for the gamma distribution is given by. The mean of the gamma distribution is αβ and the variance (square of the standard deviation) is αβ2.	What is the mean of gamma distribution
5903	Convenience sampling is a type of non-probability sampling, which doesn't include random selection of participants. The opposite is probability sampling, where participants are randomly selected, and each has an equal chance of being chosen.	Is a convenience sample random
6808	Bivariate analysis means the analysis of bivariate data. It is one of the simplest forms of statistical analysis, used to find out if there is a relationship between two sets of values. It usually involves the variables X and Y.	How do you use bivariate analysis
1917	The t distribution (aka, Student's t-distribution) is a probability distribution that is used to estimate population parameters when the sample size is small and/or when the population variance is unknown.	What is the use of T distribution
5388	When part of the memory network is activated, activation spreads along the associative pathways to related areas in memory. This spread of activation serves to make these related areas of the memory network more available for further cognitive processing (Balota & Lorch, 1986).	What is the effect of spreading activation
4307	Random Forest is less computationally expensive and does not require a GPU to finish training. A random forest can give you a different interpretation of a decision tree but with better performance. Neural Networks will require much more data than an everyday person might have on hand to actually be effective.	Which classifier is better random forests or deep neural networks
4604	Intuitively, two random variables X and Y are independent if knowing the value of one of them does not change the probabilities for the other one. In other words, if X and Y are independent, we can write P(Y=y|X=x)=P(Y=y), for all x,y.	Does random variables imply independence
7934	The probability of an outcome is interpreted as the long-run proportion of the time that the outcome would occur, if the experiment were repeated indefinitely. That is, probability is long-term relative frequency.	How do you interpret long run relative frequency
1814	The easiest way to convert categorical variables to continuous is by replacing raw categories with the average response value of the category. cutoff : minimum observations in a category. All the categories having observations less than the cutoff will be a different category.	How do you convert a categorical variable to a continuous variable
834	Most deep learning methods use neural network architectures, which is why deep learning models are often referred to as deep neural networks.  A CNN convolves learned features with input data, and uses 2D convolutional layers, making this architecture well suited to processing 2D data, such as images.	How networks do deep learning
5745	Random error varies unpredictably from one measurement to another, while systematic error has the same value or proportion for every measurement. Random errors are unavoidable, but cluster around the true value.	What is the difference between random errors and non random errors in experimental data
6289	Preparing Your Dataset for Machine Learning: 8 Basic Techniques That Make Your Data BetterArticulate the problem early.Establish data collection mechanisms.Format data to make it consistent.Reduce data.Complete data cleaning.Decompose data.Rescale data.Discretize data.	How do you prepare data for machine learning
553	“Candidate Sampling” training methods involve constructing a training task in which for each. training example. , we only need to evaluate. for a small set of candidate classes.	What is candidate sampling in machine learning
3282	When it comes to machine learning, topology is not as ubiquitous as local geometry, but in almost all cases where local geometry is useful so is topology.	Is topology useful for machine learning
523	Pearson's product moment correlation coefficient (r) is given as a measure of linear association between the two variables: r² is the proportion of the total variance (s²) of Y that can be explained by the linear regression of Y on x. 1-r² is the proportion that is not explained by the regression.	What is the correlation coefficient in a linear regression
8246	The comparison - wise error rate is the probability of a Type I error set by the experimentor for evaluating each comparison. The experiment - wise error rate is the probability of making at least one Type I error when performing the whole set of comparisons.	What is comparison wise error rate
7695	– Validation set: A set of examples used to tune the parameters of a classifier, for example to choose the number of hidden units in a neural network. – Test set: A set of examples used only to assess the performance of a fully-specified classifier. These are the recommended definitions and usages of the terms.	What is the difference between a validation set and a test set
271	"The set of all ""normal"" Turing machines, i.e., the set of all Turing machines, can compute all computable functions.  The difference is that a single universal Turing machine can simulate the computation of all computable functions depending on how you interpret its input."	Why do we differentiate between universal Turing machines and normal Turing machines
228	To find the confidence interval in R, create a new data. frame with the desired value to predict. The prediction is made with the predict() function. The interval argument is set to 'confidence' to output the mean interval.	How do you predict confidence intervals in R
1073	API KPIs (Key Performance Indicators) Defining the key performance indicators (KPIs) for APIs being used is a critical part of understanding not just how they work but how well they can work and the impact they have on your services, users or partners.	What is API and KPI
1273	For the Wilcoxon test, a p-value is the probability of getting a test statistic as large or larger assuming both distributions are the same. In addition to a p-value we would like some estimated measure of how these distributions differ. The wilcox. test function provides this information when we set conf.int = TRUE .	What is the importance of the P value in a hypothesis test such as the Wilcoxon rank sum test
1458	Simply put, homoscedasticity means “having the same scatter.” For it to exist in a set of data, the points must be about the same distance from the line, as shown in the picture above. The opposite is heteroscedasticity (“different scatter”), where points are at widely varying distances from the regression line.	What does homoscedasticity mean in regression
1258	Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid, which was trained in a supervised way to learn several computer vision tasks.	What is the purpose of rectifier functions in neural networks
753	MANOVA is useful in experimental situations where at least some of the independent variables are manipulated. It has several advantages over ANOVA. First, by measuring several dependent variables in a single experiment, there is a better chance of discovering which factor is truly important.	Why use a Manova instead of Anova
56	The joint probability density function (joint pdf) is a function used to characterize the probability distribution of a continuous random vector. It is a multivariate generalization of the probability density function (pdf), which characterizes the distribution of a continuous random variable.	What is a joint density function
506	"The core idea is that we cannot know exactly how well an algorithm will work in practice (the true ""risk"") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the ""empirical"" risk)."	What is empirical risk in machine learning
99	Moment generating functions are a way to find moments like the mean(μ) and the variance(σ2). They are an alternative way to represent a probability distribution with a simple one-variable function.	What is a moment generating function used for
3186	Software Testing MethodologiesFunctional vs. Non-functional Testing.  Unit Testing. Unit testing is the first level of testing and is often performed by the developers themselves.  Integration Testing.  System Testing.  Acceptance Testing.  Performance Testing.  Security Testing.  Usability Testing.More items	What are different testing techniques
7699	Shading units (or stream processors) are small processors within the graphics card that are responsible for processing different aspects of the image.  This means that the more shading units that a graphics card has, the faster it will be able to allocate power to process the workload.	What are shading units
3959	0:087:41Suggested clip · 120 secondsHow to Create a Multiple Regression Equation - Business Statistics YouTubeStart of suggested clipEnd of suggested clip	How do you find the equation of a multiple regression
4178	Neural networks are designed to work just like the human brain does. In the case of recognizing handwriting or facial recognition, the brain very quickly makes some decisions. For example, in the case of facial recognition, the brain might start with “It is female or male?	What is Neural Network example
1011	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution.	Why is distribution important in statistics
5341	Start by learning key data analysis tools such as Microsoft Excel, Python, SQL and R. Excel is the most widely used spreadsheet program and is excellent for data analysis and visualization. Enroll in one of the free Excel courses and learn how to use this powerful software.	How do I learn data analysis
773	Image recognition is the process of identifying and detecting an object or a feature in a digital image or video. This concept is used in many applications like systems for factory automation, toll booth monitoring, and security surveillance. Typical image recognition algorithms include: Optical character recognition.	What is recognition in image processing
1976	The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent solely on the current state and time elapsed.	What are the characteristics of Markov process
4080	Specific Jobs in AIMachine Learning Researchers.AI Engineer.Data Mining and Analysis.Machine Learning Engineer.Data Scientist.Business Intelligence (BI) Developer.	What can we do after learning machine learning
5869	Formally, calibration is the documented comparison of the measurement device to be calibrated against a traceable reference device. The reference standard may be also referred as a “calibrator.” Logically, the reference is more accurate than the device to be calibrated.	What is meant by calibration
42	The power of AI and robotics combined In theory, if you combine AI and a robot, you get an artificially intelligent robot with a high level of autonomy, able to optimize tasks it is assigned to do and “learn”. In this case, AI serves as the “brain” of the robot, while the sensors and mechanical parts act as the “body”.	What is the relationship between artificial intelligence and robots
3741	Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables. For instance, when we predict rent based on square feet alone that is simple linear regression.	What is the difference between simple linear regression and multiple linear regression
4592	As the area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test, the areas under ROC curves are used to compare the usefulness of tests. The term ROC stands for Receiver Operating Characteristic.	What is area under the receiver operating characteristic curve
7349	If you want to solve some real-world problems and design a cool product or algorithm, then having machine learning skills is not enough. You would need good working knowledge of data structures.  So you've decided to move beyond canned algorithms and start to code your own machine learning methods.	Is data structures and algorithms required for machine learning
1266	The main difference between stratified sampling and cluster sampling is that with cluster sampling, you have natural groups separating your population.  In stratified sampling, a sample is drawn from each strata (using a random sampling method like simple random sampling or systematic sampling).	What is the difference between simple random cluster systematic and stratified sampling techniques
6388	In stratified sampling, a random sample is drawn from each of the strata, whereas in cluster sampling only the selected clusters are sampled. A common motivation of cluster sampling is to reduce costs by increasing sampling efficiency.	What is random vs cluster sampling
947	The null hypothesis is generally denoted as H0. It states the exact opposite of what an investigator or an experimenter predicts or expects. It basically defines the statement which states that there is no exact or actual relationship between the variables. The alternative hypothesis is generally denoted as H1.	What is null hypothesis and alternative hypothesis
922	Matplotlib is a plotting library for Python. It is used along with NumPy to provide an environment that is an effective open source alternative for MatLab.	Is Matplotlib part of NumPy
1096	See the section on order statistics. One of the most important properties of the beta distribution, and one of the main reasons for its wide use in statistics, is that it forms a conjugate family for the success probability in the binomial and negative binomial distributions.	Why is the beta distribution important
662	7:0910:40Suggested clip · 88 secondsInterpreting SPSS Output for Factor Analysis - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you interpret a factor analysis in SPSS
8512	0:3910:15Suggested clip · 118 secondsConducting a Multiple Regression using Microsoft Excel Data YouTubeStart of suggested clipEnd of suggested clip	How do you do a regression analysis with multiple variables
2550	"At a bare minimum, collect around 1000 examples. For most ""average"" problems, you should have 10,000 - 100,000 examples. For “hard” problems like machine translation, high dimensional data generation, or anything requiring deep learning, you should try to get 100,000 - 1,000,000 examples."	How much data do you need for machine learning
1041	To test for non-time-series violations of independence, you can look at plots of the residuals versus independent variables or plots of residuals versus row number in situations where the rows have been sorted or grouped in some way that depends (only) on the values of the independent variables.	How do you check if a linear regression model violates the independence assumption
357	To learn this course one needs to have enough knowledge in Python and its libraries such as NumPy, Matplotlib, Jupyter, and TensorFlow. Also, this course requires Python 3.5 or Python 3.6. Click here to learn.	What should I learn for TensorFlow
1282	General steps to calculate the mean squared error from a set of X and Y values:Find the regression line.Insert your X values into the linear regression equation to find the new Y values (Y').Subtract the new Y value from the original to get the error.Square the errors.Add up the errors.Find the mean.	How do you calculate the mean square error
6167	A log-linear model is a mathematical model that takes the form of a function whose logarithm equals a linear combination of the parameters of the model, which makes it possible to apply (possibly multivariate) linear regression.	What does log linear mean
4400	To use the more formal terms for bias and variance, assume we have a point estimator ˆθ of some parameter or function θ. Then, the bias is commonly defined as the difference between the expected value of the estimator and the parameter that we want to estimate: Bias=E[ˆθ]−θ.	How do you calculate variance and bias
1995	Rejecting or failing to reject the null hypothesis If our statistical analysis shows that the significance level is below the cut-off value we have set (e.g., either 0.05 or 0.01), we reject the null hypothesis and accept the alternative hypothesis.	What does it mean if you reject the null hypothesis
5267	An experimental group is the group in a scientific experiment where the experimental procedure is performed.  A control group is a group separated from the rest of the experiment where the independent variable being tested is not exposed. You just studied 4 terms!	What is the difference between an experimental group and a control group quizlet
4129	2:5910:12Suggested clip · 118 secondsCalculating fractal dimensions - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the fractal dimension
7517	This cross-sectional sample provides us with a snapshot of that population, at that one point in time.  Panel data differs from pooled cross-sectional data across time, because it deals with the observations on the same subjects in different times whereas the latter observes different subjects in different time periods.	What is the difference between cross sectional data and panel data
328	Unconscious racial stereotypes are a major example of implicit bias. In other words, having an automatic preference for one race over another without even being aware of this bias.	What is implicit bias example
589	Statistically, the presence of an interaction between categorical variables is generally tested using a form of analysis of variance (ANOVA). If one or more of the variables is continuous in nature, however, it would typically be tested using moderated multiple regression.	How do you find interaction between two variables
2264	The minimum sample size is 100 Most statisticians agree that the minimum sample size to get any kind of meaningful result is 100. If your population is less than 100 then you really need to survey all of them.	What is the minimum number for a sample size
3266	Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.	Is deep learning supervised or unsupervised learning
7428	Definition: Quota sampling is a sampling methodology wherein data is collected from a homogeneous group. It involves a two-step process where two variables can be used to filter information from the population. It can easily be administered and helps in quick comparison.	What do you mean by quota sampling
554	Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called empirical risk minimization.	What is training a model in machine learning
5488	Percent Error Calculation StepsSubtract one value from another.  Divide the error by the exact or ideal value (not your experimental or measured value).  Convert the decimal number into a percentage by multiplying it by 100.Add a percent or % symbol to report your percent error value.	How do you find the measurement error
1367	Some popular examples of unsupervised learning algorithms are: k-means for clustering problems. Apriori algorithm for association rule learning problems.	Which of the following is an example of unsupervised learning
4695	LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place. LDA is a matrix factorization technique.	How does LDA modeling work
8284	Scales effectively with data: Deep networks scale much better with more data than classical ML algorithms.  With classical ML algorithms this quick and easy fix doesn't work even nearly as well and more complex methods are often required to improve accuracy.	Can deep learning scale better
1772	Image compression with principal component analysis is a frequently occurring application of the dimension reduction technique.  As the number of principal components used to project the new data increases, the quality and representation compared to the original image improve.	How principal components is useful for image compression
4975	Positive feedback helps motivation, boosts confidence, and shows people you value them. It helps people to understand and develop their skills. And all this has a positive impact on individual, team, and organisational performance.	How does positive feedback make you feel
7000	1 AnswerTake as central point of your confidence interval the sum of central points of every confidence interval (45+70+35=150 minutes).Take as radius of your interval the square root of the sum of the squares of the radius of every confidence interval √52+102+52=12.25.	How do you sum confidence intervals
4703	Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).	What statistical methods are used to analyze data
5477	Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis.	How do you evaluate cosine similarity
7142	Now we'll check out the proven way to improve the accuracy of a model:Add more data. Having more data is always a good idea.  Treat missing and Outlier values.  Feature Engineering.  Feature Selection.  Multiple algorithms.  Algorithm Tuning.  Ensemble methods.	How can the accuracy of a linear regression model be improved
29	To check for heteroscedasticity, you need to assess the residuals by fitted value plots specifically. Typically, the telltale pattern for heteroscedasticity is that as the fitted values increases, the variance of the residuals also increases.	How do you account for heteroskedasticity in regression
2293	In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.  Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex.	What is meant by convolution neural network
1101	In statistics, a two-tailed test is a method in which the critical area of a distribution is two-sided and tests whether a sample is greater than or less than a certain range of values. It is used in null-hypothesis testing and testing for statistical significance.	What does a two tailed test mean in statistics
6113	A heuristic is a mental shortcut that allows people to solve problems and make judgments quickly and efficiently. These rule-of-thumb strategies shorten decision-making time and allow people to function without constantly stopping to think about their next course of action.	How do heuristics affect decision making
1286	Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).	What does regression model mean
1750	Multivariate Regression is a method used to measure the degree at which more than one independent variable (predictors) and more than one dependent variable (responses), are linearly related.  A mathematical model, based on multivariate regression analysis will address this and other more complicated questions.	What is multivariable regression
805	Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  Factor analysis aims to find independent latent variables.	What does a factor analysis tell you
5137	The independent variable is the variable the experimenter changes or controls and is assumed to have a direct effect on the dependent variable.  The dependent variable is the variable being tested and measured in an experiment, and is 'dependent' on the independent variable.	What is the independent variable in an experiment
1226	Variance measures how far a set of data is spread out. A variance of zero indicates that all of the data values are identical.  A high variance indicates that the data points are very spread out from the mean, and from one another. Variance is the average of the squared distances from each point to the mean.	What does the variance tell us
367	In statistics, principal component regression (PCR) is a regression analysis technique that is based on principal component analysis (PCA).  In PCR, instead of regressing the dependent variable on the explanatory variables directly, the principal components of the explanatory variables are used as regressors.	How is principal component analysis used in regression
5916	3 Answers. Since your response is ordinal then you should use ordinal regression. At a very high level, the main difference ordinal regression and linear regression is that with linear regression the dependent variable is continuous and ordinal the dependent variable is ordinal.	Can you use ordinal data in a regression
3778	The formula for response rate is to take the number of responses returned and divide it by the number of surveys sent out, and multiply the result by 100.	How do you calculate positive response rate
2247	Time Series analysis is “an ordered sequence of values of a variable at equally spaced time intervals.” It is used to understand the determining factors and structure behind the observed data, choose a model to forecast, thereby leading to better decision making.	What is Time series analysis used for
5435	Sample size refers to the number of participants or observations included in a study. This number is usually represented by n. The size of a sample influences two statistical properties: 1) the precision of our estimates and 2) the power of the study to draw conclusions.	What is sample and sample size
1531	For every time domain waveform there is a corresponding frequency domain waveform, and vice versa. For example, a rectangular pulse in the time domain coincides with a sinc function [i.e., sin(x)/x] in the frequency domain.  Waveforms that correspond to each other in this manner are called Fourier transform pairs.	What is a Fourier transform pair
3046	Definition : A random experiment is an experiment or a process for which the outcome cannot be predicted with certainty. Definition : The sample space (denoted S) of a random experiment is the set of all possible outcomes.	What is random experiment as used in probability
2270	A curve that represents the cumulative frequency distribution of grouped data on a graph is called a Cumulative Frequency Curve or an Ogive. Representing cumulative frequency data on a graph is the most efficient way to understand the data and derive results.	What is the graphical representation of cumulative frequency distribution
6540	Sampling is used any time data is to be gathered. Data cannot be collected until the sample size (how much) and sample frequency (how often) have been determined. Sampling should be periodically reviewed.	Under what circumstances sampling is used
1718	A sequence of random variables X1, X2, X3, ⋯ converges in probability to a random variable X, shown by Xn p→ X, if limn→∞P(|Xn−X|≥ϵ)=0, for all ϵ>0.	How do you show convergence in probability
469	In Computer science (especially Machine learning) Pruning means simplifying/compressing and optimizing a Decision tree by removing sections of the tree that are uncritical and redundant to classify instances.	How is a decision tree pruned
2623	The reality is that stepwise regression is less effective the larger the number of potential explanatory variables. Stepwise regression does not solve the Big-Data problem of too many explanatory variables. Big Data exacerbates the failings of stepwise regression.	Why you should not use stepwise regression
805	Sequential is the easiest way to build a model in Keras. It allows you to build a model layer by layer. Each layer has weights that correspond to the layer the follows it. We use the 'add()' function to add layers to our model. We will add two layers and an output layer.	What is sequential model in deep learning
5908	Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true .	What is log loss in logistic regression
1242	The Fundamental Counting Principle If one event has p possible outcomes, and another event has m possible outcomes, then there are a total of p • m possible outcomes for the two events. Rolling two six-sided dice: Each die has 6 equally likely outcomes, so the sample space is 6 • 6 or 36 equally likely outcomes.	How many different outcomes are possible for 6 rolls of a die
1215	Multinomial logistic regression is used when you have a categorical dependent variable with two or more unordered levels (i.e. two or more discrete outcomes).  One level of the dependent variable is chosen as the reference category. This is typically the most common or the most frequent category.	What is a multinomial variable
1430	Hashing provides a more reliable and flexible method of data retrieval than any other data structure. It is faster than searching arrays and lists. In the same space it can retrieve in 1.5 probes anything stored in a tree that will otherwise take log n probes.	What is hashing and its advantages
650	Dual boot is completely safe if the operating systems are installed properly with correct GRUB configuration. The main advantage of having multiple operating systems is that, you get the best performance for your work if you are working on the particular operating system's native platforms, tools, etc.	Is Dual booting a good idea
8544	You can get the feature importance of each feature of your dataset by using the feature importance property of the model. Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.	How do you select important features in Python
572	A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.  Neural networks can adapt to changing input; so the network generates the best possible result without needing to redesign the output criteria.	What is neural network in simple words
26	The confidence interval (CI) is a range of values that's likely to include a population value with a certain degree of confidence. It is often expressed a % whereby a population means lies between an upper and lower interval.	What is a confidence interval in statistics
7496	"Mixed effect logistic regression is a type of multilevel model.  ""Mixed effect logistic"" would usually refer to cases where the outcome has 2 levels. Multinomial logistic regression is used when the dependent variable has more than two levels and they cannot be ordered."	What are the differences between mixed effect logistic regression and multinomial logistic regression
5134	Bias can creep into algorithms in several ways. AI systems learn to make decisions based on training data, which can include biased human decisions or reflect historical or social inequities, even if sensitive variables such as gender, race, or sexual orientation are removed.	How can AI systems be biased
4893	The major difference between the Mann-Whitney U and the Kruskal-Wallis H is simply that the latter can accommodate more than two groups. Both tests require independent (between-subjects) designs and use summed rank scores to determine the results.	What is the difference between Kruskal Wallis test and Mann Whitney test
3230	The false positive rate is calculated as FP/FP+TN, where FP is the number of false positives and TN is the number of true negatives (FP+TN being the total number of negatives). It's the probability that a false alarm will be raised: that a positive result will be given when the true value is negative.	How do you determine a false positive rate
8102	Statistics Needed for Data Science For example, data analysis requires descriptive statistics and probability theory, at a minimum. These concepts will help you make better business decisions from data. Key concepts include probability distributions, statistical significance, hypothesis testing, and regression.	What statistics do you need for data science
1928	A random variable with a Gaussian distribution is said to be normally distributed, and is called a normal deviate. Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known.	What is Gaussian distribution used for
7905	Simple linear regression is appropriate when the following conditions are satisfied. The dependent variable Y has a linear relationship to the independent variable X. To check this, make sure that the XY scatterplot is linear and that the residual plot shows a random pattern.	How do you know if a linear regression model is appropriate
1237	Originally Answered: Why do we often make the i.i.d. assumption in machine learning? Plain and simple answer is faster computation and less messy. Training models take longer than people would like even with distributed computing.	In Machine Learning why do we often apply the assumption of Independent and Identically Distributed records
1046	Here are applications of Reinforcement Learning: Robotics for industrial automation. Business strategy planning. Machine learning and data processing.	Which application is example of reinforcement learning
1376	There are three basic rules associated with probability: the addition, multiplication, and complement rules. The addition rule is used to calculate the probability of event A or event B happening; we express it as: P(A or B) = P(A) + P(B) - P(A and B)	What are the 3 laws of probability
5082	CONCLUSION. There are three primary goals of survival analysis, to estimate and interpret survival and / or hazard functions from the survival data; to compare survival and / or hazard functions, and to assess the relationship of explanatory variables to survival time.	Why is survival analysis used
286	The nominator is the joint probability and the denominator is the probability of the given outcome.  This is the conditional probability: P(A∣B)=P(A∩B)P(B) This is the Bayes' rule: P(A∣B)=P(B|A)∗P(A)P(B).	What is the difference between conditional probability and Bayes Theorem
6796	In probability, and statistics, a multivariate random variable or random vector is a list of mathematical variables each of whose value is unknown, either because the value has not yet occurred or because there is imperfect knowledge of its value.  Normally each element of a random vector is a real number.	What is a multivariate variable
4957	Deep learning when data comes from different sources Multimodal learning suggests that when a number of our senses — visual, auditory, kinesthetic — are being engaged in the processing of information, we understand and remember more. By combining these modes, learners can combine information from different sources.	What is multimodal deep learning
2982	Variance (σ2) in statistics is a measurement of the spread between numbers in a data set. That is, it measures how far each number in the set is from the mean and therefore from every other number in the set.	How do you explain variance
780	An algorithm, for the non-programmers among us, is a set of instructions that take an input, A, and provide an output, B, that changes the data involved in some way. Algorithms have a wide variety of applications. In math, they can help calculate functions from points in a data set, among much more advanced things.	How do algorithms work
55	The covariance between X and Y is defined as Cov(X,Y)=E[(X−EX)(Y−EY)]=E[XY]−(EX)(EY).The covariance has the following properties:Cov(X,X)=Var(X);if X and Y are independent then Cov(X,Y)=0;Cov(X,Y)=Cov(Y,X);Cov(aX,Y)=aCov(X,Y);Cov(X+c,Y)=Cov(X,Y);Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z);more generally,	How do you find the covariance of X and Y
5213	Mean of General discrete uniform distribution The expected value of discrete uniform random variable is E ( X ) = a + b 2 .	What are the expected moments of a uniform discrete distribution
7499	Cross Validation:Split randomly data in train and test set.Focus on train set and split it again randomly in chunks (called folds).Let's say you got 10 folds; train on 9 of them and test on the 10th.Repeat step three 10 times to get 10 accuracy measures on 10 different and separate folds.More items	What are the best steps strategies to perform cross validation on time series data
8295	1. The Canny edge detector is a linear filter because it uses the Gaussian filter to blur the image and then uses the linear filter to compute the gradient. Solution False. Though it does those things, it also has non-linear operations: thresholding, hysteresis, non-maximum suppression.	Is Canny filter linear
1028	According to my POV model accuracy is more important and its all depends on the training data.  Model performance can be improved using distributed computing and parallelizing over the scored assets, whereas accuracy has to be carefully built during the model training process.	Which is more important to you model accuracy or model performance
83	The likelihood function is given by: L(p|x) ∝p4(1 − p)6. The likelihood of p=0.5 is 9.77×10−4, whereas the likelihood of p=0.1 is 5.31×10−5.	How do you find the likelihood function
1849	Parametric tests are those that make assumptions about the parameters of the population distribution from which the sample is drawn. This is often the assumption that the population data are normally distributed. Non-parametric tests are “distribution-free” and, as such, can be used for non-Normal variables.	What is difference between parametric and nonparametric tests
1410	If you don't know your population mean (μ) but you do know the standard deviation (σ), you can find a confidence interval for the population mean, with the formula: x̄ ± z* σ / (√n),  Step 1: Subtract the confidence level (Given as 95 percent in the question) from 1 and then divide the result by two.	How are confidence intervals calculated
7594	The first, and most important limitation, which is present in all inferential statistics, is that you are providing data about a population that you have not fully measured, and therefore, cannot ever be completely sure that the values/statistics you calculate are correct.	What are the limitations of using inferential statistics
915	An artificial neural network's learning rule or learning process is a method, mathematical logic or algorithm which improves the network's performance and/or training time.  Depending upon the process to develop the network there are three main models of machine learning: Unsupervised learning. Supervised learning.	What is learning in neural networks
3118	The Q-Q plot, or quantile-quantile plot, is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a Normal or exponential.  If both sets of quantiles came from the same distribution, we should see the points forming a line that's roughly straight.	What does a QQ plot help you to test
1231	When q-learning is performed we create what's called a q-table or matrix that follows the shape of [state, action] and we initialize our values to zero. We then update and store our q-values after an episode. This q-table becomes a reference table for our agent to select the best action based on the q-value.	What does the Q table in Q learning algorithm represent
631	The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve.	Why is it important for data to be normally distributed
5518	Regression analysis refers to assessing the relationship between the outcome variable and one or more variables.  For example, a correlation of r = 0.8 indicates a positive and strong association among two variables, while a correlation of r = -0.3 shows a negative and weak association.	What is meant by correlation and regression analysis
1157	Vector space model or term vector model is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms.  The model is used to represent documents in an n-dimensional space. But a “document” can mean any object you're trying to model.	What is vector space in machine learning
464	Accuracy is the percentage of correctly classifies instances out of all instances.  Kappa or Cohen's Kappa is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset.	What is accuracy and Kappa
547	Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.	What does a normal curve indicate
1008	Prior probability represents what is originally believed before new evidence is introduced, and posterior probability takes this new information into account.  A posterior probability can subsequently become a prior for a new updated posterior probability as new information arises and is incorporated into the analysis.	What is prior and posterior probability
7835	12 Tips to boost your multitasking skillsAccept your limits. To better manage task organization, be aware of your limits, especially those you can't control.  Distinguish urgent from important.  Learn to concentrate.  Avoid distractions.  Work in blocks of time.  Work on related tasks together.  Learn to supervise.  Plan ahead.More items•	How can I learn multitasking
125	[′au̇t‚pu̇t ‚yü·nət] (computer science) In computers, a unit which delivers information from the computer to an external device or from internal storage to external storage.	What do you mean by output unit explain its function
361	The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.	What is area under precision recall curve
1200	RELU activation solves this by having a gradient slope of 1, so during backpropagation, there isn't gradients passed back that are progressively getting smaller and smaller. but instead they are staying the same, which is how RELU solves the vanishing gradient problem.	How does ReLU solve vanishing gradient problem
3234	A nonlinear relationship is a type of relationship between two entities in which change in one entity does not correspond with constant change in the other entity.  However, nonlinear entities can be related to each other in ways that are fairly predictable, but simply more complex than in a linear relationship.	What is a nonlinear association
1150	Adaptive learning is one technique for providing personalized learning, which aims to provide efficient, effective, and customized learning paths to engage each student. Adaptive learning systems use a data-driven approach to adjust the path and pace of learning, enabling the delivery of personalized learning at scale.	How does adaptive learning work
7662	Linear models, or regression models, trace the the distribution of the dependent variable (Y) – or some characteristic of the distribution (the mean) – as a function of the independent variables (Xs).  This shows the conditional distribution of improvement value.	What does a linear model mean
5203	A loss function is used to optimize a machine learning algorithm. The loss is calculated on training and validation and its interpretation is based on how well the model is doing in these two sets.  An accuracy metric is used to measure the algorithm's performance in an interpretable way.	What is validation Loss and Validation accuracy
3318	The error function reports back the difference between the estimated reward at any given state or time step and the actual reward received.  When this is paired with a stimulus that accurately reflects a future reward, the error can be used to associate the stimulus with the future reward.	What is temporal difference error
637	Overfitting occurs when a statistical model or machine learning algorithm captures the noise of the data.  Specifically, underfitting occurs if the model or algorithm shows low variance but high bias. Underfitting is often a result of an excessively simple model.	What is Overfitting and Underfitting in machine learning
2057	"Ridge and lasso regression allow you to regularize (""shrink"") coefficients. This means that the estimated coefficients are pushed towards 0, to make them work better on new data-sets (""optimized for prediction""). This allows you to use complex models and avoid over-fitting at the same time."	Why do we use Ridge and lasso regression
3383	Confounding means the distortion of the association between the independent and dependent variables because a third variable is independently associated with both. A causal relationship between two variables is often described as the way in which the independent variable affects the dependent variable.	What is confounding in statistics
468	We can set a threshold value to classify all the values greater than threshold as 1 and lesser then that as 0. That's how the Y is predicted and we get 'Y-predicted'. The default value for threshold on which we generally get a Confusion Matrix is 0.50.	What is threshold in confusion matrix
5141	I daresay that dimensionality reduction is necessary when we are lacking an acceptable balance between bias and variance. Some learning algorithms have some kind of 'built in' dimensionality reduction like the Relevance Vector Machine or Random Forests (to name two that are widely used).	When should dimensionality reduction be used for classification
1373	Most computational models of supervised learning rely only on labeled training examples, and ignore the possible role of unlabeled data.  We present an algorithm and experimental results demonstrating that unlabeled data can significantly improve learning accuracy in certain practical problems.	Can unlabeled data be helpful for supervised learning
5688	Overview. Describe the problem.   Data and model. What data did you use to address the question, and how did you do it?   Results. In your results section, include any figures and tables necessary to make your case.   Conclusion.	How do you write a data analysis
7869	Image processing is a method to perform some operations on an image, to get an enhanced image or to extract some useful information from it.  However, to get an optimized workflow and to avoid losing time, it is important to process images after the capture, in a post-processing step.	Why is image processing important
6994	Discrete Probability Distributions If a random variable is a discrete variable, its probability distribution is called a discrete probability distribution. An example will make this clear. Suppose you flip a coin two times.	What is discrete probability distribution
1276	The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data.	What is the purpose of convolution layer
5246	A non-convex optimization problem is any problem where the objective or any of the constraints are non-convex, as pictured below. Such a problem may have multiple feasible regions and multiple locally optimal points within each region.	What is Nonconvex optimization
487	Machine learning is an AI technique where the algorithms are given data and are asked to process without a predetermined set of rules and regulations whereas Predictive analysis is the analysis of historical data as well as existing external data to find patterns and behaviors.	What is machine learning how does it fit into predictive analytics
6420	In statistical mechanics, entropy is an extensive property of a thermodynamic system. It quantifies the number Ω of microscopic configurations (known as microstates) that are consistent with the macroscopic quantities that characterize the system (such as its volume, pressure and temperature).	What is entropy and its properties
6222	A probability sampling method is any method of sampling that utilizes some form of random selection. In order to have a random selection method, you must set up some process or procedure that assures that the different units in your population have equal probabilities of being chosen.	What do you mean by probability sampling
4381	The idea of ensemble classification is to learn not just one classifier but a set of classifiers, called an ensemble of classifiers, and then to combine their predictions for the classification of unseen instances using some form of voting.	What is ensemble classification
1648	The range is influenced too much by extreme values.	Which difficulty of range as a measure of variability is overcome by interquartile range
789	Facial recognition is a way of recognizing a human face through technology. A facial recognition system uses biometrics to map facial features from a photograph or video. It compares the information with a database of known faces to find a match.	How does face recognition work
1382	"Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a dataset. MDS is used to translate ""information about the pairwise 'distances' among a set of n objects or individuals"" into a configuration of n points mapped into an abstract Cartesian space."	What is multidimensional scaling used for
5228	Both can learn and become expert in an area and both are mortal. The main difference is, humans can forget but neural networks cannot. Once fully trained, a neural net will not forget. Whatever a neural network learns is hard-coded and becomes permanent.	What are the differences and similarities of neural networks and the human brain
182	Convolution is a mathematical way of combining two signals to form a third signal. It is the single most important technique in Digital Signal Processing. Using the strategy of impulse decomposition, systems are described by a signal called the impulse response.	What is the use of convolution in signal processing
355	Weight is the parameter within a neural network that transforms input data within the network's hidden layers. A neural network is a series of nodes, or neurons. Within each node is a set of inputs, weight, and a bias value.	What is the use of weights in neural network
7473	A probability event can be defined as a set of outcomes of an experiment.  Thus, an event is a subset of the sample space, i.e., E is a subset of S. There could be a lot of events associated with a given sample space. For any event to occur, the outcome of the experiment must be an element of the set of event E.	What is event and its types
1232	Abstract. A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training.  It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.	What is unsupervised representation learning
7218	Implementing Deep Learning Methods and Feature Engineering for Text Data: FastText. Overall, FastText is a framework for learning word representations and also performing robust, fast and accurate text classification. The framework is open-sourced by Facebook on GitHub.	Is FastText deep learning
747	We can compare the quality of two estimators by looking at the ratio of their MSE. If the two estimators are unbiased this is equivalent to the ratio of the variances which is defined as the relative efficiency. rndr = n + 1 n · n n + 1 θ.	How do you calculate relative efficiency
2084	In logistic regression, a set of observations whose values deviate from the expected range and produce extremely large residuals and may indicate a sample peculiarity is called outliers. These outliers can unduly influence the results of the analysis and lead to incorrect inferences.	Is logistic regression affected by outliers
1082	R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.  So, if the R2 of a model is 0.50, then approximately half of the observed variation can be explained by the model's inputs.	What does R Squared mean in stats
6594	Definition: Entropy is a measure of uncertainty of a random variable. The entropy of a discrete random variable X with alphabet X is H(X) = -) p(x) log p(2) DEX When the base of the logarithm is 2, entropy is measured in bits.  (Note: you can prove this by assigning a variable pi to the probability of outcome i.	What is the entropy of a random variable
1099	A Hash Value (also called as Hashes or Checksum) is a string value (of specific length), which is the result of calculation of a Hashing Algorithm. Hash Values have different uses.	What is the hash value of a string
4926	The asymptotic variance-covariance matrix can be used to calculate confidence intervals and to test hypotheses about the variance components. In this example, the variance for the estimated Var(STOREID) is 65787.226. The positive square root of this number gives the standard error for Var(STOREID), which is 256.49.	What is asymptotic covariance matrix
832	Deep Reinforcement Learning: From Toys to Enteprise When paired with simulations, reinforcement learning is a powerful tool for training AI models that can help increase automation or optimize operational efficiency of sophisticated systems such as robotics, manufacturing, and supply chain logistics.	What is reinforcement learning good for
2195	Most model-performance measures are based on the comparison of the model's predictions with the (known) values of the dependent variable in a dataset. For an ideal model, the predictions and the dependent-variable values should be equal. In practice, it is never the case, and we want to quantify the disagreement.	How do you measure the performance of a model
562	The calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction.  Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level.	What is probability calibration
1040	Machine bias is the effect of erroneous assumptions in machine learning processes. Bias reflects problems related to the gathering or use of data, where systems draw improper conclusions about data sets, either because of human intervention or as a result of a lack of cognitive assessment of data.	What is machine bias
14	The pre-attention phase is an automatic process which happens unconsciously. The second stage is focused attention in which an individual takes all of the observed features and combines them to make a complete perception. This second stage process occurs if the object doesn't stand out immediately.	What are the two stages of processing in the feature integration theory
4012	The value of the z-score tells you how many standard deviations you are away from the mean. If a z-score is equal to 0, it is on the mean. A positive z-score indicates the raw score is higher than the mean average. For example, if a z-score is equal to +1, it is 1 standard deviation above the mean.	What is Z value in statistics
532	For quick and visual identification of a normal distribution, use a QQ plot if you have only one variable to look at and a Box Plot if you have many. Use a histogram if you need to present your results to a non-statistical public. As a statistical test to confirm your hypothesis, use the Shapiro Wilk test.	How do you test if your data is normally distributed
882	Selection bias is the term used to describe the situation where an analysis has been conducted among a subset of the data (a sample) with the goal of drawing conclusions about the population, but the resulting conclusions will likely be wrong (biased), because the subgroup differs from the population in some important	What is selection bias Why is it important and how can you avoid it
6731	Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.  Explicit regression gradient boosting algorithms were subsequently developed by Jerome H.	What does gradient boosting mean
4869	Canonical discriminant analysis is a dimension-reduction technique related to principal component analysis and canonical correlation.  This maximal multiple correlation is called the first canonical correlation. The coefficients of the linear combination are the canonical coefficients or canonical weights.	What is canonical discriminant analysis
7944	3 neurons	How many neurons are in the output layer
6980	An artificial neural network is an attempt to simulate the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a humanlike manner. ANNs are created by programming regular computers to behave as though they are interconnected brain cells.	What do you mean by artificial neural network
6982	Support Vector Machine can also be used as a regression method, maintaining all the main features that characterize the algorithm (maximal margin). The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences.	Can SVM be used for regression
50	In a standard normal distribution (with mean 0 and standard deviation 1), the first and third quartiles are located at -0.67448 and +0.67448 respectively. Thus the interquartile range (IQR) is 1.34896.	What is the interquartile range of a normal distribution
5792	LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate's activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.	How does Lstm overcomes vanishing gradient problem
5348	Consider the normal distribution N(100, 10). To find the percentage of data below 105.3, that is P(x < 105.3), standartize first: P(x < 105.3) = P ( z < 105.3 − 100 10 ) = P(z < 0.53). Then find the proportion corresponding to 0.53 in Table A: look for the intersection of the row labeled 0.5 and the column labeled .	How do you find the standard normal distribution percentage
1039	The greater the value, the higher the weight for that feature. The Formula! The Weighted Mean Center is calculated by multiplying the x and y coordinate by the weight for that feature and summing all for both x and y individually, and then dividing this by the sum of all the weights.	How do you find the weighted mean center
148	Independent EventsTwo events A and B are said to be independent if the fact that one event has occurred does not affect the probability that the other event will occur.If whether or not one event occurs does affect the probability that the other event will occur, then the two events are said to be dependent.	How do you know if an event is independent or dependent
6080	The AUC for the ROC can be calculated using the roc_auc_score() function. Like the roc_curve() function, the AUC function takes both the true outcomes (0,1) from the test set and the predicted probabilities for the 1 class. It returns the AUC score between 0.0 and 1.0 for no skill and perfect skill respectively.	How is AUC calculated
7564	"Systematic sampling is easier to do than random sampling. In systematic sampling, the list of elements is ""counted off"". That is, every kth element is taken.  Stratified sampling also divides the population into groups called strata."	What is the difference between systematic sampling and stratified sampling
661	To conduct a stratified analysis we can identify six major steps which have a specific chronology:Conduct a crude analysis.  Identify the potential effect modifiers or confounding factors.  Measure the effect of exposure on outcome within each stratum.  Look for effect modification.  Look for confounding.More items•	How do you do stratified analysis
726	Algorithms are methods or procedures taken in other to get a task done or solve a problem, while Models are well-defined computations formed as a result of an algorithm that takes some value, or set of values, as input and produces some value, or set of values as output.	What is the difference between a model and an algorithm
1426	"In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input."	What is the role of activation function in neural network
268	The first component is the definition: Two variables are independent when the distribution of one does not depend on the the other.  If the probabilities of one variable remains fixed, regardless of whether we condition on another variable, then the two variables are independent.	What does it mean if two variables are independent
4506	Modes, medians, and frequencies are the appropriate statistical tools to use. If you have designed a series of questions that when combined measure a particular trait, you have created a Likert scale. Use means and standard deviations to describe the scale.	What statistical analysis should I use for Likert scale data
3498	You can improve your pattern recognition skills by practising. Now you know that patterns can appear in numbers, objects, symbols, music and more, you can pay attention to this. Looking and listening while being aware that there are patterns in things most of the time, helps you to eventually find them easier.	How do I improve my pattern recognition skills
854	Non-probability sampling is often used because the procedures used to select units for inclusion in a sample are much easier, quicker and cheaper when compared with probability sampling. This is especially the case for convenience sampling.	Why do we use non probability sampling
3300	If the static shape is not fully defined, the dynamic shape of a Tensor t can be determined by evaluating tf. shape(t) . On the other hand you can extract the static shape by using x. get_shape().	How do you find the shape of a tensor
101	A confidence interval is a range of values that is likely to contain an unknown population parameter. If you draw a random sample many times, a certain percentage of the confidence intervals will contain the population mean. This percentage is the confidence level.	What is the relationship between the confidence interval and the confidence level
8689	Local Outlier Factor (LOF) The local outlier factor [43] is the most well-known local anomaly detection algorithm and also introduced the idea of local anomalies first. Today, its idea is carried out in many nearest-neighbor based algorithms, such as in the ones described below.	Which algorithm will you use for anomaly detection
7769	Edge detection is an image processing technique for finding the boundaries of objects within images. It works by detecting discontinuities in brightness. Edge detection is used for image segmentation and data extraction in areas such as image processing, computer vision, and machine vision.	Why do we need edge detection
2831	A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.  Neural networks can adapt to changing input; so the network generates the best possible result without needing to redesign the output criteria.	What is a neural network algorithm
1042	"Random forest is a supervised learning algorithm. The ""forest"" it builds, is an ensemble of decision trees, usually trained with the “bagging” method. The general idea of the bagging method is that a combination of learning models increases the overall result."	What type of algorithm is random forest
797	Step 1: Divide your confidence level by 2: .95/2 = 0.475. Step 2: Look up the value you calculated in Step 1 in the z-table and find the corresponding z-value. The z-value that has an area of .475 is 1.96. Step 3: Divide the number of events by the number of trials to get the “P-hat” value: 24/160 = 0.15.	How do you find the z score for a 95 confidence interval
1112	"Time efficiency - a measure of amount of time for an algorithm to execute. Space efficiency - a measure of the amount of memory needed for an algorithm to execute. Asymptotic dominance - comparison of cost functions when n is large. That is, g asymptotically dominates f if g dominates f for all ""large"" values of n."	What is asymptotic efficiency of algorithm
2763	The area under the normal curve is equal to 1.0. Normal distributions are denser in the center and less dense in the tails. Normal distributions are defined by two parameters, the mean (μ) and the standard deviation (σ). 68% of the area of a normal distribution is within one standard deviation of the mean.	What is the difference between normal curve and normal distribution
4757	The hazard function is the instantaneous rate of failure at a given time. Characteristics of a hazard function are frequently associated with certain products and applications. Different hazard functions are modeled with different distribution models.	What is the hazard rate in reliability
4456	Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.	What does batch normalization layer do
1208	ReLu bounded negative outputs to 0 & above. This works well in hidden layers than the final output layer.  It is not typical, since in this case, the ouput value is not bounded in a range.	Why ReLU is not used in output layer
96	Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.	What does the root mean square error tell you
3825	For a discrete random variable, x, the probability distribution is defined by a probability mass function, denoted by f(x). This function provides the probability for each value of the random variable.	What is probability distribution function of a random variable
2885	Use imputation for the missing values. When the response is missing, we can use a predictive model to predict the missing response, then create a new fully-observed dataset containing the predictions instead of the missing values, and finally re-estimate the predictive model in this expanded dataset.	How can I deal with missing values in a predictive model
6056	Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. For example, height and weight are related; taller people tend to be heavier than shorter people. An intelligent correlation analysis can lead to a greater understanding of your data.	What is correlation in statistics with example
5678	Therefore, a low test–retest reliability correlation might be indicative of a measure with low reliability, of true changes in the persons being measured, or both. That is, in the test–retest method of estimating reliability, it is not possible to separate the reliability of measure from its stability.	What does low test retest reliability mean
631	Batch gradient descent computes the gradient using the whole dataset. This is great for convex, or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution, either local or global.  Stochastic gradient descent (SGD) computes the gradient using a single sample.	What is the difference between batch gradient descent and stochastic gradient descent
680	A correlation coefficient is a numerical measure of some type of correlation, meaning a statistical relationship between two variables.	Which method of measuring correlation measures any type of relationship
7768	The problem is that probability and odds have different properties that give odds some advantages in statistics.  For example, in logistic regression the odds ratio represents the constant effect of a predictor X, on the likelihood that one outcome will occur.	Why can we interpret the coefficients of a logistic regression model using odds ratios
379	A random walk on a graph is a very special case of a Markov chain. Unlike a general Markov chain, random walk on a graph enjoys a property called time symmetry or reversibility.	Is a random walk a Markov chain
2901	We will run the ANOVA using the five-step approach.Set up hypotheses and determine level of significance. H0: μ1 = μ2 = μ3 = μ4 H1: Means are not all equal α=0.05.Select the appropriate test statistic.  Set up decision rule.  Compute the test statistic.  Conclusion.	What are the steps to carry out analysis of variance
700	When the null hypothesis is true and you reject it, you make a type I error. The probability of making a type I error is α, which is the level of significance you set for your hypothesis test. An α of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis.	When you reject the null hypothesis in a two tailed test at the 0.05 level of significance the probability you are making a Type I error is
2260	In statistics, a two-tailed test is a method in which the critical area of a distribution is two-sided and tests whether a sample is greater or less than a range of values.  If the sample being tested falls into either of the critical areas, the alternative hypothesis is accepted instead of the null hypothesis.	What is a two sided hypothesis test
3510	Statistical Methods for Finding the Best Regression ModelAdjusted R-squared and Predicted R-squared: Generally, you choose the models that have higher adjusted and predicted R-squared values.  P-values for the predictors: In regression, low p-values indicate terms that are statistically significant.More items•	How do you know which regression is the best
65	The T distribution is similar to the normal distribution, just with fatter tails. Both assume a normally distributed population. T distributions have higher kurtosis than normal distributions. The probability of getting values very far from the mean is larger with a T distribution than a normal distribution.	What is the difference between normal and T distribution
1218	"We can use the regression line to predict a value of ""Y"" for any ""X"" score. The steepness of the angle of the regression line is called its slope. It is the amount of change in ""Y"" that we can expect for any unit change in ""X""."	What does a steep regression line mean
194	Variance (σ2) in statistics is a measurement of the spread between numbers in a data set. That is, it measures how far each number in the set is from the mean and therefore from every other number in the set.	What is a variance in statistics
1614	Autocorrelation represents the degree of similarity between a given time series and a lagged version of itself over successive time intervals. Autocorrelation measures the relationship between a variable's current value and its past values.	What does the autocorrelation function tell you
31	Applications and considerations. n-gram models are widely used in statistical natural language processing. In speech recognition, phonemes and sequences of phonemes are modeled using a n-gram distribution. For parsing, words are modeled such that each n-gram is composed of n words.	What is the use of n grams
3209	Improving recall involves adding more accurately tagged text data to the tag in question. In this case, you are looking for the texts that should be in this tag but are not, or were incorrectly predicted (False Negatives). The best way to find these kind of texts is to search for them using keywords.	How do you increase recall in machine learning
1064	If the absolute value of the t-value is greater than the critical value, you reject the null hypothesis. If the absolute value of the t-value is less than the critical value, you fail to reject the null hypothesis.	What does it mean if the T value is less than the critical value
3542	Statistical modeling is the process of applying statistical analysis to a dataset. A statistical model is a mathematical representation (or mathematical model) of observed data.	What is statistical Modelling for data analysis
6105	A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.  If you accept most classes of problems can be reduced to functions, this statement implies a neural network can, in theory, solve any problem.	Can neural networks solve any problem
5385	Weighted grade calculation The weighted grade is equal to the sum of the product of the weights (w) in percent (%) times the grade (g): Weighted grade = w1×g1+ w2×g2+ w3×g3+	How do you calculate a weighted grade
7889	"The mean is the most common measure of center. It is what most people think of when they hear the word ""average"". However, the mean is affected by extreme values so it may not be the best measure of center to use in a skewed distribution. The median is the value in the center of the data."	Which measure of location is affected by extreme value
3406	Despite the sample population being selected in advance, systematic sampling is still thought of as being random if the periodic interval is determined beforehand and the starting point is random.	Is systematic sampling random
7028	Definition. Imitation is the ability to recognize and reproduce others' actions – By extension, imitation learning is a means of learning and developing new skills from observing these skills performed by another agent.	What is imitation learning
1673	A t-value is the relative error difference in contrast to the null hypothesis. A p-value, is the statistical significance of a measurement in how correct a statistical evidence part, is.	What is the difference between a t value and p value
3842	A one-tailed test is also known as a directional hypothesis or directional test. A two-tailed test, on the other hand, is designed to examine both sides of a specified data range to test whether a sample is greater than or less than the range of values.	What is the differences between marginal distribution and the two tailed test
1202	·4 min read N-gram is probably the easiest concept to understand in the whole machine learning space, I guess. An N-gram means a sequence of N words. So for example, “Medium blog” is a 2-gram (a bigram), “A Medium blog post” is a 4-gram, and “Write on Medium” is a 3-gram (trigram).	What is N gram in NLP
343	Proportional-integral-derivative (PID) controllers that can automatically select their own tuning parameters sound good but face challenges.  The exercise is conceptually simple: Choose the gain, rate, and reset parameters that define the relative magnitude of the P, I, and D contributions to the overall control effort.	What is PID auto tuning
32	The Monty Hall problem has confused people for decades. In the game show, Let's Make a Deal, Monty Hall asks you to guess which closed door a prize is behind. The answer is so puzzling that people often refuse to accept it! The problem occurs because our statistical assumptions are incorrect.	Why does the Monty Hall problem work
6209	The basic idea behind a neural network is to simulate (copy in a simplified but reasonably faithful way) lots of densely interconnected brain cells inside a computer so you can get it to learn things, recognize patterns, and make decisions in a humanlike way.	What does a neural network do
763	It is used when we want to predict the value of a variable based on the value of two or more other variables.  For example, you could use multiple regression to understand whether exam performance can be predicted based on revision time, test anxiety, lecture attendance and gender.	What is an example of Multiple regression analysis
8682	The Bayesian framework for machine learning states that you start out by enumerating all reasonable models of the data and assigning your prior belief P(M) to each of these models. Then, upon observing the data D, you evaluate how probable the data was under each of these models to compute P(D|M).	What is Bayesian machine learning
1376	A regression coefficient is the same thing as the slope of the line of the regression equation. The equation for the regression coefficient that you'll find on the AP Statistics test is: B1 = b1 = Σ [ (xi – x)(yi – y) ] / Σ [ (xi – x)2]. “y” in this equation is the mean of y and “x” is the mean of x.	What is the formula for regression coefficient
1338	There are several differences between these two frameworks. Keras is a neural network library while TensorFlow is the open-source library for a number of various tasks in machine learning. TensorFlow provides both high-level and low-level APIs while Keras provides only high-level APIs.	Is keras same as TensorFlow
4672	The frame refers to the list of units (eg, persons, households, businesses, etc) in the survey population.  It determines how well a target population is covered, and affects the choice of the data collection method.	What is a population frame
2020	Answer. A negative path loading is basically the same as a negative regression coefficient. I.e., For a path loading from X to Y it is the predicted increase in Y for a one unit increase on X holding all other variables constant. So a negative coefficient just means that as X increases, Y is predicted to decrease.	What does a negative path coefficient mean
756	You can use tf. function to make graphs out of your programs. It is a transformation tool that creates Python-independent dataflow graphs out of your Python code. This will help you create performant and portable models, and it is required to use SavedModel .  function works under the hood so you can use it effectively.	What is TF function
200	When you perform a t-test, you're usually trying to find evidence of a significant difference between population means (2-sample t) or between the population mean and a hypothesized value (1-sample t). The t-value measures the size of the difference relative to the variation in your sample data.	What is at test and p value
8550	Multivariate interpolation is the interpolation of functions of more than one variable. Methods include bilinear interpolation and bicubic interpolation in two dimensions, and trilinear interpolation in three dimensions. They can be applied to gridded or scattered data.	What are the methods of interpolation
7549	The least squares approach limits the distance between a function and the data points that the function explains. It is used in regression analysis, often in nonlinear regression modeling in which a curve is fit into a set of data. Mathematicians use the least squares method to arrive at a maximum-likelihood estimate.	Why do we use least square method
4323	KNN for Classification And the inverse, use an even number for K when you have an odd number of classes. Ties can be broken consistently by expanding K by 1 and looking at the class of the next most similar instance in the training dataset.	How does K nearest neighbor classification break ties
4018	The covariance between X and Y is defined as Cov(X,Y)=E[(X−EX)(Y−EY)]=E[XY]−(EX)(EY).The covariance has the following properties:Cov(X,X)=Var(X);if X and Y are independent then Cov(X,Y)=0;Cov(X,Y)=Cov(Y,X);Cov(aX,Y)=aCov(X,Y);Cov(X+c,Y)=Cov(X,Y);Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z);more generally,	How do you find the probability of covariance
464	If you reduce the random error of a data set, you reduce the width (FULL WIDTH AT HALF MAXIMUM) of a distribution, or the counting noise (POISSON NOISE) of a measurement. Usually, you can reduce random error by simply taking more measurements.	How do you reduce random error
5770	var·i·ance ra·ti·o (F), the distribution of the ratio of two independent estimates of the same variance from a gaussian distribution based on samples of sizes (n + 1) and (m + 1), respectively.	What is a variance ratio
8027	8 Radial Basis Function Networks. Radial basis function (RBF) networks are a commonly used type of artificial neural network for function approximation problems.  An RBF network is a type of feed forward neural network composed of three layers, namely the input layer, the hidden layer and the output layer.	What is basis function in neural network
152	Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.	How do you explain clustering
937	The expected number of false positives if the rate is set at 5% should be 5%. In general, this rate is higher, because investigators fail to include all sources of uncertainty when calculating the expected false positive rate.	What is an acceptable false discovery rate
1243	Word vectors are simply vectors of numbers that represent the meaning of a word.  In simpler terms, a word vector is a row of real-valued numbers (as opposed to dummy numbers) where each point captures a dimension of the word's meaning and where semantically similar words have similar vectors.	What is vector representation of words
6	In programming languages In Fortran, R, APL, J and Wolfram Language (Mathematica), it is done through simple multiplication operator * , whereas the matrix product is done through the function matmul , %*% , +.	Which of the following operator is used for hadamard product two matrices in R
288	To find the interquartile range (IQR), ​first find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1.	How do you determine the interquartile range
373	Differentiation and integration can help us solve many types of real-world problems. We use the derivative to determine the maximum and minimum values of particular functions (e.g. cost, strength, amount of material used in a building, profit, loss, etc.).	What are some real life applications of integration and differentiation
1651	The variance of a set of numbers is the mean squared deviation from the mean. It is a measure of how spread out the set of numbers is.  The estimation variance is the variance of that large set of values. It measures how much, well, variance there is in an estimator from sample to sample.	Is there a difference between estimation variance and sample variance
8586	A knowledge-based system (KBS) is a form of artificial intelligence (AI) that aims to capture the knowledge of human experts to support decision-making. Examples of knowledge-based systems include expert systems, which are so called because of their reliance on human expertise.	What are knowledge based systems explain with an example
3816	Decision Tree - Classification. Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed.  Decision trees can handle both categorical and numerical data	How are decision tree used for classification
784	In simpler terms, optimizers shape and mold your model into its most accurate possible form by futzing with the weights. The loss function is the guide to the terrain, telling the optimizer when it's moving in the right or wrong direction.	What is the function of Optimizer
975	The difference between combinations and permutations is ordering. With permutations we care about the order of the elements, whereas with combinations we don't. For example, say your locker “combo” is 5432. If you enter 4325 into your locker it won't open because it is a different ordering (aka permutation).	How do you know when to use a permutation instead of a combination
5045	Neural Turing Machines can take input and output and learn algorithms that map from one to the other.  This means that once they have learned that algorithm, they can take a given input, and they can extrapolate based on that algorithm to any variable output.	How does a neural turing machine work
733	Quantization, in mathematics and digital signal processing, is the process of mapping input values from a large set (often a continuous set) to output values in a (countable) smaller set, often with a finite number of elements. Rounding and truncation are typical examples of quantization processes.	What is the need of quantization
1134	Multimodal means having or using a variety of modes or methods to do something. Multimodal is a general term that can be used in many different contexts. It also has more specific uses in the fields of statistics and transportation.	What does multimodal mean
2976	General Properties of Probability Distributions The sum of all probabilities for all possible values must equal 1. Furthermore, the probability for a particular value or range of values must be between 0 and 1. Probability distributions describe the dispersion of the values of a random variable.	What are the two properties of a probability distribution
3099	Batch normalization (also known as batch norm) is a method used to make artificial neural networks faster and more stable through normalization of the input layer by re-centering and re-scaling.	What is the purpose of batch normalization
6172	In other words, discriminative models are used to specify outputs based on inputs (by models such as Logistic regression, Neural networks and Random forests), while generative models generate both inputs and outputs (for example, by Hidden Markov model, Bayesian Networks and Gaussian mixture model).	Is Random Forest generative or discriminative
1	Odds Ratio is a measure of the strength of association with an exposure and an outcome.OR > 1 means greater odds of association with the exposure and outcome.OR = 1 means there is no association between exposure and outcome.OR < 1 means there is a lower odds of association between the exposure and outcome.	How do you interpret odds ratio
8669	Best! Logistic regression chooses the class that has the biggest probability. In case of 2 classes, the threshold is 0.5: if P(Y=0) > 0.5 then obviously P(Y=0) > P(Y=1). The same stands for the multiclass setting: again, it chooses the class with the biggest probability (see e.g. Ng's lectures, the bottom lines).	How do you choose the threshold for logistic regression
8338	Weighted percentages allow you to account for this. All you have to do is convert the percentage the assignment is worth into a decimal and multiply that by your grade. To convert, just divide the percentage of your final grade the assignment represents by 100.	How do you calculate weighted percentages
5028	To convert a logit ( glm output) to probability, follow these 3 steps:Take glm output coefficient (logit)compute e-function on the logit using exp() “de-logarithimize” (you'll get odds then)convert odds to probability using this formula prob = odds / (1 + odds) .	How do you convert odds ratio to logit
3892	EAD, along with loss given default (LGD) and the probability of default (PD), are used to calculate the credit risk capital of financial institutions. Banks often calculate an EAD value for each loan and then use these figures to determine their overall default risk.	What is PD EAD and LGD
1583	Neural networks are widely used in unsupervised learning in order to learn better representations of the input data.  This process doesn't give you clusters, but it creates meaningful representations that can be used for clustering. You could, for instance, run a clustering algorithm on the hidden layer's activations.	Can neural network be used for clustering
1209	Balance between discriminator & generator We can improve GAN by turning our attention in balancing the loss between the generator and the discriminator. Unfortunately, the solution seems elusive. We can maintain a static ratio between the number of gradient descent iterations on the discriminator and the generator.	How can I improve my gan training
4171	Feature selection methods are intended to reduce the number of input variables to those that are believed to be most useful to a model in order to predict the target variable. Feature selection is primarily focused on removing non-informative or redundant predictors from the model.	What are feature selection techniques in machine learning
8320	Prior probability, in Bayesian statistical inference, is the probability of an event before new data is collected.	What does prior probability mean
470	It can be human- or machine-generated. Examples of unstructured data include: Media: Audio and video files, images. Text files: Word docs, PowerPoint presentations, email, chat logs.	Is image an unstructured data
155	Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are. Instead, they need to be recoded into a series of variables which can then be entered into the regression model.	Can you use categorical variables in linear regression
6431	There is a popular method known as elbow method which is used to determine the optimal value of K to perform the K-Means Clustering Algorithm. The basic idea behind this method is that it plots the various values of cost with changing k. As the value of K increases, there will be fewer elements in the cluster.	How can we choose a good K for K means clustering
1087	So, if we want to say how widely scattered some measurements are, we use the standard deviation. If we want to indicate the uncertainty around the estimate of the mean measurement, we quote the standard error of the mean. The standard error is most useful as a means of calculating a confidence interval.	What is the difference between standard error and standard deviation
5574	2:316:15Suggested clip · 118 secondsUnit Conversion the Easy Way (Dimensional Analysis) - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you convert dimensional analysis
294	"In statistical hypothesis testing, a type I error is the rejection of a true null hypothesis (also known as a ""false positive"" finding or conclusion; example: ""an innocent person is convicted""), while a type II error is the non-rejection of a false null hypothesis (also known as a ""false negative"" finding or conclusion"	What is the difference between Type 1 and Type 2 error in statistics
3	Continuous variables can take on an unlimited number of values between the lowest and highest points of measurement. Continuous variables include such things as speed and distance.  Discrete data are associated with a limited number of possible values.	What is continuous variable
875	Monte Carlo tree search algorithm	Which AI method does AlphaGo use
6419	The most significant difference between regression vs classification is that while regression helps predict a continuous quantity, classification predicts discrete class labels. There are also some overlaps between the two types of machine learning algorithms.	What is difference between regression and classification
4549	The median divides the data into a lower half and an upper half. The lower quartile is the middle value of the lower half. The upper quartile is the middle value of the upper half. The following figure shows the median, quartiles and interquartile range.	What is median quartile
8556	Content validity: Is the test fully representative of what it aims to measure?  Criterion validity: Do the results correspond to a different test of the same thing?	What is the difference between construct validity and criterion validity
6521	The lower class boundary is found by subtracting 0.5 units from the lower class limit and the upper class boundary is found by adding 0.5 units to the upper class limit. The difference between the upper and lower boundaries of any class.	How do you find upper and lower boundaries in statistics
5973	Econometrics originally came from statistics. In general statistics is more general than econometrics, since while econometrics focuses in Statistical Inference, Statistics also deals with other important fields such as Design of Experiments and Sampling techiniques.	What is the difference between econometrics and statistics
6855	Vector autoregression (VAR) is a statistical model used to capture the relationship between multiple quantities as they change over time.  VAR models generalize the single-variable (univariate) autoregressive model by allowing for multivariate time series. VAR models are often used in economics and the natural sciences.	What is VAR model in econometrics
22	-A field is a variable that exists inside of an object, while a parameter is a variable inside a method whose value is passed in from outside.	Which of the following are differences between a field and a parameter
748	Shift-invariance: this means that if we shift the input in time (or shift the entries in a vector) then the output is shifted by the same amount. Mathematically, we can say that if f(x(t)) = y(t), shift invariance means that f(x(t + ⌧)) = y(t + ⌧).	What does shift invariant mean
300	Arrange data points from smallest to largest and locate the central number. This is the median. If there are 2 numbers in the middle, the median is the average of those 2 numbers. The mode is the number in a data set that occurs most frequently.	How do you find the mean median and mode of a data set
1203	Under the batch processing model, a set of data is collected over time, then fed into an analytics system. In other words, you collect a batch of information, then send it in for processing. Under the streaming model, data is fed into analytics tools piece-by-piece. The processing is usually done in real time.	What is the difference between batch processing and stream processing
4089	While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation.	How is XGBoost different from gradient boosting
2410	0:315:15Suggested clip · 110 secondsMultinomial Distributions: Examples (Basic Probability and Statistics YouTubeStart of suggested clipEnd of suggested clip	How do you solve a multinomial distribution
5858	Security groups are tied to an instance whereas Network ACLs are tied to the subnet. i.e. Network Access control lists are applicable at the subnet level, so any instance in the subnet with an associated NACL will follow rules of NACL.  This means any instances within the subnet group gets the rule applied.	What is the difference between security groups and network access control list
618	If you have severely imbalanced classes, you can get high overall accuracy without much effort — but without generating any good insights. The overall accuracy might be high, but for the minority class, you will have very low recall.	Why is imbalanced data a problem
3197	Logits are values that are used as input to softmax. To understand this better click here this is official by tensorflow.  Therefore, +ive logits correspond to probability of greater than 0.5 and negative corresponds to a probability value of less than 0.5. Sometimes they are also refer to inverse of sigmoid function.	What is from Logits in Tensorflow
301	There are two types of hierarchical clustering, Divisive and Agglomerative.	What are the two types of hierarchical clustering
7312	There are two main differences between regression and structural equation modelling. The first is that SEM allows us to develop complex path models with direct and indirect effects. This allows us to more accurately model causal mechanisms we are interested in. The second key difference is to do with measurement.	What is the difference between regression and structural equation modeling
3474	Variable screening is the process of filtering out irrelevant variables, with the aim to reduce the dimensionality from ultrahigh to high while retaining all important variables.  The main theme of this thesis is to develop variable screening and variable selection methods for high dimensional data analysis.	What is variable screening
621	The mean ^ n . of these values is the expected value of the estimator :^. (3+2+5+3+6+5)/6 = 24/6 = 4. Thus, the expected value of the estimator is 4; this is denoted as E( ).	How do you calculate expected estimator
1146	XOR function	Which logic function can be performed using a 2 layered neural network
1097	Select a random sample.Create 2 or more groups by manipulating the levels of an IV.Use random assignment to select participants to a group.Measure the same dependent variable in each group. Use inferential statistics to compare differences between groups.	What are the different ways to assign participants to groups
2335	The main nonparametric tests are:1-sample sign test.  1-sample Wilcoxon signed rank test.  Friedman test.  Goodman Kruska's Gamma: a test of association for ranked variables.Kruskal-Wallis test.  The Mann-Kendall Trend Test looks for trends in time-series data.Mann-Whitney test.  Mood's Median test.More items•	What are nonparametric tests
2394	"Statistical conclusion validity is the degree to which conclusions about the relationship among variables based on the data are correct or ""reasonable"".  Statistical conclusion validity involves ensuring the use of adequate sampling procedures, appropriate statistical tests, and reliable measurement procedures."	How do statisticians decide if their conclusions are valid
1316	"(It is also possible to integrate existing knowledge with data - one way of doing that is using Bayesian statistics, in fact!)  Since Bayesian statistics provides a framework for updating ""knowledge"", it is in fact used a whole lot in machine learning."	Is Bayesian statistics useful for machine learning
1415	A hierarchical linear regression is a special form of a multiple linear regression analysis in which more variables are added to the model in separate steps called “blocks.” This is often done to statistically “control” for certain variables, to see whether adding variables significantly improves a model's ability to	What is hierarchical linear regression
6508	What is the best way to store data used for Natural Language Processing?stream data from disk when you can.  inverted indexes each get their own text file (more relevant for search, maybe not what you're doing)use sparse data structures (e.g. sparse matrix) as much as possible when you need to load stuff into memory.	How does NLP store data
728	Sampling distributions are important for inferential statistics. In practice, one will collect sample data and, from these data, estimate parameters of the population distribution. Thus, knowledge of the sampling distribution can be very useful in making inferences about the overall population.	What is the importance of sampling distribution
702	An artificial neural network (ANN) is the piece of a computing system designed to simulate the way the human brain analyzes and processes information.  ANNs have self-learning capabilities that enable them to produce better results as more data becomes available.	What is artificial neural network explain with example
4491	The mean, median, and mode of a normal distribution are equal. The area under the normal curve is equal to 1.0. Normal distributions are denser in the center and less dense in the tails. Normal distributions are defined by two parameters, the mean (μ) and the standard deviation (σ).	What is the relation between mean mode and median in a normal distribution
974	• h is the Vapnik Chervonenkis (VC) dimension and is a measure of the capacity or complexity of the machine.	What is VC dimension in SVM
7199	Diana Borsa, Bilal Piot, Rémi Munos, Olivier Pietquin. Download PDF. Observational learning is a type of learning that occurs as a function of observing, retaining and possibly replicating or imitating the behaviour of another agent.	What is observation in reinforcement learning
8037	"Loading MNIST handwritten digits datasetLoading MNIST handwritten digits dataset. Loading the MNIST dataset.Introduction.  Required Libraries.  scikit-learn: fetch_mldata.  Check the folder structure.  Download and store the dataset in local.  Load the dataset.Finally, the variable ""mnist"" will contain the data!More items•"	How do I import a Mnist dataset
4649	Path analysis is a special case of SEM.  Most of the models that you will see in the literature are SEM rather than path analyses. The main difference between the two types of models is that path analysis assumes that all variables are measured without error. SEM uses latent variables to account for measurement error.	What is the difference between path analysis and SEM
8428	"Analysis of covariance is used to test the main and interaction effects of categorical variables on a continuous dependent variable, controlling for the effects of selected other continuous variables, which co-vary with the dependent. The control variables are called the ""covariates."""	What is analysis of covariance used for
8520	Sparse signals are characterized by a few nonzero coefficients in one of their transformation domains. This was the main premise in designing signal compression algorithms. Compressive sensing as a new approach employs the sparsity property as a precondition for signal recovery.	What is sparse signal processing
3536	Most websites like Amazon, YouTube, and Netflix use collaborative filtering as a part of their sophisticated recommendation systems. You can use this technique to build recommenders that give suggestions to a user on the basis of the likes and dislikes of similar users.	Does Netflix use collaborative filtering
14	In its initial test, the Altman Z-Score was found to be 72% accurate in predicting bankruptcy two years prior to the event. In subsequent tests over 31 years up until 1999, the model was found to be 80-90% accurate in predicting bankruptcy one year prior to the event.	How accurate is Altman Z score
141	four types	How many types of informed search methods are in artificial intelligence
4159	The name tells you how to calculate it. You subtract the regression-predicted values from the actual values, square them (to get rid of directionality), take their average, then take the square root of the average.	How do you evaluate the accuracy of a regression result
2390	In statistics, uniform distribution is a probability distribution where all outcomes are equally likely. Discrete uniform distributions have a finite number of outcomes. A continuous uniform distribution is a statistical distribution with an infinite number of equally likely measurable values.	How does a uniform discrete distribution differ from a uniform continuous distribution
3259	Most economic variables are constrained to be positive, and their empirical distributions may be quite non-normal (think of the income distribution). When logs are applied, the distributions are better behaved. Taking logs also reduces the extrema in the Page 7 data, and curtails the effects of outliers.	Why do we log Variables in Econometrics
2803	It is used in studies with a repeated measures or a matched pairs design, where the data meets the requirements for a parametric test (level of measurement is interval or better, data is drawn from a population that has a normal distribution, the variances of the two samples are not significantly different).	Why would you use a related t test
540	The majority of neural networks are fully connected from one layer to another. These connexions are weighted; the higher the number the greater influence one unit has on another, similar to a human brain. As the data goes through each unit the network is learning more about the data.	How are artificial neural networks similar to the brain
1025	Some of the more common ways to normalize data include:Transforming data using a z-score or t-score.  Rescaling data to have values between 0 and 1.  Standardizing residuals: Ratios used in regression analysis can force residuals into the shape of a normal distribution.Normalizing Moments using the formula μ/σ.More items	How do you normalize data in statistics
6005	A score of 1 indicates that the data are one standard deviation from the mean, while a Z-score of -1 places the data one standard deviation below the mean. The higher the Z-score, the further from the norm the data can be considered to be.	What is the z score for 1 standard deviation
5872	Test Procedure in SPSS StatisticsClick Analyze > Regression > Binary Logistic  Transfer the dependent variable, heart_disease, into the Dependent: box, and the independent variables, age, weight, gender and VO2max into the Covariates: box, using the buttons, as shown below:  Click on the button.More items	How do you do a simple logistic regression in SPSS
4298	In many applications including econometrics and biostatistics a fixed effects model refers to a regression model in which the group means are fixed (non-random) as opposed to a random effects model in which the group means are a random sample from a population.	What are fixed effects in regression
5546	During the initial stages of survey research, researchers usually prefer using convenience sampling as it's quick and easy to deliver results. Even if many statisticians avoid implementing this technique, it is vital in situations where you intend to get insights in a shorter period or without investing too much money.	When is the convenience sampling technique appropriate
919	Data wrangling is the process of gathering, selecting, and transforming data to answer an analytical question. Also known as data cleaning or “munging”, legend has it that this wrangling costs analytics professionals as much as 80% of their time, leaving only 20% for exploration and modeling.	What is the meaning of data wrangling
4978	Loss value implies how poorly or well a model behaves after each iteration of optimization. An accuracy metric is used to measure the algorithm's performance in an interpretable way. The accuracy of a model is usually determined after the model parameters and is calculated in the form of a percentage.	What is loss value
4444	Bayesian statistics are indispensable when what you need is to evaluate a decision or conclusion in light of the available evidence. Quality control would be impossible without Bayesian statistics.	Is Bayesian statistics useful
1130	A normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution.  Since the distribution has a mean of 0 and a standard deviation of 1, the Z column is equal to the number of standard deviations below (or above) the mean.	How is the normal distribution related to the standard normal distribution
8566	- Quora. Non-significant variables on univariate analysis became significant on multivariate analysis?  Yes, it is possible that when you add more predictors (X2, X3 and so forth) in a multiple regression, X1 can become a statistically significant predictor.	Non significant variables on univariate analysis became significant on multivariate analysis Is this possible And how can we explain this
5520	·10 min read. In this article, I will present to you the most sophisticated optimization algorithms in Deep Learning that allow neural networks to learn faster and achieve better performance. These algorithms are Stochastic Gradient Descent with Momentum, AdaGrad, RMSProp, and Adam Optimizer.	What are the optimizers for deep learning
2946	The Binomial Theorem: Formulas. The Binomial Theorem is a quick way (okay, it's a less slow way) of expanding (or multiplying out) a binomial expression that has been raised to some (generally inconveniently large) power. For instance, the expression (3x – 2)10 would be very painful to multiply out by hand.	How does binomial theorem work
6734	And here are seven things you can do about that missing data:Listwise Deletion: Delete all data from any participant with missing values.  Recover the Values: You can sometimes contact the participants and ask them to fill out the missing values.	What do you do with missing values in a data set
462	A frequency distribution is a table that shows “classes” or “intervals” of data entries with a count of the number of entries in each class. The frequency f of a class is the number of data entries in the class.  The “class width” is the distance between the lower limits of consecutive classes.	Is class width the same as class interval
6395	The goal of training is to minimize a loss. This loss describes the objective that the autoencoder tries to reach. When our goal is to merely reconstruct the input as accurately as possible, two major types of loss function are typically used: Mean squared error and Kullback-Leibler (KL) divergence.	Which loss function is used for Autoencoder
2528	It's not bad to do, necessarily, but it's not a good habit to get into. Standardising variables when it's not necessary to do so leaves interpretation issues, and can lead to sloppy thinking. Also, remember that standardisation needs to be applied in the same way to all data sets that are used for a given built model.	How bad is it to standardize dummy variables
439	Fitting the XGBoost algorithm to conduct a multiclass classification.Data PreperationLoad the RBGlass1 dataset.convert the variable Site from a factor to numeric.Simulate a third class (furnace) from the data.Bind the new class to the original data.Subtract 1 from the Site names so they start at 0.Print out a summary()	How do you do multiclass classification in R
1051	A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous. If the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable. Decision boundaries are not always clear cut.	What is decision boundary in pattern recognition
1268	So, if you are constrained either by the size of the data or the number of trials you want to try, you may have to go with random forests. There is one fundamental difference in performance between the two that may force you to choose Random Forests over Gradient Boosted Machines (GBMs).	When would one use Random Forests over Gradient Boosted Machines GBMs
61	Linear regression is used to find the best fitting line between all the points of your dataset (by computing the minimum of a given distance), it does not, in itself, reduce the dimensionality of your data.	Why cant we use linear regression for dimension reduction
6348	7 Advantages of Robots in the WorkplaceSafety. Safety is the most obvious advantage of utilizing robotics.  Speed. Robots don't get distracted or need to take breaks.  Consistency. Robots never need to divide their attention between a multitude of things.  Perfection. Robots will always deliver quality.  Happier Employees.  Job Creation.  Productivity.	What are the positive effects of robots
579	The criticism was against the claim that Bayes' Theorem should be seen as foundational to the field. The debate went in a philosophical direction, with claims and counterclaims about whether real-world probabilities can ever be known to us. The controversy subsided only when the protagonists retired and died.	What is the criticism against the use of Bayes theorem in the probability theory
1392	The philosophy of information (PI) is a branch of philosophy that studies topics relevant to computer science, information science and information technology. It includes: the critical investigation of the conceptual nature and basic principles of information, including its dynamics, utilisation and sciences.	What is a state of information philosophy
793	Control Charts: A discrete distribution is one in which the data can only take on certain values, for example integers. A continuous distribution is one in which data can take on any value within a specified range (which may be infinite).	What is the difference between discrete and continuous probability distributions
772	Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem.  Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer.	What is Softmax layer in CNN
5733	Since medical tests can't be absolutely true, false positive and false negative are two problems we have to deal with. A false positive can lead to unnecessary treatment and a false negative can lead to a false diagnostic, which is very serious since a disease has been ignored.	What is more dangerous false negative or false positive
335	One of the simplest and yet most important models in time series forecasting is the random walk model. This model assumes that in each period the variable takes a random step away from its previous value, and the steps are independently and identically distributed in size (“i.i.d.”).	What is the random walk model
1277	"Latent Class (LC) segmentation operates under the assumption that there are groups underlying the data that give rise to segments. These groups are ""latent"" or not directly observable. LC techniques use formal statistical modeling to get at these segments, unlike most other segmentation methods."	What is latent class segmentation
6376	Box and Tiao (1973) define a noninformative prior as a prior which provides little information relative to the experiment. Bernardo and Smith (1994) use a similar definition, they say that noninformative priors have minimal effect relative to the data, on the final inference.	What is a non informative prior
3101	Answer: An example of a superset can be that if B is a proper superset of A, then all elements of A shall be in B but B shall have at least one element whose existence does not take place in A.  In contrast, a proper subset contains elements of the original set but not all.	What is the difference between subset and superset
2832	Mathematical Statistics TopicsCombinatorics and basic set theory notation.Probability definitions and properties.Common discrete and continuous distributions.Bivariate distributions.Conditional probability.Random variables, expectation, variance.Univariate and bivariate transformations.More items	What are the main topics in statistics
8546	In statistics, nonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. The data are fitted by a method of successive approximations.	What are nonlinear regression models
1289	5:1515:11Suggested clip · 109 secondsStatQuest: Linear Discriminant Analysis (LDA) clearly explained YouTubeStart of suggested clipEnd of suggested clip	How do you do linear discriminant analysis
1091	Normality: Data have a normal distribution (or at least is symmetric) Homogeneity of variances: Data from multiple groups have the same variance. Linearity: Data have a linear relationship. Independence: Data are independent.	What are four main assumptions for parametric statistics
4053	The standard score (more commonly referred to as a z-score) is a very useful statistic because it (a) allows us to calculate the probability of a score occurring within our normal distribution and (b) enables us to compare two scores that are from different normal distributions.	What is Z score used for in statistics
1102	We can reduce the size of a Tensorflow Model using the below mentioned methods: Freezing: Convert the variables stored in a checkpoint file of the SavedModel into constants stored directly in the model graph. This reduces the overall size of the model.	How do I reduce the size of a TensorFlow model
2301	It is said that because the shape of the constraint in LASSO is a diamond, the least squares solution obtained might touch the corner of the diamond such that it leads to a shrinkage of some variable. However, in ridge regression, because it is a circle, it will often not touch the axis.	Why does ridge regression shrinkage coefficients
455	As seen Table 1, for the single label classification, labels (category) are mutually exclusive and each instance is assigned to only one category. On the other hand, in the multi-label classification, the labels are interrelated and each instance corresponds to multiple class labels ( Table 2).	What is single label classification
3461	BFS stands for Breadth First Search. DFS stands for Depth First Search. 2. BFS(Breadth First Search) uses Queue data structure for finding the shortest path. DFS(Depth First Search) uses Stack data structure.	What is DFS and BFS
5777	"""Neural plasticity"" refers to the capacity of the nervous system to modify itself, functionally and structurally, in response to experience and injury.  This chapter discusses how plasticity is necessary not only for neural networks to acquire new functional properties, but also for them to remain robust and stable."	What is plasticity in neural networks
3583	Hickam's dictum	What's the opposite of Occam's razor
87	Loss Function The localization loss is a smooth L1 loss between the predicted bounding box correction and the true values. The coordinate correction transformation is same as what R-CNN does in bounding box regression.	What is localization loss
4295	Inter-observer variation is the amount of variation between the results obtained by two or more observers examining the same material. Intra-observer variation is the amount of variation one observer experiences when observing the same material more than once.	What is interobserver variability
4038	In descriptive statistics, a time series is defined as a set of random variables ordered with respect to time. Time series are studied both to interpret a phenomenon, identifying the components of a trend, cyclicity, seasonality and to predict its future values.	What is time series data in machine learning
445	Distance metric learning (or simply, metric learning) aims at automatically constructing task-specific distance metrics from (weakly) supervised data, in a machine learning manner. The learned distance metric can then be used to perform various tasks (e.g., k-NN classification, clustering, information retrieval).	What is distance metric learning
2088	The Adaptive Sliding Window (ADWIN) algorithm [8] is another popular, window-based detector for coping with concept drift. Assuming a stream of examples x_1,x_2,\ldots , x_n, produced by some distribution at time t, these serve as inputs to ADWIN to produce sliding window W.	Which algorithm uses sliding window to manage concept drift
1046	We can use MLE in order to get more robust parameter estimates. Thus, MLE can be defined as a method for estimating population parameters (such as the mean and variance for Normal, rate (lambda) for Poisson, etc.) from sample data such that the probability (likelihood) of obtaining the observed data is maximized.	Do we ever use maximum likelihood estimation
5244	In project management terms, an s-curve is a mathematical graph that depicts relevant cumulative data for a project—such as cost or man-hours—plotted against time.  An s-curve in project management is typically used to track the progress of a project.	What does an S curve represent
3391	In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on its own.	What is the difference between supervised and unsupervised classification
2797	Forward chaining starts from known facts and applies inference rule to extract more data unit it reaches to the goal. Backward chaining starts from the goal and works backward through inference rules to find the required facts that support the goal.  Backward chaining reasoning applies a depth-first search strategy.	What is forward and backward chaining in AI
1214	Confidence intervals and hypothesis tests are similar in that they are both inferential methods that rely on an approximated sampling distribution. Confidence intervals use data from a sample to estimate a population parameter. Hypothesis tests use data from a sample to test a specified hypothesis.	What is the relationship between hypothesis testing and confidence interval
969	In machine learning, a hyperparameter is a parameter whose value is used to control the learning process.  An example of a model hyperparameter is the topology and size of a neural network. Examples of algorithm hyperparameters are learning rate and mini-batch size.	What is Hyperparameter in neural network
7967	Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration.  Usually, a number that can be divided into the total dataset size. stochastic mode: where the batch size is equal to one.	What is batch size machine learning
7891	Artificial intelligence can dramatically improve the efficiencies of our workplaces and can augment the work humans can do. When AI takes over repetitive or dangerous tasks, it frees up the human workforce to do work they are better equipped for—tasks that involve creativity and empathy among others.	How does AI affect humanity
4586	Gap Statistic Method Hence, the optimal choice of k is the value that maximizes the gap (meaning that the clustering structure is far away from a random uniform distribution of points). We can then use k=5 from the Gap Statistic method to use in KMeans and visualize the clustering result.	Which K value in K means clustering produces the best clustering Optimisation result
5526	In probability theory and statistics, the beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parameterized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable and control the shape of the distribution.	What is A and B in beta distribution
1103	A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.	What is a term document incidence matrix
1418	The reason why Convolutional Neural Networks (CNNs) do so much better than classic neural networks on images and videos is that the convolutional layers take advantage of inherent properties of images. Simple feedforward neural networks don't see any order in their inputs.	Why convolutional neural networks are better suited for image recognition than fully connected networks
998	The Wilcoxon signed rank sum test is another example of a non-parametric or distribution free test (see 2.1 The Sign Test). As for the sign test, the Wilcoxon signed rank sum test is used is used to test the null hypothesis that the median of a distribution is equal to some value.	Which type of test is the Wilcoxon rank sum test an example of
6924	At every node, a set of possible split points is identified for every predictor variable. The algorithm calculates the improvement in purity of the data that would be created by each split point of each variable. The split with the greatest improvement is chosen to partition the data and create child nodes.	What is splitting variable in decision tree
171	The sample standard deviation (s) is a point estimate of the population standard deviation (σ). The sample mean (̄x) is a point estimate of the population mean, μ The sample variance (s2 is a point estimate of the population variance (σ2).	What is the point estimate of the population standard deviation
4017	Gradient Boosting Machines vs. XGBoost.  While regular gradient boosting uses the loss function of our base model (e.g. decision tree) as a proxy for minimizing the error of the overall model, XGBoost uses the 2nd order derivative as an approximation.	What's the difference between gradient boosting and XGBoost
2189	Hindsight bias is a psychological phenomenon that allows people to convince themselves after an event that they had accurately predicted it before it happened. This can lead people to conclude that they can accurately predict other events.	What does hindsight bias mean
6996	An operation which can produce some well-defined outcomes, is called an experiment. Each outcome is called an event. An experiment in which all possible outcomes are known and the exact outcome cannot be predicted in advance, is called a random experiment.	What is the difference between experiment and random experiment
2973	In statistics, Poisson regression is a generalized linear model form of regression analysis used to model count data and contingency tables.  A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables.	Is Poisson regression linear
209	So, assuming a 15% survey response rate, we see that you should send your NPS survey to 1,700 customers. What if you're a smaller company and don't have enough customers to send the recommended number of invitations?	What is a good sample size for NPS
1006	A random variable can be either discrete (having specific values) or continuous (any value in a continuous range). The use of random variables is most common in probability and statistics, where they are used to quantify outcomes of random occurrences.	Why are statistics random variables
739	The only difference from Ridge regression is that the regularization term is in absolute value.  Lasso method overcomes the disadvantage of Ridge regression by not only punishing high values of the coefficients β but actually setting them to zero if they are not relevant.	How does Lasso differ from ridge regression
1209	Normal distribution describes continuous data which have a symmetric distribution, with a characteristic 'bell' shape. Binomial distribution describes the distribution of binary data from a finite sample.  Poisson distribution describes the distribution of binary data from an infinite sample.	What is the difference between binomial Poisson and normal distributions
4181	Bias allows you to shift the activation function by adding a constant (i.e. the given bias) to the input. Bias in Neural Networks can be thought of as analogous to the role of a constant in a linear function, whereby the line is effectively transposed by the constant value.	What is bias in neural network
1349	Selection sortClassSorting algorithmWorst-case performanceО(n2) comparisons, О(n) swapsBest-case performanceО(n2) comparisons, O(1) swapsAverage performanceО(n2) comparisons, О(n) swapsWorst-case space complexityO(1) auxiliary1 more row	Which selection algorithm has better best case performance
4139	6 Types of Regression Models in Machine Learning You Should Know AboutLinear Regression.Logistic Regression.Ridge Regression.Lasso Regression.Polynomial Regression.Bayesian Linear Regression.	What are the different types of regression models
5097	Von Mises stress is a value used to determine if a given material will yield or fracture. It is mostly used for ductile materials, such as metals.	Under what conditions do we use Von Mises equivalent stress
7778	Bootstrapping is a type of resampling where large numbers of smaller samples of the same size are repeatedly drawn, with replacement, from a single original sample. For example, let's say your sample was made up of ten numbers: 49, 34, 21, 18, 10, 8, 6, 5, 2, 1. You randomly draw three numbers 5, 1, and 49.	What is an example of bootstrapping
4411	For a large sample size, Sample Variance will be a better estimate of Population variance so even if population variance is unknown, we can use the Z test using sample variance. Similarly, for a Large Sample, we have a high degree of freedom.	Do I use Z or t test
169	An artificial neural network is an attempt to simulate the network of neurons that make up a human brain so that the computer will be able to learn things and make decisions in a humanlike manner. ANNs are created by programming regular computers to behave as though they are interconnected brain cells.	What is a simple explanation of how artificial neural networks work 1
431	A Statistical Model is the use of statistics to build a representation of the data and then conduct analysis to infer any relationships between variables or discover insights. Machine Learning is the use of mathematical and or statistical models to obtain a general understanding of the data to make predictions.	What is statistical model in machine learning
7418	You probably have a numerical stability issue. This may happen due to zero division or any operation that is making a number(s) extremely big.	Why do l get NaN values when l train my neural network with a rectified linear unit
5310	Limitations include its sample size requirements, difficulty of interpretation when there are large numbers of categories (20 or more) in the independent or dependent variables, and tendency of the Cramer's V to produce relative low correlation measures, even for highly significant results.	What are the assumptions and limitations of chi square test
3758	There are two types of quantitative data, which is also referred to as numeric data: continuous and discrete. As a general rule, counts are discrete and measurements are continuous. Discrete data is a count that can't be made more precise. Typically it involves integers.	What are the two types of quantitative data
5942	Typical examples are the linear operator of multiplication by and differentiation in .	Is Multiplication a linear operator
5555	Related units One newton equals one kilogram metre per second squared. Therefore, the unit metre per second squared is equivalent to newton per kilogram, N·kg−1, or N/kg. Thus, the Earth's gravitational field (near ground level) can be quoted as 9.8 metres per second squared, or the equivalent 9.8 N/kg.	Is N kg equal to m/s 2
7705	How to Calculate a Confusion MatrixYou need a test dataset or a validation dataset with expected outcome values.Make a prediction for each row in your test dataset.From the expected outcomes and predictions count: The number of correct predictions for each class.	How do you solve a confusion matrix
483	Like all of the least squares methods discussed so far, weighted least squares is an efficient method that makes good use of small data sets. It also shares the ability to provide different types of easily interpretable statistical intervals for estimation, prediction, calibration and optimization.	Why do we use weighted least squares
1359	To train a generative model we first collect a large amount of data in some domain (e.g., think millions of images, sentences, or sounds, etc.) and then train a model to generate data like it. The intuition behind this approach follows a famous quote from Richard Feynman: “What I cannot create, I do not understand.”	How do you train a generative model
3129	Stacked Generalization or “Stacking” for short is an ensemble machine learning algorithm. It involves combining the predictions from multiple machine learning models on the same dataset, like bagging and boosting.	What is stacking in machine learning
2809	"AUC stands for ""Area under the ROC Curve."" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). Figure 5. AUC (Area under the ROC Curve). AUC provides an aggregate measure of performance across all possible classification thresholds."	What is AUC in classification
5691	Hash algorithms have been around for decades and are used for applications such as table lookups. For example, you can use a person's name and address as a hash key used by a hash algorithm. The output of the hash algorithm will be a pointer into a table where the person's information will be stored.	Where is hashing algorithm used
981	Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum.	How does Adam Optimizer work
3398	The two ratios are both used in the Capital Assets Pricing Model (CAPM)Alpha= R – Rf – beta (Rm-Rf)R represents the portfolio return.Rf represents the risk-free rate of return.Beta represents the systematic risk of a portfolio.Rm represents the market return, per a benchmark.	How do you calculate alpha in CAPM
1143	Chi-Square goodness of fit test is a non-parametric test that is used to find out how the observed value of a given phenomena is significantly different from the expected value.  In Chi-Square goodness of fit test, sample data is divided into intervals.	What is the chi square goodness of fit test used for
719	The Sobel filter is used for edge detection. It works by calculating the gradient of image intensity at each pixel within the image.  The result shows how abruptly or smoothly the image changes at each pixel, and therefore how likely it is that that pixel represents an edge.	How does Sobel edge detection work
4610	When you conduct a study that looks at a single variable, that study involves univariate data. For example, you might study a group of college students to find out their average SAT scores or you might study a group of diabetic patients to find their weights. Bivariate data is when you are studying two variables.	What is univariate and bivariate analysis with examples
2608	Naive Bayes Classifier example by hand and how to do in Scikit-Learn, You can use any kind of predictor in a naive Bayes classifier, as long as you can specify a discriminative linear models take a mixture of categorical and continuous predictors.	Can a naive Bayes algorithm be used for both continuous and categorical data
715	Train Loss is the value of the objective function that you are minimizing. This value could be a positive or negative number, depending on the specific objective function of your training data. The training loss is calculated over the entire training dataset.	What is the training loss
5693	Unlike the independent-samples t-test, the Mann-Whitney U test allows you to draw different conclusions about your data depending on the assumptions you make about your data's distribution.  These different conclusions hinge on the shape of the distributions of your data, which we explain more about later.	Why use Mann Whitney U test instead of t test
1887	An autoregressive (AR) model predicts future behavior based on past behavior. It's used for forecasting when there is some correlation between values in a time series and the values that precede and succeed them.  Where simple linear regression and AR models differ is that Y is dependent on X and previous values for Y.	Why are models autoregressive
8333	Implementing Stochastic Gradient Descent (SGD) with Python# import the necessary packages.import matplotlib.pyplot as plt.from sklearn.datasets.samples_generator import make_blobs.import numpy as np.import argparse.def sigmoid_activation(x):# compute and return the sigmoid activation value for a.# given input value.More items•	How do you implement stochastic gradient descent in Python
110	BACKWARD STEPWISE REGRESSION is a stepwise regression approach that begins with a full (saturated) model and at each step gradually eliminates variables from the regression model to find a reduced model that best explains the data. Also known as Backward Elimination regression.	What is backward stepwise regression
3049	The number of true positives is placed in the top left cell of the confusion matrix. The data rows (emails) belonging to the positive class (spam) and incorrectly classified as negative (normal emails). These are called False Negatives (FN).	What is a positive class in a confusion matrix
4661	"Heuristics are the ""shortcuts"" that humans use to reduce task complexity in judgment and choice, and biases are the resulting gaps between normative behavior and the heuristically determined behavior (Kahneman et al., 1982)."	What is the difference between a heuristic and a bias
1723	To determine whether the correlation between variables is significant, compare the p-value to your significance level. Usually, a significance level (denoted as α or alpha) of 0.05 works well. An α of 0.05 indicates that the risk of concluding that a correlation exists—when, actually, no correlation exists—is 5%.	How do you find the significance level of a correlation
1202	"The machine operates on an infinite memory tape divided into discrete ""cells"". The machine positions its ""head"" over a cell and ""reads"" or ""scans"" the symbol there.  The Turing machine was invented in 1936 by Alan Turing, who called it an ""a-machine"" (automatic machine)."	How does a Turing machine work
1473	There are many types of motors are available in today's market, but mostly Tiny pager motors, servo motors, linear motors, stepper motors and DC geared motors are used in industrial robots according to their application area.	Which motor is used in robots
4280	The probability of each value of the discrete random variable is between 0 and​ 1, inclusive, and the sum of all the probabilities is 1.  It is the expected value of a discrete random variable.	What is a discrete probability distribution What are the two conditions
5995	A good knowledge representation system must have properties such as: Representational Accuracy: It should represent all kinds of required knowledge. Inferential Adequacy: It should be able to manipulate the representational structures to produce new knowledge corresponding to the existing structure.	What are the properties of good knowledge representation techniques
2631	Skewness refers to distortion or asymmetry in a symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed. Skewness can be quantified as a representation of the extent to which a given distribution varies from a normal distribution.	What is skew in statistics
170	A decision tree is a flowchart-like diagram that shows the various outcomes from a series of decisions. It can be used as a decision-making tool, for research analysis, or for planning strategy. A primary advantage for using a decision tree is that it is easy to follow and understand.	What is decision tree explain with diagram
1765	Bimodal Distribution: Two Peaks. The bimodal distribution has two peaks.  However, if you think about it, the peaks in any distribution are the most common number(s). The two peaks in a bimodal distribution also represent two local maximums; these are points where the data points stop increasing and start decreasing.	How do you describe bimodal distribution
1331	Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value.	When should I use ridge regression
3953	Machine learning, on the other hand, refers to a group of techniques used by data scientists that allow computers to learn from data. These techniques produce results that perform well without programming explicit rules.  Although data science includes machine learning, it is a vast field with many different tools.	Is data science the same as machine learning
1474	In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.  In setting a learning rate, there is a trade-off between the rate of convergence and overshooting.	What is learning rate in machine learning
8517	"Symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level ""symbolic"" (human-readable) representations of problems, logic and search.  Production rules connect symbols in a relationship similar to an If-Then statement."	What is symbolic machine learning
1307	Natural Language Processing (NLP) is the sub-branch of Data Science that attempts to extract insights from “text.” Thus, NLP is assuming an important role in Data Science.	Is NLP part of data science
1242	5 Most Important Methods For Statistical Data AnalysisMean. The arithmetic mean, more commonly known as “the average,” is the sum of a list of numbers divided by the number of items on the list.  Standard Deviation.  Regression.  Sample Size Determination.  Hypothesis Testing.	What is the best statistical analysis technique
1029	Regularized regression is a type of regression where the coefficient estimates are constrained to zero. The magnitude (size) of coefficients, as well as the magnitude of the error term, are penalized. Complex models are discouraged, primarily to avoid overfitting.	What is regularization coefficient
560	Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant.	What is the problem with Collinearity
426	If you are already a programmer and has basic knowledge of how it works. I would say 2 days to a month to learn it. Toby Thain, Started at around 10 years old. Still learning.	How long will it take to learn data structures and algorithms
5130	An important way of checking whether a regression, simple or multiple, has achieved its goal to explain as much variation as possible in a dependent variable while respecting the underlying assumption, is to check the residuals of a regression.  If groups of observations were overlooked, they'll show up in the residuals.	Why is it important to study residuals when reviewing results of a regression model
1135	In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.  Given these hyperparameters, the training algorithm learns the parameters from the data.	What are hyperparameters in machine learning
7766	1 Answer. In fact for a sample space containing 2 possible outcomes Ω={a,b}, the event space contains 4 events, F={a,b,ab,∅}. In general, for a sample space containing n possible outcomes, the event space is the power set of the sample space, so contains 2n events.	How many events are in a sample space
531	Recurrent neural networks, or RNNs, are a type of artificial neural network that add additional weights to the network to create cycles in the network graph in an effort to maintain an internal state.	Is recurrent neural network deep learning
5016	A stationary time series is one whose properties do not depend on the time at which the series is observed. 14. Thus, time series with trends, or with seasonality, are not stationary — the trend and seasonality will affect the value of the time series at different times.	What is a stationary time series
1068	Communication TheoriesActor-Network Theory (ANT)  Adaptive Structuration Theory (AST)  Agenda Setting Theory.  Cognitive Dissonance Theory.  Groupthink.  Priming.  Social Exchange Theory.  Social Learning Theory.More items	What are the different theories of communication
5540	Stationarity. A common assumption in many time series techniques is that the data are stationary. A stationary process has the property that the mean, variance and autocorrelation structure do not change over time.	What is stationarity in time series analysis
801	The year is a categorical variable. The ratio between two years is not meaningful which is why its not appropriate to classify it as a quantitative variable.	Is year a quantitative or categorical variable
8083	A finite population is a collection of objects or individuals that are objects of research that occupy a certain area. It clear boundaries that distinguish these population groups from other populations.	What is a finite population in statistics
465	To predict a continuous value, you need to adjust your model (regardless whether it is Recurrent or Not) to the following conditions:Use a linear activation function for the final layer.Chose an appropriate cost function (square error loss is typically used to measure the error of predicting real values)	How can we make a neural network to predict a continuous variable
1105	Blended learning, also known as b-Learning, is a combination of offline and online instruction where students interact with the instructor, the material, and other students through both a physical classroom and an online platform.	What is blended learning in offline mode
8095	Generalization is a term used to describe a model's ability to react to new data. Generalization is the ability of your model, after being trained to digest new data and make accurate predictions.	What is generalized neural network model
764	The law of averages is a lay term used to express a belief that outcomes of a random event will “even out” within a small sample. The law of averages says it's due to land on black! ” Of course, the wheel has no memory and its probabilities do not change according to past results.	What is the law of averages in statistics
5674	Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them.	What is Federated machine learning
3227	A 100MP smartphone camera will likely have pixels that are too small for even pixel-binning to make a big difference. As we know it today, pixel-binning might not be able to produce great results with a 100MP camera, as the pixels could be far too small.  Both of these devices offer a 12MP 1.4-micron pixel main camera.	Is pixel binning bad
813	A probability distribution is a statistical function that describes all the possible values and likelihoods that a random variable can take within a given range.  These factors include the distribution's mean (average), standard deviation, skewness, and kurtosis.	What is a probability distribution in statistics
590	Descriptive statistics summarize the characteristics of a data set. Inferential statistics allow you to test a hypothesis or assess whether your data is generalizable to the broader population.	What is the difference between descriptive and inferential statistics
817	The first step in backward elimination is pretty simple, you just select a significance level, or select the P-value. Usually, in most cases, a 5% significance level is selected. This means the P-value will be 0.05. You can change this value depending on the project.	What is significance level in backward elimination
332	The normal curve is called Mesokurtic curve. If the curve of a distribution is peaked than a normal or mesokurtic curve then it is referred to as a Leptokurtic curve. If a curve is less peaked than a normal curve, it is called as a Platykurtic curve. That's why kurtosis of normal distribution equal to three.	Why kurtosis of normal distribution is 3
778	A population is the entire group that you want to draw conclusions about. A sample is the specific group that you will collect data from. The size of the sample is always less than the total size of the population.	What is a sample and population in statistics
1210	Depending on how the machine learning systems are used, such biases could result in lower customer service experiences, reduced sales and revenue, unfair or possibly illegal actions, and potentially dangerous conditions.	How does bias affect machine learning
1319	The first step in backward elimination is pretty simple, you just select a significance level, or select the P-value. Usually, in most cases, a 5% significance level is selected. This means the P-value will be 0.05. You can change this value depending on the project.	What is p value in backward elimination
2075	You might also see this written as something like “An unbiased estimator is when the mean of the statistic's sampling distribution is equal to the population's parameter.” This essentially means the same thing: if the statistic equals the parameter, then it's unbiased.	How do you prove an estimator is unbiased
1403	A non-correlated asset is exactly what sounds like: an asset whose value isn't tied to larger fluctuations in the traditional markets. Yes, it's true that broad market movements can impact any asset, even those considered traditionally non-correlated.	What are uncorrelated assets
2819	Fuzzy logic is useful for commercial and practical purposes. It can control machines and consumer products. It may not give accurate reasoning, but acceptable reasoning. Fuzzy logic helps to deal with the uncertainty in engineering.	Artificial Intelligence How useful is fuzzy logic in designing AI
145	A control group is a set of experimental samples or subjects that are kept separate and aren't exposed to the independent variable.  A controlled experiment is one in which every parameter is held constant except for the experimental (independent) variable.	What is the difference between a control variable and a control group
5799	“The benefit to using a one-tailed test is that it requires fewer subjects to reach significance. A two-tailed test splits your significance level and applies it in both directions. Thus, each direction is only half as strong as a one-tailed test, which puts all the significance in one direction.	What is the primary benefit of conducting a one tailed test instead of a two tailed test
1020	With cluster sampling, in contrast, the sample includes elements only from sampled clusters. Multistage sampling. With multistage sampling, we select a sample by using combinations of different sampling methods. For example, in Stage 1, we might use cluster sampling to choose clusters from a population.	What is the difference between cluster and multistage sampling
338	Coefficients of linear discriminants: Shows the linear combination of predictor variables that are used to form the LDA decision rule. for example, LD1 = 0.91*Sepal.	What is coefficients of linear Discriminants
7111	Word2vec is a group of related models that are used to produce word embeddings.  Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space.	What is word2vec word Embeddings
7657	An endogenous variable is a variable in a statistical model that's changed or determined by its relationship with other variables within the model. In other words, an endogenous variable is synonymous with a dependent variable, meaning it correlates with other factors within the system being studied.	What does it mean for a variable to be endogenous
1158	A machine learning pipeline is used to help automate machine learning workflows. They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative.	What is a pipeline in ML
3050	"Than sentence examplesHe thinks you are better than us.  He has lived more than eighty years.  Alex had been hiding more than a father.  Less than a week later she passed another milestone.  No one could have been more private than Josh.  ""That's all right,"" returned the man's voice, more pleasantly than before.More items"	How do you use than in a sentence
3449	Random error is always present in a measurement. It is caused by inherently unpredictable fluctuations in the readings of a measurement apparatus or in the experimenter's interpretation of the instrumental reading.  They can be estimated by comparing multiple measurements, and reduced by averaging multiple measurements.	What causes random error
4735	: being, relating to, or involving statistical methods that assign probabilities or distributions to events (such as rain tomorrow) or parameters (such as a population mean) based on experience or best guesses before experimentation and data collection and that apply Bayes' theorem to revise the probabilities and	What does Bayesian mean
2914	Calculation. The formula given in most textbooks is Skew = 3 * (Mean – Median) / Standard Deviation. This is known as an alternative Pearson Mode Skewness. You could calculate skew by hand.	How do you find the skew of a distribution
4768	Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph. How sent_tokenize works ? The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk.	How does NLTK sentence Tokenizer work
4966	On a technical note, estimation of a latent variable is done by analyzing the variance and covariance of the indicators. The measurement model of a latent variable with effect indicators is the set of relationships (modeled as equations) in which the latent variable is set as the predictor of the indicators.	How do you calculate latent variables
1266	Consider these five ways to promote active learning in your own classroom.Make Videos and Photographs Engaging. They say a picture is worth 1,000 words.  Have to Lecture? Keep It Interactive.  Incorporate Games and Puzzles.  Harness the Power of Social Collaboration.  Assign Flexible Projects.	How do you promote active learning
2928	There are two sets of degrees of freedom; one for the numerator and one for the denominator. For example, if F follows an F distribution and the number of degrees of freedom for the numerator is four, and the number of degrees of freedom for the denominator is ten, then F ~ F 4,10.	What are the numerator and denominator degrees of freedom for the F test
1176	Predictive models use known results to develop (or train) a model that can be used to predict values for different or new data. The modeling results in predictions that represent a probability of the target variable (for example, revenue) based on estimated significance from a set of input variables.	What is predictive modeling techniques
1149	Dependent events influence the probability of other events – or their probability of occurring is affected by other events. Independent events do not affect one another and do not increase or decrease the probability of another event happening.	What is dependent and independent events in probability
5042	A distribution is skewed if one of its tails is longer than the other. The first distribution shown has a positive skew. This means that it has a long tail in the positive direction. The distribution below it has a negative skew since it has a long tail in the negative direction.	What does a skewed distribution mean
8510	There are three main methods for handling continuous variables in naive Bayes classifiers, namely, the normal method (parametric approach), the kernel method (non parametric approach) and discretization.	How does naive Bayes handle continuous variables
5374	The expected value (i.e. the mean) of a uniform random variable X is: E(X) = (1/2) (a + b) This is also written equivalently as: E(X) = (b + a) / 2. “a” in the formula is the minimum value in the distribution, and “b” is the maximum value.	What is the mean of a uniform distribution
4741	The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers.	How do you choose the value of k in KNN algorithm
529	Generally, z-tests are used when we have large sample sizes (n > 30), whereas t-tests are most helpful with a smaller sample size (n < 30). Both methods assume a normal distribution of the data, but the z-tests are most useful when the standard deviation is known.	When is the t test preferred to the Z test
2416	Improving the PF can maximize current-carrying capacity, improve voltage to equipment, reduce power losses, and lower electric bills. The simplest way to improve power factor is to add PF correction capacitors to the electrical system. PF correction capacitors act as reactive current generators.	How can we control the power factor
1020	four	How many types of agents are there in artificial intelligence
679	The probability distribution of a discrete random variable can always be represented by a table. For example, suppose you flip a coin two times.  The probability of getting 0 heads is 0.25; 1 head, 0.50; and 2 heads, 0.25. Thus, the table is an example of a probability distribution for a discrete random variable.	What is a probability distribution example
5736	Exploding gradients are a problem where large error gradients accumulate and result in very large updates to neural network model weights during training. This has the effect of your model being unstable and unable to learn from your training data.	What is exploding gradient problem
1413	In the ARCH(q) process the conditional variance is specified as a linear function of past sample variances only, whereas the GARCH(p, q) process allows lagged conditional variances to enter as well. This corresponds to some sort of adaptive learning mechanism.	What is the difference between Arch and Garch model
2258	For example, a network with two variables in the input layer, one hidden layer with eight nodes, and an output layer with one node would be described using the notation: 2/8/1. I recommend using this notation when describing the layers and their size for a Multilayer Perceptron neural network.	How many nodes are in a hidden layer
973	Random samples are the best method of selecting your sample from the population of interest. The advantages are that your sample should represent the target population and eliminate sampling bias. The disadvantage is that it is very difficult to achieve (i.e. time, effort and money).	What is the advantage and disadvantage of simple random sampling
7224	"In statistics, the coefficient of determination, denoted R2 or r2 and pronounced ""R squared"", is the proportion of the variance in the dependent variable that is predictable from the independent variable(s)."	Is R or R 2 the correlation coefficient
3161	In probability theory and statistics, the Poisson distribution (/ˈpwɑːsɒn/; French pronunciation: ​[pwasɔ̃]), named after French mathematician Siméon Denis Poisson, is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these	What is Poisson distribution in probability
7494	The time spent waiting between events is often modeled using the exponential distribution. For example, suppose that an average of 30 customers per hour arrive at a store and the time between arrivals is exponentially distributed.	Where is exponential distribution used
5425	The correlation coefficient is a statistical measure of the strength of the relationship between the relative movements of two variables. The values range between -1.0 and 1.0.  Since oil companies earn greater profits as oil prices rise, the correlation between the two variables is highly positive.	How do you describe the correlation coefficient
1342	Instance-based methods are sometimes referred to as lazy learning methods because they delay processing until a new instance must be classified. The nearest neighbors of an instance are defined in terms of Euclidean distance.	Why instance based learning is called as lazy learning
779	AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.  AUC is scale-invariant.	What is AUC score in machine learning
66	Stratified random sampling is one common method that is used by researchers because it enables them to obtain a sample population that best represents the entire population being studied, making sure that each subgroup of interest is represented. All the same, this method of research is not without its disadvantages.	Why is stratified random sampling used
2017	Particle filtering uses a set of particles (also called samples) to represent the posterior distribution of some stochastic process given noisy and/or partial observations.  The state-space model can be nonlinear and the initial state and noise distributions can take any form required.	What is filtered in particle filtering
619	Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.	What does normal distribution mean
653	Using proper validation techniques helps you understand your model, but most importantly, estimate an unbiased generalization performance.Splitting your data.  k-Fold Cross-Validation (k-Fold CV)  Leave-one-out Cross-Validation (LOOCV)  Nested Cross-Validation.  Time Series CV.  Comparing Models.	How do you validate a model
5050	The SD is usually more useful to describe the variability of the data while the variance is usually much more useful mathematically. For example, the sum of uncorrelated distributions (random variables) also has a variance that is the sum of the variances of those distributions.	Which is better variance or standard deviation
575	Use of AI in Following Things/Fields/Areas:Virtual Assistant or Chatbots.Agriculture and Farming.Autonomous Flying.Retail, Shopping and Fashion.Security and Surveillance.Sports Analytics and Activities.Manufacturing and Production.Live Stock and Inventory Management.More items•	What are the various areas where AI Artificial intelligence can be used
1120	Greedy algorithms produce good solutions on some mathematical problems, but not on others. Most problems for which they work will have two properties: Greedy choice property. We can make whatever choice seems best at the moment and then solve the subproblems that arise later.	What are the characteristics of greedy algorithm
4472	A Power Spectral Density (PSD) is the measure of signal's power content versus frequency.  Therefore, while the power spectrum calculates the area under the signal plot using the discrete Fourier Transform, the power spectrum density assigns units of power to each unit of frequency and thus, enhances periodicities.	What is the difference between power spectrum and power spectral density
4632	0:041:23Suggested clip · 72 secondsQuick Example - Find the Area to the Right Of a Z-Score - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How is technology used to find the area to the right of Z
2155	Take the sum of all the deviations (they should all be positive numbers because of the absolute value operation), then divide by the number of deviations you have added together. This result is the average deviation from the mean.	How do you calculate the average deviation
302	"Recommender systems are an important class of machine learning algorithms that offer ""relevant"" suggestions to users. Categorized as either collaborative filtering or a content-based system, check out how these approaches work along with implementations to follow from example code."	What is recommender system in machine learning
1074	The distinction between probability and likelihood is fundamentally important: Probability attaches to possible results; likelihood attaches to hypotheses. Explaining this distinction is the purpose of this first column. Possible results are mutually exclusive and exhaustive.	What is difference between likelihood and probability
1793	The survival function is S(t) = Pr(T >t)=1 − F(t). – The survival function gives the probability that a subject will survive past time t.	How do you find the survival function
1360	Best practices – Machine Learning models and applicationsIdentify the business problem and the right success metrics.  Begin with it.  Gather correct data.  Move the algorithms instead of your data.  Initiate tests before the actual launch.  Avoid data dropping while machine learning algorithms train.  Keep away from objectives that are unaligned.  Keep using codes.More items•	What are some best practices for training machine learning models
6590	Jensen's inequality states that this line is everywhere at least as large as f(x). pf(x1) + (1 − p)f(x2) ≥ f(px1 + (1 − p)x2). If f is (doubly) differentiable then f is convex if and only if d2f/dx2 ≥ 0. Now consider a probability distribution P on a set M and a function X assigning real values X(m) for m ∈ M.	How do you prove Jensen's inequality
1380	As a rule of thumb, I'd say that SVMs are great for relatively small data sets with fewer outliers.  Also, deep learning algorithms require much more experience: Setting up a neural network using deep learning algorithms is much more tedious than using an off-the-shelf classifiers such as random forests and SVMs.	Is SVM deep learning
4090	Backpropagation and computing gradients. According to the paper from 1989, backpropagation: repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector.	How are weights adjusted in backpropagation
7419	Bayes' theorem, named after 18th-century British mathematician Thomas Bayes, is a mathematical formula for determining conditional probability. Conditional probability is the likelihood of an outcome occurring, based on a previous outcome occurring.	What is Bayes theorem probability
1117	The general idea is that machine learning, while not always the perfect choice, can be powerful in modeling time series data due to its ability to handle non-linear data. The feature engineering applied to the time series data in a machine learning approach is the key to how successful the model will be.	Data Science Can machine learning be used for time series analysis
322	No, you don't. You'll get an equivalent solution whether you apply some kind of linear scaling or not.  Then linear scaling can change the results dramatically. That's actually another reason to do feature scaling, but since you asked about simple linear regression, I won't go into that.	Do I need to do feature scaling for simple linear regression
5541	The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer.	How does neural network determine the number of hidden layers
741	In basic terms, A MANOVA is an ANOVA with two or more continuous response variables.  MANCOVA compares two or more continuous response variables (e.g. Test Scores and Annual Income) by levels of a factor variable (e.g. Level of Education), controlling for a covariate (e.g. Number of Hours Spent Studying).	What is the difference between Manova and Mancova
3824	The word stochastic is jargon for random. A stochastic process is a system which evolves in time while undergoing chance fluctuations. We can describe such a system by defining a family of random variables, {X t }, where X t measures, at time t, the aspect of the system which is of interest.	What is meant by stochastic process
5217	"Like random forests, gradient boosting is a set of decision trees. The two main differences are:  Combining results: random forests combine results at the end of the process (by averaging or ""majority rules"") while gradient boosting combines results along the way."	What is the difference between random forest and gradient boosted trees
7294	The problem is we always prefer an output having highest probability or lowest distance from reference as our answer and while we are dealing with it, KNN will always give same output for a given set of input repeatedly tested. That means it is quit deterministic.	Is K nearest neighbors example of deterministic algorithm
78	Each class will have a “lower class limit” and an “upper class limit” which are the lowest and highest numbers in each class. The “class width” is the distance between the lower limits of consecutive classes.	What is the difference between class size and class width
8358	December 1955	When was AI first introduced
1387	Definition. The Likelihood Ratio (LR) is the likelihood that a given test result would be expected in a patient with the target disorder compared to the likelihood that that same result would be expected in a patient without the target disorder.	What does the likelihood ratio mean
1512	A chi-square test is used when you want to see if there is a relationship between two categorical variables. In SPSS, the chisq option is used on the statistics subcommand of the crosstabs command to obtain the test statistic and its associated p-value.	Which test would be appropriate to test multiple variables in SPSS statistics
1290	Z-tests are statistical calculations that can be used to compare population means to a sample's. T-tests are calculations used to test a hypothesis, but they are most useful when we need to determine if there is a statistically significant difference between two independent sample groups.	What is the difference between the z test and the t test
1201	Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.	What does a normal distribution model
1385	The notation for the uniform distribution is X ~ U(a, b) where a = the lowest value of x and b = the highest value of x. The probability density function is f(x)=1b−a f ( x ) = 1 b − a for a ≤ x ≤ b.	How do you do a uniform distribution
172	Expected Value and Variance. This is also written equivalently as: E(X) = (b + a) / 2. “a” in the formula is the minimum value in the distribution, and “b” is the maximum value.	How do you find the expected value of a uniform distribution
794	When the two options are available, lemmatization will always be a better option than stemming.  But if you can apply a lemmatizer, it will always give you a better result, because lemmatizers rely on correct language data (dictionaries) to identify a word with its lemma.	Is it advisable to choose lemmatization over stemming in NLP
2647	To summarise, here's what you need to master before being able to learn and understand artificial intelligence:Advanced Math (e.g. correlation algorithms) and Stats.Programming language.Machine Learning.PATIENCE – yes, on top of everything you need lots of patience.	What do I need to learn for artificial intelligence
2275	someone who stands apart from others of his or her group, as by differing behavior, beliefs, or religious practices: scientists who are outliers in their views on climate change.	Who could you describe as an outlier
4847	"A confusion matrix is a table that is often used to describe the performance of a classification model (or ""classifier"") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing."	What is meant by confusion matrix in R
1087	Answer: You would first split the dataset into training and test sets, or perhaps use cross-validation techniques to further segment the dataset into composite sets of training and test sets within the data.	What evaluation approaches would you work to gauge the effectiveness of a machine learning model
352	Examples include path analysis/ regression, repeated measures analysis/latent growth curve modeling, and confirmatory factor analysis. Participants will learn basic skills to analyze data with structural equation modeling.	What is structural equation modeling example
8344	ABM is known by different names due to its wide variety of applications, which could refer to entirely diverse methodologies. It can also be called a multi-agent system (MAS) or agent-based system (ABS).	Can we call IoT a multi agent system
8181	Unlike Monte Carlo sampling methods that are able to draw independent samples from the distribution, Markov Chain Monte Carlo methods draw samples where the next sample is dependent on the existing sample, called a Markov Chain.	What's the difference between Markov Chain Monte Carlo methods and regular Monte Carlo methods
7086	There are various ways to modify a study design to actively exclude or control confounding variables (3) including Randomization, Restriction and Matching. In randomization the random assignment of study subjects to exposure categories to breaking any links between exposure and confounders.	How do you control a confounding variable in regression
2359	2 Answers. If you have two classes (i.e. binary classification), you should use a binary crossentropy loss. If you have more than two you should use a categorical crossentropy loss.	How do I tell which loss function is suitable for image classification
8409	Gaussian processes are useful in statistical modelling, benefiting from properties inherited from the normal distribution. For example, if a random process is modelled as a Gaussian process, the distributions of various derived quantities can be obtained explicitly.	What are Gaussian processes used for
2148	In the Fourier domain image, each point represents a particular frequency contained in the spatial domain image. The Fourier Transform is used in a wide range of applications, such as image analysis, image filtering, image reconstruction and image compression.	Where do we use Fourier transform
298	No, you don't have to transform your observed variables just because they don't follow a normal distribution. Linear regression analysis, which includes t-test and ANOVA, does not assume normality for either predictors (IV) or an outcome (DV). No way!  Yes, you should check normality of errors AFTER modeling.	Does the dependent variable need to be normally distributed in linear regression
144	Message passing algorithm which is an iterative decoding algorithm factorizes the global function of many variables into product of simpler local functions, whose arguments are the subset of variables. In order to visualize this factorization we use factor graph.	What is message passing algorithm
3869	Covariance measures the total variation of two random variables from their expected values.  Obtain the data.Calculate the mean (average) prices for each asset.For each security, find the difference between each value and mean price.Multiply the results obtained in the previous step.More items	How is covariance calculated
568	Histogram normalization is a common technique that is used to enhance fine detail within an image.  Each column in the cumulative histogram is computed as the sum of all the image intensity histogram values up to and including that grey level, and then it is scaled so that the final value is 1.0.	What does it mean to normalize a histogram
6696	Convergence of random variables (sometimes called stochastic convergence) is where a set of numbers settle on a particular number. In the same way, a sequence of numbers (which could represent cars or anything else) can converge (mathematically, this time) on a single, specific number.	What does convergence mean in statistics
7263	Test method. Use the one-sample z-test to determine whether the hypothesized population proportion differs significantly from the observed sample proportion.	What test statistic is used to test a population proportion
1868	"In Kalman filtering the ""process noise"" represents the idea/feature that the state of the system changes over time, but we do not know the exact details of when/how those changes occur, and thus we need to model them as a random process."	What is process noise in Kalman filter
1049	The non-linear functions do the mappings between the inputs and response variables. Their main purpose is to convert an input signal of a node in an ANN(Artificial Neural Network) to an output signal. That output signal is now used as an input in the next layer in the stack.	Why do we use non linear activation function
540	Maximum likelihood, also called the maximum likelihood method, is the procedure of finding the value of one or more parameters for a given statistic which makes the known likelihood distribution a maximum. The maximum likelihood estimate for a parameter is denoted . For a Bernoulli distribution, (1)	What is the meaning of maximum likelihood
4527	Top Machine Learning Algorithms You Should KnowLinear Regression.Logistic Regression.Linear Discriminant Analysis.Classification and Regression Trees.Naive Bayes.K-Nearest Neighbors (KNN)Learning Vector Quantization (LVQ)Support Vector Machines (SVM)More items•	What is the best machine learning algorithm
3732	From a practical standpoint, L1 tends to shrink coefficients to zero whereas L2 tends to shrink coefficients evenly. L1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that go to zero. L2, on the other hand, is useful when you have collinear/codependent features.	Why is l2 better than l1
6958	Definition of outliers. An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal.	How do you define an outlier
4601	Descriptive studies only describe the current state of a variable, so there are no presumed cause or effects, therefore no independent and dependent variables.  Since neither variable in a correlational design is manipulated, it is impossible to determine which is the cause and which is the effect.	Why are dependent and independent variables not applicable in a descriptive type of research
3632	Abstract. The goal of statistical pattern feature extraction (SPFE) is 'low loss dimension reduction'. As the key link of pattern recognition, dimension reduction has become the research hot spot and difficulty in the fields of pattern recognition, machine learning, data mining and so on.	What is statistical feature extraction
127	Parameter selection: When SVMs are used, there are a number of parameters selected to have the best performance including: (1) parameters included in the kernel functions, (2) the trade-off parameter C, and (3) the ε-insensitivity parameter.	What are the parameters of SVM
874	Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on. If your training/validation loss are about equal then your model is underfitting. Increase the size of your model (either number of layers or the raw number of neurons per layer)	How can validation loss be improved
3855	As the formula shows, the standard score is simply the score, minus the mean score, divided by the standard deviation.	How do you find the standardized score
1275	Neural network ensemble is a learning paradigm where many neural networks are jointly used to solve a problem.  Then it assigns random weights to those networks and employs genetic algorithm to evolve the weights so that they can characterize to some extent the fitness of the neural networks in constituting an ensemble.	What is ensemble neural network
4030	Better Naive Bayes: 12 Tips To Get The Most From The Naive Bayes AlgorithmMissing Data. Naive Bayes can handle missing data.  Use Log Probabilities.  Use Other Distributions.  Use Probabilities For Feature Selection.  Segment The Data.  Re-compute Probabilities.  Use as a Generative Model.  Remove Redundant Features.More items•	How do you increase the accuracy of a naive Bayes classifier
4509	The beta function has the formula. B(\alpha,\beta) = \int_{0}^{1} {t^{\alpha-1}(1-t)^{\beta-1}dt} The case where a = 0 and b = 1 is called the standard beta distribution. The equation for the standard beta distribution is. f(x) = \frac{x^{p-1}(1-x)^{q-1}}{B(p,q)} \hspace{.3in} 0 \le x \le 1; p, q > 0.	What is beta distribution formula
410	A linear model is an equation that describes a relationship between two quantities that show a constant rate of change.	What is a linear model
2076	"Disadvantages include its ""black box"" nature, greater computational burden, proneness to overfitting, and the empirical nature of model development. An overview of the features of neural networks and logistic regression is presented, and the advantages and disadvantages of using this modeling technique are discussed."	What are the disadvantages of neural networks
1320	The standard error tells you how accurate the mean of any given sample from that population is likely to be compared to the true population mean. When the standard error increases, i.e. the means are more spread out, it becomes more likely that any given mean is an inaccurate representation of the true population mean.	What standard error tells us
567	Word embeddings are widely used nowadays in Distributional Semantics and for a variety of tasks in NLP. Embeddings can be evaluated using ex- trinsic evaluation methods, i.e. the trained em- beddings are evaluated on a specific task such as part-of-speech tagging or named-entity recogni- tion (Schnabel et al., 2015).	How are word Embeddings usually evaluated
4236	The advantage of hierarchical clustering is that it is easy to understand and implement. The dendrogram output of the algorithm can be used to understand the big picture as well as the groups in your data.	What are the benefits of hierarchical clustering
1110	The term three-stage least squares (3SLS) refers to a method of estimation that combines system equation, sometimes known as seemingly unrelated regression (SUR), with two-stage least squares estimation.  It is assumed that each equation of the system is at least just-identified.	What is 3sls regression
8416	It measures the overall difference between your data and the values predicted by your estimation model (a “residual” is a measure of the distance from a data point to a regression line). Total SS is related to the total sum and explained sum with the following formula: Total SS = Explained SS + Residual Sum of Squares.	How do you find the sum of squares of residuals
6191	Lasso regression stands for Least Absolute Shrinkage and Selection Operator.  The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero.	What's the difference between Lasso and Ridge regression
5812	A learning algorithm is a method used to process data to extract patterns appropriate for application in a new situation. In particular, the goal is to adapt a system to a specific input-output transformation task.	What are learning algorithms
7898	Experience replay enables reinforcement learning agents to memorize and reuse past experiences, just as humans replay memories for the situation at hand. Contemporary off-policy algorithms either replay past experiences uniformly or utilize a rule- based replay strategy, which may be sub-optimal.	What is the role of experience replay in reinforcement learning
4963	K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique. K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.	Which type of technique is used in K nearest neighbors
4722	Data Processing is a task of converting data from a given form to a much more usable and desired form i.e. making it more meaningful and informative. Using Machine Learning algorithms, mathematical modelling and statistical knowledge, this entire process can be automated.	What is data processing in machine learning
642	"The Chi-square test is intended to test how likely it is that an observed distribution is due to chance. It is also called a ""goodness of fit"" statistic, because it measures how well the observed distribution of data fits with the distribution that is expected if the variables are independent."	What is the chi square test used for and what does it tell you
5074	The most used algorithm to train neural networks is gradient descent. We'll define it later, but for now hold on to the following idea: the gradient is a numeric calculation allowing us to know how to adjust the parameters of a network in such a way that its output deviation is minimized.	What is a gradient in neural network
182	A sequence of random variables is covariance stationary if all the terms of the sequence have the same mean, and if the covariance between any two terms of the sequence depends only on the relative positions of the two terms, that is, on how far apart they are located from each other, and not on their absolute position	How do you prove covariance stationary
548	The whole procedure involved is called the sample design. The term sample survey is used for a detailed study of the sample. In general, the term sample survey is used for any study conducted on the sample taken from some real world data. A complete list of all the units in a population is called the sampling frame.	What is the difference between sampling design and the sampling frame
1339	4:5317:59Suggested clip · 119 secondsHow to Use SPSS-Hierarchical Multiple Regression - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you control for variables in multiple regression SPSS
1244	Trends are determined by a combination of volume and how much time it takes to create volume. In other words, one-day growth is trending, while 30 days is just more news.  Because the number of tweets using the hashtag, #FreddieGrey, built up over time, volume increased at the same rate of traffic.	How are trending topics determined
8645	Difference between K Means and Hierarchical clustering Hierarchical clustering can't handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2).	Which is better K means or hierarchical clustering
7730	Does not affect R2 or adjusted R2 (since these estimate the POPULATION variances which are not conditional on X)	Does Heteroskedasticity affect R Squared
4929	In linear regression the independent variables can be categorical and/or continuous. But, when you fit the model if you have more than two category in the categorical independent variable make sure you are creating dummy variables.	Can linear regression be used for categorical variables
127	0:382:54Suggested clip · 70 secondsClass Boundaries - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the class boundaries
705	"A dependent variable is what you measure in the experiment and what is affected during the experiment. The dependent variable responds to the independent variable. It is called dependent because it ""depends"" on the independent variable."	What is referred to as the dependent variable
4233	To address this issue, there are a few techniques we can apply. One method is to randomly resample from the minority classes (West and East) in our training dataset to meet the highest class-specific sample size, essentially copying random minority records.	What are the strategies to address the class imbalance problem
4960	Hidden layers allow for the function of a neural network to be broken down into specific transformations of the data.  For example, a hidden layer functions that are used to identify human eyes and ears may be used in conjunction by subsequent layers to identify faces in images.	Why are hidden layers used in neural networks
4679	Gibbs sampling is commonly used for statistical inference (e.g. determining the best value of a parameter, such as determining the number of people likely to shop at a particular store on a given day, the candidate a voter will most likely vote for, etc.).	What is Gibbs sampling used for
804	Introduction to Poisson Regression Poisson regression is also a type of GLM model where the random component is specified by the Poisson distribution of the response variable which is a count. When all explanatory variables are discrete, log-linear model is equivalent to poisson regression model.	What is a Poisson regression model
4091	The direct approximation of the binomial by the Poisson says that a binomial(n,p) random variable has approximately the same distribution as a Poisson(np) random variable when np is large.	When can you approximate binomial with Poisson
729	The first type of process mining is discovery. A discovery technique takes an event log and produces a process model without using any a-priori information.  The second type of process mining is conformance. Here, an existing process model is compared with an event log of the same process.	What are the two process of mining
2330	A rank-2 tensor gets two rotation matrices. This pattern generalizes to tensors of arbitrary rank. In a particular coordinate system, a rank-2 tensor can be expressed as a square matrix, but one should not marry the concepts of tensors and matrices, just like one should think of vectors simply as arrays of numbers.	What is a rank 2 tensor
3876	A sampling distribution is a probability distribution of a statistic obtained from a larger number of samples drawn from a specific population.  It describes a range of possible outcomes that of a statistic, such as the mean or mode of some variable, as it truly exists a population.	What does a sampling distribution represent
6928	Propel your business processes to the next level with process mining technology and use RPA to increase your organization's productivity.  UiPath Process Mining allows businesses to holistically understand their processes and identify process improvement opportunities to increase efficiency and reduce costs.	What is process mining in UiPath
5135	There are ways, however, to try to maintain objectivity and avoid bias with qualitative data analysis:Use multiple people to code the data.  Have participants review your results.  Verify with more data sources.  Check for alternative explanations.  Review findings with peers.	How do you avoid reporting bias
3979	The variance of the sampling distribution of the mean is computed as follows: That is, the variance of the sampling distribution of the mean is the population variance divided by N, the sample size (the number of scores used to compute a mean).	What is the variance of the sample average
6163	Subsampling reduces the image size by removing information all together. Usually when you subsample, you also interpolate or smooth the image so that you reduce aliasing.  Usually, the chrominance values are filtered then subsampled by 1/2 or even 1/4 of that of the intensity.	What is subsampling in image processing
4375	Q-learning is called off-policy because the updated policy is different from the behavior policy, so Q-Learning is off-policy. In other words, it estimates the reward for future actions and appends a value to the new state without actually following any greedy policy.	Why Q learning is off policy
822	MAP Growth uses the RIT (Rasch Unit) scale to help you measure and compare academic growth. Specifically, the scale measures levels in academic difficulty. The RIT scale extends equally across all grades, making it possible to compare a student's score at various points throughout his or her education.	What is a Rasch Unit
718	Interpret the key results for Fit Mixed Effects ModelStep 1: Determine whether the random terms significantly affect the response.Step 2: Determine whether the fixed effect terms significantly affect the response.Step 3: Determine how well the model fits your data.Step 4: Evaluate how each level of a fixed effect term affects the response.More items	How do you read mixed model results
871	The property of maximality of entropy has been used to determine the conditions of equilibrium of an isolated system.	What determines the equilibrium position for an isolated system
6791	If you use natural log values for your dependent variable (Y) and keep your independent variables (X) in their original scale, the econometric specification is called a log-linear model. These models are typically used when you think the variables may have an exponential growth relationship.	Why we use log linear model
1163	The union of two sets is a new set that contains all of the elements that are in at least one of the two sets.  The intersection of two sets is a new set that contains all of the elements that are in both sets.	What is the difference between union and intersection in statistics
1623	Fig. 1Determine the number of nearest neighbours (K values).Compute the distance between test sample and all the training samples.Sort the distance and determine nearest neighbours based on the K-th minimum distance.Assemble the categories of the nearest neighbours.More items•	How do you measure the effectiveness of KNN
5531	Local interactions in space can give rise to large scale spatio temporal patterns (e.g. (spiral) waves, spatio-temporal chaos (turbulence), stationary (Turing-type) patterns and transitions between these modes). Their occurrence and properties are largely independent of the precise interaction structure.	What is spatio temporal dynamics
216	Gradient Descent is an optimization algorithm used for minimizing the cost function in various machine learning algorithms. It is basically used for updating the parameters of the learning model.  But if the number of training examples is large, then batch gradient descent is computationally very expensive.	What is gradient descent algorithm with example
3400	The term linear model implies that the model is specified as a linear combination of features. Based on training data, the learning process computes one weight for each feature to form a model that can predict or estimate the target value.	What is a linear model in machine learning
916	Convergence in distribution means that as n goes to infinity, Xn and Y will have the same distribution function. Convergence in probability means that with probability 1, X = Y.	What is the difference between convergence in distribution and convergence in probability
2331	Concept Review. In a population whose distribution may be known or unknown, if the size ( n) of samples is sufficiently large, the distribution of the sample means will be approximately normal. The mean of the sample means will equal the population mean.	Can the mean from one sample target the population mean
4109	The Mutual Information score expresses the extent to which observed frequency of co-occurrence differs from what we would expect (statistically speaking). In statistically pure terms this is a measure of the strength of association between words x and y.	What is mutual information score
84	The Bayes theorem describes the probability of an event based on the prior knowledge of the conditions that might be related to the event. If we know the conditional probability , we can use the bayes rule to find out the reverse probabilities .	What is the difference between Bayes Theorem and conditional probability and how do I know when to apply them
7909	Here is a six-step formula for building your core expert systems.Step One: Define All Deliverables.  Step Two: Lay Out the Process.  Step Three: Determine the Optimal Level of Expertise for Each Step.  Step Four: Control for Consistency.  Step Five: Map Out the Key Components of Your Expert System to Refine First.More items•	How do you develop an expert system
3451	Rejection Regions and Alpha Levels You, as a researcher, choose the alpha level you are willing to accept. For example, if you wanted to be 95% confident that your results are significant, you would choose a 5% alpha level (100% – 95%). That 5% level is the rejection region.	How do you find the rejection region
2975	In statistics, a negatively skewed (also known as left-skewed) distribution is a type of distribution in which more values are concentrated on the right side (tail) of the distribution graph while the left tail of the distribution graph is longer.	What does it mean when data is negatively skewed
1075	1)It enhances the learner's motivation and leads to more effective learning. 2)It provides learners with more opportunities for English communication in a non-native environment. 3) It caters to the individual needs of learners at all levels.	What are the benefits of autonomous learning
431	7 Techniques to Handle Imbalanced DataUse the right evaluation metrics.  Resample the training set.  Use K-fold Cross-Validation in the right way.  Ensemble different resampled datasets.  Resample with different ratios.  Cluster the abundant class.  Design your own models.	How do you handle an unbalanced data set
2795	Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes.  Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes. The algorithm selection is also based on type of target variables.	How does a tree decide where to split
547	Causation explicitly applies to cases where action A {quote:right}Causation explicitly applies to cases where action A causes outcome B. {/quote} causes outcome B. On the other hand, correlation is simply a relationship. Action A relates to Action B—but one event doesn't necessarily cause the other event to happen.	How do you tell the difference between correlation and causation
450	To find the mean, add up the values in the data set and then divide by the number of values that you added. To find the median, list the values of the data set in numerical order and identify which value appears in the middle of the list.	How do you evaluate the mean and median
4299	Constraint satisfaction problems (CSPs) are mathematical questions defined as a set of objects whose state must satisfy a number of constraints or limitations. CSPs represent the entities in a problem as a homogeneous collection of finite constraints over variables, which is solved by constraint satisfaction methods.	What do you mean by constraint satisfaction problem
262	TL;DR: Sparsity means most of the weights are 0. This can lead to an increase in space and time efficiency. Detailed version: In general, neural networks are represented as tensors. Each layer of neurons is represented by a matrix.  A matrix in which most entries are 0 is called a sparse matrix.	What is sparsity in deep learning
395	Keras is a high-level interface and uses Theano or Tensorflow for its backend. It runs smoothly on both CPU and GPU. Keras supports almost all the models of a neural network – fully connected, convolutional, pooling, recurrent, embedding, etc. Furthermore, these models can be combined to build more complex models.	Is keras a part of TensorFlow
7458	The difference between Dense and Sparse. When used as adjectives, dense means having relatively high density, whereas sparse means having widely spaced intervals. Dense is also noun with the meaning: a thicket.	What is the difference between sparse and dense
2739	The random variable in the chi-square distribution is the sum of squares of df standard normal variables, which must be independent.  The chi-square distribution curve is skewed to the right, and its shape depends on the degrees of freedom df. For df > 90, the curve approximates the normal distribution.	Why is the chi square distribution skewed
1254	A boxplot is a standardized way of displaying the distribution of data based on a five number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”).  It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.	What do box plots tell you
3233	There are four types of artificial intelligence: reactive machines, limited memory, theory of mind and self-awareness.	What are the types of artificial intelligence
2494	Properties of a normal distribution The mean, mode and median are all equal. The curve is symmetric at the center (i.e. around the mean, μ). Exactly half of the values are to the left of center and exactly half the values are to the right. The total area under the curve is 1.	What are 3 characteristics of a normal curve
3710	In statistics, bivariate data is data on each of two variables, where each value of one of the variables is paired with a value of the other variable.  For example, bivariate data on a scatter plot could be used to study the relationship between stride length and length of legs.	What is the meaning of bivariate data
1523	"The empirical (or experimental) probability of an event is an ""estimate"" that an event will occur based upon how often the event occurred after collecting data from an experiment in a large number of trials.  With theoretical probability, you do not actually conduct an experiment."	What is the difference between an empirical and theoretical distribution
1129	In general, data structures are used to implement the physical forms of abstract data types. This can be translated into a variety of applications, such as displaying a relational database as a binary tree. In programming languages, data structures are used to organize code and information in a digital space.	What are data structures used for
4019	Counterintuitive as it may be, supervised algorithms (particularly logistic regression and random forest) tend to outperform unsupervised ones on discrete classification and categorization tasks, where data is relatively structured and well-labeled.	Is it possible for unsupervised learning algorithms to outperform supervised ones
1085	From Wikipedia, the free encyclopedia. Error-driven learning is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback. It is a type of reinforcement learning.	In machine learning what is error driven learning
5486	Cross-Validation is a very powerful tool. It helps us better use our data, and it gives us much more information about our algorithm performance. In complex machine learning models, it's sometimes easy not pay enough attention and use the same data in different steps of the pipeline.	Why is cross validation a better choice for testing
1754	The loss given default (LGD) is an important calculation for financial institutions projecting out their expected losses due to borrowers defaulting on loans. The expected loss of a given loan is calculated as the LGD multiplied by both the probability of default and the exposure at default.	What is loss given default formula
2702	A target function, in machine learning, is a method for solving a problem that an AI algorithm parses its training data to find. Once an algorithm finds its target function, that function can be used to predict results (predictive analysis).	What is the target function in machine learning
86	Main limitation of Linear Regression is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is rarely linearly separable. It assumes that there is a straight-line relationship between the dependent and independent variables which is incorrect many times.	What are the drawbacks of the linear model
5519	For years, people have been forecasting weather patterns, economic and political events, sports outcomes, and more.  Because we try to predict so many different events, there are a wide variety of ways in which forecasts can be developed.	What is forecasting in machine learning
43	It's a Gaussian distribution in more than one dimension at a time. Nothing tricky in the combining itself, just a straightforward Cartesian-style combination.	What is an intuitive explanation for the multivariate Gaussian distribution aka multivariate normal
7203	In Gradient Descent or Batch Gradient Descent, we use the whole training data per epoch whereas, in Stochastic Gradient Descent, we use only single training example per epoch and Mini-batch Gradient Descent lies in between of these two extremes, in which we can use a mini-batch(small portion) of training data per epoch	What is the difference between stochastic gradient descent and gradient descent
855	In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data.	What is MAP decision rule
2782	The work efficiency formula is efficiency = output / input, and you can multiply the result by 100 to get work efficiency as a percentage. This is used across different methods of measuring energy and work, whether it's energy production or machine efficiency.	How do we calculate efficiency
6816	Frequency distribution in statistics is a representation that displays the number of observations within a given interval.  Frequency distributions are particularly useful for normal distributions, which show the observations of probabilities divided among standard deviations.	What is the frequency distribution in statistics
7061	Both PLS and PCA are used for dimension reduction. Partial Least Squares, use the annotated label to maximize inter-class variance.  Principal components are focus on maximize correlation. The main difference is that the PCA is unsupervised method and PLS is supervised method.	What is the difference between PCA and PLS
452	Randomization in an experiment means random assignment of treatments. This way we can eliminate any possible biases that may arise in the experiment. Good. Randomization in an experiment is important because it minimizes bias responses.	What does it mean for the experiment to be randomized
1498	A low R-squared value indicates that your independent variable is not explaining much in the variation of your dependent variable - regardless of the variable significance, this is letting you know that the identified independent variable, even though significant, is not accounting for much of the mean of your	What does a low R squared value mean
5311	Observer bias (also called experimenter bias or research bias) is the tendency to see what we expect to see, or what we want to see. When a researcher studies a certain group, they usually come to an experiment with prior knowledge and subjective feelings about the group being studied.	What is meant by experimenter bias
5883	If X and Y are normed vector spaces (a special type of TVS), then L is bounded if and only if there exists some M ≥ 0 such that for all x in X, ||Lx||Y ≤ M ||x||X. The smallest such M, denoted by ||L||, is called the operator norm of L.	How do you show an operator is bounded
2644	The weights of artificial neural networks must be initialized to small random numbers. This is because this is an expectation of the stochastic optimization algorithm used to train the model, called stochastic gradient descent.  About the need for nondeterministic and randomized algorithms for challenging problems.	Why are the neural networks weights initialised with random values
967	Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic.	What is LDA topic Modelling
1372	Linear Regression, intuitively is a regression algorithm with a Linear approach. We try to predict a continuous value of a given data point by generalizing on the data that we have in hand. The linear part indicates that we are using a linear approach in generalizing over the data.	What is the intuition behind linear regression
666	Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are: Continuous Bag-of-Words, or CBOW model. Continuous Skip-Gram Model.	Which is the best model used in Word2Vec algorithm for word embedding
3359	The multinomial distribution models the outcome of n experiments, where the outcome of each trial has a categorical distribution, such as rolling a k-sided dice n times.  While the trials are independent, their outcomes X are dependent because they must be summed to n.	Is multinomial distribution independent
1164	Factor analysis is a statistical data reduction and analysis technique that strives to explain correlations among multiple outcomes as the result of one or more underlying explanations, or factors. The technique involves data reduction, as it attempts to represent a set of variables by a smaller number.	What is factor analysis What is the basic purpose of factor analysis
2786	The C parameter trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly by giving the model freedom to select more samples as support vectors.	Which parameter in SVM is responsible for tradeoff between misclassification and simplicity of model
8538	Java, Python, Lisp, Prolog, and C++ are major AI programming language used for artificial intelligence capable of satisfying different needs in the development and designing of different software.  It answers the question, 'what is the language used for artificial intelligence?	How artificial intelligence is programmed
478	When instead of one, there are two independent samples then K-S two sample test can be used to test the agreement between two cumulative distributions. The null hypothesis states that there is no difference between the two distributions. The D-statistic is calculated in the same manner as the K-S One Sample Test.	What is the null hypothesis for KS test
5090	Define Population Distribution; and sketch a graph: The population distribution gives the values of the variable for all the individuals in the population.  The sampling distribution shows the statistic values from all the possible samples of the same size from the population. It is a distribution of the statistic.	What is the difference between a sampling distribution and a population distribution
4914	Each tree is created from a different sample of rows and at each node, a different sample of features is selected for splitting. Each of the trees makes its own individual prediction. These predictions are then averaged to produce a single result.	How does random forest regression predict
796	"Since the observed values for y vary about their means y, the multiple regression model includes a term for this variation. In words, the model is expressed as DATA = FIT + RESIDUAL, where the ""FIT"" term represents the expression 0 + 1x1 + 2x2 +  xp."	What is the formula for multiple linear regression
2992	The binomial distribution is a common discrete distribution used in statistics, as opposed to a continuous distribution, such as the normal distribution.	Is binomial distribution continuous or discrete
3093	As a rule of thumb, I'd say that SVMs are great for relatively small data sets with fewer outliers.  Also, deep learning algorithms require much more experience: Setting up a neural network using deep learning algorithms is much more tedious than using an off-the-shelf classifiers such as random forests and SVMs.	What makes deep learning more efficient than support vector machines
6213	It does this by using a means of representing knowledge called, semantic networks. These use graphical methods to describe relationships between concepts and events to describe common sense activities.	How is common sense knowledge represented
8051	Explanation: Randomized quick sort chooses a random element as a pivot. It is done so as to avoid the worst case of quick sort in which the input array is already sorted.	What is a randomized quicksort *
21	Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.  Tokenization can be done to either separate words or sentences.	Why tokenization is important in NLP
5931	"A Gaussian filter is a linear filter. It's usually used to blur the image or to reduce noise. If you use two of them and subtract, you can use them for ""unsharp masking"" (edge detection). The Gaussian filter alone will blur edges and reduce contrast."	What is Gaussian filtering in image processing
8017	A batch is the complete dataset.  Iterations is the number of batches of data the algorithm has seen (or simply the number of passes the algorithm has done on the dataset). Epochs is the number of times a learning algorithm sees the complete dataset.	What is the difference between epoch batch and iteration in deep learning
2478	In very rare cases, you can have a false-positive result. This means you're not pregnant but the test says you are. You could have a false-positive result if you have blood or protein in your pee. Certain drugs, such as tranquilizers, anticonvulsants, hypnotics, and fertility drugs, could cause false-positive results.	Why would a pregnancy test give a false positive
6350	Hierarchical clustering is an instance of the agglomerative or bottom-up approach, where we start with each data point as its own cluster and then combine clusters based on some similarity measure.	Which type of clustering is used for big data
4034	Specifically, we can compute the probability that a discrete random variable equals a specific value (probability mass function) and the probability that a random variable is less than or equal to a specific value (cumulative distribution function).	What is the difference between cumulative distribution function and probability mass function
7392	- Population Based Training - It is open-source. The library connected with DeepMind's paper ( [1711.09846] Population Based Training of Neural Networks ) should be enough to start with something.	Is there any open source implementation of population based hyperparameter tuning from DeepMind
1850	The geometric distribution represents the number of failures before you get a success in a series of Bernoulli trials. This discrete probability distribution is represented by the probability density function: f(x) = (1 − p)x − 1p.	What does geometric distribution mean
8339	SVD, or Singular Value Decomposition, is one of several techniques that can be used to reduce the dimensionality, i.e., the number of columns, of a data set.  SVD is an algorithm that factors an m x n matrix, M, of real or complex values into three component matrices, where the factorization has the form USV*.	How does SVD help in dimensionality reduction
8167	Fourier analysis is used in electronics, acoustics, and communications. Many waveforms consist of energy at a fundamental frequency and also at harmonic frequencies (multiples of the fundamental). The relative proportions of energy in the fundamental and the harmonics determines the shape of the wave.	What is Fourier analysis used for
17	One is that larger learning rates increase the noise on the stochastic gradient, which acts as an implicit regularizer.  If you find your model overfitting with a low learning rate, the minima you're falling into might actually be too sharp and cause the model to generalize poorly.	Does learning rate affect Overfitting
4796	Emotional artificial intelligence, also called Emotion AI or affective computing, is being used to develop machines that are capable of reading, interpreting, responding to, and imitating human affect—the way we, as humans, experience and express emotions.	What is emotional artificial intelligence
1467	�(�) = x �−1 e−xdx. then f(x �, �) will be a probability density function since it is nonnegative and it integrates | to one. Definition. The distribution with p.d.f. f(x �, �) is called Gamma distribution with | parameters � and � and it is denoted as �(�, �).	What is the pdf of gamma distribution
1421	In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.	What is hyper parameter tuning in machine learning
916	Unsupervised learning is the Holy Grail of Deep Learning. The goal of unsupervised learning is to create general systems that can be trained with little data.  Today Deep Learning models are trained on large supervised datasets. Meaning that for each data, there is a corresponding label.	Can deep learning be used for unsupervised learning
7228	Growth curve analysis, or trajectory analysis, is a specialized set of techniques for modeling change over time.  Growth curve analysis is a data reduction technique: it is used to summarize longitudinal data into a smooth curve defined by relatively few parameters for descriptive purposes or further inquiry.	What is growth curve analysis
799	9:3122:36Suggested clip · 72 seconds3.5: Mathematics of Gradient Descent - Intelligence and Learning YouTubeStart of suggested clipEnd of suggested clip	How do you solve gradient descent problems
71	In simplest manner, svm without kernel is a single neural network neuron but with different cost function. If you add a kernel function, then it is comparable with 2 layer neural nets.  In simplest manner, svm without kernel is a single neural network neuron but with different cost function.	Is a support vector machine a neural network
574	Process Mining Process mining uses event commits and application logs to decipher a business process. Process DiscoveryAI-powered process discovery uses computer vision and machine intelligence to observe users and uncover deep process variants from digital traces of human work.	What is the difference between process mining and process discovery
19	R-squared measures the proportion of the variation in your dependent variable (Y) explained by your independent variables (X) for a linear regression model. Adjusted R-squared adjusts the statistic based on the number of independent variables in the model.	What is adjusted R squared
843	Businesses use data mining techniques to identify potentially useful information in their data, in order to aid business decision making processes. Machine learning is utilized in order to improve these decision making models.	How is machine learning used in data mining
4245	Though the name is a mouthful, the concept behind this is very simple. To tell briefly, LDA imagines a fixed set of topics. Each topic represents a set of words. And the goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics.	How does LDA algorithm work
849	It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance. In this tutorial, you will discover the rectified linear activation function for deep learning neural networks.	Why is the ReLU activation function used the most often in neural networks for computer vision
387	An interval scale is one where there is order and the difference between two values is meaningful. Examples of interval variables include: temperature (Farenheit), temperature (Celcius), pH, SAT score (200-800), credit score (300-850).	What type of variable is test score
5169	Variability refers to how spread out a group of data is. The common measures of variability are the range, IQR, variance, and standard deviation. Data sets with similar values are said to have little variability while data sets that have values that are spread out have high variability.	How do you explain variation in statistics
655	Apply the sigmoid function as the final activation function of CNN network which is as below. The train and validation data set is little bit different, it has additional images that has multiple classes in a given images.	What is sigmoid in CNN
678	"Target is the ""correct"" or desidered value for the respose associate to one input. Usually, this value will be compared with the output (the response of the neural network) to guide the learning process involving the weight changes."	What is target in neural network
442	The Kalman filter uses a system's dynamic model (e.g., physical laws of motion), known control inputs to that system, and multiple sequential measurements (such as from sensors) to form an estimate of the system's varying quantities (its state) that is better than the estimate obtained by using only one measurement	How does Kalman filter work
2047	To measure test-retest reliability, you conduct the same test on the same group of people at two different points in time. Then you calculate the correlation between the two sets of results.	How do you calculate test retest reliability
1060	ELIZA is an early natural language processing computer program created from 1964 to 1966 at the MIT Artificial Intelligence Laboratory by Joseph Weizenbaum.  As such, ELIZA was one of the first chatterbots and one of the first programs capable of attempting the Turing test.	What is Eliza in artificial intelligence
7966	"In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM).  This approach is called the ""kernel trick"". Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors."	What is kernel in support vector regression
7047	A person who engages in banditry is known as a bandit and primarily commits crimes such as extortion, robbery, and murder, either as an individual or in groups. Banditry is a vague concept of criminality and in modern usage can be synonymous for gangsterism, brigandage, marauding, and thievery.	What did bandits do
7878	Consider that:You choose door 1. Monty shows you a goat behind door 2.If the car is behind door 1, Monty will not choose it.  If the car is behind door 2, Monty will always open door 3, as he never reveals the car.If the car is behind door 3, Monty will open door 2 100% of the time.	How do you prove the Monty Hall problem
1777	The tool of normal approximation allows us to approximate the probabilities of random variables for which we don't know all of the values, or for a very large range of potential values that would be very difficult and time consuming to calculate.	Why do we use normal approximation
5843	Stochastic control or stochastic optimal control is a sub field of control theory that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system.	What is a stochastic control system
8391	Bayesian inference refers to statistical inference where uncertainty in inferences is quantified using probability.  Statistical models specify a set of statistical assumptions and processes that represent how the sample data is generated. Statistical models have a number of parameters that can be modified.	What is a Bayesian model
6250	Now, every textbook on linear algebra gives the following definition of a linear operator: an operator T: V—> W between two vector spaces V and W over the same field ! F is said to be linear if it satisfies the conditions of additivity, viz. T(u + v)=T(u)+T(v)	What makes an operator linear
5724	The standard normal distribution is a normal distribution with a mean of zero and standard deviation of 1.  For the standard normal distribution, 68% of the observations lie within 1 standard deviation of the mean; 95% lie within two standard deviation of the mean; and 99.9% lie within 3 standard deviations of the mean.	What does standard normal distribution mean
566	Yes. The Sobel operator approximates a horizontal gradient and a vertical gradient on an image by convolving it with two kernels, and . The kernel itself can be decomposed into the product of a averaging operator and a differentiating operator.	Is the Sobel operator a linear filter
4237	Key Differences between AI, ML, and NLP Machine Learning and Artificial Intelligence are the terms often used together but aren't the same. ML is an application of AI.  The main technology used in NLP (Natural Language Processing) which mainly focuses on teaching natural/human language to computers.	What is the difference between NLP and Machine Learning
6700	Log-loss measures the accuracy of a classifier. It is used when the model outputs a probability for each class, rather than just the most likely class. Log-loss measures the accuracy of a classifier. It is used when the model outputs a probability for each class, rather than just the most likely class.	Why do we use log loss
895	"Logistic regression can be binomial, ordinal or multinomial. Binomial or binary logistic regression deals with situations in which the observed outcome for a dependent variable can have only two possible types, ""0"" and ""1"" (which may represent, for example, ""dead"" vs. ""alive"" or ""win"" vs. ""loss"")."	What are the types of logistic regression
7045	Standard interpretation of the ordered logit coefficient is that for a one unit increase in the predictor, the response variable level is expected to change by its respective regression coefficient in the ordered log-odds scale while the other variables in the model are held constant.	How do you interpret ordered logit coefficients
5835	The essential difference between the set and the multiset is that in a set the keys must be unique, while a multiset permits duplicate keys.  In both sets and multisets, the sort order of components is the sort order of the keys, so the components in a multiset that have duplicate keys may appear in any order.	What is the difference between a set and a multiset
7035	The disadvantages: Convenience samples do not produce representative results. If you need to extrapolate to the target population, convenience samples aren't going to get you there.	What is the problem with convenience sampling
596	ReLu refers to the Rectifier Unit, the most commonly deployed activation function for the outputs of the CNN neurons. Mathematically, it's described as: Unfortunately, the ReLu function is not differentiable at the origin, which makes it hard to use with backpropagation training.	What is ReLU in CNN
928	Every neuron has input connections and output connections. These connections simulate the behavior of the synapses in the brain. The same way that synapses in the brain transfer the signal from one neuron to another, connections pass information between artificial neurons.	How do artificial neurons learn
5382	The main benefit claimed for feature selection, which is the main focus in this manuscript, is that it increases classification accuracy. It is believed that removing non-informative signal can reduce noise, and can increase the contrast between labelled groups.	Does feature selection improve classification accuracy
641	Bivariate analysis is one of the simplest forms of quantitative (statistical) analysis. It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them. Bivariate analysis can be helpful in testing simple hypotheses of association.	What is the purpose of bivariate analysis
7532	In single-link (or single linkage) hierarchical clustering, we merge in each step the two clusters whose two closest members have the smallest distance (or: the two clusters with the smallest minimum pairwise distance).  A single-link clustering also closely corresponds to a weighted graph's minimum spanning tree.	What is linkage in hierarchical clustering
818	K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.  In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.	What is K means algorithm in machine learning
436	How to Use K-means Cluster Algorithms in Predictive AnalysisPick k random items from the dataset and label them as cluster representatives.Associate each remaining item in the dataset with the nearest cluster representative, using a Euclidean distance calculated by a similarity function.Recalculate the new clusters' representatives.More items	How is K means clustering used in prediction
1110	Events are dependent if the outcome of one event affects the outcome of another. For example, if you draw two colored balls from a bag and the first ball is not replaced before you draw the second ball then the outcome of the second draw will be affected by the outcome of the first draw.	How do you solve dependent events
755	Systematic sampling is a type of probability sampling method in which sample members from a larger population are selected according to a random starting point but with a fixed, periodic interval. This interval, called the sampling interval, is calculated by dividing the population size by the desired sample size.	What is systematic sampling
1127	So unlike biological neurons, artificial neurons don't just “fire”: they send continuous values instead of binary signals. Depending on their activation functions, they might somewhat fire all the time, but the strength of these signals varies.	In what way are artificial neurons functionally different from biological neurons
7970	Any point (x) from a normal distribution can be converted to the standard normal distribution (z) with the formula z = (x-mean) / standard deviation. z for any particular x value shows how many standard deviations x is away from the mean for all x values.	How do you convert normal distribution to normal distribution
2714	To overcome the issue of the curse of dimensionality, Dimensionality Reduction is used to reduce the feature space with consideration by a set of principal features.	Which is considered as a solution for curse of dimensionality
1062	cortex	What part of the brain is responsible for pattern recognition
6402	Feature extraction is a process of dimensionality reduction by which an initial set of raw data is reduced to more manageable groups for processing. A characteristic of these large data sets is a large number of variables that require a lot of computing resources to process.	What is feature extraction in deep learning
1141	Descriptive Analytics tells you what happened in the past.  Predictive Analytics predicts what is most likely to happen in the future. Prescriptive Analytics recommends actions you can take to affect those outcomes.	What is the difference between descriptive and predictive statistics
776	KMeans is a clustering algorithm which divides observations into k clusters. Since we can dictate the amount of clusters, it can be easily used in classification where we divide data into clusters which can be equal to or more than the number of classes.	Can clustering be used for classification
8633	The relationship between margin of error and sample size is simple: As the sample size increases, the margin of error decreases.  If you think about it, it makes sense that the more information you have, the more accurate your results are going to be (in other words, the smaller your margin of error will get).	How are sampling error and sample size related
2556	In particular, a random experiment is a process by which we observe something uncertain. After the experiment, the result of the random experiment is known. An outcome is a result of a random experiment. The set of all possible outcomes is called the sample space.	What is the meaning of random experiment
3700	Dummy variables are useful because they enable us to use a single regression equation to represent multiple groups. This means that we don't need to write out separate equation models for each subgroup. The dummy variables act like 'switches' that turn various parameters on and off in an equation.	Why do we use dummy variables in regression
1394	How to Calculate a Confusion MatrixYou need a test dataset or a validation dataset with expected outcome values.Make a prediction for each row in your test dataset.From the expected outcomes and predictions count: The number of correct predictions for each class.	How do you use Confusion Matrix
457	This implies that bias and variance of an estimator are complementary to each other i.e. an estimator with high bias will vary less(have low variance) and an estimator with high variance will have less bias(as it can vary more to fit/explain/estimate the data points).	What is the difference between the bias and variance of an estimator
692	Chebyshev's inequality, also known as Chebyshev's theorem, is a statistical tool that measures dispersion in a data population.  The theorem states that no more than 1 / k2 of the distribution's values will be more than k standard deviations away from the mean.	What does Chebyshev's inequality measure
2022	Optimization is the most essential ingredient in the recipe of machine learning algorithms. It starts with defining some kind of loss function/cost function and ends with minimizing the it using one or the other optimization routine.	What is optimization techniques in machine learning
45	We can use the median with the interquartile range, or we can use the mean with the standard deviation.	Can standard deviation be used with median
7913	Why gradient clipping accelerates training: A theoretical justification for adaptivity.  These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption.	Why gradient clipping accelerates training a theoretical justification for adaptivity
2999	As the formula shows, the standard score is simply the score, minus the mean score, divided by the standard deviation. Therefore, let's return to our two questions.	What is a standard score How do you find the standard score for a particular data value
1716	Learning of probability helps you in making informed decisions about likelihood of events, based on a pattern of collected data. In the context of data science, statistical inferences are often used to analyze or predict trends from data, and these inferences use probability distributions of data.	How is probability used in data science
901	The decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes. The ID3 algorithm builds decision trees using a top-down greedy search approach through the space of possible branches with no backtracking.	Which method is used in decision tree algorithm
1312	Sigmoid function, unlike step function, introduces non-linearity into our neural network model.  This non-linear activation function, when used by each neuron in a multi-layer neural network, produces a new “representation” of the original data, and ultimately allows for non-linear decision boundary, such as XOR.	What is a sigmoid function in neural networks
3739	Examples of multivariate regression Example 2. A doctor has collected data on cholesterol, blood pressure, and weight. She also collected data on the eating habits of the subjects (e.g., how many ounces of red meat, fish, dairy products, and chocolate consumed per week).	What are some examples of multivariate data
1603	The false discovery rate (FDR) is a statistical approach used in multiple hypothesis testing to correct for multiple comparisons.  The FDR is defined as the expected proportion of false discoveries, i.e., incorrectly rejected null hypothesis, among all discoveries (Benjamini and Hochberg 1995).	What does false discovery rate mean
37	A simple random sample is similar to a random sample. The difference between the two is that with a simple random sample, each object in the population has an equal chance of being chosen. With random sampling, each object does not necessarily have an equal chance of being chosen.	How do you know if something is a simple random sample
504	The other major way to estimate inter-rater reliability is appropriate when the measure is a continuous one. There, all you need to do is calculate the correlation between the ratings of the two observers. For instance, they might be rating the overall level of activity in a classroom on a 1-to-7 scale.	How do you measure inter observer reliability
795	With cluster sampling, the researcher divides the population into separate groups, called clusters. Then, a simple random sample of clusters is selected from the population.  For example, given equal sample sizes, cluster sampling usually provides less precision than either simple random sampling or stratified sampling.	What is the difference between simple random sampling and cluster sampling
2902	A high pass filter can be formed by placing a capacitor in series with an inverting gain stage as shown in Figure 11.13.	How is the high pass filters formed
134	"Stochastic Gradient Descent: you would randomly select one of those training samples at each iteration to update your coefficients. Online Gradient Descent: you would use the ""most recent"" sample at each iteration. There is no stochasticity as you deterministically select your sample."	Difference between stochastic gradient descent and online learning
4793	The clarity of visual features are excellent inputs to Deep Learning models. Because images can learn from themselves in a semi-supervised manner, there is no data required.	Which of the following best characterizes why computer vision is such a sweet spot for deep learning
913	In short, fourier series is for periodic signals and fourier transform is for aperiodic signals. Fourier series is used to decompose signals into basis elements (complex exponentials) while fourier transforms are used to analyze signal in another domain (e.g. from time to frequency, or vice versa).	What is difference between Fourier series and Fourier transform
7772	In statistics, the residual sum of squares (RSS), also known as the sum of squared residuals (SSR) or the sum of squared estimate of errors (SSE), is the sum of the squares of residuals (deviations predicted from actual empirical values of data).	What does the sum of squares due to error measure
876	Whereas the null hypothesis of the two-sample t test is equal means, the null hypothesis of the Wilcoxon test is usually taken as equal medians. Another way to think of the null is that the two populations have the same distribution with the same median.	What is the null hypothesis for a Wilcoxon test
8023	To find the critical value, follow these steps.Compute alpha (α): α = 1 - (confidence level / 100)Find the critical probability (p*): p* = 1 - α/2.To express the critical value as a z-score, find the z-score having a cumulative probability equal to the critical probability (p*).More items	How do you find the critical value of Z with a significance level
974	Statistically Valid Sample Size Criteria Probability or percentage: The percentage of people you expect to respond to your survey or campaign. Confidence: How confident you need to be that your data is accurate. Expressed as a percentage, the typical value is 95% or 0.95.	What is a statistically valid sample size
2942	"Click on the triangle-shaped icon located at the top right corner of the panel, and then choose ""Save Path"". Next, select ""Clipping Path"" from the same drop-down menu. A new dialog box will appear with a variety of clipping path settings. Make sure your path is selected, and then click OK."	How do you do a clipping path
6829	Related calculationsFalse positive rate (α) = type I error = 1 − specificity = FP / (FP + TN) = 180 / (180 + 1820) = 9%False negative rate (β) = type II error = 1 − sensitivity = FN / (TP + FN) = 10 / (20 + 10) = 33%Power = sensitivity = 1 − βLisää kohteita…	How do you calculate false positive rate from sensitivity and specificity
583	To measure test-retest reliability, you conduct the same test on the same group of people at two different points in time. Then you calculate the correlation between the two sets of results.	How do you measure test retest reliability
3539	In probability theory and statistics, the beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parameterized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable and control the shape of the distribution.	What is beta distribution in statistics
2931	An association rule has two parts: an antecedent (if) and a consequent (then). An antecedent is an item found within the data.  Support is an indication of how frequently the items appear in the data. Confidence indicates the number of times the if-then statements are found true.	What is support and confidence in association rule mining
2092	Inductive Learning is where we are given examples of a function in the form of data (x) and the output of the function (f(x)). The goal of inductive learning is to learn the function for new data (x). Classification: when the function being learned is discrete. Regression: when the function being learned is continuous.	What is inductive machine learning
7709	There are four main types of probability sample.Simple random sampling. In a simple random sample, every member of the population has an equal chance of being selected.  Systematic sampling.  Stratified sampling.  Cluster sampling.	What are the 4 types of probability sampling
1350	receiver operating characteristic curve	What does ROC mean in statistics
8638	The easiest[A] way to evaluate the actual value of a Tensor object is to pass it to the Session. run() method, or call Tensor. eval() when you have a default session (i.e. in a with tf. Session(): block, or see below).	How do you evaluate a tensor in TensorFlow
2765	To find the mean absolute deviation of the data, start by finding the mean of the data set. Find the sum of the data values, and divide the sum by the number of data values. Find the absolute value of the difference between each data value and the mean: |data value – mean|.	How do you get the mean absolute deviation
611	To address this issue, there is a modification to Cohen's kappa called weighted Cohen's kappa.  The weighted kappa is calculated using a predefined table of weights which measure the degree of disagreement between the two raters, the higher the disagreement the higher the weight.	What are the differences between Cohens Kappa and weighted Kappa
6787	NAT (Network Address Translation) is a feature of the Firewall Software Blade and replaces IPv4 and IPv6 addresses to add more security. You can enable NAT for all SmartDashboard objects to help manage network traffic. NAT protects the identity of a network and does not show internal IP addresses to the Internet.	What is a NAT policy
4984	An F-test (Snedecor and Cochran, 1983) is used to test if the variances of two populations are equal. This test can be a two-tailed test or a one-tailed test. The two-tailed version tests against the alternative that the variances are not equal.	Is the F test a one tailed test or a two tailed test
1119	Multinomial logistic regression (often just called 'multinomial regression') is used to predict a nominal dependent variable given one or more independent variables. It is sometimes considered an extension of binomial logistic regression to allow for a dependent variable with more than two categories.	When would you use multinomial regression
6274	Nonparametric statistics refers to a statistical method in which the data are not assumed to come from prescribed models that are determined by a small number of parameters; examples of such models include the normal distribution model and the linear regression model.	What is non parametric distribution
701	What Are Moments in Statistics?Moments About the MeanFirst, calculate the mean of the values.Next, subtract this mean from each value.Then raise each of these differences to the sth power.Now add the numbers from step #3 together.Finally, divide this sum by the number of values we started with.	How do you find the moment in statistics
835	Random assignment is however a process of randomly assigning subjects to experimental or control groups. This is a standard practice in true experimental research to ensure that treatment groups are similar (equivalent) to each other and to the control group, prior to treatment administration.	Are based on the idea that subjects are randomly assigned to groups
5593	A quantile defines a particular part of a data set, i.e. a quantile determines how many values in a distribution are above or below a certain limit. Special quantiles are the quartile (quarter), the quintile (fifth) and percentiles (hundredth).	What does a quantile mean
203	A classification is an ordered set of related categories used to group data according to its similarities. It consists of codes and descriptors and allows survey responses to be put into meaningful categories in order to produce useful data. A classification is a useful tool for anyone developing statistical surveys.	What is meant by classification in statistics
4734	6 Practices to enhance the performance of a Text ClassificationDomain Specific Features in the Corpus. For a classification problem, it is important to choose the test and training corpus very carefully.  Use An Exhaustive Stopword List.  Noise Free Corpus.  Eliminating features with extremely low frequency.  Normalized Corpus.  Use Complex Features: n-grams and part of speech tags.	How can text classification models be improved
7332	An eager algorithm executes immediately and returns a result. A lazy algorithm defers computation until it is necessary to execute and then produces a result.  Eager algorithms are easier to understand and debug. They can also be highly optimized for a single use case (e.g. filter ).	What is an eager algorithm
8129	In representation learning, features are extracted from unlabeled data by training a neural network on a secondary, supervised learning task.  When applying deep learning to natural language processing (NLP) tasks, the model must simultaneously learn several language concepts: the meanings of words.	How is NLP used in representation learning
3215	Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model.	What is data augmentation in machine learning
4035	Artificial intelligence has close connections with philosophy because both use concepts that have the same names and these include intelligence, action, consciousness, epistemology, and even free will.  These factors contributed to the emergence of the philosophy of artificial intelligence.	Is artificial intelligence possible philosophy
7995	Conclusion. Cross-Validation is a very powerful tool. It helps us better use our data, and it gives us much more information about our algorithm performance. In complex machine learning models, it's sometimes easy not pay enough attention and use the same data in different steps of the pipeline.	Why is validation important in machine learning
920	Clustering is done based on a similarity measure to group similar data objects together. This similarity measure is most commonly and in most applications based on distance functions such as Euclidean distance, Manhattan distance, Minkowski distance, Cosine similarity, etc. to group objects in clusters.	What are the similarity measures of clustering
1347	A composite hypothesis test contains more than one parameter and more than one model. In a simple hypothesis test, the probability density functions for both the null hypothesis (H0) and alternate hypothesis (H1) are known.	What is the difference between simple and composite hypothesis
275	Receptive field, region in the sensory periphery within which stimuli can influence the electrical activity of sensory cells.	Where are receptive fields located
3302	The cumulative density function gives you the probability of a random variable being on or below a certain value. The quantile function is the opposite of that. i.e. you give it a probability and it tells you the random variable value.  A quartile is the value of the quantile at the probabilities 0.25, 0.5 and 0.75.	What is the difference between quantile and quartile
7374	In general, if the data is normally distributed, parametric tests should be used. If the data is non-normal, non-parametric tests should be used.	How do you know what statistical test to use
398	So when you perform t-test for comparison of two means or ANOVA forr comparison of multiple means. You need dummy variables. In your case if the data is categorical you'll definitely need to convert them so simultaneously they are becoming dummy by themselves. Hence YES, you can use these tests for categorical data.	Can you do a t test with categorical data
2447	Server clustering refers to a group of servers working together on one system to provide users with higher availability. These clusters are used to reduce downtime and outages by allowing another server to take over in the event of an outage. Here's how it works. A group of servers are connected to a single system.	What is cluster and how it works
1400	The shape of any distribution can be described by its various 'moments'. The first four are: 1) The mean, which indicates the central tendency of a distribution. 2) The second moment is the variance, which indicates the width or deviation.	What is a moment of a distribution
8114	A common cause of sampling bias lies in the design of the study or in the data collection procedure, both of which may favor or disfavor collecting data from certain classes or individuals or in certain conditions.  Figure 1: Possible sources of bias occurring in the selection of a sample from a population.	What causes sampling bias
6338	12:3117:15Suggested clip · 120 secondsLogistic Regression in R, Clearly Explained!!!! - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you fit a logistic model in R
5357	Dependent events: Two events are dependent when the outcome of the first event influences the outcome of the second event. The probability of two dependent events is the product of the probability of X and the probability of Y AFTER X occurs.	How do you find the probability of multiple dependent events
1466	A bounding box is an imaginary rectangle that serves as a point of reference for object detection and creates a collision box for that object. Data annotators draw these rectangles over images, outlining the object of interest within each image by defining its X and Y coordinates.	What is bounding box in image processing
7722	The Least Squares Regression Line is the line that makes the vertical distance from the data points to the regression line as small as possible. It's called a “least squares” because the best line of fit is one that minimizes the variance (the sum of squares of the errors).	What does Least Squares line mean
918	Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data. Boosting is an iterative technique which adjusts the weight of an observation based on the last classification.	What is the difference between boosting and bagging
8652	Response bias is a general term for a wide range of tendencies for participants to respond inaccurately or falsely to questions. These biases are prevalent in research involving participant self-report, such as structured interviews or surveys.	What is response bias in surveys
1490	The standard deviation formula may look confusing, but it will make sense after we break it down.  Step 1: Find the mean.Step 2: For each data point, find the square of its distance to the mean.Step 3: Sum the values from Step 2.Step 4: Divide by the number of data points.Step 5: Take the square root.	How do you find the standard deviation between two sets of data
1059	“Human error” is not a source of experimental error. You must classify specific errors as random or systematic and identify the source of the error. Human error cannot be stated as experimental error.	Is random error human error
777	"Binning is a way to group a number of more or less continuous values into a smaller number of ""bins"". For example, if you have data about a group of people, you might want to arrange their ages into a smaller number of age intervals."	When should you use binning
2746	Interpret the key results for Binary Logistic RegressionStep 1: Determine whether the association between the response and the term is statistically significant.Step 2: Understand the effects of the predictors.Step 3: Determine how well the model fits your data.Step 4: Determine whether the model does not fit the data.	How do you interpret logistic regression results
1728	The Mann Whitney U test, sometimes called the Mann Whitney Wilcoxon Test or the Wilcoxon Rank Sum Test, is used to test whether two samples are likely to derive from the same population (i.e., that the two populations have the same shape).	When would you use a Mann Whitney U test
461	Bias in data can result from: survey questions that are constructed with a particular slant. choosing a known group with a particular background to respond to surveys. reporting data in misleading categorical groupings.	How can data be biased
649	Tensorflow is the most famous library used in production for deep learning models.  However TensorFlow is not that easy to use. On the other hand, Keras is a high level API built on TensorFlow (and can be used on top of Theano too). It is more user-friendly and easy to use as compared to TF.	Is Keras better than Tensorflow for deep learning
5772	If you are working on a classification problem, the best score is 100% accuracy. If you are working on a regression problem, the best score is 0.0 error. These scores are an impossible to achieve upper/lower bound.	What is a good accuracy for machine learning
456	Converting a Covariance Matrix to a Correlation Matrix First, use the DIAG function to extract the variances from the diagonal elements of the covariance matrix. Then invert the matrix to form the diagonal matrix with diagonal elements that are the reciprocals of the standard deviations.	How do you convert a covariance matrix to a correlation matrix
965	In the context of neural networks, a perceptron is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network.	What is Perceptron in neural network
5305	In the field of artificial intelligence, inference engine is a component of the system that applies logical rules to the knowledge base to deduce new information. The first inference engines were components of expert systems.  The knowledge base stored facts about the world.	What is an inference engine in AI
5	In fact, linear regression analysis works well, even with non-normal errors.	Can I perform a multiple regression on non normal data
332	Let's take a look at some of the important business problems solved by machine learning.Manual data entry.  Detecting Spam.  Product recommendation.  Medical Diagnosis.  Customer segmentation and Lifetime value prediction.  Financial analysis.  Predictive maintenance.  Image recognition (Computer Vision)	What types of problems can machine learning solve
6626	R is now used by over 50% of data miners. R, Python, and SQL were the most popular programming languages. Python, Lisp/Clojure, and Unix tools showest the highest growth in 2012, while Java and MATLAB slightly declined in popularity.	What language is used for data mining
1276	The four elements of a descriptive statistics problem include population/sample, tables/graphs, identifying patterns, and A. data.	What are the four elements of a descriptive statistics problem
231	The 7 Steps of Machine Learning1 - Data Collection. The quantity & quality of your data dictate how accurate our model is.  2 - Data Preparation. Wrangle data and prepare it for training.  3 - Choose a Model.  4 - Train the Model.  5 - Evaluate the Model.  6 - Parameter Tuning.  7 - Make Predictions.	What are the steps in designing a machine learning problem
1482	The neuron is the basic working unit of the brain, a specialized cell designed to transmit information to other nerve cells, muscle, or gland cells. Neurons are cells within the nervous system that transmit information to other nerve cells, muscle, or gland cells. Most neurons have a cell body, an axon, and dendrites.	What are the functions of neurons
7289	Taking the square root of the variance gives us the units used in the original scale and this is the standard deviation. Standard deviation is the measure of spread most commonly used in statistical practice when the mean is used to calculate central tendency. Thus, it measures spread around the mean.	Where do we use standard deviation and variance
1366	Many time series show periodic behavior. This periodic behavior can be very complex. Spectral analysis is a technique that allows us to discover underlying periodicities. To perform spectral analysis, we first must transform data from time domain to frequency domain.	Why is spectral analysis performed
1300	0:294:16Suggested clip · 116 secondsGeometric distribution moment generating function - YouTubeYouTubeStart of suggested clipEnd of suggested clip	How do you find the moment generating function of a geometric distribution
4325	In a multilevel model, we use random variables to model the variation between groups. An alternative approach is to use an ordinary regression model, but to include a set of dummy variables to represent the differences between the groups. The multilevel approach offers several advantages.	What is Multi Level Regression
641	Quantum fields are matter.  The simplest “practical” quantum field theory is quantum electromagnetism. In it, two fields exist: the electromagnetic field and the “electron field”. These two fields continuously interact with each other, energy and momentum are transferred, and excitations are created or destroyed.	What is a field in QFT
3940	Restricted Boltzmann Machines are used to analyze and find out these underlying factors. The analysis of hidden factors is performed in a binary way, i.e, the user only tells if they liked (rating 1) a specific movie or not (rating 0) and it represents the inputs for the input/visible layer.	How does a restricted Boltzmann machine work
4540	Class boundaries are the data values which separate classes. They are not part of the classes or the dataset. The lower class boundary of a class is defined as the average of the lower limit of the class in question and the upper limit of the previous class.	What is class boundaries in statistics
